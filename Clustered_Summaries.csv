SerialNo,Session_Summary,Cleaned_Summary,Lecture_Label,tsne-2d-one,tsne-2d-two,umap-2d-one,umap-2d-two,Lecture_Name
1,"we started our lecture with a recap of previous lecture particularly about the difference between statistically similar values and statistically significant values. we then started a new topic called as multiple linear regression. as the name suggests, it is a linear regression but dependent upon multiple features. we looked how things like sales depend on multiple factors like age, earning, family size etc. the things on which our y is dependent are known as features. our goal is to represent y as a linear combination of x1, x2, x3, ..., xn. suppose y = b0 + b1x1 + b2x2 + ...... our objective is to find the values of b0, b1, b2 which we do by a method called as gradient descent which is a numerical method very similar to newton raphson method, but used for n dimensions. prof then showed an illustration on excel where the y was dependent on 5 features. we performed mlr and had a look at the p-values of the calculated coefficients. every coefficient had a p-value greater than 0.05 except the intercept which shows that the probability of them being correct is very low. we then started dropping the features one by one which had the highest p-value. there also is a metric known as f value where f = msr/mse. higher f corresponds to better model. as we dropped the features, f value started to increase.","we started our lecture with a recap of previous lecture particularly about the difference between statistically similar values and statistically significant values. we then started a new topic called as multiple linear regression. as the name suggests, it is a linear regression but dependent upon multiple features. we looked how things like sales depend on multiple factors like age, earning, family size etc. the things on which our y is dependent are known as features. our goal is to represent y as a linear combination of x1, x2, x3, ..., xn. suppose y = b0 + b1x1 + b2x2 + ...... our objective is to find the values of b0, b1, b2 which we do by a method called as gradient descent which is a numerical method very similar to newton raphson method, but used for n dimensions. prof then showed an illustration on excel where the y was dependent on 5 features. we performed mlr and had a look at the p-values of the calculated coefficients. every coefficient had a p-value greater than 0.05 except the intercept which shows that the probability of them being correct is very low. we then started dropping the features one by one which had the highest p-value. there also is a metric known as f value where f = msr/mse. higher f corresponds to better model. as we dropped the features, f value started to increase.",2,16.94262,8.096779,12.411322,4.212075,"regression, regressions, features"
2,"in this session, we explored various feature encoding techniques, essential for converting categorical and textual data into numerical representations suitable for machine learning models. initially, we discussed vectorization and one-hot encoding. vectorization is a general approach to transform textual or categorical data into numerical vectors. one-hot encoding specifically converts categorical variables into binary vectors, creating separate columns for each category. this method is widely used in multiclass and multilabel classification problems but can introduce the curse of dimensionality when dealing with variables having many unique categories.

we then examined label encoding and integer encoding. label encoding assigns each categorical class a unique integer value. while this method is straightforward, it implies an ordinal relationship between categories, which may not always be appropriate. integer encoding is particularly useful when the categorical variable represents ordinal data (categories with a meaningful order), as it preserves the inherent ordering.

due to the limitations of one-hot encoding, such as increased dimensionality, we introduced binary encoding (compact encoding). binary encoding efficiently represents multiple categories using fewer columnsâ€”for example, just three columns can encode up to eight distinct classes. this approach helps mitigate dimensionality issues while retaining meaningful category distinctions.

additionally, we covered frequency encoding and target encoding. frequency encoding involves assigning each category a numerical value based on its frequency within the dataset. target encoding replaces categories with values derived from the target variable (such as the mean target value for each category), effectively capturing relationships between categories and the outcome variable.

finally, we briefly touched upon methods for converting textual data into numerical vectors through vectorization techniques, setting the stage for deeper exploration in future sessions.","in this session, we explored various feature encoding techniques, essential for converting categorical and textual data into numerical representations suitable for machine learning models. initially, we discussed vectorization and one-hot encoding. vectorization is a general approach to transform textual or categorical data into numerical vectors. one-hot encoding specifically converts categorical variables into binary vectors, creating separate columns for each category. this method is widely used in multiclass and multilabel classification problems but can introduce the curse of dimensionality when dealing with variables having many unique categories. we then examined label encoding and integer encoding. label encoding assigns each categorical class a unique integer value. while this method is straightforward, it implies an ordinal relationship between categories, which may not always be appropriate. integer encoding is particularly useful when the categorical variable represents ordinal data (categories with a meaningful order), as it preserves the inherent ordering. due to the limitations of one-hot encoding, such as increased dimensionality, we introduced binary encoding (compact encoding). binary encoding efficiently represents multiple categories using fewer columns for example, just three columns can encode up to eight distinct classes. this approach helps mitigate dimensionality issues while retaining meaningful category distinctions. additionally, we covered frequency encoding and target encoding. frequency encoding involves assigning each category a numerical value based on its frequency within the dataset. target encoding replaces categories with values derived from the target variable (such as the mean target value for each category), effectively capturing relationships between categories and the outcome variable. finally, we briefly touched upon methods for converting textual data into numerical vectors through vectorization techniques, setting the stage for deeper exploration in future sessions.",3,-42.69363,3.4189084,0.02768619,6.3129735,"categorical, categorization, categorise"
3,"population and sample were further discussed upon. sample is a good and representative part of the population. sample is used to predict/estimate the population. attributes and operations such as count, mode , median, mean, std. deviation , variance , add, multiply, divide and subtract. different level of measurement were classified on the basis of these attributes and operations. population has parameter(s) while sample has statistic. we estimate parameters on the basis of these statistic. linear data model was discussed upon from the sales vs advt. expenditure . any model can be fitted upon any set of data but it might not be the best fit. even a point can be considered a model although a naive model. as sample size increases the estimation of parameters gets better and better. based on th sample a slr can be fit with the equation y=î²o+î²1 , where î²o is the bias in the data , as the sample size gets closer and closer to the population this bias reduces. bias represents the sum of the effect of all the unknown(unaccounted for) variables. î²o and î²1 are estimates of the population parameters. we can determine confidence intervals around the estimated parameters , the size of the confidence interval depends upon the level of confidence. as the level of confidence increases the confidence interval gets wider and wider. to find the best fit line we equate the sum of squares of errors of data points from the line to 0. we use ei^2 because it does not depend upon the direction gives the euclidean distance rather than the manhattan distance we get in the case of |ei| . upon partial differentiation of the sum of squares of error we get the values of a and b of the equation y=ax + b.
b= mean(y) - a*mean(x) 
a=(mean(xy) - mean(x) * mean(y) )/(mean(xâ²) - (mean(x))â² )
î²o and î²1are point estimates and we need to arrive at the possible interval within which their value lies such that there is a very high chance that î²op and î²1p (i.e the population parameters) will lie within those intervals respectively.","population and sample were further discussed upon. sample is a good and representative part of the population. sample is used to predict/estimate the population. attributes and operations such as count, mode , median, mean, std. deviation , variance , add, multiply, divide and subtract. different level of measurement were classified on the basis of these attributes and operations. population has parameter(s) while sample has statistic. we estimate parameters on the basis of these statistic. linear data model was discussed upon from the sales vs advt. expenditure . any model can be fitted upon any set of data but it might not be the best fit. even a point can be considered a model although a naive model. as sample size increases the estimation of parameters gets better and better. based on th sample a slr can be fit with the equation y= o+ 1 , where o is the bias in the data , as the sample size gets closer and closer to the population this bias reduces. bias represents the sum of the effect of all the unknown(unaccounted for) variables. o and 1 are estimates of the population parameters. we can determine confidence intervals around the estimated parameters , the size of the confidence interval depends upon the level of confidence. as the level of confidence increases the confidence interval gets wider and wider. to find the best fit line we equate the sum of squares of errors of data points from the line to 0. we use ei^2 because it does not depend upon the direction gives the euclidean distance rather than the manhattan distance we get in the case of |ei| . upon partial differentiation of the sum of squares of error we get the values of a and b of the equation y=ax + b. b= mean(y) - a*mean(x) a=(mean(xy) - mean(x) * mean(y) )/(mean(x ) - (mean(x)) ) o and 1are point estimates and we need to arrive at the possible interval within which their value lies such that there is a very high chance that op and 1p (i.e the population parameters) will lie within those intervals respectively.",1,34.161472,-6.936126,16.333784,3.7468805,"population, models, estimating"
4,"we first looked at all the summaries and observed from the graph that number of people who are submitting the summaries are decreasing with the session and the average number of words in the summaries are increasing.
then we took an example where the function looked similar to sine curve and with one feature x1, we engineered other features as x1â²,x1â³,x1â´.this is called polynomial regression.from this we obtained a p-value.then we added another feature which was sin(x1). the p value obtained in second case was found less than that of first case which suggested that the sine feature that was added is significant for this model.however, adding large number of features such that they do not give a better estimation beyond a certain point is not preferred and will decrease the adjusted râ² values.also,if a single model can represent the data good enough then it should always be chosen over having multiple models.
then we discussed about parametric method like neural network. in this there is an input layer which takes in the features and then another layer which performs computations. if computations are performed in multiple layers it is referred to as deep learning. input layer is connected to computational layers through links which represent the degrees of freedom. increasing the degrees of freedom can lead to overfitting.","we first looked at all the summaries and observed from the graph that number of people who are submitting the summaries are decreasing with the session and the average number of words in the summaries are increasing. then we took an example where the function looked similar to sine curve and with one feature x1, we engineered other features as x1 ,x1 ,x1 .this is called polynomial regression.from this we obtained a p-value.then we added another feature which was sin(x1). the p value obtained in second case was found less than that of first case which suggested that the sine feature that was added is significant for this model.however, adding large number of features such that they do not give a better estimation beyond a certain point is not preferred and will decrease the adjusted r values.also,if a single model can represent the data good enough then it should always be chosen over having multiple models. then we discussed about parametric method like neural network. in this there is an input layer which takes in the features and then another layer which performs computations. if computations are performed in multiple layers it is referred to as deep learning. input layer is connected to computational layers through links which represent the degrees of freedom. increasing the degrees of freedom can lead to overfitting.",0,-2.966825,1.9048488,8.898137,3.9197586,"models, feature, features"
5,midsem metrics for evaluation and also discussion of exercise and feedback on it,midsem metrics for evaluation and also discussion of exercise and feedback on it,6,11.54947,19.017876,9.533401,5.403641,"summarizing, summarize, summarization"
6,"the lecture explained how we use samples to understand populations. a sample is a small group taken from a population that should represent it well. this lets us study the sample to estimate the population's parameters. these estimates, calculated from the sample, are called statistics.
we then discussed regression. a good model takes the data from the sample to find a slope and an intercept of a regression line. these are our best estimates of the true values for the population. however, point estimates aren't very accurate. they're good, but they're not great. so to make our estimation more reliable, we form an interval estimate that allows us to say something like, ""we are 95% confident that the true value lies in this range.""
points estimates are different for different samples and better if sample becomes larger.
we then considered how to determine the best-fit line. among the methods available, such as minimizing the sum of errors or mod of errors , these were not the best approaches. instead, we opted to minimize the sum of squared errors, which yields better results because it reduces the effect of larger errors by magnifying them.
finally, we found a direct formula to compute the slope and intercept. it is known as closed form since we do not require iteration.","the lecture explained how we use samples to understand populations. a sample is a small group taken from a population that should represent it well. this lets us study the sample to estimate the population's parameters. these estimates, calculated from the sample, are called statistics. we then discussed regression. a good model takes the data from the sample to find a slope and an intercept of a regression line. these are our best estimates of the true values for the population. however, point estimates aren't very accurate. they're good, but they're not great. so to make our estimation more reliable, we form an interval estimate that allows us to say something like, ""we are 95% confident that the true value lies in this range."" points estimates are different for different samples and better if sample becomes larger. we then considered how to determine the best-fit line. among the methods available, such as minimizing the sum of errors or mod of errors , these were not the best approaches. instead, we opted to minimize the sum of squared errors, which yields better results because it reduces the effect of larger errors by magnifying them. finally, we found a direct formula to compute the slope and intercept. it is known as closed form since we do not require iteration.",1,32.742912,-8.989129,16.5552,4.1989493,"population, models, estimating"
7,"in class, we started by talking about how to measure key population parameters like the mean and variance.
sir used an example to explain the central limit theorem, where we tried to estimate the extra work hours of managers in a company. we worked with a small sample of 18 observations, assuming it represented the whole population, and then we plotted the probability density function (pdf) for the sample mean.
after that, we learned about confidence intervals, and how we use the t-distribution when our sample size is less than 3, sir also explained why confidence intervals are so useful in real lifeâ€”they help us make better decisions based on data. 
we touched on other important statistical terms, like t-values and z-values, and how to calculate them. we also discussed what it means when results are â€œstatistically different,â€ or when we can confidently say that two things are significantly different based on statistical tests.

the class wrapped up with a quick introduction to p-values and multiple linear regression.","in class, we started by talking about how to measure key population parameters like the mean and variance. sir used an example to explain the central limit theorem, where we tried to estimate the extra work hours of managers in a company. we worked with a small sample of 18 observations, assuming it represented the whole population, and then we plotted the probability density function (pdf) for the sample mean. after that, we learned about confidence intervals, and how we use the t-distribution when our sample size is less than 3, sir also explained why confidence intervals are so useful in real life they help us make better decisions based on data. we touched on other important statistical terms, like t-values and z-values, and how to calculate them. we also discussed what it means when results are statistically different, or when we can confidently say that two things are significantly different based on statistical tests. the class wrapped up with a quick introduction to p-values and multiple linear regression.",7,32.089077,7.9035406,14.602179,2.4632158,"statistics, statistical, statisticsâ"
8,"explained logistic regression, which is used for binary classification. we discussed clustering, a method for grouping similar data points. true positive and true negative concepts were highlighted for model evaluation. outlier detection techniques were introduced to identify rare occurrences in data. the lecture also covered loss functions, which guide model training. finally, we explored different approaches to solving logistic regression effectively.","explained logistic regression, which is used for binary classification. we discussed clustering, a method for grouping similar data points. true positive and true negative concepts were highlighted for model evaluation. outlier detection techniques were introduced to identify rare occurrences in data. the lecture also covered loss functions, which guide model training. finally, we explored different approaches to solving logistic regression effectively.",13,-6.373016,-8.659547,8.513382,5.209682,"classification, classifying, classifications"
9,"we looked at the course summary we initially talked about ways of improving the quality of our results, either by improving the sample, the method, or fine tuning/properly using the method.
we studies the errors in the residual scatter plots, and since the errors were distributed very systematically, we looked at how introduction of new variables (with polynomial relations with known variables) make the model better.
we looked at why one model is not fitting a data well, and then looked at multiple models, the results or different statistical quantities which we were getting and then tried to derive the meaning of them.
we looked at neural networks and deep klearning methods.
we discussed logistic regression, sigmoid function at the end","we looked at the course summary we initially talked about ways of improving the quality of our results, either by improving the sample, the method, or fine tuning/properly using the method. we studies the errors in the residual scatter plots, and since the errors were distributed very systematically, we looked at how introduction of new variables (with polynomial relations with known variables) make the model better. we looked at why one model is not fitting a data well, and then looked at multiple models, the results or different statistical quantities which we were getting and then tried to derive the meaning of them. we looked at neural networks and deep klearning methods. we discussed logistic regression, sigmoid function at the end",13,-2.3002892,-2.9795945,9.191719,4.788039,"classification, classifying, classifications"
10,"the lecture began with a recap of confidence intervals. using an example, we analyzed points a and b within the confidence interval range to determine whether they were truly distinct or if their values appeared by chance. another point, d, which was outside the confidence interval, was identified as statistically significant. since the probability of obtaining d from the sample was very low, we concluded that it was statistically different from a and b.  

next, we introduced embedding vectors as a way to process data. we converted data into vectors that included various features, allowing for feature engineering.  

following this, we discussed multiple linear regression (mlr). the objective of mlr is to express a dependent variable (y) as a linear combination of independent features (xâ‚, xâ‚‚, xâ‚ƒ, ...). like simple linear regression, mlr aims to minimize the sum of squared errors to determine the best-fit coefficients for the features in the model.  

the lecture then covered the matrices used in the derivation of mlr and the cost function which helps quantify the error. the gradient descent process was then introduced to optimize the model parameters.  

we also discussed the f-value, calculated as the mean square regression (msr) divided by the mean square error (mse). a higher f-value indicates a better model fit.  

next, we explored the role of the p-value in multiple linear regression. if a feature's p-value is greater than 0.05, it suggests that zero falls within the confidence interval, meaning that the feature is not statistically significant. based on this, we learned that features with high p-values can be dropped from the model, as their presence does not significantly impact the predictions. features with the highest p-values are the least significant and should be removed to improve the model's efficiency.","the lecture began with a recap of confidence intervals. using an example, we analyzed points a and b within the confidence interval range to determine whether they were truly distinct or if their values appeared by chance. another point, d, which was outside the confidence interval, was identified as statistically significant. since the probability of obtaining d from the sample was very low, we concluded that it was statistically different from a and b. next, we introduced embedding vectors as a way to process data. we converted data into vectors that included various features, allowing for feature engineering. following this, we discussed multiple linear regression (mlr). the objective of mlr is to express a dependent variable (y) as a linear combination of independent features (x , x , x , ...). like simple linear regression, mlr aims to minimize the sum of squared errors to determine the best-fit coefficients for the features in the model. the lecture then covered the matrices used in the derivation of mlr and the cost function which helps quantify the error. the gradient descent process was then introduced to optimize the model parameters. we also discussed the f-value, calculated as the mean square regression (msr) divided by the mean square error (mse). a higher f-value indicates a better model fit. next, we explored the role of the p-value in multiple linear regression. if a feature's p-value is greater than 0.05, it suggests that zero falls within the confidence interval, meaning that the feature is not statistically significant. based on this, we learned that features with high p-values can be dropped from the model, as their presence does not significantly impact the predictions. features with the highest p-values are the least significant and should be removed to improve the model's efficiency.",2,18.048662,6.353295,12.796107,4.253656,"regression, regressions, features"
11,"problem with heatmap is that they don't capture sufficient information but they play important role when we have multiple parameters and it is not feasible to plot all parameters therefore heatmap helps in pair wise analysis.variance influence factor explain a parameter based other all other parameters excluding itself cif  is xi=f( xj not equals xi ) indentify the vif of the features of the vif is greater than threshold then remove it principle component analysis is an application of singular value decomposition principle component are necessarily orthogonal to each other and can be equal to the dimension of the data but if we have for say 10 principle component we can decide which parameters we will choose to continue with if we have 2 dimensional data and we do pca then it may get converted to one dimension hence help in reducing the dimensionality of the data pcs are the weighted sum of all the parameters and the weights are known as loading in general vif needs to do first as multicollinearity is a big problem than pca . pca helps you with predictions but we cannot do what if/ sensitivity type of analysis like how much u changes when we change a x(parameter) as they are not real parameters. pca has  multiple uses dimension reduction , prediction models ,  visualization( during eda) for the purpose of understanding the data , captures variance , data reduction ( data compression without loosing too much of data ), it is sensitive to data scaling therefore we have to normalize the data , tsne  follows t distribution it is stochastic in nature and gradient descent is involved in this , it is a lossy transformation ( we loose exactness of data  but we get relative closeness of the data )","problem with heatmap is that they don't capture sufficient information but they play important role when we have multiple parameters and it is not feasible to plot all parameters therefore heatmap helps in pair wise analysis.variance influence factor explain a parameter based other all other parameters excluding itself cif is xi=f( xj not equals xi ) indentify the vif of the features of the vif is greater than threshold then remove it principle component analysis is an application of singular value decomposition principle component are necessarily orthogonal to each other and can be equal to the dimension of the data but if we have for say 10 principle component we can decide which parameters we will choose to continue with if we have 2 dimensional data and we do pca then it may get converted to one dimension hence help in reducing the dimensionality of the data pcs are the weighted sum of all the parameters and the weights are known as loading in general vif needs to do first as multicollinearity is a big problem than pca . pca helps you with predictions but we cannot do what if/ sensitivity type of analysis like how much u changes when we change a x(parameter) as they are not real parameters. pca has multiple uses dimension reduction , prediction models , visualization( during eda) for the purpose of understanding the data , captures variance , data reduction ( data compression without loosing too much of data ), it is sensitive to data scaling therefore we have to normalize the data , tsne follows t distribution it is stochastic in nature and gradient descent is involved in this , it is a lossy transformation ( we loose exactness of data but we get relative closeness of the data )",11,-19.147757,4.056911,10.460503,13.415756,"pca, heatmap, heatmaps"
12,"in today's session, we first analyzed the summaries we write after every class using the pivot table in excel, in which we see the number of people who submitted the summary, the average word length of summaries, the minimum number of characters in any summary and the maximum number of character in any summary. we plot the data collected through summaries into a histogram, which tells about the shape of the data, and then the box plot to further analyze the data. we find the descriptive statistics of the data and scatter plots of data points. we also see how many times a person has submitted the summary, and then we plot the changes in the summary over time, including changes in maximum words and minimum words in the summaries. then, we also analyzed other data with different columns of data, and at last, we plotted the data, including the outliers. at the end of the class, we got the assessment of exercise 2 by the teaching assistant, including some points which are as follows: proper naming of files; proper plots with titles, labeling, and legend; excel file submission as (.xlsx) form and not (.csv) as they don't contain the formulas we have used for performing the data analysis; formatting the document in a proper format; providing examples if asked; using a spreadsheet if mentioned, making all the necessary plots and calculating correct descriptive statistics; adding residual diagnostics, talking about all the metrics obtained and not just r^2; creating a dataset with large variance; rmse, f-statistic, r^2 vs std. dev. plots; mentioning the ci interval also with the description about it.","in today's session, we first analyzed the summaries we write after every class using the pivot table in excel, in which we see the number of people who submitted the summary, the average word length of summaries, the minimum number of characters in any summary and the maximum number of character in any summary. we plot the data collected through summaries into a histogram, which tells about the shape of the data, and then the box plot to further analyze the data. we find the descriptive statistics of the data and scatter plots of data points. we also see how many times a person has submitted the summary, and then we plot the changes in the summary over time, including changes in maximum words and minimum words in the summaries. then, we also analyzed other data with different columns of data, and at last, we plotted the data, including the outliers. at the end of the class, we got the assessment of exercise 2 by the teaching assistant, including some points which are as follows: proper naming of files; proper plots with titles, labeling, and legend; excel file submission as (.xlsx) form and not (.csv) as they don't contain the formulas we have used for performing the data analysis; formatting the document in a proper format; providing examples if asked; using a spreadsheet if mentioned, making all the necessary plots and calculating correct descriptive statistics; adding residual diagnostics, talking about all the metrics obtained and not just r^2; creating a dataset with large variance; rmse, f-statistic, r^2 vs std. dev. plots; mentioning the ci interval also with the description about it.",6,-10.1930895,25.413107,7.7740235,10.173612,"summarizing, summarize, summarization"
13,"first we calculate sample mean,which is considered the approximate of population mean ,then std dev of population is calculated by dividing population std dev by the root of sample space and when the sample size is less we can use t-statistic instead of normal distribution,then we do linear regression as y=b0+b1x and we use hypothesis testing to check for dependency on the variable and if p-value is less than 0.05 then it can be concluded that the coefficient is not 0. anova compares the statistical equivalence of multiple means or variables.
the f-statistic is the ratio of the mean square regression (msr) to the mean square error (mse), used to evaluate the overall model significance.","first we calculate sample mean,which is considered the approximate of population mean ,then std dev of population is calculated by dividing population std dev by the root of sample space and when the sample size is less we can use t-statistic instead of normal distribution,then we do linear regression as y=b0+b1x and we use hypothesis testing to check for dependency on the variable and if p-value is less than 0.05 then it can be concluded that the coefficient is not 0. anova compares the statistical equivalence of multiple means or variables. the f-statistic is the ratio of the mean square regression (msr) to the mean square error (mse), used to evaluate the overall model significance.",7,33.064735,0.09603559,14.842681,2.9956834,"statistics, statistical, statisticsâ"
14,"we started off by discussing that with nominal and ordinal data, our observation or data points have an additional characteristic property called as a label. labels basically help us to identify which class that observation belongs to. 
then we took a small detour to understand standard error in detail. we understood that standard error is the variance of the means of different samples of a population. also we discussed about the number of bins which should be considered while plotting histograms. it depends on the data and how deep do we need to analyse the data. if we need to understand broad trends, we can use lesser number of bins, while if we want to find some minute trends, we need to use more bins. 
we came back to logistic regression and discussed about the logistic unit. a logistic unit gives us the probability of an observation being in a particular class. we use the sigmoid function in order to get the exact probabilities, and then map the probability to a particular class. so p(y|x) is calculated as the sigmoid function evaluated at a = w1x1 + w2x2 + w3x3. now there will always be a difference between the probability value and the actual class label. the probability value is denoted by â€™pâ€™ while the known class outcomes are denoted using â€˜tâ€™. our aim with logistic regression is to maximise the likelihood of our predicted outcomes being close to the targets, or to reduce the difference between the probability value and the target value. so â€˜tâ€™ is the observed outcome i.e. 0 or 1, â€˜pâ€™ is the probability of t=1 and thus, 1-p gives the probability of t=0. our aim is to maximise p or 1-p based on the value of t. now to ease out our calculations and maximisation, we take the log of the probability products, so that they can convert to a summation, and that gives us our loss function which we need to minimise.
we then moved on to some quality metrics which we need to assess our model. we discussed about the confusion matrix and the various terms, true positives, true negatives, false positives and false negatives. we discussed about accuracy, and how it is not a very good measure in case there is a class imbalance i.e. one class is under-represented and has very few observations. we understood precision, which is defined as of the events that we have detected, how many have we detected correctly. recall is defined as of a specific class, how many have we detected correctly. f1 value is defined is the harmonic mean of precision and recall. these terms give us a better picture of the data and prevent us from being misled by the accuracy. 
lastly, the tas of the course gave us insights about our assignment submissions.","we started off by discussing that with nominal and ordinal data, our observation or data points have an additional characteristic property called as a label. labels basically help us to identify which class that observation belongs to. then we took a small detour to understand standard error in detail. we understood that standard error is the variance of the means of different samples of a population. also we discussed about the number of bins which should be considered while plotting histograms. it depends on the data and how deep do we need to analyse the data. if we need to understand broad trends, we can use lesser number of bins, while if we want to find some minute trends, we need to use more bins. we came back to logistic regression and discussed about the logistic unit. a logistic unit gives us the probability of an observation being in a particular class. we use the sigmoid function in order to get the exact probabilities, and then map the probability to a particular class. so p(y|x) is calculated as the sigmoid function evaluated at a = w1x1 + w2x2 + w3x3. now there will always be a difference between the probability value and the actual class label. the probability value is denoted by p while the known class outcomes are denoted using t . our aim with logistic regression is to maximise the likelihood of our predicted outcomes being close to the targets, or to reduce the difference between the probability value and the target value. so t is the observed outcome i.e. 0 or 1, p is the probability of t=1 and thus, 1-p gives the probability of t=0. our aim is to maximise p or 1-p based on the value of t. now to ease out our calculations and maximisation, we take the log of the probability products, so that they can convert to a summation, and that gives us our loss function which we need to minimise. we then moved on to some quality metrics which we need to assess our model. we discussed about the confusion matrix and the various terms, true positives, true negatives, false positives and false negatives. we discussed about accuracy, and how it is not a very good measure in case there is a class imbalance i.e. one class is under-represented and has very few observations. we understood precision, which is defined as of the events that we have detected, how many have we detected correctly. recall is defined as of a specific class, how many have we detected correctly. f1 value is defined is the harmonic mean of precision and recall. these terms give us a better picture of the data and prevent us from being misled by the accuracy. lastly, the tas of the course gave us insights about our assignment submissions.",10,12.534424,-18.047813,8.962061,-1.3862275,"classifications, histograms, histogram"
15,"in today's lecture, sir discussed steps to solve problem in ds, 1. understanding the problem 2. exploring data analysis (eda) 3. visualization.  the method crisp-dm (cross industry standard process for data mining) is like a sop for problem solving; it has six steps which are run cyclically : business understanding,  data understanding,  data preparation,  modeling, evaluation, deployment. a term known as heteroscadacity means that the variance is also varing (the assumption that the variance remains same for all data is no longer valid). then ta explained steps of eda with examples of pima india diabetes dataset, ganga water quality 2012, aqi of different areas in mumbai. then he discussed about box plot, matrix plot,etc helps analysis and how class imbalance affect the analysis. after that we learnt how to handle missing data (mcar, mar & mnar are types of missing data). there are with univariate and multivariate approaches to fix missing data. univariate solution involves options like 1. delete the entire column/row of that particular entry or 2. replace the missing data with mean, median or mode whichever is suitable. multivariate approach involves options like knn (kth nearest neighbor), mice, fatal regression model,etc. then we learnt how to handle outliers (outliers arises due to 1. data corruption,  2. faulty measurements & 3. true outliers). handling outliers also involves univariate(isolation forest) and multivariate approach(db scan). there are techniques through which one can determine which value is outliers (one of the example is you drop the value of the outlier if it lies outside the certain tolerance interval of iqr(inter-quantile range). also, median is not influenced by the outliers whereas mean is influenced by the outliers. thus median is oftenly used to isolate outliers.","in today's lecture, sir discussed steps to solve problem in ds, 1. understanding the problem 2. exploring data analysis (eda) 3. visualization. the method crisp-dm (cross industry standard process for data mining) is like a sop for problem solving; it has six steps which are run cyclically : business understanding, data understanding, data preparation, modeling, evaluation, deployment. a term known as heteroscadacity means that the variance is also varing (the assumption that the variance remains same for all data is no longer valid). then ta explained steps of eda with examples of pima india diabetes dataset, ganga water quality 2012, aqi of different areas in mumbai. then he discussed about box plot, matrix plot,etc helps analysis and how class imbalance affect the analysis. after that we learnt how to handle missing data (mcar, mar & mnar are types of missing data). there are with univariate and multivariate approaches to fix missing data. univariate solution involves options like 1. delete the entire column/row of that particular entry or 2. replace the missing data with mean, median or mode whichever is suitable. multivariate approach involves options like knn (kth nearest neighbor), mice, fatal regression model,etc. then we learnt how to handle outliers (outliers arises due to 1. data corruption, 2. faulty measurements & 3. true outliers). handling outliers also involves univariate(isolation forest) and multivariate approach(db scan). there are techniques through which one can determine which value is outliers (one of the example is you drop the value of the outlier if it lies outside the certain tolerance interval of iqr(inter-quantile range). also, median is not influenced by the outliers whereas mean is influenced by the outliers. thus median is oftenly used to isolate outliers.",9,-14.581311,16.359465,9.103767,8.924123,"dataâ, analyse, analyses"
16,"today, we built on our understanding of statistical significance before shifting gears to python, where we were assigned tutorials to prepare for the next class.  

we also looked at how data science algorithms analyze session summaries using a relative strength approach. this method calculates the distance between different submissions based on scatter plot coefficients, helping to evaluate patterns effectively.  

later, we dove into **multiple linear regression**, where a dependent variable is influenced by multiple independent factors. a key concept we explored was the **f-value**, which helps gauge a modelâ€™s effectiveness by comparing explained and unexplained varianceâ€”a higher f-value suggests a better fit. using a dataset, we applied error metrics and compared f-values, getting some hands-on experience in assessing model performance.","today, we built on our understanding of statistical significance before shifting gears to python, where we were assigned tutorials to prepare for the next class. we also looked at how data science algorithms analyze session summaries using a relative strength approach. this method calculates the distance between different submissions based on scatter plot coefficients, helping to evaluate patterns effectively. later, we dove into **multiple linear regression**, where a dependent variable is influenced by multiple independent factors. a key concept we explored was the **f-value**, which helps gauge a model s effectiveness by comparing explained and unexplained variance a higher f-value suggests a better fit. using a dataset, we applied error metrics and compared f-values, getting some hands-on experience in assessing model performance.",13,-6.982253,-5.154506,9.350138,5.390138,"classification, classifying, classifications"
17,"in todays class , we first discussed about exploratory data analysis. we explored pivot table tool in excel . in which we worked first with the class data of summaries which we submit. we learnt how to arrange data according to different parameters, how to arrange them in rows and columns .then we worked eda on chemical lab data in excel sheet. we first arranged data according to time , ie , years then months then dates. and then assessed them like by finding the mean , minimum value , maximum value and so on. we them came across a 0 minimum value of a particular year , on exploring more , there were 3 days with no sensor reading , hence 0 . we then checked mean , median , mode and so on using data analysis tool and from that value, we concluded that, in assymetric distribution, mean is not around in the middle of min and max values . also its histogram will be on the one side. we can also check kurtosos and skewness to prove it. 
then we check outliers and anomalies. 
we used correlations for different features to bring out relationships in a heat map. 
in histograms , we can see outliers and can remove them to perform eda on the rest of the data.
then the tas  gave review about the e2 assignment","in todays class , we first discussed about exploratory data analysis. we explored pivot table tool in excel . in which we worked first with the class data of summaries which we submit. we learnt how to arrange data according to different parameters, how to arrange them in rows and columns .then we worked eda on chemical lab data in excel sheet. we first arranged data according to time , ie , years then months then dates. and then assessed them like by finding the mean , minimum value , maximum value and so on. we them came across a 0 minimum value of a particular year , on exploring more , there were 3 days with no sensor reading , hence 0 . we then checked mean , median , mode and so on using data analysis tool and from that value, we concluded that, in assymetric distribution, mean is not around in the middle of min and max values . also its histogram will be on the one side. we can also check kurtosos and skewness to prove it. then we check outliers and anomalies. we used correlations for different features to bring out relationships in a heat map. in histograms , we can see outliers and can remove them to perform eda on the rest of the data. then the tas gave review about the e2 assignment",9,-11.4456625,25.528553,7.7932043,10.256006,"dataâ, analyse, analyses"
18,firstly we started with a hands on excel session sir started with teaching us how to calculate beta0 and beta1 and eventually y_cap and error values. later sir questioned the class regarding the prediction of of data points beyond the regression line which is in turn unpredictable due to sample constraints. later sir emphasized regarding accessing the model by judging the noise and it's behavior which is supposed to be totally random for a good model. further we learnt the definition and characteristics of a histogram briefly and judging whether an error distribution is random or not. a random error distribution should follow a normal/gaussian distribution which is due to the error being dependent upon many-many unknown variables. further we understood the use of data analysis toolpack. further sir discussed the coefficient of determination. for the case of simple linear regression is exactly the same as the square of the correlation coefficient 'r' between x and y which doesn't hold true for multiple linear regression. the class ends with derivation of centralâ limitâ theory,firstly we started with a hands on excel session sir started with teaching us how to calculate beta0 and beta1 and eventually y_cap and error values. later sir questioned the class regarding the prediction of of data points beyond the regression line which is in turn unpredictable due to sample constraints. later sir emphasized regarding accessing the model by judging the noise and it's behavior which is supposed to be totally random for a good model. further we learnt the definition and characteristics of a histogram briefly and judging whether an error distribution is random or not. a random error distribution should follow a normal/gaussian distribution which is due to the error being dependent upon many-many unknown variables. further we understood the use of data analysis toolpack. further sir discussed the coefficient of determination. for the case of simple linear regression is exactly the same as the square of the correlation coefficient 'r' between x and y which doesn't hold true for multiple linear regression. the class ends with derivation of central limit theory,5,21.587645,-1.5939969,13.779273,4.548632,"regression, statistical, statistics"
19,"in the beginning of the class we discussed a few questions raised by students about the previous topics discussed in class: learnt about expectation algebra, histogram showing a particular distribution and how many bins to consider, when to consider a division between two clusters and what boundary to consider, model which changes regression expression according to ranges of x either by going for two different forests or some other method. 
after that we continued our discussion from previous class about logistic unit and logistic regression. soft max function redistributes numbers around 0 and 1 so that they can be used. if p>0.5 then we push it into one class and otherwise, we push it to another class. then we saw the notations of logistic regression with 3 model examples. y1 and y2 are known outcomes, which can also be referred to as 't'. then we learnt about how to calculate the weights wi. we do this such that the likelihood of getting desired targets is maximized (another way of saying minimize the differences). y is known as the target and denoted by t.  we find likelihood using the function, and maximising l, we get the desired weights wi. the expression for l should satisfy the two requirements, that when t=1 we would be getting maximum p and when t=0 we should be getting maximum (1-p). likelihood is a product based function, but it's easier to deal when in summation form, so we take log of likelihood. it's better to do this way, because maxima are always unstable, while minima are stable points. use gradient descent method to do so. also, we defined something like n ( learning rate). then we saw the confusion matrix (cases of tn, fn, fp and tp, and the examples that are associated with them). false negatives are more disastrous in cases of scenarios like being sick but told that you're not. definition of accuracy: being able to correctly identify the situation, how close the measurement is to the actual value. definition of precision: of the events that you detected, how many did you do correctly, how close the measurements of the same observations are to each other. definition of recall: of a specific class, how many could you correctly identify. f1 value is defined as the harmonic mean value of precision and recall. 
in the end of the class we discussed the topics of assignment e1, and the common mistakes most people made, like directly using kurtosis function from excel without checking if we need the positive kurtosis or negative kurtosis, and in q1 which involved comparison between the two models, the slope terms weren't same which most people just subtracted intercept term from predictions and used it for comparison. general: excel file should be submitted along with calculations and formulas, not just values. and put in more effort during documentation and organize well.","in the beginning of the class we discussed a few questions raised by students about the previous topics discussed in class: learnt about expectation algebra, histogram showing a particular distribution and how many bins to consider, when to consider a division between two clusters and what boundary to consider, model which changes regression expression according to ranges of x either by going for two different forests or some other method. after that we continued our discussion from previous class about logistic unit and logistic regression. soft max function redistributes numbers around 0 and 1 so that they can be used. if p>0.5 then we push it into one class and otherwise, we push it to another class. then we saw the notations of logistic regression with 3 model examples. y1 and y2 are known outcomes, which can also be referred to as 't'. then we learnt about how to calculate the weights wi. we do this such that the likelihood of getting desired targets is maximized (another way of saying minimize the differences). y is known as the target and denoted by t. we find likelihood using the function, and maximising l, we get the desired weights wi. the expression for l should satisfy the two requirements, that when t=1 we would be getting maximum p and when t=0 we should be getting maximum (1-p). likelihood is a product based function, but it's easier to deal when in summation form, so we take log of likelihood. it's better to do this way, because maxima are always unstable, while minima are stable points. use gradient descent method to do so. also, we defined something like n ( learning rate). then we saw the confusion matrix (cases of tn, fn, fp and tp, and the examples that are associated with them). false negatives are more disastrous in cases of scenarios like being sick but told that you're not. definition of accuracy: being able to correctly identify the situation, how close the measurement is to the actual value. definition of precision: of the events that you detected, how many did you do correctly, how close the measurements of the same observations are to each other. definition of recall: of a specific class, how many could you correctly identify. f1 value is defined as the harmonic mean value of precision and recall. in the end of the class we discussed the topics of assignment e1, and the common mistakes most people made, like directly using kurtosis function from excel without checking if we need the positive kurtosis or negative kurtosis, and in q1 which involved comparison between the two models, the slope terms weren't same which most people just subtracted intercept term from predictions and used it for comparison. general: excel file should be submitted along with calculations and formulas, not just values. and put in more effort during documentation and organize well.",10,11.058308,-17.117447,9.06804,-1.496033,"classifications, histograms, histogram"
20,"a sample is just a smaller group taken from a larger population. any calculations we make from a sample, like averages, variances, or standard deviations, are called statistics, while the same calculations done on the entire population are known as parameters. the goal is often to estimate these population parameters using statistics from a sample.
now, in the case of simple linear regression, weâ€™re trying to find a straight line that best fits our data so we can predict one thing based on another. the equation of this line is written as: 
y = î²â‚€ +  î²â‚x 
where:
y is the predicted value,
x is the input value, and
î²â‚€ and î²â‚ are the parameters of the line.
these parameters, î²â‚€ and î²â‚, represent the intercept and the slope of the line, and they're estimates of the population parameters. to get the best estimates for these, we want to minimize the errorsâ€”basically, the difference between the actual data points and the line we're drawing.
we do this by minimizing the sum of squared errors which are just the squared differences between the predicted and actual values. it's a way to avoid problems that would arise from just using raw differences (like positive and negative errors canceling each other out). minimizing these squared differences gives us the best-fit line.
to find the actual values of î²â‚€ and î²â‚, we use a method called partial differentiation to minimize the errors. the formulas that come out of this are:
î²â‚€ = mean(y) - î²â‚ * mean(x)
î²â‚ = (mean(xy) - mean(x) * mean(y)) / (mean(xâ²) - mean(x)â²)
these give us the estimated values of the slope and intercept, which help us understand the relationship between the variables in our data. by using these estimates, we can get a good approximation of the population parameters and make predictions from our model.","a sample is just a smaller group taken from a larger population. any calculations we make from a sample, like averages, variances, or standard deviations, are called statistics, while the same calculations done on the entire population are known as parameters. the goal is often to estimate these population parameters using statistics from a sample. now, in the case of simple linear regression, we re trying to find a straight line that best fits our data so we can predict one thing based on another. the equation of this line is written as: y = + x where: y is the predicted value, x is the input value, and and are the parameters of the line. these parameters, and , represent the intercept and the slope of the line, and they're estimates of the population parameters. to get the best estimates for these, we want to minimize the errors basically, the difference between the actual data points and the line we're drawing. we do this by minimizing the sum of squared errors which are just the squared differences between the predicted and actual values. it's a way to avoid problems that would arise from just using raw differences (like positive and negative errors canceling each other out). minimizing these squared differences gives us the best-fit line. to find the actual values of and , we use a method called partial differentiation to minimize the errors. the formulas that come out of this are: = mean(y) - * mean(x) = (mean(xy) - mean(x) * mean(y)) / (mean(x ) - mean(x) ) these give us the estimated values of the slope and intercept, which help us understand the relationship between the variables in our data. by using these estimates, we can get a good approximation of the population parameters and make predictions from our model.",1,32.249065,-9.314625,16.59812,4.1868773,"population, models, estimating"
21,https://docs.google.com/forms/d/e/1faipqlsffcpe8ytvk7pee7cslz0xgjhjk3_a8y7jo1abmkldxnrim4a/closedform,https://docs.google.com/forms/d/e/1faipqlsffcpe8ytvk7pee7cslz0xgjhjk3_a8y7jo1abmkldxnrim4a/closedform,12,8.242522,-7.1775813,9.8443365,3.832401,"classifiers, logistic, roc"
22,"in todayâ€™s class we learnt about logistic regression. we began by understanding the classification problem which is about determining a decision boundary between two classes .the logistic unit takes multiple input features and produces an outcome (0 or 1). to achieve this, we need a function that maps inputs to probabilities. the sigmoid function ensures the output is always between 0 and 1, which gives probabilities. we then learnt about maximum likelihood estimation, which minimizes negative log-likelihood to find optimal weights. gradient descent minimizes the error function. then we got better understanding through some examples. we also have several metrics to decide how good is our classifier.","in today s class we learnt about logistic regression. we began by understanding the classification problem which is about determining a decision boundary between two classes .the logistic unit takes multiple input features and produces an outcome (0 or 1). to achieve this, we need a function that maps inputs to probabilities. the sigmoid function ensures the output is always between 0 and 1, which gives probabilities. we then learnt about maximum likelihood estimation, which minimizes negative log-likelihood to find optimal weights. gradient descent minimizes the error function. then we got better understanding through some examples. we also have several metrics to decide how good is our classifier.",10,9.616125,-18.573257,9.037876,-1.7081339,"classifications, histograms, histogram"
23,"we discussed that whatever be the sample distribution, mean distribution is normal. we then defined sample error as sigma/root n where sigma of population is assumed to be close to sample sigma which we know. histogram of these frequency distribution of sample is often divided by total area so that it is made to unity distribution. and when bin size is reduced then histogram smoothens out. confidence interval 95% and t distribution in some cases. in the end discussed multiple regression using anova technique.","we discussed that whatever be the sample distribution, mean distribution is normal. we then defined sample error as sigma/root n where sigma of population is assumed to be close to sample sigma which we know. histogram of these frequency distribution of sample is often divided by total area so that it is made to unity distribution. and when bin size is reduced then histogram smoothens out. confidence interval 95% and t distribution in some cases. in the end discussed multiple regression using anova technique.",7,39.660522,0.9531235,15.4061985,2.4965339,"statistics, statistical, statisticsâ"
24,"we learnt about parameter / statistics
also about dependent variables and dependent variable
when y= box+b1
bo is bias
we also learnt about confident interval range

bo and b1 are point estimations and we need to arrive at the possible interval within which their value be such that is a very high chance that bop and b1 p will be within intervals respectively",we learnt about parameter / statistics also about dependent variables and dependent variable when y= box+b1 bo is bias we also learnt about confident interval range bo and b1 are point estimations and we need to arrive at the possible interval within which their value be such that is a very high chance that bop and b1 p will be within intervals respectively,1,37.04555,-4.8864455,16.408781,3.6239395,"population, models, estimating"
25,started with confidence interval. learnt more concepts of statistics and data analysis.,started with confidence interval. learnt more concepts of statistics and data analysis.,6,27.50394,10.054569,12.749975,5.589982,"summarizing, summarize, summarization"
26,"in today's session, we plotted x and y values for a given dataset and calculated a and b of the regression line using the x and y data, respectively, using ms excel. then, we plotted the histogram graph of the errors, i.e., actual values of y minus the predicted values of y. the height of the histogram represents the no. of values in the bin of a particular class width. using a histogram, we observed whether the data was uniform or random. we saw an example of a fitted line on non-linear data and the resulting scatter plot of the error values{ei} displaying a distinct pattern, stating the model has failed to pick up the inherent pattern in the data. using the data analysis tool pack in excel, we created a summary output of the given data showing various numbers like r-square, p-value, etc. a good model explains variations in the data. the measure of the total variation in the given dataset(sst) is equal to the sum of the total variation explained by the regression model(ssr) and the variation not explained by the model, attributed to random errors(sse) and one another term that gives zero on summing it up. dividing ssr with sst gives you the value of the coefficient of determination(r-square). for slr, c.o.d. is equal to the square of the correlation coefficient of r. a positive correlation in the data means the value of x increases, and the value of y also increases, while a negative correlation means if x increases then y decreases. if we take a population and calculate the means of different samples we have taken, the histogram of these means will follow a normal distribution. the standard error of the distribution is equal to the standard deviation of the population divided by the square root of the size of the sample. a random sample with a finite mean and standard deviation will approach a normal distribution as the value of n becomes sufficiently large(central limit theorem).","in today's session, we plotted x and y values for a given dataset and calculated a and b of the regression line using the x and y data, respectively, using ms excel. then, we plotted the histogram graph of the errors, i.e., actual values of y minus the predicted values of y. the height of the histogram represents the no. of values in the bin of a particular class width. using a histogram, we observed whether the data was uniform or random. we saw an example of a fitted line on non-linear data and the resulting scatter plot of the error values{ei} displaying a distinct pattern, stating the model has failed to pick up the inherent pattern in the data. using the data analysis tool pack in excel, we created a summary output of the given data showing various numbers like r-square, p-value, etc. a good model explains variations in the data. the measure of the total variation in the given dataset(sst) is equal to the sum of the total variation explained by the regression model(ssr) and the variation not explained by the model, attributed to random errors(sse) and one another term that gives zero on summing it up. dividing ssr with sst gives you the value of the coefficient of determination(r-square). for slr, c.o.d. is equal to the square of the correlation coefficient of r. a positive correlation in the data means the value of x increases, and the value of y also increases, while a negative correlation means if x increases then y decreases. if we take a population and calculate the means of different samples we have taken, the histogram of these means will follow a normal distribution. the standard error of the distribution is equal to the standard deviation of the population divided by the square root of the size of the sample. a random sample with a finite mean and standard deviation will approach a normal distribution as the value of n becomes sufficiently large(central limit theorem).",5,24.52671,-2.4691317,14.126688,4.499774,"regression, statistical, statistics"
27,"we started the class with a thorough discussion of the mid-semester paper, in which sir plotted and did every part and showed us. after that, the ta came to give comments regarding assignment 3; she said that the overall quality of the report was increasing as assignments were submitted. after that, in the last 10- 15 mins of the class, we discussed on the flowchart for the data problems.","we started the class with a thorough discussion of the mid-semester paper, in which sir plotted and did every part and showed us. after that, the ta came to give comments regarding assignment 3; she said that the overall quality of the report was increasing as assignments were submitted. after that, in the last 10- 15 mins of the class, we discussed on the flowchart for the data problems.",6,-3.924719,21.980316,8.209828,9.6441345,"summarizing, summarize, summarization"
28,"during exploratory data analysis we get insights on type of data and understand problems present in the data. we can remove outliers based on trend followed by data: if all data follows a particular trend and some doesn't follow we can remove this data. we can use data smoothening to reduce noise in data. real data has a lot of fluctuations. this can make it difficult to find trends ir pattern in such data. simple moving average sma consider a window around every data points and average the values. window width can be varied to adjust the level of smoothening. based on application we can take window width. we can take moving average at a point as average of all past points or average of some past and some future points. there will be problems at the end of data as there won't be any points to the past of starting of data. so we have to define accordingly at end points. the moving average reduce the noise and maintains the characteristics data. another way is exponential moving average that weight nearby samples more while averaging. this is used in time series forecasting.
in data analysis, it is crucial to handle missing values and outliers before proceeding with further steps like calculating moving averages. ignoring these aspects can lead to distorted results, such as sudden changes or incorrect trend patterns. handling missing values can be done by either ignoring, discarding, or using methods like regression to fill in the gaps. outliers should be removed or adjusted to prevent them from skewing the analysis. after addressing these issues, the data is ready for further analysis, where both visual and mathematical methods are used to ensure accuracy. normalization transforms data to a 0-1 range. in this case, normalization had little effect on the target values and coefficients, with minimal changes except for one variable (x2). this shows that scaling didn't significantly impact the model. for the next messy dataset, a clustering algorithm is suggested. in the process of data analysis, independent normalization of variables (x and y) can impact the results, especially when using algorithms like k-means clustering, which rely on euclidean distance. when the data is normalized, it alters the shape and relationships between variables, leading to more refined results. algorithms sensitive to scaling, such as k-means, are particularly affected, and normalization helps produce more accurate clusters by balancing the influence of different features.


a standard normal distribution has a mean of 0 and variance of 1. to standardize data, the formula is used, where is an observation, is the mean, and is the standard deviation. standardization does not change the shape of the data's distribution, meaning the transformed data retains the original distribution's shape. this method is often used in statistical tests, especially hypothesis testing, where normal distribution is required. data transformations like box-cox are applied to achieve normality before performing such tests. descriptive statistics of the transformations should be reviewed to understand their impact.
the box-cox transformation is a technique used to make data more normally distributed. it involves raising each observation to a power (lambda), with the optimal lambda value determined through a process called maximum likelihood estimation. in this case, the lambda value is 0.17, which is chosen to best transform the data into a normal distribution. this method is useful when algorithms assume normality in the data, ensuring that the data fits those assumptions.
the box-cox transformation and similar techniques, like square root or cube root transformations, pull data points closer together, particularly those that are far from the mean. this helps in dealing with skewed distributions. for a lambda value close to 0 in box-cox, the transformation approximates ; if lambda is 0, it uses the log transformation. the result is a more normally distributed dataset, which is essential for certain algorithms. when applying transformations like box-cox or logarithmic transformations to features (e.g., x1), these same transformations must be applied consistently to future or test data to ensure accurate predictions. additionally, sometimes transformations must be reversed to interpret results in their original context. various scaling methods, such as standardization and normalization, are commonly used to handle such transformations.
the condition where data variance changes across different levels of the data is known as heteroscedasticity.
the process of scaling data, such as using log transformations, reduces the emphasis on large values and minimizes the impact of variations in data. this is crucial because many algorithms, especially those that rely on euclidean distance or hierarchical clustering, are sensitive to data scaling. various scaling methods are used to bring the data to a common scale while preserving the variation within the dataset. however, in some cases, transformations like the box-cox transformation are applied to also change the shape of the data for better analysis.
when preparing data for analysis, steps include identifying and fixing missing values, exploring correlations between features, and creating visual representations like scatter plots or matrix plots. scaling transformations are often applied before conducting these analyses to ensure accurate and consistent results.
the issue of data imbalance arises when certain classes are underrepresented in a dataset compared to other classes. this is particularly problematic in classification tasks. in the example given, there are four classes: red, green, blue, and white, where the white class is underrepresented and often merged with the blue class. when subjecting these classes to classification, boundaries are created by the algorithm based on the dominant class, leading to misclassification of the underrepresented class.
as a result, false negatives occur, where observations belonging to the underrepresented class are wrongly classified as another class. this is evident in confusion matrices and the performance metrics like precision and recall, where the underrepresented class shows poor classification. the algorithm focuses on correctly classifying the majority class, but the minority class suffers from poor performance.
data imbalance occurs when one class significantly dominates over others in a dataset, causing challenges for learning algorithms. this imbalance can negatively affect the algorithmâ€™s ability to learn, particularly in cases like medical diagnosis (e.g., diabetes detection) or fraud detection, where underrepresented classes are crucial. algorithms may become biased toward the majority class, leading to misclassification and poor performance for minority classes.
an example of this is in the detection of exoplanets based on flux values from distant star systems. most stars do not have planets, meaning the dataset is heavily skewed, with 99.3% representing stars without planets and only 0.7% representing stars with planets. in this case, the algorithm may focus on the majority class (stars without planets) and miss important patterns in the minority class.
to address this, data needs to be balanced or projected into a lower-dimensional space for better visualization and understanding. algorithms require different performance metrics (e.g., precision, recall) to detect the presence of data imbalance and ensure that the model learns effectively from both the majority and minority classes. neglecting this can lead to biased models that perform poorly on critical underrepresented classes.

in cases of data imbalance, particularly when the disparity between majority and minority classes is not extreme, dropping a small number of values from the majority class can be acceptable without significantly affecting the sample's representativeness. however, if the difference is significant, dropping too many values from the majority class may lead to a sample that is no longer representative of the population.
one approach to address data imbalance is oversampling the minority class by duplicating its data points. however, this method does not introduce new information; it merely replicates existing data, which can water down the dataset without solving the core issue of imbalance.
a more effective method is using synthetic minority over-sampling technique (smote). instead of duplicating data points, smote generates synthetic samples by selecting a minority class data point and creating new points as linear interpolations between it and its nearest neighbors. this method enhances the representation of the minority class while maintaining diversity in the dataset. by creating synthetic samples, smote helps improve the balance without simply duplicating existing data, providing a better way to handle data imbalance in machine learning tasks.
some other data balancing are adasyn. tomek links are used to to undersample data. majority class with their nearest neighbor being a minority class sample are removed. smote and tomek links are done one after other to get nice data.","during exploratory data analysis we get insights on type of data and understand problems present in the data. we can remove outliers based on trend followed by data: if all data follows a particular trend and some doesn't follow we can remove this data. we can use data smoothening to reduce noise in data. real data has a lot of fluctuations. this can make it difficult to find trends ir pattern in such data. simple moving average sma consider a window around every data points and average the values. window width can be varied to adjust the level of smoothening. based on application we can take window width. we can take moving average at a point as average of all past points or average of some past and some future points. there will be problems at the end of data as there won't be any points to the past of starting of data. so we have to define accordingly at end points. the moving average reduce the noise and maintains the characteristics data. another way is exponential moving average that weight nearby samples more while averaging. this is used in time series forecasting. in data analysis, it is crucial to handle missing values and outliers before proceeding with further steps like calculating moving averages. ignoring these aspects can lead to distorted results, such as sudden changes or incorrect trend patterns. handling missing values can be done by either ignoring, discarding, or using methods like regression to fill in the gaps. outliers should be removed or adjusted to prevent them from skewing the analysis. after addressing these issues, the data is ready for further analysis, where both visual and mathematical methods are used to ensure accuracy. normalization transforms data to a 0-1 range. in this case, normalization had little effect on the target values and coefficients, with minimal changes except for one variable (x2). this shows that scaling didn't significantly impact the model. for the next messy dataset, a clustering algorithm is suggested. in the process of data analysis, independent normalization of variables (x and y) can impact the results, especially when using algorithms like k-means clustering, which rely on euclidean distance. when the data is normalized, it alters the shape and relationships between variables, leading to more refined results. algorithms sensitive to scaling, such as k-means, are particularly affected, and normalization helps produce more accurate clusters by balancing the influence of different features. a standard normal distribution has a mean of 0 and variance of 1. to standardize data, the formula is used, where is an observation, is the mean, and is the standard deviation. standardization does not change the shape of the data's distribution, meaning the transformed data retains the original distribution's shape. this method is often used in statistical tests, especially hypothesis testing, where normal distribution is required. data transformations like box-cox are applied to achieve normality before performing such tests. descriptive statistics of the transformations should be reviewed to understand their impact. the box-cox transformation is a technique used to make data more normally distributed. it involves raising each observation to a power (lambda), with the optimal lambda value determined through a process called maximum likelihood estimation. in this case, the lambda value is 0.17, which is chosen to best transform the data into a normal distribution. this method is useful when algorithms assume normality in the data, ensuring that the data fits those assumptions. the box-cox transformation and similar techniques, like square root or cube root transformations, pull data points closer together, particularly those that are far from the mean. this helps in dealing with skewed distributions. for a lambda value close to 0 in box-cox, the transformation approximates ; if lambda is 0, it uses the log transformation. the result is a more normally distributed dataset, which is essential for certain algorithms. when applying transformations like box-cox or logarithmic transformations to features (e.g., x1), these same transformations must be applied consistently to future or test data to ensure accurate predictions. additionally, sometimes transformations must be reversed to interpret results in their original context. various scaling methods, such as standardization and normalization, are commonly used to handle such transformations. the condition where data variance changes across different levels of the data is known as heteroscedasticity. the process of scaling data, such as using log transformations, reduces the emphasis on large values and minimizes the impact of variations in data. this is crucial because many algorithms, especially those that rely on euclidean distance or hierarchical clustering, are sensitive to data scaling. various scaling methods are used to bring the data to a common scale while preserving the variation within the dataset. however, in some cases, transformations like the box-cox transformation are applied to also change the shape of the data for better analysis. when preparing data for analysis, steps include identifying and fixing missing values, exploring correlations between features, and creating visual representations like scatter plots or matrix plots. scaling transformations are often applied before conducting these analyses to ensure accurate and consistent results. the issue of data imbalance arises when certain classes are underrepresented in a dataset compared to other classes. this is particularly problematic in classification tasks. in the example given, there are four classes: red, green, blue, and white, where the white class is underrepresented and often merged with the blue class. when subjecting these classes to classification, boundaries are created by the algorithm based on the dominant class, leading to misclassification of the underrepresented class. as a result, false negatives occur, where observations belonging to the underrepresented class are wrongly classified as another class. this is evident in confusion matrices and the performance metrics like precision and recall, where the underrepresented class shows poor classification. the algorithm focuses on correctly classifying the majority class, but the minority class suffers from poor performance. data imbalance occurs when one class significantly dominates over others in a dataset, causing challenges for learning algorithms. this imbalance can negatively affect the algorithm s ability to learn, particularly in cases like medical diagnosis (e.g., diabetes detection) or fraud detection, where underrepresented classes are crucial. algorithms may become biased toward the majority class, leading to misclassification and poor performance for minority classes. an example of this is in the detection of exoplanets based on flux values from distant star systems. most stars do not have planets, meaning the dataset is heavily skewed, with 99.3% representing stars without planets and only 0.7% representing stars with planets. in this case, the algorithm may focus on the majority class (stars without planets) and miss important patterns in the minority class. to address this, data needs to be balanced or projected into a lower-dimensional space for better visualization and understanding. algorithms require different performance metrics (e.g., precision, recall) to detect the presence of data imbalance and ensure that the model learns effectively from both the majority and minority classes. neglecting this can lead to biased models that perform poorly on critical underrepresented classes. in cases of data imbalance, particularly when the disparity between majority and minority classes is not extreme, dropping a small number of values from the majority class can be acceptable without significantly affecting the sample's representativeness. however, if the difference is significant, dropping too many values from the majority class may lead to a sample that is no longer representative of the population. one approach to address data imbalance is oversampling the minority class by duplicating its data points. however, this method does not introduce new information; it merely replicates existing data, which can water down the dataset without solving the core issue of imbalance. a more effective method is using synthetic minority over-sampling technique (smote). instead of duplicating data points, smote generates synthetic samples by selecting a minority class data point and creating new points as linear interpolations between it and its nearest neighbors. this method enhances the representation of the minority class while maintaining diversity in the dataset. by creating synthetic samples, smote helps improve the balance without simply duplicating existing data, providing a better way to handle data imbalance in machine learning tasks. some other data balancing are adasyn. tomek links are used to to undersample data. majority class with their nearest neighbor being a minority class sample are removed. smote and tomek links are done one after other to get nice data.",9,-20.056002,11.828432,10.126951,10.057804,"dataâ, analyse, analyses"
29,"we began by using pivot tables in excel to combine big data grouping and summing up values to speedily calculate the important metrics such as totals, averages and counts. we followed this with exploratory data analysis (eda), where we employed summary statistics like mean median, variance and standard deviation together with visualization tools like histograms box plot '' and scatter plots to derive insights about data distribution finally we briefly discussed issues such as class imbalance and presented feature engineering and data transformation methods that are important to improve data quality and ready data for deeper examination.","we began by using pivot tables in excel to combine big data grouping and summing up values to speedily calculate the important metrics such as totals, averages and counts. we followed this with exploratory data analysis (eda), where we employed summary statistics like mean median, variance and standard deviation together with visualization tools like histograms box plot '' and scatter plots to derive insights about data distribution finally we briefly discussed issues such as class imbalance and presented feature engineering and data transformation methods that are important to improve data quality and ready data for deeper examination.",6,-13.981413,26.27942,7.248898,9.960773,"summarizing, summarize, summarization"
30,"we started with the talk for trying to evaluate the various population parameters like mean, variance, etc. sir emphasized on the central limit theorem oven a counter example of estimating the extra work hours 1of managers of a particular company where we considered a small single sample of 18 observations which was assumed to represent the population futher we plotted the pdf for the sample mean and later learnt about confidence intervals and the use of t distribution when the observation count is less than 30. further sir explained the practical significance of confidence intervals. moving on sir discussed other statical parameters like t value, z, etc and how to calculate them. also the term ""statistically different"" was introduced. the class ended with discussion on p value and sir briefly introduced multiple linear regression","we started with the talk for trying to evaluate the various population parameters like mean, variance, etc. sir emphasized on the central limit theorem oven a counter example of estimating the extra work hours 1of managers of a particular company where we considered a small single sample of 18 observations which was assumed to represent the population futher we plotted the pdf for the sample mean and later learnt about confidence intervals and the use of t distribution when the observation count is less than 30. further sir explained the practical significance of confidence intervals. moving on sir discussed other statical parameters like t value, z, etc and how to calculate them. also the term ""statistically different"" was introduced. the class ended with discussion on p value and sir briefly introduced multiple linear regression",7,32.243576,6.968428,14.624137,2.4731166,"statistics, statistical, statisticsâ"
31,"today we discussed statistical significance before moving on to python, where tutorials were assigned to prepare for our next class.

in this session, we learned how data science algorithms judge session summaries with a relative strength method where distances between submissions were calculated using scatter plot coefficients.

later, we studied multiple linear regression, where the dependent variable is influenced by more than one independent variable. we learned about f-value, which measures model effectiveness by measuring explained versus unexplained varianceâ€”a higher f-value indicative of a better model. using a dataset, we calculated error metrics and compared f-values to practically assess model performance.","today we discussed statistical significance before moving on to python, where tutorials were assigned to prepare for our next class. in this session, we learned how data science algorithms judge session summaries with a relative strength method where distances between submissions were calculated using scatter plot coefficients. later, we studied multiple linear regression, where the dependent variable is influenced by more than one independent variable. we learned about f-value, which measures model effectiveness by measuring explained versus unexplained variance a higher f-value indicative of a better model. using a dataset, we calculated error metrics and compared f-values to practically assess model performance.",13,-7.264204,-4.7669606,9.452415,5.300426,"classification, classifying, classifications"
32,"we discussed the idea that using one model is often better than combining two models, as combining them might lead to discontinuities at the boundary. then, we moved on to logistic regression, introducing the concept of a logistic unit, which has two possible states: 0 and 1. we considered a function in the form of a linear combination of input variables: w1x1 + w2x2 + w3x3 + b. since the output needs to be either 0 or 1, we introduced the sigmoid function, which converts this linear function into a probability value.

the probability function p(y|x) was then defined as sigma(w^t x + b). if a point belongs to class 1, the predicted value should be close to 1; otherwise, it should be close to 0. the goal was to maximize the likelihood of the predicted outcome matching the actual target. when t = 1, we maximize p, and when t = 0, we maximize (1 - p). this led to the definition of the likelihood function as a product over all data points: p^t (1 - p)^(1 - t). to make optimization easier, we took the log of this function, leading to the log-likelihood expression. since finding a minimum is often more stable than finding a maximum, we defined j as -log l and minimized it using gradient descent. the weight parameters w1, w2, ..., wn were updated iteratively with a learning rate (eta).

we also discussed that the decision boundary does not necessarily have to be linear. then, we introduced the confusion matrix, which contains true positives, false positives, true negatives, and false negatives. the accuracy metric was defined as (tp + tn) / total. however, it was noted that accuracy is not always a good measure, especially when data is imbalanced. if one class has significantly more data points than the other, accuracy might still appear high despite poor performance on the minority class. to address this, we introduced precision and recall as alternative evaluation metrics.

towards the end of the session, the tas discussed assignment 1, pointing out common mistakes and providing suggestions for improvement.","we discussed the idea that using one model is often better than combining two models, as combining them might lead to discontinuities at the boundary. then, we moved on to logistic regression, introducing the concept of a logistic unit, which has two possible states: 0 and 1. we considered a function in the form of a linear combination of input variables: w1x1 + w2x2 + w3x3 + b. since the output needs to be either 0 or 1, we introduced the sigmoid function, which converts this linear function into a probability value. the probability function p(y|x) was then defined as sigma(w^t x + b). if a point belongs to class 1, the predicted value should be close to 1; otherwise, it should be close to 0. the goal was to maximize the likelihood of the predicted outcome matching the actual target. when t = 1, we maximize p, and when t = 0, we maximize (1 - p). this led to the definition of the likelihood function as a product over all data points: p^t (1 - p)^(1 - t). to make optimization easier, we took the log of this function, leading to the log-likelihood expression. since finding a minimum is often more stable than finding a maximum, we defined j as -log l and minimized it using gradient descent. the weight parameters w1, w2, ..., wn were updated iteratively with a learning rate (eta). we also discussed that the decision boundary does not necessarily have to be linear. then, we introduced the confusion matrix, which contains true positives, false positives, true negatives, and false negatives. the accuracy metric was defined as (tp + tn) / total. however, it was noted that accuracy is not always a good measure, especially when data is imbalanced. if one class has significantly more data points than the other, accuracy might still appear high despite poor performance on the minority class. to address this, we introduced precision and recall as alternative evaluation metrics. towards the end of the session, the tas discussed assignment 1, pointing out common mistakes and providing suggestions for improvement.",10,10.626933,-18.3637,8.980488,-1.6369902,"classifications, histograms, histogram"
33,"in today's session proff discussed about mid sem exam and how to approach the problem. he started with describing the problem and how some of the datas were missing and inconsistent, then he started solving the question by doing eda, then how to deal with outliers and by analysis we concluded that there is no need of clustering as there were many parameters and then made the correlation matrix using which only 6 of the parameters were truly independent. also for the second part  the model made in first part was not applicable to second one as they were for different population.
then ta told about e3 assignment and in last 15 minutes sir briefly discussed about curse of dimensionality due to which there was increase in sparsity, complexity.
and at last we discussed about vif vs r square graph","in today's session proff discussed about mid sem exam and how to approach the problem. he started with describing the problem and how some of the datas were missing and inconsistent, then he started solving the question by doing eda, then how to deal with outliers and by analysis we concluded that there is no need of clustering as there were many parameters and then made the correlation matrix using which only 6 of the parameters were truly independent. also for the second part the model made in first part was not applicable to second one as they were for different population. then ta told about e3 assignment and in last 15 minutes sir briefly discussed about curse of dimensionality due to which there was increase in sparsity, complexity. and at last we discussed about vif vs r square graph",13,-4.0946465,13.01844,9.303268,8.035608,"classification, classifying, classifications"
34,"in the equation ""y=f(x)"" , before ml f was deduced by manually by plotting data points and curve fitting. ml uses various algorithms such as slr, mlr, logistic regression , random forest , k-means clustering and hierarchical clustering out of which the last two come under unsupervised learning methods while the prior 4 are supervised learning techniques. topics of statistics were discussed upon . the different levels of measurement which are :-
i)nominal_(discrete): such as gender , color classifications into different categories.
ii)ordinal_(discrete): such as grades , unlike nominal these can be ordered.(classification)
iii)interval_(continuous): such as temperature, pressure references are arbitrary.(regression)
iv)ratio_(continuous): height, weight , salary zero is fixed.(regression)
in the equation ""y=f(x)"", y is known label and x is known as features .
when both of them are available supervised learning methods are used .
when we don't have labels unsupervised learning methods are used.
population and sample(small but representative part of the population) were discussed upon too.","in the equation ""y=f(x)"" , before ml f was deduced by manually by plotting data points and curve fitting. ml uses various algorithms such as slr, mlr, logistic regression , random forest , k-means clustering and hierarchical clustering out of which the last two come under unsupervised learning methods while the prior 4 are supervised learning techniques. topics of statistics were discussed upon . the different levels of measurement which are :- i)nominal_(discrete): such as gender , color classifications into different categories. ii)ordinal_(discrete): such as grades , unlike nominal these can be ordered.(classification) iii)interval_(continuous): such as temperature, pressure references are arbitrary.(regression) iv)ratio_(continuous): height, weight , salary zero is fixed.(regression) in the equation ""y=f(x)"", y is known label and x is known as features . when both of them are available supervised learning methods are used . when we don't have labels unsupervised learning methods are used. population and sample(small but representative part of the population) were discussed upon too.",4,-23.828905,-15.557701,1.7247372,0.39881343,"classification, classifying, classifications"
35,"we talked about predicting things using several factors. this is called multiple linear regression.  we use different inputs (like x1, x2, etc.) to predict an outcome (y).  the basic idea is:

y = a0 + a1x1 + a2x2 + â€¦ + anxn

we also use the f-value to see how accurate our prediction is. it compares how much of the variation we explain to how much we don't. a higher f-value usually means a better model which predicts.

we looked at some data and calculated different errors and statistics. we learned that there's no single ""good"" f-value. it's more sensible to compare f-values between different predictionsâ€”the higher one again is the  better one.","we talked about predicting things using several factors. this is called multiple linear regression. we use different inputs (like x1, x2, etc.) to predict an outcome (y). the basic idea is: y = a0 + a1x1 + a2x2 + + anxn we also use the f-value to see how accurate our prediction is. it compares how much of the variation we explain to how much we don't. a higher f-value usually means a better model which predicts. we looked at some data and calculated different errors and statistics. we learned that there's no single ""good"" f-value. it's more sensible to compare f-values between different predictions the higher one again is the better one.",10,31.513697,-14.123579,16.134838,4.167644,"classifications, histograms, histogram"
36,"sample should be good and representative
statistic: attribute calculated for sample
parameter: attribute estimated for population

simple linear regression:
only one predictor variable (x)
assume linear model y = (a * x) + b
where b is the inherent bias; accommodates the effect of all the unknown variables

how to assess if the values of a and b are good (the model is a good fit or not) ?
prediction using model = yhat
error = y - yhat
size of dataset : n (n samples indexed i = 1 to n)
sum of squared error = sum_{i = 1 to n} [(y - yhat) ^ 2]
closed form solution is known but we have zero confidence in how well they estimate the true values of a and b for the population.

confidence increases with the size of the interval where the true values of a and b are most likely to be.",sample should be good and representative statistic: attribute calculated for sample parameter: attribute estimated for population simple linear regression: only one predictor variable (x) assume linear model y = (a * x) + b where b is the inherent bias; accommodates the effect of all the unknown variables how to assess if the values of a and b are good (the model is a good fit or not) ? prediction using model = yhat error = y - yhat size of dataset : n (n samples indexed i = 1 to n) sum of squared error = sum_{i = 1 to n} [(y - yhat) ^ 2] closed form solution is known but we have zero confidence in how well they estimate the true values of a and b for the population. confidence increases with the size of the interval where the true values of a and b are most likely to be.,1,33.730663,-11.092476,16.579088,4.1633444,"population, models, estimating"
37,"i learned about four levels of measurement of statistics: nominal,ordinal,interval and ratio. i understood about supervised and unsupervised learning. the difference between population and sample is clear after the session. also the data we have captured till now is nothing compared to population. to handle such large data, we need more computational power. we should also look upon whether our model is good or not. testing is really important. i also got to know about one hot encoding (ohe) which is used because we can't just assign a number to nominal data as it by default assumes an ordering. we use vectors instead of numbers to tackle this problem which is also known as ohe. also got to know fundamental difference between interval and ratio level. also just basic names of supervised and unsupervised learning methods.","i learned about four levels of measurement of statistics: nominal,ordinal,interval and ratio. i understood about supervised and unsupervised learning. the difference between population and sample is clear after the session. also the data we have captured till now is nothing compared to population. to handle such large data, we need more computational power. we should also look upon whether our model is good or not. testing is really important. i also got to know about one hot encoding (ohe) which is used because we can't just assign a number to nominal data as it by default assumes an ordering. we use vectors instead of numbers to tackle this problem which is also known as ohe. also got to know fundamental difference between interval and ratio level. also just basic names of supervised and unsupervised learning methods.",4,-23.707464,-10.909537,1.7493719,0.50976795,"classification, classifying, classifications"
38,"from today's class i learned about the difference between sample and population data and how they are related and how we use various sample dataset to estimate the information about population and to estimate with some level of confidence we introduce the concept of confidence intervals. then we moved on to simple linear regression where i understand the least squares error and based on that we minimise it with respect to weights to get the best fit line. but there can be different best fit line depending upon sample dala and the mean of sample will always lie on the best fit line with least squares error . then we derive the values of a,b(weights) in case of simple linear regression but if there are multiple features then the calculation of wegaireh by directly doing differentiation and minimising it can be very difficult so we will look for some kind of algorithms in next class.","from today's class i learned about the difference between sample and population data and how they are related and how we use various sample dataset to estimate the information about population and to estimate with some level of confidence we introduce the concept of confidence intervals. then we moved on to simple linear regression where i understand the least squares error and based on that we minimise it with respect to weights to get the best fit line. but there can be different best fit line depending upon sample dala and the mean of sample will always lie on the best fit line with least squares error . then we derive the values of a,b(weights) in case of simple linear regression but if there are multiple features then the calculation of wegaireh by directly doing differentiation and minimising it can be very difficult so we will look for some kind of algorithms in next class.",1,27.729347,-9.693633,15.642548,4.104505,"population, models, estimating"
39,"sir said 3 ways are available or improving result quality. 
(1)	sample improvement
(2)	mehod improvement
(3)	for fine tuning
now sir started talking about mlr with many features,i.e., mlr for non linear cases.
adjustedrsquare decreases as more terms are included. p values makes  only the significant term to remain at the end.
(polynomial regression) we took help of feature engineering for features in form of polynomials or trigonometric functons.  
sir said about 
(1) backward eng
(2) forward eng
(3) overfit issues
we then talked about trigonometry and straight line. a single model model which is the combination of the two is required.
sir then talked about random forest, parametric, non parametric and dela analysis.
sir then asked us if we knew about neural networks and only few people know about it.
then he said that chatgpt is also a neural network with billions of features and requires a lot of data space.","sir said 3 ways are available or improving result quality. (1) sample improvement (2) mehod improvement (3) for fine tuning now sir started talking about mlr with many features,i.e., mlr for non linear cases. adjustedrsquare decreases as more terms are included. p values makes only the significant term to remain at the end. (polynomial regression) we took help of feature engineering for features in form of polynomials or trigonometric functons. sir said about (1) backward eng (2) forward eng (3) overfit issues we then talked about trigonometry and straight line. a single model model which is the combination of the two is required. sir then talked about random forest, parametric, non parametric and dela analysis. sir then asked us if we knew about neural networks and only few people know about it. then he said that chatgpt is also a neural network with billions of features and requires a lot of data space.",0,-1.1724225,-0.62328106,9.131987,4.247289,"models, feature, features"
40,"to improve the likelihood of accurate predictions, logistic regression is used to generate weighted values. we maximize p if t=1 and if t=0 we maximize 1-p. because working with product terms can be complex, it is necessary to utilize a logarithm to simplify computations by converting them into summations. we evaluate model performance using a confusion matrix consisting of true negatives, false positives, false negatives, and true positives. important measures include accuracy , precision and recall (the model's capacity to identify all actual positive examples).the f1 score is a harmonic mean of precision and recall, a balanced statistic that doesnt burden off accuracy alone.","to improve the likelihood of accurate predictions, logistic regression is used to generate weighted values. we maximize p if t=1 and if t=0 we maximize 1-p. because working with product terms can be complex, it is necessary to utilize a logarithm to simplify computations by converting them into summations. we evaluate model performance using a confusion matrix consisting of true negatives, false positives, false negatives, and true positives. important measures include accuracy , precision and recall (the model's capacity to identify all actual positive examples).the f1 score is a harmonic mean of precision and recall, a balanced statistic that doesnt burden off accuracy alone.",10,14.460126,-22.242615,8.634509,-1.6777564,"classifications, histograms, histogram"
41,"logistic regression and clustering.
we want to maximize the likelihood of our predicted outcomes being close to the targets. 
maximizing likelihood is same as minimizing the error function.
we create a confusion matrix:-true negative,false positive,false negative,true positive.
terms we defined :
precision:of the events we have detected how many have we detected correctly 
f1 value-harmonic mean of precision and accuracy","logistic regression and clustering. we want to maximize the likelihood of our predicted outcomes being close to the targets. maximizing likelihood is same as minimizing the error function. we create a confusion matrix:-true negative,false positive,false negative,true positive. terms we defined : precision:of the events we have detected how many have we detected correctly f1 value-harmonic mean of precision and accuracy",10,15.49677,-20.407564,8.702082,-1.6988392,"classifications, histograms, histogram"
42,"we are trying to fit y = f(x)
y is dependent variable
x is independent variable

in simple linear regression, y = bâ° + bâ¹x
bâ° and bâ¹ are estimates of population parameters 
as sample size increase, prediction error will decrease, the statistics will get better 

ei = (yi - yi')
ei is point error
total error is accumulation of point errors
if e = sum(ei), positive and negative will cancel, hence can not be used. if e = sum(|ei|), sphere of influence is diamond shaped. if e = sum(eiâ²), sphere of influence is circular. 

for simple linear regression, we have closed form solution.","we are trying to fit y = f(x) y is dependent variable x is independent variable in simple linear regression, y = b + b x b and b are estimates of population parameters as sample size increase, prediction error will decrease, the statistics will get better ei = (yi - yi') ei is point error total error is accumulation of point errors if e = sum(ei), positive and negative will cancel, hence can not be used. if e = sum(|ei|), sphere of influence is diamond shaped. if e = sum(ei ), sphere of influence is circular. for simple linear regression, we have closed form solution.",1,34.101894,-12.262303,16.56191,4.222578,"population, models, estimating"
43,"we began by discussing the quality of the sample and stressing the importance of its representativeness to the population for reliable analysis. a distinction was made between statistics (which are calculated on samples) and parameters (which are estimated on populations). key measures related to the mean, such as count, mode, standard deviation, median, were introduced. next, we turn to the concept of simple linear regression (slr), discussing how it establishes relationships between variables. an interesting point is made regarding the lack of fit of individual data points, as class representativeness summarizes the diversity of a class. the concept of bias in slr is introduced, emphasizing how it reflects the influence of other factors on the output variable y.","we began by discussing the quality of the sample and stressing the importance of its representativeness to the population for reliable analysis. a distinction was made between statistics (which are calculated on samples) and parameters (which are estimated on populations). key measures related to the mean, such as count, mode, standard deviation, median, were introduced. next, we turn to the concept of simple linear regression (slr), discussing how it establishes relationships between variables. an interesting point is made regarding the lack of fit of individual data points, as class representativeness summarizes the diversity of a class. the concept of bias in slr is introduced, emphasizing how it reflects the influence of other factors on the output variable y.",7,30.888954,4.941912,15.191769,3.171384,"statistics, statistical, statisticsâ"
44,"1. from the given dataset, we visualised the given dataset in the form of scatterplot and fitted a regression line by calculating constants (slopes and intercept) for the regression line from the data itself in excel
2. next we plotted the errors and visualised the errors in the form of histograms and learnt that for the errors to be random, the errors must be distributed so that their pattern cant be predicted
3. we then discussed that if the errors arent randomly distributed, the model isnt that good and has more scopes of improvement
4. then we discussed the population and sample distribution, the more the representative of population the samplr is, the more its mean is expected to be near the population mean
5. we then utilised data analysis toolpack, and performed linear regression on the original dataset and introduced ourselves to various metric such as confidence interval, r-square (r2), standard error etc
6. the variance of data (y or dependent variable) gives rise to the sse and ssr, the and the equation r2 + sse/sst = 1, implies that r2 which is the square of corelation cefficient, the closer it is to one for a simple linear regression model, the good the model is
7. we also then discussed anout the relation beween standard deviation and variance, the relation between standard deviation of population to the sample that is sigma/sqrt(n).","1. from the given dataset, we visualised the given dataset in the form of scatterplot and fitted a regression line by calculating constants (slopes and intercept) for the regression line from the data itself in excel 2. next we plotted the errors and visualised the errors in the form of histograms and learnt that for the errors to be random, the errors must be distributed so that their pattern cant be predicted 3. we then discussed that if the errors arent randomly distributed, the model isnt that good and has more scopes of improvement 4. then we discussed the population and sample distribution, the more the representative of population the samplr is, the more its mean is expected to be near the population mean 5. we then utilised data analysis toolpack, and performed linear regression on the original dataset and introduced ourselves to various metric such as confidence interval, r-square (r2), standard error etc 6. the variance of data (y or dependent variable) gives rise to the sse and ssr, the and the equation r2 + sse/sst = 1, implies that r2 which is the square of corelation cefficient, the closer it is to one for a simple linear regression model, the good the model is 7. we also then discussed anout the relation beween standard deviation and variance, the relation between standard deviation of population to the sample that is sigma/sqrt(n).",5,22.552418,-2.3648593,14.216163,4.3319354,"regression, statistical, statistics"
45,"in today's class, we started with the discussion on metrics which determines the quality of logistics regression or classification. the score or accuracy is not a good metric in some cases, instead f1-score is better than accuracy. then we discussed the roc graph (receiver operating characteristic curve), plot between tp rate and fp rate. auc(area under the curve) of this plot tells about the quality of classification. if auc is near 1, then classification is good; whereas when auc is near 0.5, there is no or worst classification. after that, we discussed how to calculate precision and recall for a given classification problem from it's confusion matrix. in ml, a regression problem can be solved with classification, but the converse need not to be true.
clustering is usually done on unsupervised data. we discussed two types of clustering algorithms 1. k-means clustering and 2. hierarchical clustering. sir also explained the method by which both algorithms work. in k-means, we have to predecide the number of clusters whereas hierarchical does not need to predefine number of clusters. hierarchical clustering uses tree like structure known as dendrogram. we also discussed linkages related to hierarchical clustering.","in today's class, we started with the discussion on metrics which determines the quality of logistics regression or classification. the score or accuracy is not a good metric in some cases, instead f1-score is better than accuracy. then we discussed the roc graph (receiver operating characteristic curve), plot between tp rate and fp rate. auc(area under the curve) of this plot tells about the quality of classification. if auc is near 1, then classification is good; whereas when auc is near 0.5, there is no or worst classification. after that, we discussed how to calculate precision and recall for a given classification problem from it's confusion matrix. in ml, a regression problem can be solved with classification, but the converse need not to be true. clustering is usually done on unsupervised data. we discussed two types of clustering algorithms 1. k-means clustering and 2. hierarchical clustering. sir also explained the method by which both algorithms work. in k-means, we have to predecide the number of clusters whereas hierarchical does not need to predefine number of clusters. hierarchical clustering uses tree like structure known as dendrogram. we also discussed linkages related to hierarchical clustering.",8,-3.1018946,-19.403727,6.476872,0.40232348,"classification, clusterings, classifying"
46,"so today we started off with the basics of data science, where we discussed that y = f(x) is basically the crux of data science, where we try to find different unknowns based on the data given to us. y is known as labels, while x is known as features (plural as it can be represented as a vector i.e. a collection of multiple entities). we discussed that the main motive of ml is to fit the most appropriate curve which can depict the given observational data, and unlike human calculations, which will lead us to some mathematical equations, the output of ml algorithms is an empirical equation. we talked about 4 algorithms i.e. simple linear regression, multiple linear regression, random forest and logistic regression. 
then we moved on to discuss the levels of measurement. there are 4 levels:
i) nominal - no ordering, only classification is possible. differentiation based on some characteristics, discrete. eg. gender, colour
ii) ordinal - inherent order between different groups. discrete. eg. grades
iii) interval - only the difference between entries matters, reference is arbitrary. continuous. eg. temperature
iv) ratio - reference is well defined. continuous. eg. height, weight

we also discussed the appropriate way of encoding data for using the algorithms. we should not use numbers directly as that may change our level of measurement from nominal to ordinal. instead we should use as many variables as there are categories, and use one hot encoding to create a vector, where each bit represents a particular character. 
we also suggested that nominal and ordinal level data can be classified into different groups, whereas interval and ratio level data can use regression to find relations between different observations. 
finally, we started with unsupervised learning, which is the branch of ml which deals with data where we do not have labels y, and we basically need to form clusters or groups based on some characteristic of the data points. there are 2 main classification algorithms, k - means clustering and hierarchial clustering.","so today we started off with the basics of data science, where we discussed that y = f(x) is basically the crux of data science, where we try to find different unknowns based on the data given to us. y is known as labels, while x is known as features (plural as it can be represented as a vector i.e. a collection of multiple entities). we discussed that the main motive of ml is to fit the most appropriate curve which can depict the given observational data, and unlike human calculations, which will lead us to some mathematical equations, the output of ml algorithms is an empirical equation. we talked about 4 algorithms i.e. simple linear regression, multiple linear regression, random forest and logistic regression. then we moved on to discuss the levels of measurement. there are 4 levels: i) nominal - no ordering, only classification is possible. differentiation based on some characteristics, discrete. eg. gender, colour ii) ordinal - inherent order between different groups. discrete. eg. grades iii) interval - only the difference between entries matters, reference is arbitrary. continuous. eg. temperature iv) ratio - reference is well defined. continuous. eg. height, weight we also discussed the appropriate way of encoding data for using the algorithms. we should not use numbers directly as that may change our level of measurement from nominal to ordinal. instead we should use as many variables as there are categories, and use one hot encoding to create a vector, where each bit represents a particular character. we also suggested that nominal and ordinal level data can be classified into different groups, whereas interval and ratio level data can use regression to find relations between different observations. finally, we started with unsupervised learning, which is the branch of ml which deals with data where we do not have labels y, and we basically need to form clusters or groups based on some characteristic of the data points. there are 2 main classification algorithms, k - means clustering and hierarchial clustering.",4,-24.491327,-15.02583,1.6808367,0.40939608,"classification, classifying, classifications"
47,"in today's session we worked on a given data in excel and plotted a scatter plot for respective x and y values, establishing the relationship between the two variables for equation y = a + bx. and using linear regression formulas calculated the parameters and plotted regression line. then we explored many regression metrics such as sse( sum of squares errors), mse( mean squared error ), rmse ( root mean squared error), and mae ( mean absolute error) to assess how the linear regression model is performing. these metrics helped to know the accuracy of the model built by comparing the actual and predicted values. also we learnt that the coefficients which aare. derived from sample data are the estimates population parameters.

we also understood the concept of sampling distributions wherein multiple representative samples drawn from the population and then we calculated the sample mean from the data and notice that these means 10 to common normal distribution as per central limit theorem regardless of the original population distribution provided that the sample size is sufficiently large. using this principle we can calculate the confidence intervals for the model parameters.

also we learnt that with larger sample sizes standard error of sampling distribution decreases and uncertainty also decreases which leads to improving the precision of estimates. you also plotted the histogram for the errors and do the best fit line using linear regression.","in today's session we worked on a given data in excel and plotted a scatter plot for respective x and y values, establishing the relationship between the two variables for equation y = a + bx. and using linear regression formulas calculated the parameters and plotted regression line. then we explored many regression metrics such as sse( sum of squares errors), mse( mean squared error ), rmse ( root mean squared error), and mae ( mean absolute error) to assess how the linear regression model is performing. these metrics helped to know the accuracy of the model built by comparing the actual and predicted values. also we learnt that the coefficients which aare. derived from sample data are the estimates population parameters. we also understood the concept of sampling distributions wherein multiple representative samples drawn from the population and then we calculated the sample mean from the data and notice that these means 10 to common normal distribution as per central limit theorem regardless of the original population distribution provided that the sample size is sufficiently large. using this principle we can calculate the confidence intervals for the model parameters. also we learnt that with larger sample sizes standard error of sampling distribution decreases and uncertainty also decreases which leads to improving the precision of estimates. you also plotted the histogram for the errors and do the best fit line using linear regression.",5,22.309164,-3.0488548,14.569518,4.5793,"regression, statistical, statistics"
48,"fri 31/01

train vs test (unseen) data split for any ml model.


loss of degree of freedom in r-squared. when one knows the mean (x bar) the number of unkown observations in the sample drops by 1. however, larger the population the smaller the relative change in degree of freedoms. h.w. check if dropping a variable in x vector brings up adjusted r-squared. 

linear regression need not output a line as the relatipn between y and x vector. 

sklearn or scikitlearn has many builtin functions including linearregression class. a quantile plot was presented in class.

the notion of better models comes from comes from a few metrics. aic & bic are two such metrics - lower the better.

different tests for close to normal distribution. skewness (0), kurtosis (3), jarque-bera test (0.05-2.00), omnibus, durbin-watson (2) test among others.","fri 31/01 train vs test (unseen) data split for any ml model. loss of degree of freedom in r-squared. when one knows the mean (x bar) the number of unkown observations in the sample drops by 1. however, larger the population the smaller the relative change in degree of freedoms. h.w. check if dropping a variable in x vector brings up adjusted r-squared. linear regression need not output a line as the relatipn between y and x vector. sklearn or scikitlearn has many builtin functions including linearregression class. a quantile plot was presented in class. the notion of better models comes from comes from a few metrics. aic & bic are two such metrics - lower the better. different tests for close to normal distribution. skewness (0), kurtosis (3), jarque-bera test (0.05-2.00), omnibus, durbin-watson (2) test among others.",2,9.73196,1.1020532,11.009205,4.643208,"regression, regressions, features"
49,"the following were discussed in today's class
1. a short discussion about a website where one can play with multiple aspects of neural networks by interacting with it directly.
2. most of the discussion revolved around this section where various classification model performance model analysis tools/numbers/metrics were discussed. some of them are
	a) confusion matrix: overall glimpse of the model performance, showing a matrix of predictions vs outcomes
	b) precision: this metric is defined for every class. precision of a class shows how precise is the model when in classifying data as this class(tp/tp+fp)
	c) recall: this metric is defined for every class. this shows how well the model is able to recall this particular class(tp/tp+fn)
	d) f1-score: harmonic mean of precision and recall. signifies how good the model is. accuracy metric is not a very good metric for model performance analysis as it doesn't capture 	misclassification of under-represented class: example of fraudulent transactions which have very less representation(npci example)
	e) roc(receiver operating characteristic) curve: explains the trade off of increasing tpr with fpr of the given model. auc(area under the curve) signifies how much one has to trade 	off tpr to decrease fpr.
3. clustering problem(unsupervised learning problem). example of customer segmentation was discussed. then went on to clustering algorithms like k-means, hierarchical clustering etc.","the following were discussed in today's class 1. a short discussion about a website where one can play with multiple aspects of neural networks by interacting with it directly. 2. most of the discussion revolved around this section where various classification model performance model analysis tools/numbers/metrics were discussed. some of them are a) confusion matrix: overall glimpse of the model performance, showing a matrix of predictions vs outcomes b) precision: this metric is defined for every class. precision of a class shows how precise is the model when in classifying data as this class(tp/tp+fp) c) recall: this metric is defined for every class. this shows how well the model is able to recall this particular class(tp/tp+fn) d) f1-score: harmonic mean of precision and recall. signifies how good the model is. accuracy metric is not a very good metric for model performance analysis as it doesn't capture misclassification of under-represented class: example of fraudulent transactions which have very less representation(npci example) e) roc(receiver operating characteristic) curve: explains the trade off of increasing tpr with fpr of the given model. auc(area under the curve) signifies how much one has to trade off tpr to decrease fpr. 3. clustering problem(unsupervised learning problem). example of customer segmentation was discussed. then went on to clustering algorithms like k-means, hierarchical clustering etc.",13,0.09359283,-16.459923,6.784566,0.2282194,"classification, classifying, classifications"
50,"the session covered key steps in data preprocessing, starting with outlier analysis and removal. next, it addressed noise reduction using data smoothing techniques like simple moving average (sma) and exponential moving average (ema), where a larger window size results in more smoothing. the discussion then moved to gradient descent as an optimization technique. normalization methods were introduced, followed by the box-cox transformation and logarithmic transformations, which help stabilize variance and normalize data before time series analysis. additionally, maximum likelihood estimation (mle) was discussed. the session also touched on the kepler exoplanet dataset and the synthetic minority over-sampling technique (smote) for handling imbalancedâ data.","the session covered key steps in data preprocessing, starting with outlier analysis and removal. next, it addressed noise reduction using data smoothing techniques like simple moving average (sma) and exponential moving average (ema), where a larger window size results in more smoothing. the discussion then moved to gradient descent as an optimization technique. normalization methods were introduced, followed by the box-cox transformation and logarithmic transformations, which help stabilize variance and normalize data before time series analysis. additionally, maximum likelihood estimation (mle) was discussed. the session also touched on the kepler exoplanet dataset and the synthetic minority over-sampling technique (smote) for handling imbalanced data.",11,-23.87299,8.190675,9.955193,10.280443,"pca, heatmap, heatmaps"
51,"in  today's session, we focused on regression metrics, sampling distributions, and the central limit theorem (clt).we were also introduced to the idea that regression coefficients derived from sample data are estimates of population parameters, emphasizing the importance of evaluating their reliability.to understand this ,we examined the concept of sampling distributions, where multiple representative samples are drawn from a population to calculate sample means. these means tend to form a normal distribution as per the central limit theorem, regardless of the original population distribution, provided the sample size is sufficiently large. this principle allows statisticians to infer population characteristics and calculate confidence intervals for model parameters. we also saw metrics like sse (sum of squared errors), mse (mean squared error), rmse (root mean squared error), and mae (mean absolute error), which are necessary for analysing the performance of linear regression models. these metrics help to explain variations in the data and its accuracy in predicting outcomes.
by reducing uncertainty with larger sample sizes, the standard error of the sampling distribution decreases, improving the precision of estimates. and then we made this on excel sheet and plotted histogram for errors and drew best fit line using linear regression.","in today's session, we focused on regression metrics, sampling distributions, and the central limit theorem (clt).we were also introduced to the idea that regression coefficients derived from sample data are estimates of population parameters, emphasizing the importance of evaluating their reliability.to understand this ,we examined the concept of sampling distributions, where multiple representative samples are drawn from a population to calculate sample means. these means tend to form a normal distribution as per the central limit theorem, regardless of the original population distribution, provided the sample size is sufficiently large. this principle allows statisticians to infer population characteristics and calculate confidence intervals for model parameters. we also saw metrics like sse (sum of squared errors), mse (mean squared error), rmse (root mean squared error), and mae (mean absolute error), which are necessary for analysing the performance of linear regression models. these metrics help to explain variations in the data and its accuracy in predicting outcomes. by reducing uncertainty with larger sample sizes, the standard error of the sampling distribution decreases, improving the precision of estimates. and then we made this on excel sheet and plotted histogram for errors and drew best fit line using linear regression.",5,26.964987,1.1339049,14.602947,4.0374546,"regression, statistical, statistics"
52,"today's class started with a brief discussion on our upcoming projects and assignments. then we moved onto encoding of categorical variables. we studied in brief various encoding methods, their advantages and disadvantages.
1. one hot encoding: good if the independent variables are not ordinal. if the output is not ordinal, this is not preferred. but this increases the dimension of the dataset, bringing along the problems of high dimensionality.
2. binary encoding: instead of giving separate columns to each of the bit, combine all the columns into one binary number column.
3. integer encoding: has a sense of order. it can't be used for nominal labels.
we also discussed other methods of encoding like frequency and target encoding. then we started discussing about text processing. any text processing application involves converting text into numbers of some sort and then make meaning out of them. mostly words called stop words that don't add much meaning to sentences are removed and a dictionary is made out of the rest of the words for further processing.","today's class started with a brief discussion on our upcoming projects and assignments. then we moved onto encoding of categorical variables. we studied in brief various encoding methods, their advantages and disadvantages. 1. one hot encoding: good if the independent variables are not ordinal. if the output is not ordinal, this is not preferred. but this increases the dimension of the dataset, bringing along the problems of high dimensionality. 2. binary encoding: instead of giving separate columns to each of the bit, combine all the columns into one binary number column. 3. integer encoding: has a sense of order. it can't be used for nominal labels. we also discussed other methods of encoding like frequency and target encoding. then we started discussing about text processing. any text processing application involves converting text into numbers of some sort and then make meaning out of them. mostly words called stop words that don't add much meaning to sentences are removed and a dictionary is made out of the rest of the words for further processing.",3,-43.93085,5.3940105,0.19810244,6.4364414,"categorical, categorization, categorise"
53,"population and sample differ in that a sample is a subset of the population. attributes calculated for a sample are called statistics, while those for a population are called parameters. parameters can be estimated using sample statistics (e.g., mean, variance, standard deviation).

in simple linear regression, the goal is to find the best-fit line (y = î²â‚€ + î²â‚x) for making predictions. here, î²â‚€ and î²â‚ are estimates of population parameters. confidence intervals for these estimates are calculated by minimizing the sum of squared errors (î£(eáµ¢â²), where eáµ¢ = yáµ¢ - (î²â‚€ + î²â‚xáµ¢)).

the estimates for î²â‚€ and î²â‚ are derived as:
î²â‚€ = mean(y) - î²â‚ * mean(x)
î²â‚ = (mean(xy) - mean(x) * mean(y)) / (mean(xâ²) - (mean(x))â²).","population and sample differ in that a sample is a subset of the population. attributes calculated for a sample are called statistics, while those for a population are called parameters. parameters can be estimated using sample statistics (e.g., mean, variance, standard deviation). in simple linear regression, the goal is to find the best-fit line (y = + x) for making predictions. here, and are estimates of population parameters. confidence intervals for these estimates are calculated by minimizing the sum of squared errors ( (e ), where e = y - ( + x )). the estimates for and are derived as: = mean(y) - * mean(x) = (mean(xy) - mean(x) * mean(y)) / (mean(x ) - (mean(x)) ).",1,31.399422,-8.687469,16.362371,4.27286,"population, models, estimating"
54,"we talked about the plan for the remaining 10 lectures and the group project.

started the class by learning about function encoding.

saw how to change categories like red, blue, green into a function with three variables.

there are two types of problems: multiclass and multilabel, and the approach depends on that.

learned about binary encoding, which is a very compact way to encode data.

discussed how to convert data using binary encoding in detail.

frequency encoding replaces category values with how often they appear in the dataset.

in target encoding, category values are replaced by the average score above 2.5.

other encoding methods include label encoding, one hot encoding, and image encoding.","we talked about the plan for the remaining 10 lectures and the group project. started the class by learning about function encoding. saw how to change categories like red, blue, green into a function with three variables. there are two types of problems: multiclass and multilabel, and the approach depends on that. learned about binary encoding, which is a very compact way to encode data. discussed how to convert data using binary encoding in detail. frequency encoding replaces category values with how often they appear in the dataset. in target encoding, category values are replaced by the average score above 2.5. other encoding methods include label encoding, one hot encoding, and image encoding.",3,-41.991093,6.2641354,0.37428683,6.0238404,"categorical, categorization, categorise"
55,"today we initiated our class with process of working on data, how ml model should be done, it is not supposed to used the entire sample to creation of ml models. we are supposed to divide the sample into trained data and test (unseen) data. the split is done randomly with a usual training to test ratio being 4:1. there are other commonly used ratio but, the decesion on which ratio to choose depended on the number of observations, that is size of the sample.
so how do we compare the outcomes the model gives us. two outcomes training metrics are like f value, r^2, confidence, and etc. the test metrics r^2 but it doesn't have  confidence interval, as we don't make statistics here. 
we also looked about overfitting and followed with data drift which is basically minute changes in data after a some interval if time.
lastly we were again given a heads up for the usage of python from the following classes and we were also shown and talked about some packages and libraries of python.","today we initiated our class with process of working on data, how ml model should be done, it is not supposed to used the entire sample to creation of ml models. we are supposed to divide the sample into trained data and test (unseen) data. the split is done randomly with a usual training to test ratio being 4:1. there are other commonly used ratio but, the decesion on which ratio to choose depended on the number of observations, that is size of the sample. so how do we compare the outcomes the model gives us. two outcomes training metrics are like f value, r^2, confidence, and etc. the test metrics r^2 but it doesn't have confidence interval, as we don't make statistics here. we also looked about overfitting and followed with data drift which is basically minute changes in data after a some interval if time. lastly we were again given a heads up for the usage of python from the following classes and we were also shown and talked about some packages and libraries of python.",2,7.561631,4.0288897,10.741335,4.8875074,"regression, regressions, features"
56,"levels of measurement : data obtained / represented in different forms should be treated differently instead of blindly feeding it to the computer

our limitation : even after having access to huge data (via internet) we are unable to utilize all of it due to computational constraints, hence we must still take a sample from it

(un)supervised learning : even in the absence of labels (""y""), we can use some techniques to predict the function (y = f(x))","levels of measurement : data obtained / represented in different forms should be treated differently instead of blindly feeding it to the computer our limitation : even after having access to huge data (via internet) we are unable to utilize all of it due to computational constraints, hence we must still take a sample from it (un)supervised learning : even in the absence of labels (""y""), we can use some techniques to predict the function (y = f(x))",9,-24.182148,-9.338946,1.5163127,0.6760827,"dataâ, analyse, analyses"
57,"ds 203 15 january, 2025 (3rd lecture)
sir started by talking about y = f(x). he first talked about old and new methods of data analysis. for this he took the example of finding the temperature difference between the two ends of a pipe using the flow rate of the fluid flowing inside the pipe. y was the temperature difference denoted by delta-t and x was the flow rate.
in the old methods we used to get the equation relating y = f(x) like dt = l23, while in the new methods we get data points and obtain a plot of the curve which relates temperature difference with the corresponding flow rate.
then he talked about the methods that we use for obtaining the plot which are:
1.	slr â€“ simple linear regression
2.	mlr â€“ multiple linear regression
3.	logistic regression
4.	random forest
he also mentioned k-means clustering and hierarchical clustering.
then he said that there are two paths which are machine-learning and the other one is statistics, in this course we would frequently move from one path to another to get the assigned task done.
he then started talking about levels of measurement. there are four levels of measurement:
1.	nominal (discrete): for example, gender, color, etc. 
to explain, in this if two people have different gender no one is superior the other, all of them are equal.
2.	ordinal (discrete): for example, grades
but now how would the computer identify these (here, gender) as distinct because we need to associate some number with each grade for the computer to recognize them.
but if we assign numbers like male -1, female-2 and so on then we are doing something that is fundamentally wrong because we are attaching higher value to one of the them whereas, they should be equal.
in order to take care of this issue we use vectors and use them as a switch. it would be difficult to show that it here but sir has drawn that very nicely in his notes (uploaded on moodle).
example: dog [1 0 0 0]
3.	interval (continuous) for example, temperature
sir said that scale on which we are measuring will not create any issues.
4.	ratio (continuous) for example, height, weight, salary
y is known as label and x is known as feature.
supervised and unsupervised learning problems
a problem in which we know both the values of the label and the features is known as supervised learning problem.

then sir defined a function 
monthly-purchases = f (salary, month of year, size of family, etc.)
then he talked about the ml categories about each of the level of measurement: 
nominal â€“ classification
ordinal â€“ classification
interval â€“ regression
ratio â€“ regression
then he talked about unsupervised learning in which we donâ€™t know the value of labels associated with features.
there is hierarchical and k-means clustering. then he explained clustering by using a graph between features.
it is difficult to explain the flow chart (on page 9) in sirâ€™s notes but it is quite clear by looking at the flow chart.

we take a representative sample out of the population in order to save time and money for data analysis.
larger the value of the population more accurate is the prediction.","ds 203 15 january, 2025 (3rd lecture) sir started by talking about y = f(x). he first talked about old and new methods of data analysis. for this he took the example of finding the temperature difference between the two ends of a pipe using the flow rate of the fluid flowing inside the pipe. y was the temperature difference denoted by delta-t and x was the flow rate. in the old methods we used to get the equation relating y = f(x) like dt = l23, while in the new methods we get data points and obtain a plot of the curve which relates temperature difference with the corresponding flow rate. then he talked about the methods that we use for obtaining the plot which are: 1. slr simple linear regression 2. mlr multiple linear regression 3. logistic regression 4. random forest he also mentioned k-means clustering and hierarchical clustering. then he said that there are two paths which are machine-learning and the other one is statistics, in this course we would frequently move from one path to another to get the assigned task done. he then started talking about levels of measurement. there are four levels of measurement: 1. nominal (discrete): for example, gender, color, etc. to explain, in this if two people have different gender no one is superior the other, all of them are equal. 2. ordinal (discrete): for example, grades but now how would the computer identify these (here, gender) as distinct because we need to associate some number with each grade for the computer to recognize them. but if we assign numbers like male -1, female-2 and so on then we are doing something that is fundamentally wrong because we are attaching higher value to one of the them whereas, they should be equal. in order to take care of this issue we use vectors and use them as a switch. it would be difficult to show that it here but sir has drawn that very nicely in his notes (uploaded on moodle). example: dog [1 0 0 0] 3. interval (continuous) for example, temperature sir said that scale on which we are measuring will not create any issues. 4. ratio (continuous) for example, height, weight, salary y is known as label and x is known as feature. supervised and unsupervised learning problems a problem in which we know both the values of the label and the features is known as supervised learning problem. then sir defined a function monthly-purchases = f (salary, month of year, size of family, etc.) then he talked about the ml categories about each of the level of measurement: nominal classification ordinal classification interval regression ratio regression then he talked about unsupervised learning in which we don t know the value of labels associated with features. there is hierarchical and k-means clustering. then he explained clustering by using a graph between features. it is difficult to explain the flow chart (on page 9) in sir s notes but it is quite clear by looking at the flow chart. we take a representative sample out of the population in order to save time and money for data analysis. larger the value of the population more accurate is the prediction.",4,-26.185825,-12.64044,1.7015727,0.2454813,"classification, classifying, classifications"
58,"analyzing the entire population to understand its behavior/characteristics is not feasible both â€“ time wise and money wise. so, we make use of (representative) samples.

attributes
attributes associated with samples as well as population are:
1.	count (frequency)
2.	mode
3.	mean
4.	standard deviation
5.	variance, and many others
operations
operations associated with samples as well as population are:
1.	count
2.	add
3.	subtract
4.	multiply
5.	divide and many others

using a sample, we can estimate the mean of the population.
attributes associated with the population are known as parameters while those associated with the sample as statistics.
we want to estimate the parameters based on the statistics.

simple linear regression has only one predictor.

y is the response variable/dependent variable/label
x is the predictor variable/independent variable/feature

on a scatter plot in which it looked like the points were scattered on the surface of a circular disc. then he pointed out a few things, which are:
1.	simple linear regression is not required in such cases.
2.	it would not be the best possible method in such cases.
3.	but if someone wants to apply it, then it can be applied.
4.	in such a case a point is a better approximation than a line, this can be understood by taking any value of the feature (x) and then by looking at the difference in its actual value at that x and the predicted value.
5.	each point can be considered as a model â€“ although a very naã¯ve one.

bias - value of the y â€“ intercept in the equation obtained by simple linear regression.

as the size of the sample which we are using to estimate the population parameters increases the estimates become better.

estimation interval or confidence interval, finding î²0 and î²1 was just the start of machine learning. we need to take into account the error that would be there in our estimation. we have no basis to say that î²0 = î²0p.

confidence interval which is that with a% probability we can say that the parameter value will lie in an interval, that interval is known as confidence interval. typically, confidence intervals are of 90% or 95% and sometimes even 99%. 100% confidence interval is the interval from -infinity to +infinity.

options : 
1.	minimize sum of all ei
2.	minimize the sum of squares of all ei.
3.	minimize the sum of perpendicular distance between all pairs of {(xi,,yi) and (xi,yi-bar)}.
4.	minimize the sum of absolute value of all ei.
here ei = yi â€“ (yi-bar)    
(1) is rejected (4) is rejected because we donâ€™t want our solutions to be biased towards any direction so we choose (2).
a professor sir/maâ€™am has done (3).","analyzing the entire population to understand its behavior/characteristics is not feasible both time wise and money wise. so, we make use of (representative) samples. attributes attributes associated with samples as well as population are: 1. count (frequency) 2. mode 3. mean 4. standard deviation 5. variance, and many others operations operations associated with samples as well as population are: 1. count 2. add 3. subtract 4. multiply 5. divide and many others using a sample, we can estimate the mean of the population. attributes associated with the population are known as parameters while those associated with the sample as statistics. we want to estimate the parameters based on the statistics. simple linear regression has only one predictor. y is the response variable/dependent variable/label x is the predictor variable/independent variable/feature on a scatter plot in which it looked like the points were scattered on the surface of a circular disc. then he pointed out a few things, which are: 1. simple linear regression is not required in such cases. 2. it would not be the best possible method in such cases. 3. but if someone wants to apply it, then it can be applied. 4. in such a case a point is a better approximation than a line, this can be understood by taking any value of the feature (x) and then by looking at the difference in its actual value at that x and the predicted value. 5. each point can be considered as a model although a very na ve one. bias - value of the y intercept in the equation obtained by simple linear regression. as the size of the sample which we are using to estimate the population parameters increases the estimates become better. estimation interval or confidence interval, finding 0 and 1 was just the start of machine learning. we need to take into account the error that would be there in our estimation. we have no basis to say that 0 = 0p. confidence interval which is that with a% probability we can say that the parameter value will lie in an interval, that interval is known as confidence interval. typically, confidence intervals are of 90% or 95% and sometimes even 99%. 100% confidence interval is the interval from -infinity to +infinity. options : 1. minimize sum of all ei 2. minimize the sum of squares of all ei. 3. minimize the sum of perpendicular distance between all pairs of {(xi,,yi) and (xi,yi-bar)}. 4. minimize the sum of absolute value of all ei. here ei = yi (yi-bar) (1) is rejected (4) is rejected because we don t want our solutions to be biased towards any direction so we choose (2). a professor sir/ma am has done (3).",1,36.689808,-7.171047,16.434385,3.5747132,"population, models, estimating"
59,"in this lecture we first learned about population and that sample should be good and representative. our goal is to predict or estimate population from sample and we can use operations like counting ,adding ,subtracting, multiplication and division. also, we have attributes like frequency or count, mode ,median ,mean, standard deviation ,variance if we estimate for population then it is called parameter and if we calculate for sample then it is called statistic. we want to estimate the parameters based on the statistics. we create a model based on how correctly can we predict the values based on that model. even a point can be considered as a model but it will be a very naive model. first we need to perform some operation on the data points like plotting scatter plot or something and from which we can see and conclude that whether we have to use simple linear regression or we have to just create a single point as a model. for a particular model bias represents the net sum of all unaccountable variables. one set of samples will give one line but some other samples might give other line so we just create one of the very possible models. we need to estimate the confidence interval for values of b0 and b1. the best way to arrive at these values is by using sum of square of errors because it will magnify the error. the mean of the observation lies on the prediction line .we have closed form solution to calculate the values of a and b. b0 and b1r point estimates and we need to arrive at the possible interval within which these values will lie such that there is a very high chance that the predictable values that is the population parameters will lie within these intervals respectively","in this lecture we first learned about population and that sample should be good and representative. our goal is to predict or estimate population from sample and we can use operations like counting ,adding ,subtracting, multiplication and division. also, we have attributes like frequency or count, mode ,median ,mean, standard deviation ,variance if we estimate for population then it is called parameter and if we calculate for sample then it is called statistic. we want to estimate the parameters based on the statistics. we create a model based on how correctly can we predict the values based on that model. even a point can be considered as a model but it will be a very naive model. first we need to perform some operation on the data points like plotting scatter plot or something and from which we can see and conclude that whether we have to use simple linear regression or we have to just create a single point as a model. for a particular model bias represents the net sum of all unaccountable variables. one set of samples will give one line but some other samples might give other line so we just create one of the very possible models. we need to estimate the confidence interval for values of b0 and b1. the best way to arrive at these values is by using sum of square of errors because it will magnify the error. the mean of the observation lies on the prediction line .we have closed form solution to calculate the values of a and b. b0 and b1r point estimates and we need to arrive at the possible interval within which these values will lie such that there is a very high chance that the predictable values that is the population parameters will lie within these intervals respectively",1,35.69393,-8.116107,16.586077,3.822625,"population, models, estimating"
60,"in class, we picked up from where we left off, focusing on some key statistical concepts. we talked about terms from the data analysis toolpack, what they mean, how theyâ€™re connected, and how to interpret them visually, including the errors and uncertainties that can come with them.
we went over cases involving beta and beta 0 in specific scenarios, and we talked about the p-value and how it helps with feature selection. we also explored multiple linear regression and discussed how certain table values are specifically tied to it. toward the end, we introduced anova, talked about the f-statistic, and why itâ€™s important for it to be largeâ€”weâ€™ll dive deeper into that next class.","in class, we picked up from where we left off, focusing on some key statistical concepts. we talked about terms from the data analysis toolpack, what they mean, how they re connected, and how to interpret them visually, including the errors and uncertainties that can come with them. we went over cases involving beta and beta 0 in specific scenarios, and we talked about the p-value and how it helps with feature selection. we also explored multiple linear regression and discussed how certain table values are specifically tied to it. toward the end, we introduced anova, talked about the f-statistic, and why it s important for it to be large we ll dive deeper into that next class.",13,3.4379048,15.49256,12.474157,5.830917,"classification, classifying, classifications"
61,"in today's lecture, we used excel to perform linear regression. we started by opening a csv file, extracting the necessary parameters, and creating a scatter plot with a fitted linear trendline. additionally, we plotted a histogram and a scatter plot of the error values, which showed a distinct pattern. these plots, along with other factors, can be analyzed to evaluate the quality of a model.","in today's lecture, we used excel to perform linear regression. we started by opening a csv file, extracting the necessary parameters, and creating a scatter plot with a fitted linear trendline. additionally, we plotted a histogram and a scatter plot of the error values, which showed a distinct pattern. these plots, along with other factors, can be analyzed to evaluate the quality of a model.",5,18.595118,-6.34415,14.242687,5.128067,"regression, statistical, statistics"
62,"started with recap: all the values in 95% interval are not really distinct it's just we got the values by chance.
statistically significant and similar situation based on where the point is in the normal distribution. learnt with analogy to beta1 that it is statistically similar to zero.
stat similar so no regression. then we studied gradient descent method for mlr. where the various variable are termed as features. and the method to deal with it get it into right form like dividing height weight to get a feature of bmi which is more important for a data of health analysis. then we did a vector analysis where there is a coefficient matrix which gets updated based on where sum of sqaured error is minimized that is minimizing the cost function. then we saw this happening in excel sheet where various parameters were studies and understood it's context as to how good a model is.  we want a high f value to compare various models. r2 should be more close to 1. but majorly played with dropping variables to get the p values for the variables under 0.05 and avoid zero in the 95% interval.",started with recap: all the values in 95% interval are not really distinct it's just we got the values by chance. statistically significant and similar situation based on where the point is in the normal distribution. learnt with analogy to beta1 that it is statistically similar to zero. stat similar so no regression. then we studied gradient descent method for mlr. where the various variable are termed as features. and the method to deal with it get it into right form like dividing height weight to get a feature of bmi which is more important for a data of health analysis. then we did a vector analysis where there is a coefficient matrix which gets updated based on where sum of sqaured error is minimized that is minimizing the cost function. then we saw this happening in excel sheet where various parameters were studies and understood it's context as to how good a model is. we want a high f value to compare various models. r2 should be more close to 1. but majorly played with dropping variables to get the p values for the variables under 0.05 and avoid zero in the 95% interval.,2,16.664232,5.494636,12.588129,4.0150337,"regression, regressions, features"
63,"in todayâ€™s class, we covered different techniques for encoding categorical data. we started with feature encoding, where we learned about one-hot encoding and vectorization. one-hot encoding is useful for categorical data, but it can lead to the curse of dimensionality when the number of unique categories is very large. vectorization, on the other hand, helps convert text data into numerical form, which we only touched on briefly toward the end of the lecture.  

we also explored label encoding, which assigns a unique integer to each category. however, if the labels have a natural order (making them ordinal rather than nominal), using integer encoding would be more meaningful. for multiclass and multilabel problems, different encoding approaches may be required depending on the complexity of the data.  

binary encoding was another method we discussed, where each category is converted into a binary format, which helps reduce dimensionality â€” for example, three binary columns can represent up to eight classes. we also learned about frequency encoding, where the frequency of a category is used as its encoded value, and target encoding, where the encoding is based on the relationship between the category and the target variable.  

overall, the session introduced us to the challenges and trade-offs involved in different encoding methods and gave us a basic idea of how to handle text data through vectorization.","in today s class, we covered different techniques for encoding categorical data. we started with feature encoding, where we learned about one-hot encoding and vectorization. one-hot encoding is useful for categorical data, but it can lead to the curse of dimensionality when the number of unique categories is very large. vectorization, on the other hand, helps convert text data into numerical form, which we only touched on briefly toward the end of the lecture. we also explored label encoding, which assigns a unique integer to each category. however, if the labels have a natural order (making them ordinal rather than nominal), using integer encoding would be more meaningful. for multiclass and multilabel problems, different encoding approaches may be required depending on the complexity of the data. binary encoding was another method we discussed, where each category is converted into a binary format, which helps reduce dimensionality for example, three binary columns can represent up to eight classes. we also learned about frequency encoding, where the frequency of a category is used as its encoded value, and target encoding, where the encoding is based on the relationship between the category and the target variable. overall, the session introduced us to the challenges and trade-offs involved in different encoding methods and gave us a basic idea of how to handle text data through vectorization.",3,-42.69871,4.6160436,0.21261826,6.233514,"categorical, categorization, categorise"
64,"we looked at f1 matrix, confusion matrix, also we need to check if the confusion matrix has actual numbers in the table, either by checking the documentation or the data. 
a very frequent problem which we face is having a cleaned and sane dataset, hence we looked at how a general data analysis problem can go on, then we looked at crisp-dm and overview of itâ€™s six steps. the ta shubham sir then explained how the data can be collected and cleaned, and explained the concept of outliers, when the data is actually an outlier (for eg seasonal demand and festivities), and their importance, and that we cannot simply ignore them. we discussed how mean, medience are related to outliers and how they are affected by it","we looked at f1 matrix, confusion matrix, also we need to check if the confusion matrix has actual numbers in the table, either by checking the documentation or the data. a very frequent problem which we face is having a cleaned and sane dataset, hence we looked at how a general data analysis problem can go on, then we looked at crisp-dm and overview of it s six steps. the ta shubham sir then explained how the data can be collected and cleaned, and explained the concept of outliers, when the data is actually an outlier (for eg seasonal demand and festivities), and their importance, and that we cannot simply ignore them. we discussed how mean, medience are related to outliers and how they are affected by it",9,-11.33981,20.921549,8.857369,8.943191,"dataâ, analyse, analyses"
65,"today's session basically started off with some review of the previous lectures, where we discussed that how if the error plot is not random, and shows some non linear relation i.e. if there is a non linear relation between the independent and dependent variable, then we need to introduce polynomials of the independent variable also as features to our model. this is part of feature engineering, and using polynomial functions of the independent variable as a feature for the mlr model, is known as polynomial regression. we also discussed how any non-linear and non-polynomial relation (eg. sin, cos, log) can also be converted to a polynomial relation using taylor series expansion. we also learnt that even complex neural networks employed some kind of polynomial regression itself, in order to make good predictions.
then we moved on to data which displays different kinds of relation in different ranges. we saw that we could use a variety of models for such data, and we could even use multiple models for a single dataset in case there was a completely different relation. however using one single model for the entire dataset is always more beneficial and easier than using multiple models, as it will create complications when implementing for the test dataset. also in such cases, we concluded that the best method of finding out the best model for our data was to try out all possible models and then choose the one with the best prediction accuracy according to us. we saw the responses of various models over a dataset and we observed that even though one model (random forest) was fitting our data highly accurately, we observed that it might be too good a fit for our model to be able to make any useful predictions on the test data. however, some other models, although not fitting the data that accurately, were doing quite well at prediction as compared to random forest. hence, choosing them would be a better choice than choosing a model which accurately fits our data but is not able to make accurate predictions. 
we then moved on to logistic regression and classification. we noted that this scheme applies to nominal and ordinal levels of measurement and not to ratios as classes are discrete and need to be dealt with in that manner itself. we also said that unlike linear regression, logistic regression was not meant to predict the data values, but rather the class in which an observation belonged. the output of such a model is the class, and we are basically trying to predict the boundaries between the classes. the expression for calculation is somewhat similar to that of linear regression, however in this case, we use weights instead of coefficients (beta) as per the nomenclature. the idea behind both the regression algorithms was similar; to reach a mean or mediocre value. in linear regression we were trying to predict the value of data points based on the mean of observations, whereas here, we are trying to find the mean boundary between classes so that we can accurately classify our observation points.","today's session basically started off with some review of the previous lectures, where we discussed that how if the error plot is not random, and shows some non linear relation i.e. if there is a non linear relation between the independent and dependent variable, then we need to introduce polynomials of the independent variable also as features to our model. this is part of feature engineering, and using polynomial functions of the independent variable as a feature for the mlr model, is known as polynomial regression. we also discussed how any non-linear and non-polynomial relation (eg. sin, cos, log) can also be converted to a polynomial relation using taylor series expansion. we also learnt that even complex neural networks employed some kind of polynomial regression itself, in order to make good predictions. then we moved on to data which displays different kinds of relation in different ranges. we saw that we could use a variety of models for such data, and we could even use multiple models for a single dataset in case there was a completely different relation. however using one single model for the entire dataset is always more beneficial and easier than using multiple models, as it will create complications when implementing for the test dataset. also in such cases, we concluded that the best method of finding out the best model for our data was to try out all possible models and then choose the one with the best prediction accuracy according to us. we saw the responses of various models over a dataset and we observed that even though one model (random forest) was fitting our data highly accurately, we observed that it might be too good a fit for our model to be able to make any useful predictions on the test data. however, some other models, although not fitting the data that accurately, were doing quite well at prediction as compared to random forest. hence, choosing them would be a better choice than choosing a model which accurately fits our data but is not able to make accurate predictions. we then moved on to logistic regression and classification. we noted that this scheme applies to nominal and ordinal levels of measurement and not to ratios as classes are discrete and need to be dealt with in that manner itself. we also said that unlike linear regression, logistic regression was not meant to predict the data values, but rather the class in which an observation belonged. the output of such a model is the class, and we are basically trying to predict the boundaries between the classes. the expression for calculation is somewhat similar to that of linear regression, however in this case, we use weights instead of coefficients (beta) as per the nomenclature. the idea behind both the regression algorithms was similar; to reach a mean or mediocre value. in linear regression we were trying to predict the value of data points based on the mean of observations, whereas here, we are trying to find the mean boundary between classes so that we can accurately classify our observation points.",0,4.7593865,-1.1689985,10.240315,4.435754,"models, feature, features"
66,"we started our discussion with how to improve the quality of results:
1. by improving the sample --> quality of the sample
                                                 --> size of the sample (increasing it)
2. improve the method
    --> use multiple methods and use the best one.
3. fine tune/properly use the methods.

then we talked about linear regression:
where output is expressed as a linear combination of independent variables.
it is not necessary that the output has to be a line (linear).
like: x1->x1, x2->x1^2, x3->x1^3
y = b0 + b1*x1 + b2*x2 + ......

although, on increasing the number of features the adjusted r^2 value might reduce as we are adding more unnecessary features.","we started our discussion with how to improve the quality of results: 1. by improving the sample --> quality of the sample --> size of the sample (increasing it) 2. improve the method --> use multiple methods and use the best one. 3. fine tune/properly use the methods. then we talked about linear regression: where output is expressed as a linear combination of independent variables. it is not necessary that the output has to be a line (linear). like: x1->x1, x2->x1^2, x3->x1^3 y = b0 + b1*x1 + b2*x2 + ...... although, on increasing the number of features the adjusted r^2 value might reduce as we are adding more unnecessary features.",2,5.7525043,-5.4838824,9.686675,4.0831637,"regression, regressions, features"
67,"class started by the introduction to pivot table. we can find various parameters of a data like average of total characters, sum, min, max and std of the total characters. we analyzed the data of the submissions of summaries. we checked the histogram  for the average number of characters. we noticed that the majority of the portion lies in the lesser number of characters. we further created a box plot to find that there were some outliers in the data. 
we calculated the skewness and kurtosis and plotted a scatter plot to support the conclusion for outliers. 
we get shape of distribution by the histogram. other methods can help us know about the outliers. 
for the data, we explored the maximum submissions, the minimum submission, the number of times they have submitted. then, we went to ""who on which date submitted what length of submission?"". there were several missing values in the overall sheet. 
sir moved to the chemical plant data. we understood how data can be calculated for every 5 min or every hour. but it will just explode the data. to find the inner fluctuations, we need to dub up data. on using pivot table, we found that a lot of data is missing. we went to the min, max and average value of the data. we found an anomaly: min value for zero for a parameter. to check the place in which the error occurred we cut down the observations into small chunks until we found it. on plotting this data, these anomalies were clearly visible. we also created histogram and box plots. it is a good practice to create all the images and view them at once. we noticed how the plots with and without outliers look different from one another.  later, we proceeded to see the documentation of the data. in the anomaly and questions were highlighted. we do not want to exclude feature that may be useful. we went on to check how many of the variables are independent. we noticed that out of these hundreds of data rows. we only need 17 principle component or 17 independent process to describe the entire process. also, we can reorganize data and bring similar observation together. this makes the heatmap look better. 
we also checked for the data obtained from a transformer operations. the incomplete can be from sensor failure or device failure. we noticed how there can be points where hypothesis can be generated and we need to decide if to accept it or reject it. quality of input depends on the quality of our exploratory data analysis.","class started by the introduction to pivot table. we can find various parameters of a data like average of total characters, sum, min, max and std of the total characters. we analyzed the data of the submissions of summaries. we checked the histogram for the average number of characters. we noticed that the majority of the portion lies in the lesser number of characters. we further created a box plot to find that there were some outliers in the data. we calculated the skewness and kurtosis and plotted a scatter plot to support the conclusion for outliers. we get shape of distribution by the histogram. other methods can help us know about the outliers. for the data, we explored the maximum submissions, the minimum submission, the number of times they have submitted. then, we went to ""who on which date submitted what length of submission?"". there were several missing values in the overall sheet. sir moved to the chemical plant data. we understood how data can be calculated for every 5 min or every hour. but it will just explode the data. to find the inner fluctuations, we need to dub up data. on using pivot table, we found that a lot of data is missing. we went to the min, max and average value of the data. we found an anomaly: min value for zero for a parameter. to check the place in which the error occurred we cut down the observations into small chunks until we found it. on plotting this data, these anomalies were clearly visible. we also created histogram and box plots. it is a good practice to create all the images and view them at once. we noticed how the plots with and without outliers look different from one another. later, we proceeded to see the documentation of the data. in the anomaly and questions were highlighted. we do not want to exclude feature that may be useful. we went on to check how many of the variables are independent. we noticed that out of these hundreds of data rows. we only need 17 principle component or 17 independent process to describe the entire process. also, we can reorganize data and bring similar observation together. this makes the heatmap look better. we also checked for the data obtained from a transformer operations. the incomplete can be from sensor failure or device failure. we noticed how there can be points where hypothesis can be generated and we need to decide if to accept it or reject it. quality of input depends on the quality of our exploratory data analysis.",9,-9.293209,23.49369,8.027476,10.175662,"dataâ, analyse, analyses"
68,"true positive rate vs fast positive rate plot for different thresholds is called receiver operating characteristics curve. for random guess model, this curve is y=x. area under this curve should be as close to 1 for good classification models.

classification problems when data labels are not available is clustering problem where model have to find clusters in dataset, one cluster means same label points.
in k-means clustering algorithm, initially k clusters are assigned randomly and cluster centers are iteratively updated based on distance from data points until cluster center stops changing.

hierarchical clustering starts with each data point as a separate cluster. gradually, the nearest clusters or points are merged step by step until all points form a single cluster. this merging process is visually represented using a dendrogram, where making a horizontal cut at any level reveals a possible clustering structure with a specific number of groups.","true positive rate vs fast positive rate plot for different thresholds is called receiver operating characteristics curve. for random guess model, this curve is y=x. area under this curve should be as close to 1 for good classification models. classification problems when data labels are not available is clustering problem where model have to find clusters in dataset, one cluster means same label points. in k-means clustering algorithm, initially k clusters are assigned randomly and cluster centers are iteratively updated based on distance from data points until cluster center stops changing. hierarchical clustering starts with each data point as a separate cluster. gradually, the nearest clusters or points are merged step by step until all points form a single cluster. this merging process is visually represented using a dendrogram, where making a horizontal cut at any level reveals a possible clustering structure with a specific number of groups.",8,-5.8690076,-21.392376,6.300155,0.38355398,"classification, clusterings, classifying"
69,"simple linear regression (slr) and the statistical foundations of sampling distributions....explains how regression metrics like 
1)sse
2) mse
3)rmse
4) mae
5) r^2
...which help assess model accuracy by measuring variation in data

repeated sampling leads to a normal distribution of sample means (as per the central limit theorem), making statistical inference possible","simple linear regression (slr) and the statistical foundations of sampling distributions....explains how regression metrics like 1)sse 2) mse 3)rmse 4) mae 5) r^2 ...which help assess model accuracy by measuring variation in data repeated sampling leads to a normal distribution of sample means (as per the central limit theorem), making statistical inference possible",5,27.707706,1.7206012,14.617102,3.9856415,"regression, statistical, statistics"
70,"in class today, sir explained various feature encoding techniques. he began with label encoding, which assigns unique integers to each category. the second method is integer encoding, suitable for ordinal variable categorization. then came binary encoding, a more compact representation, in which three columns can depict eight classes. frequency encoding involves replacing categories with their occurrence counts, while target encoding allocates values based on the target variable statistics. one-hot encoding is particularly useful when it comes to multi-class and multi-label problems, but it suffers heavily from the curse of dimensionality. in conclusion, a brief introduction to text vectorization techniques, which are techniques to convert text into numerical representations, was also introduced.","in class today, sir explained various feature encoding techniques. he began with label encoding, which assigns unique integers to each category. the second method is integer encoding, suitable for ordinal variable categorization. then came binary encoding, a more compact representation, in which three columns can depict eight classes. frequency encoding involves replacing categories with their occurrence counts, while target encoding allocates values based on the target variable statistics. one-hot encoding is particularly useful when it comes to multi-class and multi-label problems, but it suffers heavily from the curse of dimensionality. in conclusion, a brief introduction to text vectorization techniques, which are techniques to convert text into numerical representations, was also introduced.",3,-42.127804,4.918571,0.24048172,6.0606937,"categorical, categorization, categorise"
71,"in today's session, we first discuss what we are going to cover next in the course, including feature engineering, data engineering, data preparation, big data, cloud computing, and tools in the cloud, etc. we further discussed the upcoming project and exercises. then we discussed the feature encoding irrespective of the kind of dependence of y, whether it is dependent on categorical variables, they must be converted to a numerical value. there are 6 types of encoding as follows:
label encoding: we assign a unique integer to each category, like red:0, blue:1, or green:2, but these already assign an ordinal relationship in the data, and we should apply this to y rather than x. 
one-hot encoding: converting categories into vectors like red:(1,0,0), blue:(0,1,0), and green:(0,0,1), but here we increase the dimensionality in the data. 
binary encoding: converts the categorical values into separate binary columns as a:00, b:01, c:10, d:11, this is similar to pseudo one-hot encoding, but here we reduce the dimensionality comparatively, still has various columns. 
integer encoding: somewhat similar to label but used in tree-based models mainly. frequency encoding: assigning the frequency of the categorical variable in the dataset. target encoding: replaces categories with the mean of the target variable for each category. 
for the y=f(x) we simply use one-hot encoding for the x-side and the y-side, consider everything, one or multiple labels based on the problem, and apply appropriate models. we further learn about feature binning, where we change continuous numerical features into categorical features and then use the classification model. at last, we see about llm in which statistical processing can generate deterministic output and we will mainly use it for code generation.","in today's session, we first discuss what we are going to cover next in the course, including feature engineering, data engineering, data preparation, big data, cloud computing, and tools in the cloud, etc. we further discussed the upcoming project and exercises. then we discussed the feature encoding irrespective of the kind of dependence of y, whether it is dependent on categorical variables, they must be converted to a numerical value. there are 6 types of encoding as follows: label encoding: we assign a unique integer to each category, like red:0, blue:1, or green:2, but these already assign an ordinal relationship in the data, and we should apply this to y rather than x. one-hot encoding: converting categories into vectors like red:(1,0,0), blue:(0,1,0), and green:(0,0,1), but here we increase the dimensionality in the data. binary encoding: converts the categorical values into separate binary columns as a:00, b:01, c:10, d:11, this is similar to pseudo one-hot encoding, but here we reduce the dimensionality comparatively, still has various columns. integer encoding: somewhat similar to label but used in tree-based models mainly. frequency encoding: assigning the frequency of the categorical variable in the dataset. target encoding: replaces categories with the mean of the target variable for each category. for the y=f(x) we simply use one-hot encoding for the x-side and the y-side, consider everything, one or multiple labels based on the problem, and apply appropriate models. we further learn about feature binning, where we change continuous numerical features into categorical features and then use the classification model. at last, we see about llm in which statistical processing can generate deterministic output and we will mainly use it for code generation.",3,-43.109966,2.6888068,0.04573987,6.35603,"categorical, categorization, categorise"
72,"we looked at three ways to improve the quality of results: improving the sample, improving the method, and fine-tuning existing methods. from there, we moved on to multiple linear regression (mlr) for nonlinear cases, where we used feature engineering to add polynomial terms like  and trigonometric functions like . this led us to polynomial regression. we noticed that as we added more terms, adjusted  tended to drop, and only the most significant terms remained based on p-values.

we also explored forward and backward feature selection and the risk of overfitting. when dealing with nonlinear models that combine trigonometry and linear elements, we considered whether a single model could handle both instead of relying on separate approaches. random forest turned out to be a method that naturally does this by combining different models.

along the way, we compared parametric and non-parametric methods, introduced delta analysis, and briefly touched on neural networks and deep learning, particularly models with multiple hidden layers. finally, we wrapped up with classification, covering regression vs. logistic regression, the concept of weights, and the sigmoid function, including its graphs and explanations.","we looked at three ways to improve the quality of results: improving the sample, improving the method, and fine-tuning existing methods. from there, we moved on to multiple linear regression (mlr) for nonlinear cases, where we used feature engineering to add polynomial terms like and trigonometric functions like . this led us to polynomial regression. we noticed that as we added more terms, adjusted tended to drop, and only the most significant terms remained based on p-values. we also explored forward and backward feature selection and the risk of overfitting. when dealing with nonlinear models that combine trigonometry and linear elements, we considered whether a single model could handle both instead of relying on separate approaches. random forest turned out to be a method that naturally does this by combining different models. along the way, we compared parametric and non-parametric methods, introduced delta analysis, and briefly touched on neural networks and deep learning, particularly models with multiple hidden layers. finally, we wrapped up with classification, covering regression vs. logistic regression, the concept of weights, and the sigmoid function, including its graphs and explanations.",0,-1.528819,-2.930125,9.122649,4.5211954,"models, feature, features"
73,"we learnt about the closed form solution in multiple linear regression but found that it is not feasible as we have to deal with matrix inversion and multi collinearity. also during selection of features if p value is greater than 0.05 we can ignore that feature as it is not impacting much. then we learnt about train and test data today . normally we use 80-20 %. we are advised to never ever use 100% test data as that will not benefit.we also saw if râ² values of training and testing matrixes are close enough then overfit can occur. we also saw what is adjusted râ², multiple r and why is there only n-1 degree of freedom in total sum of squares when there are n observations because the formula is such that one degree of freedom is already used.","we learnt about the closed form solution in multiple linear regression but found that it is not feasible as we have to deal with matrix inversion and multi collinearity. also during selection of features if p value is greater than 0.05 we can ignore that feature as it is not impacting much. then we learnt about train and test data today . normally we use 80-20 %. we are advised to never ever use 100% test data as that will not benefit.we also saw if r values of training and testing matrixes are close enough then overfit can occur. we also saw what is adjusted r , multiple r and why is there only n-1 degree of freedom in total sum of squares when there are n observations because the formula is such that one degree of freedom is already used.",2,8.1297,8.2760515,11.452173,4.994998,"regression, regressions, features"
74,"we started our discussion with multiple linear regression. we saw that we can derive more features from given features also that might help us in fitting a better curve. with more features the adjusted r2-score decreases. and hence in a situation of more features we should analyze the p-values of all the features, the features which best signify the dataset will have lower p-value and one with higher p-values can be dropped. if we force a model to fit the training data then it would lead to overfitting and the model will not able to generalize well. the dimensionality of the problem can be reduced using methods like pca or t-sne etc. we should try multiple models from linear-regression to xgboost etc. and one which performs best should be chosen. then we switched to classification and discussed about it. we started discussing about logistic regression. looked into logit/sigmoid function and it's plot.","we started our discussion with multiple linear regression. we saw that we can derive more features from given features also that might help us in fitting a better curve. with more features the adjusted r2-score decreases. and hence in a situation of more features we should analyze the p-values of all the features, the features which best signify the dataset will have lower p-value and one with higher p-values can be dropped. if we force a model to fit the training data then it would lead to overfitting and the model will not able to generalize well. the dimensionality of the problem can be reduced using methods like pca or t-sne etc. we should try multiple models from linear-regression to xgboost etc. and one which performs best should be chosen. then we switched to classification and discussed about it. we started discussing about logistic regression. looked into logit/sigmoid function and it's plot.",0,0.0071329335,-3.0176234,9.551657,4.506269,"models, feature, features"
75,"1. we started off with the exploratory data analysis, the ability to visualise given data and analyse its various patterns based on various histogram, and plots.
2. main factors for solving a machine learning problem include domain knowledge, communication and critical thinking about problem
3. sources of data, lot of anamolies and real world data isnâ€™t close to the pre processed easy to handle data
4. got to know about crisp: cross industry process for data mining. data understanding: eda, data preperation: selecting, cleaning, construction and formatting
5. importance of hypothesis testing, the two tests that checks whether the errors follow normal distribution or not
6. talked about the data mind map consisting various problems with major branches like dependent variable and independent variable
and various problems such as column problem, heteroscelasticity, across the column problems, insuffuficent features.
7. talked about various methods to remove anamolies, one being feeding in the value of mean or the value obtained by fitting a curve, it can be the value of k nearest beighbour
8. talked about pca, about projection of higher dimensional data into a 2d plane to detect various outliers.","1. we started off with the exploratory data analysis, the ability to visualise given data and analyse its various patterns based on various histogram, and plots. 2. main factors for solving a machine learning problem include domain knowledge, communication and critical thinking about problem 3. sources of data, lot of anamolies and real world data isn t close to the pre processed easy to handle data 4. got to know about crisp: cross industry process for data mining. data understanding: eda, data preperation: selecting, cleaning, construction and formatting 5. importance of hypothesis testing, the two tests that checks whether the errors follow normal distribution or not 6. talked about the data mind map consisting various problems with major branches like dependent variable and independent variable and various problems such as column problem, heteroscelasticity, across the column problems, insuffuficent features. 7. talked about various methods to remove anamolies, one being feeding in the value of mean or the value obtained by fitting a curve, it can be the value of k nearest beighbour 8. talked about pca, about projection of higher dimensional data into a 2d plane to detect various outliers.",9,-15.024477,20.055597,8.936162,9.136598,"dataâ, analyse, analyses"
76,today we continued discussing about logistic regression the class began with the  demonstration of artificial neural networks and we learnt that selecting appropriate and relevant number of features is very important. we then revisited the logistic regression metrices and a new metric was introduced today and that was roc graph which tells us that how many true positives did the model predict before predicting the false positives and for a good model which can make a boundary between two clusters then the area under the curve must be close to 1 if the data is not good and if there is too much overlap of the data then in such cases the area under the curve will be close to 0.5. we then saw two examples of logistic regression one showing how to interpret the metrices like accuracy. precision and confusion matrix. we looked at another example where there were four clusters in the data but one had class imbalance and we could clearly see in the roc curve that specific class had very low area under the curve. we looked at clustering methods mainly k-means and hierarchical clustering. the main difference between these two is that we have to specify  number of cluster we want in k-means but the hierarchical clustering considers every data point as a cluster and then starts making broader groups. k-means works on taking the n-means and then starts calculating the euclidean distance between other points which ever is close to the nth mean it is classified into that cluster. hierarchical clustering uses indivdual points as a cluster and then finds which element is close to it forms a bigger cluster with that now distance between this cluster and other cluster is calculated and which ever is smaller is taken in to form a bigger cluster,today we continued discussing about logistic regression the class began with the demonstration of artificial neural networks and we learnt that selecting appropriate and relevant number of features is very important. we then revisited the logistic regression metrices and a new metric was introduced today and that was roc graph which tells us that how many true positives did the model predict before predicting the false positives and for a good model which can make a boundary between two clusters then the area under the curve must be close to 1 if the data is not good and if there is too much overlap of the data then in such cases the area under the curve will be close to 0.5. we then saw two examples of logistic regression one showing how to interpret the metrices like accuracy. precision and confusion matrix. we looked at another example where there were four clusters in the data but one had class imbalance and we could clearly see in the roc curve that specific class had very low area under the curve. we looked at clustering methods mainly k-means and hierarchical clustering. the main difference between these two is that we have to specify number of cluster we want in k-means but the hierarchical clustering considers every data point as a cluster and then starts making broader groups. k-means works on taking the n-means and then starts calculating the euclidean distance between other points which ever is close to the nth mean it is classified into that cluster. hierarchical clustering uses indivdual points as a cluster and then finds which element is close to it forms a bigger cluster with that now distance between this cluster and other cluster is calculated and which ever is smaller is taken in to form a bigger cluster,8,-3.3991225,-21.22836,6.289307,0.34907144,"classification, clusterings, classifying"
77,"the session focused on essential data analysis techniques, beginning with a tutorial on using pivot tables for efficient data organization and summarization. this was followed by a demonstration of exploratory data analysis (eda) on a real dataset, highlighting key insights and patterns. additionally, the teaching assistant (ta) presented exercise e2, explaining its concepts and applications. the session provided a comprehensive understanding of data exploration and analysis methods.","the session focused on essential data analysis techniques, beginning with a tutorial on using pivot tables for efficient data organization and summarization. this was followed by a demonstration of exploratory data analysis (eda) on a real dataset, highlighting key insights and patterns. additionally, the teaching assistant (ta) presented exercise e2, explaining its concepts and applications. the session provided a comprehensive understanding of data exploration and analysis methods.",6,-18.548582,27.854588,7.3529267,9.428711,"summarizing, summarize, summarization"
78,"studied about linear regression in excel using closed form solutions derived in previous class. learnt about sse, mse, rmse, mae. learnt about r_square, adjusted r_square and their significance. learnt about anova. good model is the one which explains variations of data. 
sst = ssr + sse , ssr/sst = r^2 , r^2 is called coefficient of determination = square of the correlation between x and y. 
these all metrics reflects quality of linear regression. by central limit theorem, sample means tend to be normal distributed.the expected value of such a distribution is very close to the population mean.the standard deviation of this distribution is known as the standard error is directly proportional to population's standard deviation. as the sample sizes increase, our analysis becomes more accurate.","studied about linear regression in excel using closed form solutions derived in previous class. learnt about sse, mse, rmse, mae. learnt about r_square, adjusted r_square and their significance. learnt about anova. good model is the one which explains variations of data. sst = ssr + sse , ssr/sst = r^2 , r^2 is called coefficient of determination = square of the correlation between x and y. these all metrics reflects quality of linear regression. by central limit theorem, sample means tend to be normal distributed.the expected value of such a distribution is very close to the population mean.the standard deviation of this distribution is known as the standard error is directly proportional to population's standard deviation. as the sample sizes increase, our analysis becomes more accurate.",5,24.733625,-0.38607675,14.143169,4.089236,"regression, statistical, statistics"
79,"we did linear regression on a data given to us in excel. we calculated xbar,ybar, xsquare , xbar square and thus calculated beta0 and beta1 which will help in predicting y values. we then calculated error and then we plot a scatter plot of x vs y values. also on the same graph created our line using beta0 and beta1. it helps to find how much data is visually far from our prediction. also we noted that we can only predict in the specified range of data not everywhere. we then plotted error scatterplot which was visually seen as randomised enough. we also plotted histogram which was looking uniformly distributed. we also got to know about r2 coefficient of determination which is square of correlation coefficient in case of simple linear regression. then we used analytics tool of excel to get different values of data like beta0,beta1, confidence interval values and f values. we saw there was a difference in number (degree of freedom) in ssr, sse and sst ..  overall we got in depth knowledge of how to do linear regression.","we did linear regression on a data given to us in excel. we calculated xbar,ybar, xsquare , xbar square and thus calculated beta0 and beta1 which will help in predicting y values. we then calculated error and then we plot a scatter plot of x vs y values. also on the same graph created our line using beta0 and beta1. it helps to find how much data is visually far from our prediction. also we noted that we can only predict in the specified range of data not everywhere. we then plotted error scatterplot which was visually seen as randomised enough. we also plotted histogram which was looking uniformly distributed. we also got to know about r2 coefficient of determination which is square of correlation coefficient in case of simple linear regression. then we used analytics tool of excel to get different values of data like beta0,beta1, confidence interval values and f values. we saw there was a difference in number (degree of freedom) in ssr, sse and sst .. overall we got in depth knowledge of how to do linear regression.",5,19.118898,-3.2149491,13.964645,5.2191887,"regression, statistical, statistics"
80,"i learned about the concepts of population and sample, where analyzing an entire population is often impractical, so we work with samples to estimate population parameters. using attributes like mean, median, variance, and more, we study data to uncover patterns.

for instance, in simple linear regression, a single predictor is used to predict sales, like advertisement expenditure, using the equation y = bâ‚€ + bâ‚x. here, bâ‚€ (intercept) and bâ‚ (slope) are sample-based estimates of population parameters. our aim is to find a confidence interval for these estimates to claim where the true population values likely lie.

the intercept, bâ‚€ accounts for other factors (bias), which decreases as we add more predictors. in minimizing the sum of squared errors, we compute bâ‚€ and bâ‚, and curiously, the means of the predictor and response values always lie on the best-fit line.","i learned about the concepts of population and sample, where analyzing an entire population is often impractical, so we work with samples to estimate population parameters. using attributes like mean, median, variance, and more, we study data to uncover patterns. for instance, in simple linear regression, a single predictor is used to predict sales, like advertisement expenditure, using the equation y = b + b x. here, b (intercept) and b (slope) are sample-based estimates of population parameters. our aim is to find a confidence interval for these estimates to claim where the true population values likely lie. the intercept, b accounts for other factors (bias), which decreases as we add more predictors. in minimizing the sum of squared errors, we compute b and b , and curiously, the means of the predictor and response values always lie on the best-fit line.",1,32.634666,-7.625672,16.271572,4.056038,"population, models, estimating"
81,"sir taught exploratory data analysis. he accomplished this by using pivot tables and excel. to gain an understanding of the session summary data set, we examine various elements from the pivot table, such as the mean, median, min, max, stdev, histogram, box plot, summary statistics, and scatter plot.  by doing this, we were able to identify a few outliers whose character values in the summary were noticeably higher than those of the others.  after that, we performed the same eda on a dataset pertaining to a chemical factory, which contained data entered daily for almost six years in 241 columns.finally, tas talked about our e2-submissions and shared their perspectives with us.","sir taught exploratory data analysis. he accomplished this by using pivot tables and excel. to gain an understanding of the session summary data set, we examine various elements from the pivot table, such as the mean, median, min, max, stdev, histogram, box plot, summary statistics, and scatter plot. by doing this, we were able to identify a few outliers whose character values in the summary were noticeably higher than those of the others. after that, we performed the same eda on a dataset pertaining to a chemical factory, which contained data entered daily for almost six years in 241 columns.finally, tas talked about our e2-submissions and shared their perspectives with us.",6,-10.243726,26.450119,7.652231,10.186724,"summarizing, summarize, summarization"
82,"the session started with a question of how to improve the quality of the results. one solution is to improve the sample. to improve the sample, we can either increase the sample size or increase the sample quality. but how do we know if the sample selected is of good quality. another way to improve the quality of result is to improve the method. we can use multiple method and then select the best one. but we noticed that if the nature of data changes, then even the best method may not be the best for the new data. another solution to improve quality is to fine tune or properly use the method. 
then we went back to mlr. the base idea is that in multiple linear regression the outcome may not be linear but linear combination of independent variables. 
then we saw a sample. after the regression the error showed a specific pattern like a function or polynomial. for such cases we can use polynomial regression where we create new parameter x1, x2, x3, x4, which are polynomial or function of original x parameter. ex. if the error shows the pattern like a sine function, then taking x4 = sin(x) can be helpful. and now in mlr, the coefficient for sine function should appear significant and other all parameters should drop. 
it is not possible that we get a 100% accurate model. but at least we should try there should not be too many features and overfitting. 
then we discussed in short about forward feature engineering and backward feature engineering. 
though, we can use multiple models to extract data but it always better to have a single model. 
then we analyzed several models together. models were like random forest and neural network. 
by the end of the class, we moved to classification. it is applicable when y is either nominal or ordinal. we studied about the boundaries between the groups of y and that to calculate the boundaries we need equations. for which we use the sigmoid function.","the session started with a question of how to improve the quality of the results. one solution is to improve the sample. to improve the sample, we can either increase the sample size or increase the sample quality. but how do we know if the sample selected is of good quality. another way to improve the quality of result is to improve the method. we can use multiple method and then select the best one. but we noticed that if the nature of data changes, then even the best method may not be the best for the new data. another solution to improve quality is to fine tune or properly use the method. then we went back to mlr. the base idea is that in multiple linear regression the outcome may not be linear but linear combination of independent variables. then we saw a sample. after the regression the error showed a specific pattern like a function or polynomial. for such cases we can use polynomial regression where we create new parameter x1, x2, x3, x4, which are polynomial or function of original x parameter. ex. if the error shows the pattern like a sine function, then taking x4 = sin(x) can be helpful. and now in mlr, the coefficient for sine function should appear significant and other all parameters should drop. it is not possible that we get a 100% accurate model. but at least we should try there should not be too many features and overfitting. then we discussed in short about forward feature engineering and backward feature engineering. though, we can use multiple models to extract data but it always better to have a single model. then we analyzed several models together. models were like random forest and neural network. by the end of the class, we moved to classification. it is applicable when y is either nominal or ordinal. we studied about the boundaries between the groups of y and that to calculate the boundaries we need equations. for which we use the sigmoid function.",0,4.634553,-4.593098,9.643834,4.2333145,"models, feature, features"
83,"the session began with a discussion on the mid-semester exam, where the professor went over the solutions, clarifying key concepts and addressing doubts. this was followed by the ta providing feedback on the exercise, highlighting common mistakes and areas for improvement. after these discussions, the professor moved on to teaching two important topics in machine learning and statistics.

the first topic covered was the variance inflation factor (vif), a measure used to detect multicollinearity in regression models. the professor explained that vif is defined as  represents the coefficient of determination obtained by regressing a feature on all other features. a high vif value indicates strong collinearity, which can negatively impact model performance by making coefficient estimates unreliable. understanding vif is crucial in regression analysis to ensure that independent variables are not excessively correlated.

the second topic discussed was the curse of dimensionality, a fundamental challenge in machine learning that arises when working with high-dimensional data. the professor explained that as the number of dimensions increases, data points become more sparse, making it harder to identify meaningful patterns. additionally, high-dimensional datasets increase model complexity, often leading to overfitting and poor generalization to new data. the computational cost of processing such datasets also rises significantly, requiring more time and resources. another major issue is distance distortion, where the concept of ""distance"" between data points becomes less meaningful, making it difficult to distinguish between nearest and farthest neighbors. the session emphasized the importance of addressing these challenges through techniques like feature selection and dimensionality reduction to improve model efficiency and accuracy.","the session began with a discussion on the mid-semester exam, where the professor went over the solutions, clarifying key concepts and addressing doubts. this was followed by the ta providing feedback on the exercise, highlighting common mistakes and areas for improvement. after these discussions, the professor moved on to teaching two important topics in machine learning and statistics. the first topic covered was the variance inflation factor (vif), a measure used to detect multicollinearity in regression models. the professor explained that vif is defined as represents the coefficient of determination obtained by regressing a feature on all other features. a high vif value indicates strong collinearity, which can negatively impact model performance by making coefficient estimates unreliable. understanding vif is crucial in regression analysis to ensure that independent variables are not excessively correlated. the second topic discussed was the curse of dimensionality, a fundamental challenge in machine learning that arises when working with high-dimensional data. the professor explained that as the number of dimensions increases, data points become more sparse, making it harder to identify meaningful patterns. additionally, high-dimensional datasets increase model complexity, often leading to overfitting and poor generalization to new data. the computational cost of processing such datasets also rises significantly, requiring more time and resources. another major issue is distance distortion, where the concept of ""distance"" between data points becomes less meaningful, making it difficult to distinguish between nearest and farthest neighbors. the session emphasized the importance of addressing these challenges through techniques like feature selection and dimensionality reduction to improve model efficiency and accuracy.",13,-8.694778,7.5486903,9.1824045,7.4444604,"classification, classifying, classifications"
84,we started the class by y=f(x). we have data of y and x. we use machine learning models to find f. next we discussed about levels of measurement. starting with nominal which would basically differentiate between a characteristic. examples are gender and colour. then ordinal which had an inherent order. then there were inherent like temperature and ratio like height and weight. when there is no label it becomes an unsupervised learning model. then we have to use segmentation algorithms and assign labels. we have to realise the fact that any data we analyse is a sample.,we started the class by y=f(x). we have data of y and x. we use machine learning models to find f. next we discussed about levels of measurement. starting with nominal which would basically differentiate between a characteristic. examples are gender and colour. then ordinal which had an inherent order. then there were inherent like temperature and ratio like height and weight. when there is no label it becomes an unsupervised learning model. then we have to use segmentation algorithms and assign labels. we have to realise the fact that any data we analyse is a sample.,4,-22.302694,-11.744768,1.8474386,0.72313017,"classification, classifying, classifications"
85,"the session focused on feature engineering techniques, particularly feature binning and various encoding methods used in machine learning. feature binning is a technique that converts continuous numerical features into categorical features, making it useful for classification models by grouping values into bins. this can help improve model interpretability and performance.

the discussion then moved to different encoding techniques for categorical data. label encoding assigns numerical values to categorical variables and can be used for both dependent (y) and independent (x) variables. one-hot encoding, another common method, creates binary columns for each category but can significantly increase dimensionality. binary encoding provides an alternative by converting categories into binary format and mapping them to new columns, thereby reducing dimensionality compared to one-hot encoding. additionally, frequency encoding was discussed, where categorical values are replaced by their occurrence count in the dataset. target encoding was also covered, which involves replacing categorical values with the mean of the target variable for each category. the session emphasized the importance of selecting appropriate encoding techniques based on the problem at hand, as different approaches impact model performance and interpretability in various ways.","the session focused on feature engineering techniques, particularly feature binning and various encoding methods used in machine learning. feature binning is a technique that converts continuous numerical features into categorical features, making it useful for classification models by grouping values into bins. this can help improve model interpretability and performance. the discussion then moved to different encoding techniques for categorical data. label encoding assigns numerical values to categorical variables and can be used for both dependent (y) and independent (x) variables. one-hot encoding, another common method, creates binary columns for each category but can significantly increase dimensionality. binary encoding provides an alternative by converting categories into binary format and mapping them to new columns, thereby reducing dimensionality compared to one-hot encoding. additionally, frequency encoding was discussed, where categorical values are replaced by their occurrence count in the dataset. target encoding was also covered, which involves replacing categorical values with the mean of the target variable for each category. the session emphasized the importance of selecting appropriate encoding techniques based on the problem at hand, as different approaches impact model performance and interpretability in various ways.",3,-42.477478,2.6028244,-0.02504377,6.293504,"categorical, categorization, categorise"
86,"in today's class we went deep into the simple linear regression techniques by using some data in excel. in the data we have only x and y column from which we calculated other terms such as x_bar, y_bar, xbar_sq, ybar_sq, error and many more terms. we also create the scatter plot between x and y and realized that it follows linear model. we have also plot the histogram to check what is the nature of distribution of data. if it's a bell shaped curve then it is good. the scatter plot of errors values display a distinct pattern showing that model has failed to pick-up the inherent pattern in the data. next by using the data analysis tool in excel we created the summary output of an linear regression model giving many values related to regression statistics. another interesting thing is that - one that explains most of the variations in the data is 'good model'. further we learnt about some regression statistics  short forms like  1)sst = measure of total variation in the given dataset.
2)ssr => total variation explained by the regression model and 3) sse => variation not explained by the model, attributed to random errors. coefficient of determination(r^2) which is the square of the correlation coefficient 'r' between x and y. if the x is increasing and y also increases then it have (+ve) correlation. if the x is increasing and y is decreasing then it have (-ve) correlation. we conduct some 'thought' experiments, related to estimating the population mean from the sample mean: assume that from a population we can take multiple good, representative samples, let's say k samples, each of size n. let's call each sample as s_i. using each s_i, we calculate its mean and call it m_i. for samples are good, representative samples of the population, they will result in means m_i that are close to each other. if we collect all the m_i and create a frequency table and a histogram, it's shape will be bell curved.","in today's class we went deep into the simple linear regression techniques by using some data in excel. in the data we have only x and y column from which we calculated other terms such as x_bar, y_bar, xbar_sq, ybar_sq, error and many more terms. we also create the scatter plot between x and y and realized that it follows linear model. we have also plot the histogram to check what is the nature of distribution of data. if it's a bell shaped curve then it is good. the scatter plot of errors values display a distinct pattern showing that model has failed to pick-up the inherent pattern in the data. next by using the data analysis tool in excel we created the summary output of an linear regression model giving many values related to regression statistics. another interesting thing is that - one that explains most of the variations in the data is 'good model'. further we learnt about some regression statistics short forms like 1)sst = measure of total variation in the given dataset. 2)ssr => total variation explained by the regression model and 3) sse => variation not explained by the model, attributed to random errors. coefficient of determination(r^2) which is the square of the correlation coefficient 'r' between x and y. if the x is increasing and y also increases then it have (+ve) correlation. if the x is increasing and y is decreasing then it have (-ve) correlation. we conduct some 'thought' experiments, related to estimating the population mean from the sample mean: assume that from a population we can take multiple good, representative samples, let's say k samples, each of size n. let's call each sample as s_i. using each s_i, we calculate its mean and call it m_i. for samples are good, representative samples of the population, they will result in means m_i that are close to each other. if we collect all the m_i and create a frequency table and a histogram, it's shape will be bell curved.",5,24.142195,-2.074736,14.031594,4.4788184,"regression, statistical, statistics"
87,"in this session, we learned about multiple linear regression (mlr) and how it works in real-world scenarios. while there is a mathematical formula to solve mlr, it involves complex calculations and isnâ€™t always practical. a key challenge in mlr is multi-collinearity, where some input variables are too closely related, which can make predictions less reliable.

to build a good model, we donâ€™t use the entire dataset for training. instead, we split it into 80% for training and 20% for testing. this helps us check if the model can make accurate predictions on new, unseen data. we also talked about overfitting, which happens when a model performs well on training data but fails to predict new data correctly.

we explored key performance measures like multiple r (which shows how strongly the inputs and output are related) and adjusted r2 (which checks if adding more variables actually improves the model). we also discussed why, in statistics, we divide by n-1 instead of n when calculating standard deviation.

linear regression is a parametric method, meaning it works based on fixed formulas and p-values. in contrast, non-parametric methods rely on different performance measures like r2 and mean squared error (mse). we also touched on data drift, which happens when data patterns change over time, making old models less effective.

finally, we implemented mlr in python using the ordinary least squares (ols) method, giving us hands-on experience with how regression models are built and analyzed.","in this session, we learned about multiple linear regression (mlr) and how it works in real-world scenarios. while there is a mathematical formula to solve mlr, it involves complex calculations and isn t always practical. a key challenge in mlr is multi-collinearity, where some input variables are too closely related, which can make predictions less reliable. to build a good model, we don t use the entire dataset for training. instead, we split it into 80% for training and 20% for testing. this helps us check if the model can make accurate predictions on new, unseen data. we also talked about overfitting, which happens when a model performs well on training data but fails to predict new data correctly. we explored key performance measures like multiple r (which shows how strongly the inputs and output are related) and adjusted r2 (which checks if adding more variables actually improves the model). we also discussed why, in statistics, we divide by n-1 instead of n when calculating standard deviation. linear regression is a parametric method, meaning it works based on fixed formulas and p-values. in contrast, non-parametric methods rely on different performance measures like r2 and mean squared error (mse). we also touched on data drift, which happens when data patterns change over time, making old models less effective. finally, we implemented mlr in python using the ordinary least squares (ols) method, giving us hands-on experience with how regression models are built and analyzed.",0,5.9052653,0.7701089,10.383398,4.649622,"models, feature, features"
88,"in today's class,ta gave feedback on assignment 3 that quality of our report is increasing with the and then the professor discussed the midsem paper. then in the last 15 minutes, we explored the issues that are normally faced when handling data. our professor highlighted that incorrect data is a major issue, usually due to human mistakes or system malfunctions while entering data. we also learned about the issue of incomplete data, where missing data can limit detailed analysis.","in today's class,ta gave feedback on assignment 3 that quality of our report is increasing with the and then the professor discussed the midsem paper. then in the last 15 minutes, we explored the issues that are normally faced when handling data. our professor highlighted that incorrect data is a major issue, usually due to human mistakes or system malfunctions while entering data. we also learned about the issue of incomplete data, where missing data can limit detailed analysis.",6,-5.8053617,21.156649,8.419394,9.482268,"summarizing, summarize, summarization"
89,"the session started with a quick review of the gradient descent function and then moved into logistic regression. the professor explained the confusion matrix and showed a website where you could experiment with neural network clustering. after that, students explored logistic regression code and learned about key metrics like precision, recall, and the f1 score. the roc curve was also introduced, with the false positive rate (fpr) on the x-axis and the true positive rate (tpr) on the y-axis, and it was mentioned that each class has its own curve. the session finished with a look at clustering methods, starting with k-means, where the number of clusters is set in advance, and ending with hierarchical clustering.","the session started with a quick review of the gradient descent function and then moved into logistic regression. the professor explained the confusion matrix and showed a website where you could experiment with neural network clustering. after that, students explored logistic regression code and learned about key metrics like precision, recall, and the f1 score. the roc curve was also introduced, with the false positive rate (fpr) on the x-axis and the true positive rate (tpr) on the y-axis, and it was mentioned that each class has its own curve. the session finished with a look at clustering methods, starting with k-means, where the number of clusters is set in advance, and ending with hierarchical clustering.",8,-5.7256675,-17.327072,6.241254,0.6853007,"classification, clusterings, classifying"
90,today in class we analysed the data of the previous submitted summaries of the sessions and plotted it in a graph. also at the end there was feeback about the assignments submitted and what are the possible points where we can improve while plotting graph.,today in class we analysed the data of the previous submitted summaries of the sessions and plotted it in a graph. also at the end there was feeback about the assignments submitted and what are the possible points where we can improve while plotting graph.,6,-4.2189727,22.644371,8.012994,9.938466,"summarizing, summarize, summarization"
91,"in today's session, we saw that the equation y=f(x) shows that we can find the function by identifying the data pattern of y and x. then we move onto 4 levels of measurement which are as follows:
nominal: discrete data like gender or color, which we can just classify not measure.
ordinal: discrete data like grades, which have their inherent order, we can also classify them.
interval: continuous data like temperature, here the definition of 0 is arbitrary.
ratio: continuous data like height, in which you can state 10m is twice of 5m.
we saw that we should not associate numbers directly with nominal and ordinal data types, instead we can use vectors to represent them. in y=f(x), y is called the label and the x's are features. if we know both, then to predict f we use supervised learning methods like slr, mlr, etc. if we don't know the labels, we can predict f with unsupervised learning methods like hierarchical clustering, etc. in which we form clusters of x's, analyze each, and then convert them into labeled data.
if the label data is nominal or ordinal, we use classification and regression for interval and ratio data types. then we see that whatever the size of the data is, it is always a sample that we need to analyze to get y=f(x), and further that you should back-test your model before presenting.","in today's session, we saw that the equation y=f(x) shows that we can find the function by identifying the data pattern of y and x. then we move onto 4 levels of measurement which are as follows: nominal: discrete data like gender or color, which we can just classify not measure. ordinal: discrete data like grades, which have their inherent order, we can also classify them. interval: continuous data like temperature, here the definition of 0 is arbitrary. ratio: continuous data like height, in which you can state 10m is twice of 5m. we saw that we should not associate numbers directly with nominal and ordinal data types, instead we can use vectors to represent them. in y=f(x), y is called the label and the x's are features. if we know both, then to predict f we use supervised learning methods like slr, mlr, etc. if we don't know the labels, we can predict f with unsupervised learning methods like hierarchical clustering, etc. in which we form clusters of x's, analyze each, and then convert them into labeled data. if the label data is nominal or ordinal, we use classification and regression for interval and ratio data types. then we see that whatever the size of the data is, it is always a sample that we need to analyze to get y=f(x), and further that you should back-test your model before presenting.",4,-26.470726,-15.190508,1.4038656,0.4608374,"classification, classifying, classifications"
92,"we began with an analysis of pivot tables in excel, which can be helpful for summarizing large datasets by arranging and aggregating information. we learned how to create pivot tables, re-arrange fields, and calculate important metrics like sums, averages, counts, and percentages to gain meaningful insights.

after that we learned some methods of exploratory data analysis (eda). we also learned important statistical parameters such as mean, median, variance, and standard deviation to understand the distribution of data alon with some methods of visualization such as histograms, box plots, and scatter plots that enable us to identify patterns, trends in distribution, and outliers.

later on, we discussed problems like class imbalance, where a particular category is very heavily overrepresented, and it causes biased outcomes. we touched upon feature engineering and data transformation techniques briefly, which improve the quality of the data prior to performing more intensive analysis.","we began with an analysis of pivot tables in excel, which can be helpful for summarizing large datasets by arranging and aggregating information. we learned how to create pivot tables, re-arrange fields, and calculate important metrics like sums, averages, counts, and percentages to gain meaningful insights. after that we learned some methods of exploratory data analysis (eda). we also learned important statistical parameters such as mean, median, variance, and standard deviation to understand the distribution of data alon with some methods of visualization such as histograms, box plots, and scatter plots that enable us to identify patterns, trends in distribution, and outliers. later on, we discussed problems like class imbalance, where a particular category is very heavily overrepresented, and it causes biased outcomes. we touched upon feature engineering and data transformation techniques briefly, which improve the quality of the data prior to performing more intensive analysis.",6,-14.501537,26.372032,7.23044,9.847125,"summarizing, summarize, summarization"
93,"in the lecture, we began by exploring a website called playground.tensorflow.org, which allows users to interactively experiment with machine learning models. on this site, we could select a dataset, choose features, and configure the number and size of hidden layers in the neural network. by pressing the ""run"" button, we could start training the model and observe how each neuron affects the final classification. this provided a clear visual understanding of machine learning processes and helped us build an intuition for model behavior. an interesting point was that even with a single hidden layer and appropriate features, we were able to achieve high classification accuracy on challenging datasets.

next, we discussed logistic regression. in the example code, we didn't implement a train-test split, which is something we should always do to evaluate the model's performance more reliably. we used python's logistic regression library to train the model and evaluated it using the `score` function, which in the case of logistic regression refers to the modelâ€™s accuracy, unlike in linear regression, where it corresponds to the r-squared value. 

we also covered the receiver operating characteristic (roc) curve, which is a graphical representation of a model's performance. the roc curve plots the true positive rate (tpr) against the false positive rate (fpr). a good classifier will classify most examples correctly before starting to make mistakes, which results in a steep increase in tpr with a minimal increase in fpr. we explained that for two classes, we can assume 2d gaussian distributions, each with different means. when the distributions are well separated, the overlap is minimal, leading to fewer misclassifications. however, when the distributions are less well separated, thereâ€™s more overlap, increasing the chance of classification errors. the model calculates the probability of each classification, and by adjusting the threshold, we can compute different tpr and fpr values, which form the roc curve. if the area under the roc curve is 1, the classifier is considered perfect.

following this, we explored multiclass classification. in this case, we calculated the precision, recall, and f1 score for each class. if one of the classes showed particularly low precision, recall, or f1 score, it suggested that the dataset was imbalanced, with too few instances of that class. we also generated multiple roc curves, one for each class, to evaluate the performance across all categories.

the lecture then shifted to clustering, a form of unsupervised learning where we only have feature data (x) and no target variable (y). we learned about key clustering algorithms, such as k-means and hierarchical clustering, and the importance of clustering metrics to evaluate how well the data points are grouped.

in k-means clustering, the number of clusters, k, must be specified beforehand. the algorithm starts by randomly selecting k initial cluster centers. each data point is then assigned to the nearest center, forming the initial clusters. the centers are recalculated as the mean of the points in each cluster, and the process repeats iteratively until the cluster centers no longer change, indicating convergence.

hierarchical clustering, on the other hand, does not require predefining the number of clusters. instead, it focuses on the distance between clusters using a method called linkage. there are several types of linkage, such as single, complete, average, and wardâ€™s method, which define how the distance between clusters is measured. in hierarchical clustering, the process begins with each data point as its own individual cluster. the two clusters with the smallest distance between them are merged, and this process continues iteratively. eventually, all the points are combined into a single cluster. a key feature of hierarchical clustering is that it can be represented visually through a dendrogram, a tree-like diagram that shows the sequence of merges. by cutting the dendrogram at different levels, we can obtain different numbers of clusters and visually inspect how the data points are grouped. this gives us the flexibility to explore clusters at various levels of granularity.

in conclusion, the lecture covered foundational concepts in machine learning, including model evaluation through accuracy and roc curves, as well as clustering techniques like k-means and hierarchical clustering. these topics laid the groundwork for understanding how unsupervised and supervised learning methods work in practice.","in the lecture, we began by exploring a website called playground.tensorflow.org, which allows users to interactively experiment with machine learning models. on this site, we could select a dataset, choose features, and configure the number and size of hidden layers in the neural network. by pressing the ""run"" button, we could start training the model and observe how each neuron affects the final classification. this provided a clear visual understanding of machine learning processes and helped us build an intuition for model behavior. an interesting point was that even with a single hidden layer and appropriate features, we were able to achieve high classification accuracy on challenging datasets. next, we discussed logistic regression. in the example code, we didn't implement a train-test split, which is something we should always do to evaluate the model's performance more reliably. we used python's logistic regression library to train the model and evaluated it using the `score` function, which in the case of logistic regression refers to the model s accuracy, unlike in linear regression, where it corresponds to the r-squared value. we also covered the receiver operating characteristic (roc) curve, which is a graphical representation of a model's performance. the roc curve plots the true positive rate (tpr) against the false positive rate (fpr). a good classifier will classify most examples correctly before starting to make mistakes, which results in a steep increase in tpr with a minimal increase in fpr. we explained that for two classes, we can assume 2d gaussian distributions, each with different means. when the distributions are well separated, the overlap is minimal, leading to fewer misclassifications. however, when the distributions are less well separated, there s more overlap, increasing the chance of classification errors. the model calculates the probability of each classification, and by adjusting the threshold, we can compute different tpr and fpr values, which form the roc curve. if the area under the roc curve is 1, the classifier is considered perfect. following this, we explored multiclass classification. in this case, we calculated the precision, recall, and f1 score for each class. if one of the classes showed particularly low precision, recall, or f1 score, it suggested that the dataset was imbalanced, with too few instances of that class. we also generated multiple roc curves, one for each class, to evaluate the performance across all categories. the lecture then shifted to clustering, a form of unsupervised learning where we only have feature data (x) and no target variable (y). we learned about key clustering algorithms, such as k-means and hierarchical clustering, and the importance of clustering metrics to evaluate how well the data points are grouped. in k-means clustering, the number of clusters, k, must be specified beforehand. the algorithm starts by randomly selecting k initial cluster centers. each data point is then assigned to the nearest center, forming the initial clusters. the centers are recalculated as the mean of the points in each cluster, and the process repeats iteratively until the cluster centers no longer change, indicating convergence. hierarchical clustering, on the other hand, does not require predefining the number of clusters. instead, it focuses on the distance between clusters using a method called linkage. there are several types of linkage, such as single, complete, average, and ward s method, which define how the distance between clusters is measured. in hierarchical clustering, the process begins with each data point as its own individual cluster. the two clusters with the smallest distance between them are merged, and this process continues iteratively. eventually, all the points are combined into a single cluster. a key feature of hierarchical clustering is that it can be represented visually through a dendrogram, a tree-like diagram that shows the sequence of merges. by cutting the dendrogram at different levels, we can obtain different numbers of clusters and visually inspect how the data points are grouped. this gives us the flexibility to explore clusters at various levels of granularity. in conclusion, the lecture covered foundational concepts in machine learning, including model evaluation through accuracy and roc curves, as well as clustering techniques like k-means and hierarchical clustering. these topics laid the groundwork for understanding how unsupervised and supervised learning methods work in practice.",8,-4.588958,-20.419436,6.405725,0.46036646,"classification, clusterings, classifying"
94,"in today's session we started with implementing simple functions like average, sum of squares for our features and labels and then we used them to find beta_0 and beta_1 for our linear regression line. visualizing the data using scatter plots helps us to find the patterns in out independent and dependent variables. also the errors can be represented using a scatter plot and histograms and for out data the scatter plot had a random pattern. the histogram somewhat represented a uniformity in the errors but it wasn't a normal distribution. also we can use r^2 to see how good our model actually is. if r^2 value is pretty close to 1 it represents most of the variation in the data  captured by the regression line. r^2 = r^2 for slr where r is the correlation coefficient. at the end confidence intervals were discussed and the analysis of our slr model gives out 95% c.i. for both beta_0 and beta_1 on both sides of the distribution which helps us to know the range where our actual values might lie. sampling distributions were also discussed where we take a particular set of samples with a particular size say 'n' the distribution of means of the samples is approximately gaussian or normal and the formula of std error was also discussed which is inversely proportional to sq.root(n) and hence its better to choose a bigger sample size than a sample with smaller size because choosing a bigger n value decreases the standard error and hence the uncertainity is less and our system becomes more accurate.","in today's session we started with implementing simple functions like average, sum of squares for our features and labels and then we used them to find beta_0 and beta_1 for our linear regression line. visualizing the data using scatter plots helps us to find the patterns in out independent and dependent variables. also the errors can be represented using a scatter plot and histograms and for out data the scatter plot had a random pattern. the histogram somewhat represented a uniformity in the errors but it wasn't a normal distribution. also we can use r^2 to see how good our model actually is. if r^2 value is pretty close to 1 it represents most of the variation in the data captured by the regression line. r^2 = r^2 for slr where r is the correlation coefficient. at the end confidence intervals were discussed and the analysis of our slr model gives out 95% c.i. for both beta_0 and beta_1 on both sides of the distribution which helps us to know the range where our actual values might lie. sampling distributions were also discussed where we take a particular set of samples with a particular size say 'n' the distribution of means of the samples is approximately gaussian or normal and the formula of std error was also discussed which is inversely proportional to sq.root(n) and hence its better to choose a bigger sample size than a sample with smaller size because choosing a bigger n value decreases the standard error and hence the uncertainity is less and our system becomes more accurate.",5,24.612272,-3.9449139,14.162925,4.5406036,"regression, statistical, statistics"
95,"sir started with discussion of midsem question paper. solution of midsem problem: 1. perform exploratory data analysis: we need to find problems with data set. find missing values in the dataset. we can drop rows with missing values if there are very less such rows and we have more data. or we can replace the missing values with mean or median based on distribution of data. understand the distribution of data within the columns and understand the distribution across the columns. normalise the data as required. if we have categorical data we can plot a histogram or pie chart. if there are very less data points of a particular category, just create a model that cannot detect that particular category. it is better not predict such categories with very less points. we have advantage if we have independent columns. in exam, the validation dataset were derived from a different dataset. we need to fit random forest to this kind of data. then sir discussed about solution to both parts. then one of the teaching assistant gave presentation about e3 review. then sir explained about curse of dimensionality. it describes the problems that occur when the number of features(dimensions) in a dataset increases significantly. we can address this by dimensionality reduction, feature selection, regularisation, increase in the amount of data.","sir started with discussion of midsem question paper. solution of midsem problem: 1. perform exploratory data analysis: we need to find problems with data set. find missing values in the dataset. we can drop rows with missing values if there are very less such rows and we have more data. or we can replace the missing values with mean or median based on distribution of data. understand the distribution of data within the columns and understand the distribution across the columns. normalise the data as required. if we have categorical data we can plot a histogram or pie chart. if there are very less data points of a particular category, just create a model that cannot detect that particular category. it is better not predict such categories with very less points. we have advantage if we have independent columns. in exam, the validation dataset were derived from a different dataset. we need to fit random forest to this kind of data. then sir discussed about solution to both parts. then one of the teaching assistant gave presentation about e3 review. then sir explained about curse of dimensionality. it describes the problems that occur when the number of features(dimensions) in a dataset increases significantly. we can address this by dimensionality reduction, feature selection, regularisation, increase in the amount of data.",9,-9.6197405,15.346622,9.602999,8.623595,"dataâ, analyse, analyses"
96,"for a mlr, in theory, closed form solution exists but in practicality, the closed form solution is not taken into consideration due to two reasons 1. finding inverse of large matrices is difficult and 2. there can be multi-collinearity. from a population we take sample, but in order to train ml model, entire sample should not be used. after cleaning and examining the sample, it should be divided in 80:20 ratio; 80% sample should be used to train the model (this sample data is known as training data) and the rest 20% sample should be used for testing purpose to check how well the model can predict the data (this is known as testing data or unseen data). then we discussed what is meant by overfit situation : (case1: râ²_trn=0.95 & râ²_tst=0.75, in this case the model is able to fit the training data, but is unable to predict the test data well; case2: râ²_trn=0.95 & râ²_tst=0.88, in this case the model is able to fit the training data and is able to predict the test data as well), in the above example, case1 is overfit situation, and in case2, ml model is good. after this, there was a discussion on different râ² values, i.e., like what is meant by multiple râ², râ², adjusted râ², etc. and what are their significance. also, we came to know that linear regression do not always mean fitting a straight line (linear is not for straight line), it is called linear regression because it uses linear combination of independent variables. then we discussed what are parametric models(slr & mlr which uses p-value for its operations and prediction) and non-parametric models(decision tree and random forest which rely on râ², rmse, mse ,etc. values for predicting outcomes).then we jumped into python discussed its two library 1. sklearnt and 2. statsmodel.api ; we also discussed q-q plot (quantile-quantile plot) in order to judge the that are the residuals normally distributed or not. 'sklearnt' do not give p-values, f-values, etc. so feature selection and dropping cannot happen in it. where as 'statsmodel.api' provides all the things just like excel and some even more. at last we learned about ols(ordinary least square), aic & bic (lower their values, better is the model), omnibus statistic(a number arrived from some formulation of skewness & kurtosis; lower its value, more the residual plot is near the normality), omnibus p-value(high p-value suggests that residual follows normal distribution), jarque-bera test(high p-value suggests that residual follows normal distribution) & durbin-watson test(it was something about auto-correlation between the error terms).","for a mlr, in theory, closed form solution exists but in practicality, the closed form solution is not taken into consideration due to two reasons 1. finding inverse of large matrices is difficult and 2. there can be multi-collinearity. from a population we take sample, but in order to train ml model, entire sample should not be used. after cleaning and examining the sample, it should be divided in 80:20 ratio; 80% sample should be used to train the model (this sample data is known as training data) and the rest 20% sample should be used for testing purpose to check how well the model can predict the data (this is known as testing data or unseen data). then we discussed what is meant by overfit situation : (case1: r _trn=0.95 & r _tst=0.75, in this case the model is able to fit the training data, but is unable to predict the test data well; case2: r _trn=0.95 & r _tst=0.88, in this case the model is able to fit the training data and is able to predict the test data as well), in the above example, case1 is overfit situation, and in case2, ml model is good. after this, there was a discussion on different r values, i.e., like what is meant by multiple r , r , adjusted r , etc. and what are their significance. also, we came to know that linear regression do not always mean fitting a straight line (linear is not for straight line), it is called linear regression because it uses linear combination of independent variables. then we discussed what are parametric models(slr & mlr which uses p-value for its operations and prediction) and non-parametric models(decision tree and random forest which rely on r , rmse, mse ,etc. values for predicting outcomes).then we jumped into python discussed its two library 1. sklearnt and 2. statsmodel.api ; we also discussed q-q plot (quantile-quantile plot) in order to judge the that are the residuals normally distributed or not. 'sklearnt' do not give p-values, f-values, etc. so feature selection and dropping cannot happen in it. where as 'statsmodel.api' provides all the things just like excel and some even more. at last we learned about ols(ordinary least square), aic & bic (lower their values, better is the model), omnibus statistic(a number arrived from some formulation of skewness & kurtosis; lower its value, more the residual plot is near the normality), omnibus p-value(high p-value suggests that residual follows normal distribution), jarque-bera test(high p-value suggests that residual follows normal distribution) & durbin-watson test(it was something about auto-correlation between the error terms).",2,8.821526,2.055482,10.968788,4.673232,"regression, regressions, features"
97,"to improve the quality of results we can improve the sample by either increasing quality of sample or size of sample. we can improve the method by using multiple methods and select best one. also we can fine tune and properly choose parameters. linear regression- outcome is expressed as a linear combination of independent variables. taylor seriers expansion- any function can be written as a sum of powers of x. so we can fit any curve by defining x2=x^2,x3=x^3,...... and use multiple linear regression. if we keep on increasing number of feature, adjusted r squared value decreases after a certain point. linear regression of non linear independent variables. the resulting regression method is known as polynomial regression. the technique of starting with all features and remove features one by one until the model performance reaches a peak is known as backward feature engineering. sir gave two exercises to solve based on feature selection and polynomial regression. for different datasets linear regression does not always give proper fit. there are many models under supervised machine learning. based on data and exploratory data analysis we choose which models to try on this data. each model can handle a different type of dataset better. we can use multiple models at a time also. we need to fit a single good model rather than fitting multiple models to predict output. we have to think in long term. if we fit multiple models, total cost of ownership increases. linear regression and similar methods are parametric methods. random forest is a non parametric model. with parametric model we can do delta analysis(if there is certain change in feature what would be the change in output). xg boost is also a nice non parametric model it gives residuals in better normal distribution as compared to random forest. neural networks are examples of parametric models, most of the present day ml models are based on this. if there are more than one layer it is a deep learning network. but deep learning requires a lot of data. when y is nominal or ordinal we use classification techniques. when it is internal and ratio, it becomes a regression problem. we then went on to logistic regression which is a classification method. regress- to go back to the mean. in logistic regression we try to find boundaries between groups. we need to find the boundary so that the misclassification is minimised.  the output in this case is a categorical variable which denotes to which group the given point belongs. we need a function that will convert any variable in between 0 and 1. we use the sigmoid function for this purpose.","to improve the quality of results we can improve the sample by either increasing quality of sample or size of sample. we can improve the method by using multiple methods and select best one. also we can fine tune and properly choose parameters. linear regression- outcome is expressed as a linear combination of independent variables. taylor seriers expansion- any function can be written as a sum of powers of x. so we can fit any curve by defining x2=x^2,x3=x^3,...... and use multiple linear regression. if we keep on increasing number of feature, adjusted r squared value decreases after a certain point. linear regression of non linear independent variables. the resulting regression method is known as polynomial regression. the technique of starting with all features and remove features one by one until the model performance reaches a peak is known as backward feature engineering. sir gave two exercises to solve based on feature selection and polynomial regression. for different datasets linear regression does not always give proper fit. there are many models under supervised machine learning. based on data and exploratory data analysis we choose which models to try on this data. each model can handle a different type of dataset better. we can use multiple models at a time also. we need to fit a single good model rather than fitting multiple models to predict output. we have to think in long term. if we fit multiple models, total cost of ownership increases. linear regression and similar methods are parametric methods. random forest is a non parametric model. with parametric model we can do delta analysis(if there is certain change in feature what would be the change in output). xg boost is also a nice non parametric model it gives residuals in better normal distribution as compared to random forest. neural networks are examples of parametric models, most of the present day ml models are based on this. if there are more than one layer it is a deep learning network. but deep learning requires a lot of data. when y is nominal or ordinal we use classification techniques. when it is internal and ratio, it becomes a regression problem. we then went on to logistic regression which is a classification method. regress- to go back to the mean. in logistic regression we try to find boundaries between groups. we need to find the boundary so that the misclassification is minimised. the output in this case is a categorical variable which denotes to which group the given point belongs. we need a function that will convert any variable in between 0 and 1. we use the sigmoid function for this purpose.",0,3.7984097,-2.888315,10.007598,4.244663,"models, feature, features"
98,"summary
todayâ€™s class discussion start on a topic of how to improve the quality of results ? we can do this by 1) improving the sample like quality and size of sample 2) improving the method like using multiple methods and selecting the best one 3) fine tuning of methods or properly using the methods example â€” gridsearch
the new definition of linear regression is that outcome is expressed as a linear combination of independent variables.
when using mlr for any datasets and if the error plot is not random follows a pattern. this indicates that forcing a line to model this data results in incorrect results. we need to introduce non-linear independent variables in the system so that the multiple linear regression method can use this non-linearity to produce the desired non-linear y_cap. this method is still linear regression but it is linear regression of non-linear independent variables. the method is known as polynomial regression as polynomial terms are introduced as independent variables to handle non-linearity in y. another term is feature engineering in which we introduced additional x variables to improve the performance of ml methods. there are two types of feature engineering 1) forward engineering - it starts with an empty feature set and iteratively adds one feature at a time based on their performance and 2) backward engineering - it starts with a complete set of features and removes features one by one until the model performance reaches a peak. both the techniques have their advantages and disadvantages and can be used in combination to optimise the feature selection process. next we talked about some non-parametric models like random forest, xg boost, knn, etc and their features. some glimpses of neural networks which is an example of parametric models are composed of neuronâ€™s and links with their weights. if we start increasing the layer it becomes deep learning but it needs more and more data but chances of overfitting also increases.
if the difference between r_squared of train and test data is more then it is case of overfitting. by comparing r_squared and mse value we can select best model. next we learned about the classification when y is nominal or ordinal values. if y and x are available then it is supervised learning. in logistic regression which is used for classification creates boundary along the data using sigmoid function.","summary today s class discussion start on a topic of how to improve the quality of results ? we can do this by 1) improving the sample like quality and size of sample 2) improving the method like using multiple methods and selecting the best one 3) fine tuning of methods or properly using the methods example gridsearch the new definition of linear regression is that outcome is expressed as a linear combination of independent variables. when using mlr for any datasets and if the error plot is not random follows a pattern. this indicates that forcing a line to model this data results in incorrect results. we need to introduce non-linear independent variables in the system so that the multiple linear regression method can use this non-linearity to produce the desired non-linear y_cap. this method is still linear regression but it is linear regression of non-linear independent variables. the method is known as polynomial regression as polynomial terms are introduced as independent variables to handle non-linearity in y. another term is feature engineering in which we introduced additional x variables to improve the performance of ml methods. there are two types of feature engineering 1) forward engineering - it starts with an empty feature set and iteratively adds one feature at a time based on their performance and 2) backward engineering - it starts with a complete set of features and removes features one by one until the model performance reaches a peak. both the techniques have their advantages and disadvantages and can be used in combination to optimise the feature selection process. next we talked about some non-parametric models like random forest, xg boost, knn, etc and their features. some glimpses of neural networks which is an example of parametric models are composed of neuron s and links with their weights. if we start increasing the layer it becomes deep learning but it needs more and more data but chances of overfitting also increases. if the difference between r_squared of train and test data is more then it is case of overfitting. by comparing r_squared and mse value we can select best model. next we learned about the classification when y is nominal or ordinal values. if y and x are available then it is supervised learning. in logistic regression which is used for classification creates boundary along the data using sigmoid function.",0,2.5435236,-3.8221018,9.694243,4.1422143,"models, feature, features"
99,"we learned that if we have a large number of features, then the correlation matrix is a good way to understand the pairwise relation between the parameters. 
then we compared the variance inflation factor(vif) of all the parameters.
then we aimed to reduce the parameters to reduce the complexity and computation power. we can do it by calculating the vif and removing parameters that have very high vif values. this helps reduce the complexity by minimal change in the model's accuracy.
then we studied principal component analysis. the no of principal components is equal to the dimensions of the dataset. all the principal components are orthogonal to each other.
then we understood the elbow diagram and came to know how many principal components are actually required to have a 95% confidence interval. we then accordingly removed features to simplify the dataset.
we then had a discussion about which thing to do first, vif or principal component analysis. we came to the conclusion that doing vif first is beneficial because multicollinearity is important to remove. 
also, we saw that we can write y as a function of principal components.
we do principal component analysis for three main reasons- dimension reduction, prediction models, and visualization.
then we had a look at t-distribution and t-distributed stochastic neighbour encoding(t-sne). it is a machine-learning algorithm for dimension reduction and data visualization. its main advantage is the ability to preserve local structure.","we learned that if we have a large number of features, then the correlation matrix is a good way to understand the pairwise relation between the parameters. then we compared the variance inflation factor(vif) of all the parameters. then we aimed to reduce the parameters to reduce the complexity and computation power. we can do it by calculating the vif and removing parameters that have very high vif values. this helps reduce the complexity by minimal change in the model's accuracy. then we studied principal component analysis. the no of principal components is equal to the dimensions of the dataset. all the principal components are orthogonal to each other. then we understood the elbow diagram and came to know how many principal components are actually required to have a 95% confidence interval. we then accordingly removed features to simplify the dataset. we then had a discussion about which thing to do first, vif or principal component analysis. we came to the conclusion that doing vif first is beneficial because multicollinearity is important to remove. also, we saw that we can write y as a function of principal components. we do principal component analysis for three main reasons- dimension reduction, prediction models, and visualization. then we had a look at t-distribution and t-distributed stochastic neighbour encoding(t-sne). it is a machine-learning algorithm for dimension reduction and data visualization. its main advantage is the ability to preserve local structure.",11,-15.555371,1.2598848,10.306501,12.855918,"pca, heatmap, heatmaps"
100,"today covered principal component analysis (pca) and its role in simplifying data while keeping the most valuable information. we started by looking at why itâ€™s important to remove correlated factors using the variance inflation factor (vif) before applying pca. from there, we explored how principal components (pcs) capture variance and how the elbow method helps determine the right number of pcs to use. we also discussed practical applications, like using pca for exploratory data analysis (eda), improving predictive models, and making data easier to interpret. a comparison between standard regression and principal component regression highlighted how pca can enhance model performance. we wrapped up by discussing its real-world impact and why itâ€™s such a useful tool in data analysis.","today covered principal component analysis (pca) and its role in simplifying data while keeping the most valuable information. we started by looking at why it s important to remove correlated factors using the variance inflation factor (vif) before applying pca. from there, we explored how principal components (pcs) capture variance and how the elbow method helps determine the right number of pcs to use. we also discussed practical applications, like using pca for exploratory data analysis (eda), improving predictive models, and making data easier to interpret. a comparison between standard regression and principal component regression highlighted how pca can enhance model performance. we wrapped up by discussing its real-world impact and why it s such a useful tool in data analysis.",11,-12.816691,3.1878085,10.120954,12.719225,"pca, heatmap, heatmaps"
101,"we started with discussing confusion matrix for multi class (before this, understand the meaning ascribed to rows and columns). in the midsem, there won't be derivations, it will include understanding the problem (then eda, visualisations). then we discussed crisp-dm (cross industry standard process for data mining) - 6 steps that run cyclically - business understanding, data understanding, data preparation, modeling, evaluation, and deployment. then ta discussed outliers and quartiles in boxplots. then we discussed missing values (mcar, mar and mnar). for handling missing data, we explored univariate methods (dropping rows/columns, replacing with mean, median, or mode) and multivariate approaches (knn, mice, regression models). you can detect and remove outliers if it lies outside inter-quantile range.","we started with discussing confusion matrix for multi class (before this, understand the meaning ascribed to rows and columns). in the midsem, there won't be derivations, it will include understanding the problem (then eda, visualisations). then we discussed crisp-dm (cross industry standard process for data mining) - 6 steps that run cyclically - business understanding, data understanding, data preparation, modeling, evaluation, and deployment. then ta discussed outliers and quartiles in boxplots. then we discussed missing values (mcar, mar and mnar). for handling missing data, we explored univariate methods (dropping rows/columns, replacing with mean, median, or mode) and multivariate approaches (knn, mice, regression models). you can detect and remove outliers if it lies outside inter-quantile range.",9,-16.635164,16.038038,8.627217,8.664057,"dataâ, analyse, analyses"
102,"in today's lecture, there was a discussion about population mean & variance and what is sample mean & variance and also how to evalute or estimate them. after that, we learned about central limit theorem with an example of estimation of working hours of a manager. for less sample size (typically less than thirty), we came to learn that how can we use t-distribution to analyze such data. there was also a discussion on z-distribution, p-value, confidence interval and multiple linear regression.","in today's lecture, there was a discussion about population mean & variance and what is sample mean & variance and also how to evalute or estimate them. after that, we learned about central limit theorem with an example of estimation of working hours of a manager. for less sample size (typically less than thirty), we came to learn that how can we use t-distribution to analyze such data. there was also a discussion on z-distribution, p-value, confidence interval and multiple linear regression.",7,33.024857,7.3902326,14.675019,2.5497668,"statistics, statistical, statisticsâ"
103,"touched upon how taylor expansion allows functions of x to be expressed as linear combination of powers of x

when we have just one independent variable to describe y- why not allow more variables to be included?- make variables as powers of x

this is the basis of polynomial regression

introduced to backward and forward selection
backward- get all features and keep eliminating features one-by-one

get data- preprocess- good data- multiple methods- compare- select best

is fitting one model necessary- answer is still ambiguous to me.

looked into a lot of models- like slr, svm, random forest, knn, ann and more
we were simultaneously classifying the models as parametric v/s non-parametric models

random forest good for prediction for the value of y, but lets say we want to find the delta that is change in y as we change x, then rf is not as helpful

what are neural networks?

learned about the basic of these models; special emphasis given on how anns work by associating weights to the various variables and how an ann with multiple layer is called deep learning model

moved on the next topic-
classification

it is also regression!!! ""logistic regression""?
what does the word regress mean- ""coming back to mediocrity"" (found this definition quite interesting)

logistic regression is about finding boundariesâ€¨classifier- gives you the boundary

once the model is made ie., the boundaries are determined we proceed by assigning metrics- false positive; false negative

labels: # distinct labels = # distinct classes

we are not predicting a continuous value here

we can very well have a line defining the boundary between the clusters

sigmoid function- gives 0 or 1 value- like a switch
s(a) is this functionâ€¨
a is obtained by the indep var and the weights associated with them- w1x1 + w2x2 + â€¦
so, interestingly, we need to find the weights- exactly like linear regression

using the training data to find w1, w2, â€¦","touched upon how taylor expansion allows functions of x to be expressed as linear combination of powers of x when we have just one independent variable to describe y- why not allow more variables to be included?- make variables as powers of x this is the basis of polynomial regression introduced to backward and forward selection backward- get all features and keep eliminating features one-by-one get data- preprocess- good data- multiple methods- compare- select best is fitting one model necessary- answer is still ambiguous to me. looked into a lot of models- like slr, svm, random forest, knn, ann and more we were simultaneously classifying the models as parametric v/s non-parametric models random forest good for prediction for the value of y, but lets say we want to find the delta that is change in y as we change x, then rf is not as helpful what are neural networks? learned about the basic of these models; special emphasis given on how anns work by associating weights to the various variables and how an ann with multiple layer is called deep learning model moved on the next topic- classification it is also regression!!! ""logistic regression""? what does the word regress mean- ""coming back to mediocrity"" (found this definition quite interesting) logistic regression is about finding boundaries classifier- gives you the boundary once the model is made ie., the boundaries are determined we proceed by assigning metrics- false positive; false negative labels: # distinct labels = # distinct classes we are not predicting a continuous value here we can very well have a line defining the boundary between the clusters sigmoid function- gives 0 or 1 value- like a switch s(a) is this function a is obtained by the indep var and the weights associated with them- w1x1 + w2x2 + so, interestingly, we need to find the weights- exactly like linear regression using the training data to find w1, w2,",0,4.5885463,-2.6314685,10.022607,4.117991,"models, feature, features"
104,"we kicked off by exploring session summary data with pivot tables, uncovering skewed distributions and potential outliers. we then tracked trends like summary length, submission frequency, and weekly patterns. moving on to a pressure and temperature dataset, we corrected autofilled zeros and used box plots to spot outliersâ€”keeping in mind that some outliers might hold valuable insights.

next, we dived into exploratory data analysis and got familiar with plotly in python. to wrap up, we went over assignment e3, highlighting common mistakes such as incorrect file naming, mislabeling graphs, and using scatter plots instead of bell curves for normal distributions.","we kicked off by exploring session summary data with pivot tables, uncovering skewed distributions and potential outliers. we then tracked trends like summary length, submission frequency, and weekly patterns. moving on to a pressure and temperature dataset, we corrected autofilled zeros and used box plots to spot outliers keeping in mind that some outliers might hold valuable insights. next, we dived into exploratory data analysis and got familiar with plotly in python. to wrap up, we went over assignment e3, highlighting common mistakes such as incorrect file naming, mislabeling graphs, and using scatter plots instead of bell curves for normal distributions.",9,-12.594502,24.38804,7.6879354,9.989871,"dataâ, analyse, analyses"
105,"we thoroughly discussed the midsem exam in today's lecture. it started with possible ways of gaining insights from the data like kde plots, checking the range of values in each column, filling in the missing values, box plots for detecting outliers etc. while checking the range of values, we came to know that there is a need for normalisation of data. also we generated box plots to know the number of rows of each ailment in the data. from this, a student also suggested to oversample the data for heart diseases as the no of heart diseases detected were very less. but as they were very less as compared to the other ailments, we did not use this. from the co relation heat map, there wasn't any linear dependency between the 24 columns but as discussed by the professor we cannot trust this every time. for this problem, we need to find the variance inflation factor and do an iterative process from which we came to know that only six columns are really independent and others can be written as a linear combination of these six. then some discussions on the confusion matrix and performance metrics were also done. towards the end, assessment of e3 was discussed by one of the ta and we concluded our lecture by having a small chat about curse of dimensionality that how sparsity increases with increasing dimensions and to overcome this we can have more data or reduce the dimensions of the features.","we thoroughly discussed the midsem exam in today's lecture. it started with possible ways of gaining insights from the data like kde plots, checking the range of values in each column, filling in the missing values, box plots for detecting outliers etc. while checking the range of values, we came to know that there is a need for normalisation of data. also we generated box plots to know the number of rows of each ailment in the data. from this, a student also suggested to oversample the data for heart diseases as the no of heart diseases detected were very less. but as they were very less as compared to the other ailments, we did not use this. from the co relation heat map, there wasn't any linear dependency between the 24 columns but as discussed by the professor we cannot trust this every time. for this problem, we need to find the variance inflation factor and do an iterative process from which we came to know that only six columns are really independent and others can be written as a linear combination of these six. then some discussions on the confusion matrix and performance metrics were also done. towards the end, assessment of e3 was discussed by one of the ta and we concluded our lecture by having a small chat about curse of dimensionality that how sparsity increases with increasing dimensions and to overcome this we can have more data or reduce the dimensions of the features.",9,-6.9447203,11.928823,9.547175,7.869192,"dataâ, analyse, analyses"
106,"i learned how to analyze submission data using pivot tables to gain insights into skewness, minimum and maximum values. an in-depth exploratory data analysis was conducted, including examining individual student summary lengths. with a new dataset, preliminary insights were explored, along with a discussion on edge computing. quartiles were reviewed and calculated after sorting data, while line plots (before and after outlier removal) and histograms helped visualize trends. binning was introduced as a method for converting continuous variables into discrete categories, and correlation heatmaps were used to identify relationships between parameters. trend line plots provided additional analytical insights, and there was also a brief discussion on assignment 2 submissions and some data analysis.","i learned how to analyze submission data using pivot tables to gain insights into skewness, minimum and maximum values. an in-depth exploratory data analysis was conducted, including examining individual student summary lengths. with a new dataset, preliminary insights were explored, along with a discussion on edge computing. quartiles were reviewed and calculated after sorting data, while line plots (before and after outlier removal) and histograms helped visualize trends. binning was introduced as a method for converting continuous variables into discrete categories, and correlation heatmaps were used to identify relationships between parameters. trend line plots provided additional analytical insights, and there was also a brief discussion on assignment 2 submissions and some data analysis.",6,-14.840399,24.964542,7.487563,9.697357,"summarizing, summarize, summarization"
107,"in the previous class, i learned that we rarely have access to an entire populationâ€™s data; instead, we work with a sample, known as a dataset. when training a model, we donâ€™t use the entire sample but randomly allocate about 80% of the data for training, while the remaining 20% is set aside for testing and comparing different models.

in general linear regression, we can have multiple features as well as multiple outputs, which can be expressed in matrix form.

to assess the variance captured per independent variable (degree of freedom), we adjust for the sample mean. since knowing the sample mean reduces the number of independent data points by one, the degree of freedom decreases. the sample mean is used in both ssr (sum of squares for regression) and sst (total sum of squares).","in the previous class, i learned that we rarely have access to an entire population s data; instead, we work with a sample, known as a dataset. when training a model, we don t use the entire sample but randomly allocate about 80% of the data for training, while the remaining 20% is set aside for testing and comparing different models. in general linear regression, we can have multiple features as well as multiple outputs, which can be expressed in matrix form. to assess the variance captured per independent variable (degree of freedom), we adjust for the sample mean. since knowing the sample mean reduces the number of independent data points by one, the degree of freedom decreases. the sample mean is used in both ssr (sum of squares for regression) and sst (total sum of squares).",2,13.609576,2.2192936,11.813112,4.731952,"regression, regressions, features"
108,"firstly we learned about area under the curve and probability of a specific value . 

then we learned about slr using python using sklearn and ols .
then we learned about mlr and various topics associated with it like division of data between train and test set , mlr using spreasheets and mlr using ols , feature selection in mlr . 

we learned about the interpretation of error in two models : intra and inter model interpretations 

at last we learned about handling non linearity using mlr","firstly we learned about area under the curve and probability of a specific value . then we learned about slr using python using sklearn and ols . then we learned about mlr and various topics associated with it like division of data between train and test set , mlr using spreasheets and mlr using ols , feature selection in mlr . we learned about the interpretation of error in two models : intra and inter model interpretations at last we learned about handling non linearity using mlr",13,4.0565004,3.386928,10.27911,5.0994105,"classification, classifying, classifications"
109,"approach to solving the midsem question paper
to systematically analyze the given problem, we followed a structured approach, starting with exploratory data analysis (eda). this involved examining the dataset for missing values, identifying key statistical measures such as the range, mean, median, 25th and 75th percentiles, and count, and understanding the overall distribution of the data.

upon detecting missing values, we explored various imputation strategies. given that the number of missing values was small, we opted to drop them. however, alternative approaches such as replacing them with the mean, median, or using linear interpolation could also be employed in other scenarios.

since the dataset primarily contained categorical features, we aimed to build a prediction model to classify ailments based on blood test results. to assess the interdependence among the features, we applied principal component analysis (pca) and visualized correlations using a heatmap of pairwise components. while the heatmap did not indicate significant correlation, it only accounted for pairwise relationships. to check for multicollinearity among multiple features, we computed the variance inflation factor (vif) by fitting regression models for each feature against all others and analyzing the râ² values.

given the presence of outliers and the nature of the data, we chose a tree-based model, specifically random forest, due to its robustness in handling non-linearity and outliers. since the dataset exhibited a large difference between minimum and maximum values, we applied normalization before feeding it into the model.

further analysis of the frequency distribution of categorical values revealed a significant class imbalance, with the heart disease class being heavily underrepresented. as expected, this imbalance led to poor classification performance, as reflected in the confusion matrix.

to validate our modelâ€™s generalizability, we applied the same analysis to another dataset. the results showed that the model did not fit well, suggesting that the two datasets were likely drawn from different underlying populations.

through this systematic process, we were able to derive key insights about data preprocessing, feature relationships, and model performance, emphasizing the importance of handling missing values, normalization, and addressing class imbalances in classification tasks.","approach to solving the midsem question paper to systematically analyze the given problem, we followed a structured approach, starting with exploratory data analysis (eda). this involved examining the dataset for missing values, identifying key statistical measures such as the range, mean, median, 25th and 75th percentiles, and count, and understanding the overall distribution of the data. upon detecting missing values, we explored various imputation strategies. given that the number of missing values was small, we opted to drop them. however, alternative approaches such as replacing them with the mean, median, or using linear interpolation could also be employed in other scenarios. since the dataset primarily contained categorical features, we aimed to build a prediction model to classify ailments based on blood test results. to assess the interdependence among the features, we applied principal component analysis (pca) and visualized correlations using a heatmap of pairwise components. while the heatmap did not indicate significant correlation, it only accounted for pairwise relationships. to check for multicollinearity among multiple features, we computed the variance inflation factor (vif) by fitting regression models for each feature against all others and analyzing the r values. given the presence of outliers and the nature of the data, we chose a tree-based model, specifically random forest, due to its robustness in handling non-linearity and outliers. since the dataset exhibited a large difference between minimum and maximum values, we applied normalization before feeding it into the model. further analysis of the frequency distribution of categorical values revealed a significant class imbalance, with the heart disease class being heavily underrepresented. as expected, this imbalance led to poor classification performance, as reflected in the confusion matrix. to validate our model s generalizability, we applied the same analysis to another dataset. the results showed that the model did not fit well, suggesting that the two datasets were likely drawn from different underlying populations. through this systematic process, we were able to derive key insights about data preprocessing, feature relationships, and model performance, emphasizing the importance of handling missing values, normalization, and addressing class imbalances in classification tasks.",9,-11.820691,12.405631,8.906915,8.03749,"dataâ, analyse, analyses"
110,"discussion on likelihood maximization, and weight calculation using gradient descent. the confusion matrix was introduced, explaining accuracy, precision, recall, and the f1-score. we also addressed common student questions on clustering, regression models, and histograms. lastly, we reviewed assignment e1, highlighting mistakes such as misusing the kurtosis function and incorrect model comparisons. emphasis was placed on submitting well-organized excel files with proper documentation and formulas.","discussion on likelihood maximization, and weight calculation using gradient descent. the confusion matrix was introduced, explaining accuracy, precision, recall, and the f1-score. we also addressed common student questions on clustering, regression models, and histograms. lastly, we reviewed assignment e1, highlighting mistakes such as misusing the kurtosis function and incorrect model comparisons. emphasis was placed on submitting well-organized excel files with proper documentation and formulas.",13,3.2823246,-12.593773,9.413036,5.124974,"classification, classifying, classifications"
111,"we began by looking at how outliers affect the data plots, and how much of a difference it can make (we saw a graphical representation after removing outliers and rescaling to better understand the data). we also saw how noise can change the signal data and in order to fix it we started learning about data smoothing. fluctuations due to noise can make it difficult to identify trends and patterns, and can even lead to wrong analysis, hence the need for cleaning. sma stands for simple moving average, it a moving window average, it's a form of low effort filter, to filter and check data. we start with let's say 50 data points and underlying trend is visible after that, then we have a higher window size to do more smoothing. when we come to the end points we can choose to stop the moving average according to the type of method we're choosing (if we say that moving average of a window is at the left most point of the window, then we'll have to ignore the right most end point). there are other methods like exponential moving averages that weigh nearby samples more, also used in time series forecasting. then we observed the clustering of data, and again we went through the data of session summary and saw the graphs of plotted values of number of submissions and the length of summaries. we also saw standardized and normalized transformations. we saw how a skewed distribution can turn into normal distribution with transformation, we should be aware when to apply the transformation and how it'll be useful. then we to got to see kepler exoplanet data, plotting the flux values for stars belonging to two classes. data imbalance, and we saw techniques to see data imbalance, either by under-sampling the majority class or over-sampling the minority. or, a more interesting approach is to generate synthetic data for minority, which is a type of oversampling. we understood how smote oversamples the minority class, minority classes with samples with nearest neighbor being minority class are removed.","we began by looking at how outliers affect the data plots, and how much of a difference it can make (we saw a graphical representation after removing outliers and rescaling to better understand the data). we also saw how noise can change the signal data and in order to fix it we started learning about data smoothing. fluctuations due to noise can make it difficult to identify trends and patterns, and can even lead to wrong analysis, hence the need for cleaning. sma stands for simple moving average, it a moving window average, it's a form of low effort filter, to filter and check data. we start with let's say 50 data points and underlying trend is visible after that, then we have a higher window size to do more smoothing. when we come to the end points we can choose to stop the moving average according to the type of method we're choosing (if we say that moving average of a window is at the left most point of the window, then we'll have to ignore the right most end point). there are other methods like exponential moving averages that weigh nearby samples more, also used in time series forecasting. then we observed the clustering of data, and again we went through the data of session summary and saw the graphs of plotted values of number of submissions and the length of summaries. we also saw standardized and normalized transformations. we saw how a skewed distribution can turn into normal distribution with transformation, we should be aware when to apply the transformation and how it'll be useful. then we to got to see kepler exoplanet data, plotting the flux values for stars belonging to two classes. data imbalance, and we saw techniques to see data imbalance, either by under-sampling the majority class or over-sampling the minority. or, a more interesting approach is to generate synthetic data for minority, which is a type of oversampling. we understood how smote oversamples the minority class, minority classes with samples with nearest neighbor being minority class are removed.",9,-19.050108,11.797145,10.052964,10.066337,"dataâ, analyse, analyses"
112,"lecture started with population and sample definitions. the population being a very large data set cannot be used for all operations and inference derivation hence a smaller representative data set, a sample  is selected. descriptive and detailed statistics are conducted on sample and the inferences are held to be accurate for the entire population set.
line and point models were seen. simple regression model examples were studied - y = b0 + b1x. here x is called the predictor and b0 and b1 the parameters of the regression model. confidence interval can be found by minimising the b0 term often known as bias. the best fit line is where the mean of the response and predictor values is located and the sum of the squares of the errors is minimised to reduce the error.","lecture started with population and sample definitions. the population being a very large data set cannot be used for all operations and inference derivation hence a smaller representative data set, a sample is selected. descriptive and detailed statistics are conducted on sample and the inferences are held to be accurate for the entire population set. line and point models were seen. simple regression model examples were studied - y = b0 + b1x. here x is called the predictor and b0 and b1 the parameters of the regression model. confidence interval can be found by minimising the b0 term often known as bias. the best fit line is where the mean of the response and predictor values is located and the sum of the squares of the errors is minimised to reduce the error.",1,31.35369,-5.152097,15.696223,4.034834,"population, models, estimating"
113,"when we have only sample of population (say 30 observations), how do we estimate population statistics like mean ? the idea is to assume that sample mean is close to population mean. we know that sampling distribution of the mean is normal(mu,sigma), get the sample std deviation and assume it is close to population std dev. 
we want to get the interval within which the population mean likely to lie.
""95% confidence interval"" : if you take 100 samples, the mean of 95 of those samples lie in this interval.
p-value  is area under the curve which lie outside of confidence interval.
multiple linear regression : y = b_0 + b_1x_1 + b_2x_2+...b_kx_k
anova : used to compare statistical equivalence of multiple averages simultaneously
f-statistic = mean squared regression/ mean squared error -> should be large.","when we have only sample of population (say 30 observations), how do we estimate population statistics like mean ? the idea is to assume that sample mean is close to population mean. we know that sampling distribution of the mean is normal(mu,sigma), get the sample std deviation and assume it is close to population std dev. we want to get the interval within which the population mean likely to lie. ""95% confidence interval"" : if you take 100 samples, the mean of 95 of those samples lie in this interval. p-value is area under the curve which lie outside of confidence interval. multiple linear regression : y = b_0 + b_1x_1 + b_2x_2+...b_kx_k anova : used to compare statistical equivalence of multiple averages simultaneously f-statistic = mean squared regression/ mean squared error -> should be large.",7,37.18173,3.0306737,15.334333,2.4521573,"statistics, statistical, statisticsâ"
114,"today's class start with discussion of population and sample. we want sample to estimate/predict population. for better results sample should be good and representative.
we can calculate count (frequency), mode, mean, median, standard deviation, variance.
we can also do addition, subtraction, multiplication and division. if we calculate attributes from sample then it is known as statistics and when we calculate attributes from population then it is known as parameters. the second important thing what we have learnt today is simple linear regression. we took a example of sales v/s advertisements data and created a scatter plot and draws the best fit line having equation y=b0 + b1x.  here y represents dependent variable/response variable/label and x represents independent variable/feature and the b0 represents the bias which arises due to unknown variables which influencing the model. it has only one predictor. one amazing information is that we can also represents this line as a point but it is not correct model to represent the whole data. even a point can be considered as a model although a very naive model. we do not know all variables. in the equation y=b0 + b1x, b0 and b1 are the estimates of the population parameter. also, we have different model based on different sample. the difference between predicated y^ and y is error and we take sum of all squared error to minimize the error. we can't take only sum of errors as it nullify each other and there is of no use. a confidence interval (ci) is an interval which is expected to typically contain the parameter being estimated. important reasons behind taking sum of squared error is that it doesn't magnify the error and also doesn't differentiate between directions. we have 'closed form' solution to calculate the value of a, b and b0 and b1 are known as point estimates. one last thing is that we need to arrive at the possible interval with which these values lies such that there is a very high chances that b0p and b1p will lies within those intervals respectively.","today's class start with discussion of population and sample. we want sample to estimate/predict population. for better results sample should be good and representative. we can calculate count (frequency), mode, mean, median, standard deviation, variance. we can also do addition, subtraction, multiplication and division. if we calculate attributes from sample then it is known as statistics and when we calculate attributes from population then it is known as parameters. the second important thing what we have learnt today is simple linear regression. we took a example of sales v/s advertisements data and created a scatter plot and draws the best fit line having equation y=b0 + b1x. here y represents dependent variable/response variable/label and x represents independent variable/feature and the b0 represents the bias which arises due to unknown variables which influencing the model. it has only one predictor. one amazing information is that we can also represents this line as a point but it is not correct model to represent the whole data. even a point can be considered as a model although a very naive model. we do not know all variables. in the equation y=b0 + b1x, b0 and b1 are the estimates of the population parameter. also, we have different model based on different sample. the difference between predicated y^ and y is error and we take sum of all squared error to minimize the error. we can't take only sum of errors as it nullify each other and there is of no use. a confidence interval (ci) is an interval which is expected to typically contain the parameter being estimated. important reasons behind taking sum of squared error is that it doesn't magnify the error and also doesn't differentiate between directions. we have 'closed form' solution to calculate the value of a, b and b0 and b1 are known as point estimates. one last thing is that we need to arrive at the possible interval with which these values lies such that there is a very high chances that b0p and b1p will lies within those intervals respectively.",1,35.49193,-9.134747,16.564592,3.8465083,"population, models, estimating"
115,"in todays lecture, the broad topics were
1. the core of machine learning i.e. y = f(x)
- the data available is visualised and the crux is techinques to predict a function as close to the original functions
2. introduction to levels of measurement (nominal, ordinal, interval, ratio)
-nominal mainly represents equality i.e. equal or not equal.
-ordinal represents inequalities (<= or >=) with both being discrete
-interval represents a continuous data where reference i.e. 0 is not defined ex: temperature,   year
-ratio represents a continuous data type where a reference is defined i.e. ratio is defined.
3. machine learning algorithms of various data types (classifcation for nominal and ordinal data type, regression for interval and ratio data type)
-for classification, the ordinal data type is mainly encoded into one hot encodings based on number of classes.
4. what is supervised (label present or y is known while training), unsupervised (label is absent or y is unknown as the case with majority of real world data)
5. various techniques like simple linear regression, multiple linear regression, random forest algorithm, logistic regression, k-means clustering, hierarchical clustering.","in todays lecture, the broad topics were 1. the core of machine learning i.e. y = f(x) - the data available is visualised and the crux is techinques to predict a function as close to the original functions 2. introduction to levels of measurement (nominal, ordinal, interval, ratio) -nominal mainly represents equality i.e. equal or not equal. -ordinal represents inequalities (<= or >=) with both being discrete -interval represents a continuous data where reference i.e. 0 is not defined ex: temperature, year -ratio represents a continuous data type where a reference is defined i.e. ratio is defined. 3. machine learning algorithms of various data types (classifcation for nominal and ordinal data type, regression for interval and ratio data type) -for classification, the ordinal data type is mainly encoded into one hot encodings based on number of classes. 4. what is supervised (label present or y is known while training), unsupervised (label is absent or y is unknown as the case with majority of real world data) 5. various techniques like simple linear regression, multiple linear regression, random forest algorithm, logistic regression, k-means clustering, hierarchical clustering.",4,-24.655693,-16.380014,1.5310014,0.12144328,"classification, classifying, classifications"
116,"after dropping the outliers â†’ after re-scaling the display. how do we eliminate noise? â†’ if makes it difficult to find a trend in data â†’ simple moving averages. consider a window around every data point and avg the value, window width can be varied to adjust the level of smoothing. (higher window size â†’ more smoothing). sma â†’ filling up missing value, removing outliers, eliminating noise. exponential moving averages â†’ works better time series data. we take value â†’ on both sides â†’ or only on one side. # first remove the outliers.
standardization and normalization of data. linear regression is immune to data scaling but gradient descent not. â†’ value btw [0,1] â†’ but regression statistic does not change. clustering algorithm based on euclidean distance will greatly influence by scaling the data. normalization â†’ [0,1] (xn = (x - xmin) / (xmax - xmin)). standardization â†’ ""standard normal distribution"" â†’ n(î¼,ïƒ) â†’ (0,1). # it does not change the shape of distribution of data. box cox transformation. log transformation â†’ heteroscedasticity (variance is changing).
data imbalance - certain instance of a data might show up more freq than others. ex â†’ rare disease diagnosis, forgery. â†’ undersample the majority class â†’ oversample the minority class. (1) smote. # tomek links are used to refine data.","after dropping the outliers after re-scaling the display. how do we eliminate noise? if makes it difficult to find a trend in data simple moving averages. consider a window around every data point and avg the value, window width can be varied to adjust the level of smoothing. (higher window size more smoothing). sma filling up missing value, removing outliers, eliminating noise. exponential moving averages works better time series data. we take value on both sides or only on one side. # first remove the outliers. standardization and normalization of data. linear regression is immune to data scaling but gradient descent not. value btw [0,1] but regression statistic does not change. clustering algorithm based on euclidean distance will greatly influence by scaling the data. normalization [0,1] (xn = (x - xmin) / (xmax - xmin)). standardization ""standard normal distribution"" n( , ) (0,1). # it does not change the shape of distribution of data. box cox transformation. log transformation heteroscedasticity (variance is changing). data imbalance - certain instance of a data might show up more freq than others. ex rare disease diagnosis, forgery. undersample the majority class oversample the minority class. (1) smote. # tomek links are used to refine data.",9,-20.758419,10.546941,10.122957,10.075226,"dataâ, analyse, analyses"
117,"we started today lecture with a small recap of previous lecture and looking at the problem in finding solution that are multicollinearity and matrix inversion. we then started with the splitting of sample which we do in 80-20 manner using the 80% dataset as training data and other 20% as test data. the split is done randomly. our split should be such that the r squared value of training data is nearly same as that of the test data. if r squared value of training data is considerably higher than that of test data, the model is said to be overfitted. we then discussed about multiple regression parameters which is given by excel. multiple r is root of r squared, adjusted r squared gives an idea of how much variance is captured per variable. we then moved on to the python part. we had a brief discussion on the different libraries in python like pandas, scikit learn, statsmodels etc. we concluded our lecture with a small discussion on aic and bic values, jarque bera test.","we started today lecture with a small recap of previous lecture and looking at the problem in finding solution that are multicollinearity and matrix inversion. we then started with the splitting of sample which we do in 80-20 manner using the 80% dataset as training data and other 20% as test data. the split is done randomly. our split should be such that the r squared value of training data is nearly same as that of the test data. if r squared value of training data is considerably higher than that of test data, the model is said to be overfitted. we then discussed about multiple regression parameters which is given by excel. multiple r is root of r squared, adjusted r squared gives an idea of how much variance is captured per variable. we then moved on to the python part. we had a brief discussion on the different libraries in python like pandas, scikit learn, statsmodels etc. we concluded our lecture with a small discussion on aic and bic values, jarque bera test.",2,5.327348,5.127845,10.641506,5.123911,"regression, regressions, features"
118,"today's lecture was mainly aimed at giving us a first hands on experience with data. we worked with a sample dataset on excel, where we created scatter plots and tried to implement simple linear regression. we used a tool in excel called the data analysis toolpak, which gives us a lot of information about our data and the linear regression statistics. next we discussed about histograms, which is a frequency chart showing the frequency of data distributed into various bins. 
then we studied that in an ideal case, our model should be able to harness all the predictable patterns in the data, leaving the noise or errors to be random. however, if we are able to predict the errors, that means that our model has not captured the trend between the errors. we also studied that if our outcome is dependent on a large number of unknown causes, then the distribution observed is known as a gaussian normal distribution. 
we went on to discuss that a good model is one which can explain most of the variations in our data. we defined 3 terms:
sst = measure of the total variance of the data
sse = sum of squares of the errors / noise variance
ssr = sum of squares of the total variance captured by the regression model 
then we derived a relation between these three terms to be that sst = ssr + sse. now when we divide both the sides by sst, we get a term ssr / sst on the rhs. this term is defined as the coefficient of determination, or r^2. this term tells us how close our model is in measuring the actual variance in the data. this term has a maximum value of 1, and a minimum value of 0. this term should be as close to 1 as possible, which indicates a good model. 
the term is defined as r^2, because for simple linear regression, the coefficient of determination is equal to the square of the correlation coefficient. since correlation coefficient is termed as r, the cod is termed as r^2. however this result doesn't hold true for multiple linear regression. the correlation coefficient is defined as the measure of how y changes with respect to its mean as x changes with respect to its mean. 
we moved on to define a special histogram, which captures the frequency of means of various samples of a given data. such a histogram is called the sampling distribution of the sample mean. it tells us that if we have a good representative sample, then its mean will lie very close to the mean of the population.","today's lecture was mainly aimed at giving us a first hands on experience with data. we worked with a sample dataset on excel, where we created scatter plots and tried to implement simple linear regression. we used a tool in excel called the data analysis toolpak, which gives us a lot of information about our data and the linear regression statistics. next we discussed about histograms, which is a frequency chart showing the frequency of data distributed into various bins. then we studied that in an ideal case, our model should be able to harness all the predictable patterns in the data, leaving the noise or errors to be random. however, if we are able to predict the errors, that means that our model has not captured the trend between the errors. we also studied that if our outcome is dependent on a large number of unknown causes, then the distribution observed is known as a gaussian normal distribution. we went on to discuss that a good model is one which can explain most of the variations in our data. we defined 3 terms: sst = measure of the total variance of the data sse = sum of squares of the errors / noise variance ssr = sum of squares of the total variance captured by the regression model then we derived a relation between these three terms to be that sst = ssr + sse. now when we divide both the sides by sst, we get a term ssr / sst on the rhs. this term is defined as the coefficient of determination, or r^2. this term tells us how close our model is in measuring the actual variance in the data. this term has a maximum value of 1, and a minimum value of 0. this term should be as close to 1 as possible, which indicates a good model. the term is defined as r^2, because for simple linear regression, the coefficient of determination is equal to the square of the correlation coefficient. since correlation coefficient is termed as r, the cod is termed as r^2. however this result doesn't hold true for multiple linear regression. the correlation coefficient is defined as the measure of how y changes with respect to its mean as x changes with respect to its mean. we moved on to define a special histogram, which captures the frequency of means of various samples of a given data. such a histogram is called the sampling distribution of the sample mean. it tells us that if we have a good representative sample, then its mean will lie very close to the mean of the population.",5,23.336376,-1.1782094,13.967345,4.3618917,"regression, statistical, statistics"
119,"today's lecture did not cover any significant new topics. most of our lecture revolved around the midsem problem discussion where we had an overview of the solution in step by step manner. we then were joined by tas which gave review on the e3 exercise. we then started a new topic about the curse of dimensionality. this arises when there are too less data chasing too many features. the consequences of these are overfitting, increased sparsity, increased complexity, increased computational cost and distance distortion. we then had a discussion on the heat map which we created in midsem we did not show any correlation between any variables, which was very deceptive as some variables were multicollinear. we dealt with it by measuring r square between all xi and xj where iâ‰ j in a manner such that it checks the linear combinations for all possible patterns. we then concluded our lecture with a brief discussion on vif.","today's lecture did not cover any significant new topics. most of our lecture revolved around the midsem problem discussion where we had an overview of the solution in step by step manner. we then were joined by tas which gave review on the e3 exercise. we then started a new topic about the curse of dimensionality. this arises when there are too less data chasing too many features. the consequences of these are overfitting, increased sparsity, increased complexity, increased computational cost and distance distortion. we then had a discussion on the heat map which we created in midsem we did not show any correlation between any variables, which was very deceptive as some variables were multicollinear. we dealt with it by measuring r square between all xi and xj where i j in a manner such that it checks the linear combinations for all possible patterns. we then concluded our lecture with a brief discussion on vif.",13,-6.34694,9.98656,9.460775,7.4147305,"classification, classifying, classifications"
120,"good data
	â€¢	key principles:
	â€¢	machine learning models thrive on high-quality data that is representative of the problem space.
	â€¢	properly prepared data minimizes noise and bias, ensuring better predictions and generalization.
	â€¢	characteristics:
	â€¢	relevant features, sufficient data size, and balanced representation of classes or conditions are critical.
	â€¢	handling missing values, outliers, and ensuring feature scaling or normalization are part of good data preparation.

simple linear regression (slr)
	â€¢	concept: slr models the relationship between two variables (one independent and one dependent) as a straight line, defined by the equation  y = \beta_0 + \beta_1 x , where:
	â€¢	 y : dependent variable (response).
	â€¢	 x : independent variable (predictor).
	â€¢	 \beta_0 : intercept.
	â€¢	 \beta_1 : slope (rate of change in  y  with respect to  x ).
	â€¢	point estimates:
	â€¢	coefficients  \beta_0  and  \beta_1  are calculated from the sample data.
	â€¢	they serve as estimates for the true population parameters.
	â€¢	while exact coincidence with the population parameters is unlikely, they offer useful approximations.
	â€¢	derivation of coefficients:
	â€¢	the estimates  \hat{\beta}_0  (intercept) and  \hat{\beta}_1  (slope) are derived using the least squares method, which minimizes the sum of squared residuals (differences between observed and predicted values).

\hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}, \quad
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}.

	â€¢	here,  \bar{x}  and  \bar{y}  are the means of the  x  and  y  data, respectively.
	â€¢	interval estimates:
	â€¢	beyond point estimates, interval estimates provide confidence intervals (e.g., 95%) for  \beta_0  and  \beta_1 , offering a range within which the true population parameters are likely to lie.","good data key principles: machine learning models thrive on high-quality data that is representative of the problem space. properly prepared data minimizes noise and bias, ensuring better predictions and generalization. characteristics: relevant features, sufficient data size, and balanced representation of classes or conditions are critical. handling missing values, outliers, and ensuring feature scaling or normalization are part of good data preparation. simple linear regression (slr) concept: slr models the relationship between two variables (one independent and one dependent) as a straight line, defined by the equation y = \beta_0 + \beta_1 x , where: y : dependent variable (response). x : independent variable (predictor). \beta_0 : intercept. \beta_1 : slope (rate of change in y with respect to x ). point estimates: coefficients \beta_0 and \beta_1 are calculated from the sample data. they serve as estimates for the true population parameters. while exact coincidence with the population parameters is unlikely, they offer useful approximations. derivation of coefficients: the estimates \hat{\beta}_0 (intercept) and \hat{\beta}_1 (slope) are derived using the least squares method, which minimizes the sum of squared residuals (differences between observed and predicted values). \hat{\beta}_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}, \quad \hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}. here, \bar{x} and \bar{y} are the means of the x and y data, respectively. interval estimates: beyond point estimates, interval estimates provide confidence intervals (e.g., 95%) for \beta_0 and \beta_1 , offering a range within which the true population parameters are likely to lie.",1,29.820786,-8.816674,16.221283,4.4067593,"population, models, estimating"
121,"we started with doubts in the google form and then explored logistic regression, focusing on how different variables influence outcomes and how to adjust weights to prevent overfitting. then we discussed evaluation metrics, including confusion matrix and accuracy. lastly, the tas gave feedback on assignment 1.","we started with doubts in the google form and then explored logistic regression, focusing on how different variables influence outcomes and how to adjust weights to prevent overfitting. then we discussed evaluation metrics, including confusion matrix and accuracy. lastly, the tas gave feedback on assignment 1.",13,3.7217178,-12.240374,9.329308,5.060538,"classification, classifying, classifications"
122,"### summary of todayâ€™s class: excel tools & simple linear regression (slr)

### **working with excel**

we spent about an hour exploring excel's functionalities for plotting, statistical calculations, and visualizing regression.

1. **statistical calculations:**
    - calculated metrics such as mean (xë‰,yë‰), standard deviation (xstd, ystd), and extra means (xy_).
    - used these values with the slr formula (derived in the previous class) to estimate the parameters of the regression line.
2. **visualization:**
    - plotted the regression line and visualized the errors (differences between actual and predicted values).

### **simple linear regression (slr) in depth:**

1. **prediction range for slr:**
    - predictions are reliable only within the range of the training data. slr does not extrapolate well beyond this range.
    - for predictions outside this range, advanced techniques like time series analysis are recommended.
2. **understanding plots:**
    - **histogram:** shows the frequency of values within bins.
    - **scatter plot:** visualizes data points on a 2d plane using two coordinates.
3. **error analysis:**
    - after fitting the regression line, we calculated the error for each data point and plotted its histogram.
    - **key insight:** if the error histogram follows a normal distribution, it suggests the model captures the overall relationship between input and output well.
    - in cases where errors show a pattern (e.g., parabolic input-output relationships), the regression line is not a good fit.
4. **excelâ€™s data analysis toolkit:**
    - provides key regression metrics like standard error, r2, adjusted r2, multiple r, confidence intervals, p-values, and t-values for the parameters.
5. **evaluating model performance:**
    - r square(coefficient of determination): represents the proportion of variance explained by the regression model relative to the total variance.
    - **why r square is called r square?** in slr, r2 equals the square of the correlation coefficient between input x and output y (denoted by r). however, this equivalence does not hold for multiple regression.
6. **what is standard error?**
    - the standard error measures how much the sample mean deviates, on average, from the population mean. as the sample size increases, uncertainty in predictions decreases, leading to a more accurate model. this is why a lower standard error indicates more precise predictions and a better-fitting model.
    - **given 100 observations, should we treat it as 1 sample of size 100 or 10 samples of size 10?**itâ€™s better to treat it as 1 sample of size 100. the standard error, which is equal to sigma^2 / n, will be smaller with a larger sample size n. this results in higher precision and less variability in predictions.","### summary of today s class: excel tools & simple linear regression (slr) ### **working with excel** we spent about an hour exploring excel's functionalities for plotting, statistical calculations, and visualizing regression. 1. **statistical calculations:** - calculated metrics such as mean (x ,y ), standard deviation (xstd, ystd), and extra means (xy_). - used these values with the slr formula (derived in the previous class) to estimate the parameters of the regression line. 2. **visualization:** - plotted the regression line and visualized the errors (differences between actual and predicted values). ### **simple linear regression (slr) in depth:** 1. **prediction range for slr:** - predictions are reliable only within the range of the training data. slr does not extrapolate well beyond this range. - for predictions outside this range, advanced techniques like time series analysis are recommended. 2. **understanding plots:** - **histogram:** shows the frequency of values within bins. - **scatter plot:** visualizes data points on a 2d plane using two coordinates. 3. **error analysis:** - after fitting the regression line, we calculated the error for each data point and plotted its histogram. - **key insight:** if the error histogram follows a normal distribution, it suggests the model captures the overall relationship between input and output well. - in cases where errors show a pattern (e.g., parabolic input-output relationships), the regression line is not a good fit. 4. **excel s data analysis toolkit:** - provides key regression metrics like standard error, r2, adjusted r2, multiple r, confidence intervals, p-values, and t-values for the parameters. 5. **evaluating model performance:** - r square(coefficient of determination): represents the proportion of variance explained by the regression model relative to the total variance. - **why r square is called r square?** in slr, r2 equals the square of the correlation coefficient between input x and output y (denoted by r). however, this equivalence does not hold for multiple regression. 6. **what is standard error?** - the standard error measures how much the sample mean deviates, on average, from the population mean. as the sample size increases, uncertainty in predictions decreases, leading to a more accurate model. this is why a lower standard error indicates more precise predictions and a better-fitting model. - **given 100 observations, should we treat it as 1 sample of size 100 or 10 samples of size 10?**it s better to treat it as 1 sample of size 100. the standard error, which is equal to sigma^2 / n, will be smaller with a larger sample size n. this results in higher precision and less variability in predictions.",5,20.943068,-6.1137786,14.468878,4.9030085,"regression, statistical, statistics"
123,"i learn about the add in called data analysis toolpak on excel. we used it to show regression summary for the data sheet provided and we also used average function to find xbar, year etc and hence find simple linear regression coefficients. we also plotted a scatter plot for y vs x and error and a histogram for error too. i also learnt that for simple linear reg, square of correlation coefficient = coefficient of determination","i learn about the add in called data analysis toolpak on excel. we used it to show regression summary for the data sheet provided and we also used average function to find xbar, year etc and hence find simple linear regression coefficients. we also plotted a scatter plot for y vs x and error and a histogram for error too. i also learnt that for simple linear reg, square of correlation coefficient = coefficient of determination",5,18.481825,-3.4413266,13.996562,5.2068076,"regression, statistical, statistics"
124,"observed a neural network predicting boundaries for datasets on playground.tensorflow.org, discussed about precision, accuracy, recall and f1 score. true positive and false positive rates, tp rate = (tp/(tp+fn)), fp rate = (fp/(fp+tn)), accuracy = (tp+tn)/(tp+fp+tn+fn). creating a classifier using two distributions (for positive and negative) and discussing how data can be wrongly classified in the intersection region of the two distributions. quality of classifier based on roc characteristics (sharper curve => better classifier, 45â° line => no classifier), classifier becoming better with addition of features. auc = area under the roc curve (closer the auc is to 1, better is the classifier), auc = 0.5 => no classifier. precision = (detected of a class and correctly classified observations/total observations in the class). recall = (detected of a class and correctly classified observations/total number of observations classified as being of that class). clustering introduction as unsupervised learning where label is not required (y = f(x), y not needed). k means clustering and hierarchical clustering.","observed a neural network predicting boundaries for datasets on playground.tensorflow.org, discussed about precision, accuracy, recall and f1 score. true positive and false positive rates, tp rate = (tp/(tp+fn)), fp rate = (fp/(fp+tn)), accuracy = (tp+tn)/(tp+fp+tn+fn). creating a classifier using two distributions (for positive and negative) and discussing how data can be wrongly classified in the intersection region of the two distributions. quality of classifier based on roc characteristics (sharper curve => better classifier, 45 line => no classifier), classifier becoming better with addition of features. auc = area under the roc curve (closer the auc is to 1, better is the classifier), auc = 0.5 => no classifier. precision = (detected of a class and correctly classified observations/total observations in the class). recall = (detected of a class and correctly classified observations/total number of observations classified as being of that class). clustering introduction as unsupervised learning where label is not required (y = f(x), y not needed). k means clustering and hierarchical clustering.",8,-0.057229683,-20.766369,6.900914,0.47572708,"classification, clusterings, classifying"
125,"machine learning techniques:
1.	simple linear regression
2.	multiple linear regression
3.	logistic regression
4.	random forest

unsupervised learning:
1.	k-means clustering
2.	hierarchical clustering 
levels of measurement: (4 levels)
â€¢	nominal: (discrete)
â€¢	no ordering 
â€¢	used for categorization or classification
â€¢	ordinal: (discrete)
â€¢	inherently ordering is present
â€¢	we should not assign numbers to nominal and ordinal levels of measurement
â€¢	interval: (continuous)
â€¢	fundamentally temperature is a continuous quantity but discrete due to instrumentation.
â€¢	reference point is arbitrary
â€¢	ratio: (continuous)

y=f(x)
here, y is the label and x is the feature.
if both the labels and features are present then it is called â€œsupervised learningâ€. however if the data is raw and the labels are not present the it is called â€œunsupervised learningâ€.","machine learning techniques: 1. simple linear regression 2. multiple linear regression 3. logistic regression 4. random forest unsupervised learning: 1. k-means clustering 2. hierarchical clustering levels of measurement: (4 levels) nominal: (discrete) no ordering used for categorization or classification ordinal: (discrete) inherently ordering is present we should not assign numbers to nominal and ordinal levels of measurement interval: (continuous) fundamentally temperature is a continuous quantity but discrete due to instrumentation. reference point is arbitrary ratio: (continuous) y=f(x) here, y is the label and x is the feature. if both the labels and features are present then it is called supervised learning . however if the data is raw and the labels are not present the it is called unsupervised learning .",4,-23.167887,-16.739388,1.8163092,0.32201776,"classification, classifying, classifications"
126,"we started with logistic regression and found out how we could find the weights that will drive our predictions. our aim is to bring our predicted results as close to the real targets as possible. when the actual value is 1, we want our predicted probability to be high; if it is 0, then low. this requires taking that into consideration for all of the observations in our training data.

since dealing with product terms can grow complicated, we take the log of the likelihood function, and turn multiplication into addition, so turning things over is significantly easier. maximizing this gives us the minimum amount of error possible, and hence finds us the best fit for our model.

for analysing whether or not our model was a good fit, we use a confusion matrix, which categorizes predictions into four groups:

- true negatives (tn):correctly predicted negatives 

- false positives (fp):positives wrongly predicted (false alarms) 

- false negatives (fn):negatives wrongly predicted (missing actual positives) 

- true positives (tp):correctly classified as positive 

by using all of this, we get a number of valuable performance metrics: 
- the ratio of the correctness of outputs from the model for majority of the cases. 
accuracy= (tp + tn)/total count of observations

- precision:when the model indicates something is positive, how frequently is it truly accurate? 

- recall:among all the true positive instances, how many did the model accurately recognize? 

- f1-score:a compromise between precision and recall. unlike accuracy, which may be deceptive in the presence of imbalanced data, the f1-score accounts for both precision and recall, avoiding misplaced confidence in the model's effectiveness.

in summary, precision indicates the trustworthiness of our positive predictions, recall measures our effectiveness in identifying positives, and the f1-score guarantees we remain realistic about our accuracy.","we started with logistic regression and found out how we could find the weights that will drive our predictions. our aim is to bring our predicted results as close to the real targets as possible. when the actual value is 1, we want our predicted probability to be high; if it is 0, then low. this requires taking that into consideration for all of the observations in our training data. since dealing with product terms can grow complicated, we take the log of the likelihood function, and turn multiplication into addition, so turning things over is significantly easier. maximizing this gives us the minimum amount of error possible, and hence finds us the best fit for our model. for analysing whether or not our model was a good fit, we use a confusion matrix, which categorizes predictions into four groups: - true negatives (tn):correctly predicted negatives - false positives (fp):positives wrongly predicted (false alarms) - false negatives (fn):negatives wrongly predicted (missing actual positives) - true positives (tp):correctly classified as positive by using all of this, we get a number of valuable performance metrics: - the ratio of the correctness of outputs from the model for majority of the cases. accuracy= (tp + tn)/total count of observations - precision:when the model indicates something is positive, how frequently is it truly accurate? - recall:among all the true positive instances, how many did the model accurately recognize? - f1-score:a compromise between precision and recall. unlike accuracy, which may be deceptive in the presence of imbalanced data, the f1-score accounts for both precision and recall, avoiding misplaced confidence in the model's effectiveness. in summary, precision indicates the trustworthiness of our positive predictions, recall measures our effectiveness in identifying positives, and the f1-score guarantees we remain realistic about our accuracy.",10,13.162704,-22.397034,8.705197,-1.6152505,"classifications, histograms, histogram"
127,"today we started with discussing more upon logistic regression. we talked about logistic unit, which is basically the probability of an observed data to be in the labelled class. so this is basically a conditional probability, that is p(y when x occurred). a convention we used in class is that the probabilities are denoted by p and the labels are denoted by t. to calculate the loss function here we usually use log to ease calculations.
the metrics in this model is mainly the confusion matrix. it is a 2 v 2 matrix with true positive, false positive, true negative and false negative. then there is precession and recall, precession is defined by the number of observations we classified correctly. and recall is for a particular class, that is how many have we classified correctly under that class. and another term if defined as f1 which is the harmonic mean of the above two.
and then tas gave us their findings from the exercise, from which the main point was the confusion in kurtosis and excess kurtosis. excel reports excess kurtosis.","today we started with discussing more upon logistic regression. we talked about logistic unit, which is basically the probability of an observed data to be in the labelled class. so this is basically a conditional probability, that is p(y when x occurred). a convention we used in class is that the probabilities are denoted by p and the labels are denoted by t. to calculate the loss function here we usually use log to ease calculations. the metrics in this model is mainly the confusion matrix. it is a 2 v 2 matrix with true positive, false positive, true negative and false negative. then there is precession and recall, precession is defined by the number of observations we classified correctly. and recall is for a particular class, that is how many have we classified correctly under that class. and another term if defined as f1 which is the harmonic mean of the above two. and then tas gave us their findings from the exercise, from which the main point was the confusion in kurtosis and excess kurtosis. excel reports excess kurtosis.",10,11.251225,-19.510864,8.913757,-1.4481473,"classifications, histograms, histogram"
128,"first, we discussed the large language models used to summarise the topics of the summary sent by the students. using the superset of those topics, assignments are to be evaluated. then, we discussed improving the result quality using multiple parameters. then we discussed mlr with an example of taylor series expansion. we looked at a scatter plot which resembled the function of sinx and then discussed about how the model will remove the polynomial parameter coefficients and give more weightage to trignometrical parameters for fitting the curve onto sinx. then we discussed the difference between parametric and non-parametric models and their uses like knn, random forest, etc. then there was a brief discussion about neural network. after that, sir explained in detail the mse and r-squared metrics interpretation. then, we ended the class by briefly discussing nominal and ordinal variables.","first, we discussed the large language models used to summarise the topics of the summary sent by the students. using the superset of those topics, assignments are to be evaluated. then, we discussed improving the result quality using multiple parameters. then we discussed mlr with an example of taylor series expansion. we looked at a scatter plot which resembled the function of sinx and then discussed about how the model will remove the polynomial parameter coefficients and give more weightage to trignometrical parameters for fitting the curve onto sinx. then we discussed the difference between parametric and non-parametric models and their uses like knn, random forest, etc. then there was a brief discussion about neural network. after that, sir explained in detail the mse and r-squared metrics interpretation. then, we ended the class by briefly discussing nominal and ordinal variables.",13,-3.573299,-1.4999571,9.222519,4.7044034,"classification, classifying, classifications"
129,"in todayâ€™s hands-on class, we worked with a dataset to perform regression analysis. using excel, we calculated key values like x_bar (average of x) and y_bar (average of y) manually using formulas. we explored the concept of error and its gaussian (normal) distribution, and understood the sum of squares of residuals (ssr), total sum of squares (sst), and explained sum of squares (sse). we discussed how well the variance is captured by the regression line and learned that, for simple linear regression, the coefficient of determination (r square) is the square of the correlation coefficient (r). finally, we used the data analysis toolpak in excel to quickly generate regression statistics for the dataset. additionally, we discussed sampling concepts, including calculating sample means from multiple samples, finding the mean of these sample means, and determining their variance and standard deviation. we understood that there is some relationship between the variance of sample means and the variance of the original data.","in today s hands-on class, we worked with a dataset to perform regression analysis. using excel, we calculated key values like x_bar (average of x) and y_bar (average of y) manually using formulas. we explored the concept of error and its gaussian (normal) distribution, and understood the sum of squares of residuals (ssr), total sum of squares (sst), and explained sum of squares (sse). we discussed how well the variance is captured by the regression line and learned that, for simple linear regression, the coefficient of determination (r square) is the square of the correlation coefficient (r). finally, we used the data analysis toolpak in excel to quickly generate regression statistics for the dataset. additionally, we discussed sampling concepts, including calculating sample means from multiple samples, finding the mean of these sample means, and determining their variance and standard deviation. we understood that there is some relationship between the variance of sample means and the variance of the original data.",5,21.251833,-2.7968695,14.233887,4.5434012,"regression, statistical, statistics"
130,"today's lecture started with a discussion regarding an amazing website called as playground.tensroeflow.org where we can adjust the features and hidden layers so as to classify various types of datasets and make our model flexible. then we looked into a jupyter notebook where we used logistic regression to train our model and use it for classification. prof told that to never believe the accuracy blindly as it can sometimes mislead us. for evaluation of the correctness of our model we can use various other metrics like precision, recall, confusion matrix, true positive rate, false positive rate, etc. then we discussed about when can a classifier classify the data into the wrong class. one of the reasons behind this could be the overlap between the classes int the dataset. at the end, we shifted our talk towards k mean clustering.","today's lecture started with a discussion regarding an amazing website called as playground.tensroeflow.org where we can adjust the features and hidden layers so as to classify various types of datasets and make our model flexible. then we looked into a jupyter notebook where we used logistic regression to train our model and use it for classification. prof told that to never believe the accuracy blindly as it can sometimes mislead us. for evaluation of the correctness of our model we can use various other metrics like precision, recall, confusion matrix, true positive rate, false positive rate, etc. then we discussed about when can a classifier classify the data into the wrong class. one of the reasons behind this could be the overlap between the classes int the dataset. at the end, we shifted our talk towards k mean clustering.",8,1.4067808,-17.74806,7.1499043,0.16446273,"classification, clusterings, classifying"
131,"in today's class, we first saw that the mean can be the expected value from the x sample. then we saw that different bin widths can give us different patterns in the histogram. in excel, there is a certain mechanism through which it sets the bin size initially but we need to choose the number of bins or bin width based on what we are looking for in the histogram. then we saw that to calculate the standard error, if we have just one sample, we take the standard deviation of the population to be equal to the standard deviation of that sample. then we continue with logistic regression where we name the clusters as class label 1, label 2, etc., and make boundaries to separate them. suppose we are getting a=w1.x1+w2x2+w3.x3+b then we need a function that will convert a into either 0 or 1. we use the sigmoid function p(y|x)=sigma(a)=(1/(1+(1/e^a))), the prediction is 1 if p(y/x) is greater than 0.5 and else it is zero. then we saw the metrics associated with the classification like true positive is when it is 1 or 'yes' and prediction is also 1, and false positive is when it is 0 or 'no' and predicted is 'yes'. accuracy tells us about how often the classifier is correct. suppose in a case there are 100 points of one class and 5 of another, then still if we do not consider the 5 points, the accuracy is high though, this is the case of data imbalance. then we saw precision means off the events that are detected, how many have we detected correctly and recall is off the specific class, how many did we classify correctly. at last, we got the feedback from tas regarding e1, it points out the point that we should calculate the error kurtosis using its formula and not through excel's inbuilt function, and we also that we should document our observations properly, adding all the relevant things so that the reader can understand your conclusions by reading the document.","in today's class, we first saw that the mean can be the expected value from the x sample. then we saw that different bin widths can give us different patterns in the histogram. in excel, there is a certain mechanism through which it sets the bin size initially but we need to choose the number of bins or bin width based on what we are looking for in the histogram. then we saw that to calculate the standard error, if we have just one sample, we take the standard deviation of the population to be equal to the standard deviation of that sample. then we continue with logistic regression where we name the clusters as class label 1, label 2, etc., and make boundaries to separate them. suppose we are getting a=w1.x1+w2x2+w3.x3+b then we need a function that will convert a into either 0 or 1. we use the sigmoid function p(y|x)=sigma(a)=(1/(1+(1/e^a))), the prediction is 1 if p(y/x) is greater than 0.5 and else it is zero. then we saw the metrics associated with the classification like true positive is when it is 1 or 'yes' and prediction is also 1, and false positive is when it is 0 or 'no' and predicted is 'yes'. accuracy tells us about how often the classifier is correct. suppose in a case there are 100 points of one class and 5 of another, then still if we do not consider the 5 points, the accuracy is high though, this is the case of data imbalance. then we saw precision means off the events that are detected, how many have we detected correctly and recall is off the specific class, how many did we classify correctly. at last, we got the feedback from tas regarding e1, it points out the point that we should calculate the error kurtosis using its formula and not through excel's inbuilt function, and we also that we should document our observations properly, adding all the relevant things so that the reader can understand your conclusions by reading the document.",10,13.521596,-18.305801,8.82009,-1.349315,"classifications, histograms, histogram"
132,"the lecture explained how to use interactive pivot tables to analyze data, helping calculate averages, minimums, and maximums. it highlights the value of viewing data in multiple ways to gain better insights and choosing the best methods for each situation. examples include studying chemical plant data and transformer data, where modern sensors now provide more detailed information. this approach helps spot patterns, trends, and issues, making it easier to understand complex datasets and make informed decisions. pivot tables and advanced tools simplify data exploration and visualization, turning large, complicated data into clear, actionable insights. so, in short, the focus of todayâ€™s lecture was on performing exploratory data analysis (eda).","the lecture explained how to use interactive pivot tables to analyze data, helping calculate averages, minimums, and maximums. it highlights the value of viewing data in multiple ways to gain better insights and choosing the best methods for each situation. examples include studying chemical plant data and transformer data, where modern sensors now provide more detailed information. this approach helps spot patterns, trends, and issues, making it easier to understand complex datasets and make informed decisions. pivot tables and advanced tools simplify data exploration and visualization, turning large, complicated data into clear, actionable insights. so, in short, the focus of today s lecture was on performing exploratory data analysis (eda).",6,-16.677244,26.977388,7.1680098,9.718244,"summarizing, summarize, summarization"
133,"in today's class, we started with a recap of eda (exploratory data analysis). then we learnt about fixing the missing value and handling outliers using mumbai aqi value dataset. this known as data smoothing, it can be done using simple moving average sma. higher the window size for sma, more smoothing is the curve. linear regression is immune to scaling but gradient descent method gets largely affected by scaling. then sir explained about data normalisation, standardization and box cox transformation. after normalisation, things like clustering algorithms do well; because in presence of scaled noisy data, such algorithms like k means clustering which are based on euclidean distance gets highly influenced by the scale. at last ta explained about data imbalance and how to fix it with an example of detection of exo-planets. some times a data is under represented, in that case we use methods such as smote, adasyn, borderline-smote,etc. to create new synthetic data points; and methods like tomek-link to increase the minority class data points to cope-up with class imbalance.","in today's class, we started with a recap of eda (exploratory data analysis). then we learnt about fixing the missing value and handling outliers using mumbai aqi value dataset. this known as data smoothing, it can be done using simple moving average sma. higher the window size for sma, more smoothing is the curve. linear regression is immune to scaling but gradient descent method gets largely affected by scaling. then sir explained about data normalisation, standardization and box cox transformation. after normalisation, things like clustering algorithms do well; because in presence of scaled noisy data, such algorithms like k means clustering which are based on euclidean distance gets highly influenced by the scale. at last ta explained about data imbalance and how to fix it with an example of detection of exo-planets. some times a data is under represented, in that case we use methods such as smote, adasyn, borderline-smote,etc. to create new synthetic data points; and methods like tomek-link to increase the minority class data points to cope-up with class imbalance.",9,-20.024277,10.221371,10.151835,10.054745,"dataâ, analyse, analyses"
134,"in confidence intervals, if î²â‚â€™s ci is around zero, it may still be statistically similar to zero, meaning it doesnâ€™t significantly impact y. mlr involves multiple independent variables (xâ‚, xâ‚‚, â€¦ xâ‚–) and requires feature engineering (e.g., embedding vectors or dxâ‚/dt). since thereâ€™s no closed-form solution, we use gradient descent over an n-dimensional hypersurface. model evaluation isnâ€™t just about râ²; we check error metrics (mse, rmse) and p-values to determine statistical significance and guide feature selection.","in confidence intervals, if s ci is around zero, it may still be statistically similar to zero, meaning it doesn t significantly impact y. mlr involves multiple independent variables (x , x , x ) and requires feature engineering (e.g., embedding vectors or dx /dt). since there s no closed-form solution, we use gradient descent over an n-dimensional hypersurface. model evaluation isn t just about r ; we check error metrics (mse, rmse) and p-values to determine statistical significance and guide feature selection.",2,15.274222,4.203335,12.547244,3.9775357,"regression, regressions, features"
135,"todayâ€™s session was a great mix of hands-on exploration and theory. we started by playing around with playground.tensorflow.org, tweaking the number of layers in a neural network to see how it affects performance. as we increased the degree of freedom, the model became more flexible and adapted better to the datasetâ€”until we pushed it too far and saw overfitting, where the model started memorizing patterns instead of generalizing them.

moving on to classification, we discussed the confusion matrix and broke down key terms like true positives, false positives, true negatives, and false negatives. we also saw how overlapping false positives and false negatives can make classification tricky. this naturally led us to the roc curve (receiver operating characteristic curve) and why the area under the curve (auc) matters. we learned that:

auc close to 1 means a great classifier.
auc around 0.5 (a straight diagonal line) means the model is no better than random guessing.
a key takeaway was that in real-life scenarios, data imbalance is a big challenge. for example, in fraud detection, fraudulent transactions are rare compared to normal ones, so the model often fails to detect them. thatâ€™s why accuracy alone isnâ€™t a reliable metric, and we should focus on precision, recall, and the confusion matrix instead.

shifting gears, we explored unsupervised learning and looked at k-means clusteringâ€”how data points are randomly assigned to clusters, centroids are calculated, and points are re-grouped iteratively. we also got introduced to hierarchical clustering and dendrograms, where we focused on complete linkage as a clustering method.

to wrap things up, we saw a python implementation that plotted roc curves, helping us visualize how well a classifier performs.

overall, this session gave us a clearer understanding of how classification models work, how to evaluate them properly, and how clustering helps in unsupervised learning. a great mix of concepts and application!","today s session was a great mix of hands-on exploration and theory. we started by playing around with playground.tensorflow.org, tweaking the number of layers in a neural network to see how it affects performance. as we increased the degree of freedom, the model became more flexible and adapted better to the dataset until we pushed it too far and saw overfitting, where the model started memorizing patterns instead of generalizing them. moving on to classification, we discussed the confusion matrix and broke down key terms like true positives, false positives, true negatives, and false negatives. we also saw how overlapping false positives and false negatives can make classification tricky. this naturally led us to the roc curve (receiver operating characteristic curve) and why the area under the curve (auc) matters. we learned that: auc close to 1 means a great classifier. auc around 0.5 (a straight diagonal line) means the model is no better than random guessing. a key takeaway was that in real-life scenarios, data imbalance is a big challenge. for example, in fraud detection, fraudulent transactions are rare compared to normal ones, so the model often fails to detect them. that s why accuracy alone isn t a reliable metric, and we should focus on precision, recall, and the confusion matrix instead. shifting gears, we explored unsupervised learning and looked at k-means clustering how data points are randomly assigned to clusters, centroids are calculated, and points are re-grouped iteratively. we also got introduced to hierarchical clustering and dendrograms, where we focused on complete linkage as a clustering method. to wrap things up, we saw a python implementation that plotted roc curves, helping us visualize how well a classifier performs. overall, this session gave us a clearer understanding of how classification models work, how to evaluate them properly, and how clustering helps in unsupervised learning. a great mix of concepts and application!",8,-0.91506875,-18.387424,6.815103,0.38121423,"classification, clusterings, classifying"
136,"in today's class we learn more about the regression through python modules. the very basic theory is that do not use the entire sample for creating the model. we split the data in the ratio of 80:20 to training and testing samples. this splitting helps model to perform well as compared to using the whole sample. and we done this process by randomly splitting.
from training matrics and testing matrics we get r_2. if the value of r_2 is differing much from each other then it performs well on training and testing data but not on test data.
this is called overfitting. another term is bias variance tradeoff. multiple r which is square root of r_2 which indicates correlation between y and rest of the x. the adjusted r_2 value will drop if we are adding the variables and it tells how effective the addition of new variables is. slr and mlr are parametric method of model creation as they use parametrs.
example of non-parametric model is decision tree. we also learnt about different python modules such as pandas, numpy and scikit - learn. next we see the quantile-quantile plot of residuals which tells that for good model, there should be less deviation in plot of residuals. scikit learn modules doesn't give values such as statistic values.","in today's class we learn more about the regression through python modules. the very basic theory is that do not use the entire sample for creating the model. we split the data in the ratio of 80:20 to training and testing samples. this splitting helps model to perform well as compared to using the whole sample. and we done this process by randomly splitting. from training matrics and testing matrics we get r_2. if the value of r_2 is differing much from each other then it performs well on training and testing data but not on test data. this is called overfitting. another term is bias variance tradeoff. multiple r which is square root of r_2 which indicates correlation between y and rest of the x. the adjusted r_2 value will drop if we are adding the variables and it tells how effective the addition of new variables is. slr and mlr are parametric method of model creation as they use parametrs. example of non-parametric model is decision tree. we also learnt about different python modules such as pandas, numpy and scikit - learn. next we see the quantile-quantile plot of residuals which tells that for good model, there should be less deviation in plot of residuals. scikit learn modules doesn't give values such as statistic values.",2,5.936108,4.385434,10.650043,5.034142,"regression, regressions, features"
137,"attributes: properties or features of data that describe its characteristics or dimensions. example : â€œheightâ€, â€œageâ€, â€œweightâ€ etc.

operations: actions or computations performed on attributes or data to analyze, transform or process them. example : mean, median, mode, add, subtract, multiply, divide. 

level of measurement | attribute                               | operations                             
--------------------|-----------------------------------------|----------------------------------------
nominal            | categorical, unordered                  | frequency count, mode, proportion, percentage
ordinal            | categorical, ordered                    | frequency count, mode, median, comparison
interval           | numerical, ordered, no true zero         | frequency count, mean, mode, median, addition, subtraction, variance, standard deviation
ratio              | numerical, ordered, true zero           | all interval operations, ratios

parameter: describes the entire population, and estimated using statistic.

statistic: describes a sample from the population and is directly calculated from the sample data.

simple linear regression: y=ax+b
here, y is the dependent variable or label, also called as response variable, and x is the feature or the independent variable. 

bias: introduced in models to address or approximate the effects of variables not explicitly included in the model.
if the confidence is low then, the confidence interval shrinks while if the confidence is 100% then we get an infinite confidence interval. 
we estimate the unknown parameters a and b in y=ax+b using the idea of minimizing the sum of squares of errors. 

closed form solutions: an exact and explicit mathematical expression that solves a given problem or equation in terms of known functions and constants. 

using the method of minimizing the sum of squares of errors, we get the closed form solution for the simple regression model for single predictor. however these are point estimates and we need to arrive at the possible interval in which these values lie such that there is higher chance that the population parameters lie within the intervals.","attributes: properties or features of data that describe its characteristics or dimensions. example : height , age , weight etc. operations: actions or computations performed on attributes or data to analyze, transform or process them. example : mean, median, mode, add, subtract, multiply, divide. level of measurement | attribute | operations --------------------|-----------------------------------------|---------------------------------------- nominal | categorical, unordered | frequency count, mode, proportion, percentage ordinal | categorical, ordered | frequency count, mode, median, comparison interval | numerical, ordered, no true zero | frequency count, mean, mode, median, addition, subtraction, variance, standard deviation ratio | numerical, ordered, true zero | all interval operations, ratios parameter: describes the entire population, and estimated using statistic. statistic: describes a sample from the population and is directly calculated from the sample data. simple linear regression: y=ax+b here, y is the dependent variable or label, also called as response variable, and x is the feature or the independent variable. bias: introduced in models to address or approximate the effects of variables not explicitly included in the model. if the confidence is low then, the confidence interval shrinks while if the confidence is 100% then we get an infinite confidence interval. we estimate the unknown parameters a and b in y=ax+b using the idea of minimizing the sum of squares of errors. closed form solutions: an exact and explicit mathematical expression that solves a given problem or equation in terms of known functions and constants. using the method of minimizing the sum of squares of errors, we get the closed form solution for the simple regression model for single predictor. however these are point estimates and we need to arrive at the possible interval in which these values lie such that there is higher chance that the population parameters lie within the intervals.",1,36.37308,-10.762667,16.348251,3.9365444,"population, models, estimating"
138,"the instructor conducted a tutorial on how to effectively use pivot tables, providing a detailed explanation of their functionality. additionally, they demonstrated exploratory data analysis (eda) using a real dataset from a chemical plant, showcasing practical applications of data analysis techniques. meanwhile, the teaching assistant (ta) delivered a presentation on exercise e2, offering insights and clarifications on the topic.","the instructor conducted a tutorial on how to effectively use pivot tables, providing a detailed explanation of their functionality. additionally, they demonstrated exploratory data analysis (eda) using a real dataset from a chemical plant, showcasing practical applications of data analysis techniques. meanwhile, the teaching assistant (ta) delivered a presentation on exercise e2, offering insights and clarifications on the topic.",6,-18.561523,27.773302,7.47007,9.416157,"summarizing, summarize, summarization"
139,"today's class started with a question asked in one of the session summary that is how to check the correctness of the metrics without increasing the sample size and the answer to this was to improving the method or fine tuning it. then we started to discuss about how to capture non linearity of the data set. also we discussed about the magic of multiple linear regression on how it adjusts the coefficient to make the model efficient ( example of some coefficient being the taylor series terms for sine and one coefficient being the sine itself ). then we discussed some facts that the adjusted r square decreases if features increases, some features needs to be eliminated in order to make the model stable and forcing the error to fit normally can lead to overfitting. further we discussed what are parametric methods and how we can ask them a ' what if ? ' question. also we discussed about forward and backward feature engineering on how forward uses an iterative method to add features and backward removes features to improve the accuracy. we then looked at sample fitting done by various models like random forest, xg boost, neural networks, knn etc. we deeply discussed about neural network on how they have weights associated to their features. large sample size is needed for training a neural network properly to avoid overfitting. lastly we discussed about logistic regression.","today's class started with a question asked in one of the session summary that is how to check the correctness of the metrics without increasing the sample size and the answer to this was to improving the method or fine tuning it. then we started to discuss about how to capture non linearity of the data set. also we discussed about the magic of multiple linear regression on how it adjusts the coefficient to make the model efficient ( example of some coefficient being the taylor series terms for sine and one coefficient being the sine itself ). then we discussed some facts that the adjusted r square decreases if features increases, some features needs to be eliminated in order to make the model stable and forcing the error to fit normally can lead to overfitting. further we discussed what are parametric methods and how we can ask them a ' what if ? ' question. also we discussed about forward and backward feature engineering on how forward uses an iterative method to add features and backward removes features to improve the accuracy. we then looked at sample fitting done by various models like random forest, xg boost, neural networks, knn etc. we deeply discussed about neural network on how they have weights associated to their features. large sample size is needed for training a neural network properly to avoid overfitting. lastly we discussed about logistic regression.",0,0.4399926,-2.6180818,9.386881,4.542644,"models, feature, features"
140,"in todays class (22/1/2025)
we started with a data of 100 sample points on excel calculating and analyzing the regression line via values of a and b as well as regression function on data analysis toolkit.
we analyzed error by plotting and scatter plot, later proving what we see is not always true by plotting a histogram and finding a rough uniform distribution of error. also a new statement came to me that, when the outcome is dependent on large number of uncommon causes which are random, the distribution id a normal gaussian.
we found that error was not so random in our case and concluded that there are some things/ parameters which the regression model is unable to calculate. 
next we defined what is a good model?
[1] we can determine it by the random distribution of error function and
[2] how much variation can the model explain for the provided data

later, we jumped to the data values provided by the data toolkit where we found the rsquare  values also known as coefficient of determination (ssr/sst) which is square of correlation coefficient in case of simple linear regression.
last, we discussed about the upper 95% and lower 95% confidence bound diving into gaussian distribution and mean of samples and population.","in todays class (22/1/2025) we started with a data of 100 sample points on excel calculating and analyzing the regression line via values of a and b as well as regression function on data analysis toolkit. we analyzed error by plotting and scatter plot, later proving what we see is not always true by plotting a histogram and finding a rough uniform distribution of error. also a new statement came to me that, when the outcome is dependent on large number of uncommon causes which are random, the distribution id a normal gaussian. we found that error was not so random in our case and concluded that there are some things/ parameters which the regression model is unable to calculate. next we defined what is a good model? [1] we can determine it by the random distribution of error function and [2] how much variation can the model explain for the provided data later, we jumped to the data values provided by the data toolkit where we found the rsquare values also known as coefficient of determination (ssr/sst) which is square of correlation coefficient in case of simple linear regression. last, we discussed about the upper 95% and lower 95% confidence bound diving into gaussian distribution and mean of samples and population.",5,23.01587,-4.440987,14.261422,4.7646775,"regression, statistical, statistics"
141,"we discussed about how different methods represent the confusion matrix of the logistic regression, we can know columns by either going through the documentation of the particular python library or use the support values and the dataset columns. we looked the method data scientist's have come up with to work with the data, that is crisp-dm cross industry standard process for data mining there are six steps in this business understanding, data understanding, data preparation, modelling, evaluation and deployment
in these data understanding and preparation are the key aspects. we learnt how important it is to do eda. we saw a mind map about what to do when we have different issues with the data mainly the problems are related to the data y and x values. we saw what techniques are used to tackle these problems like if there is class imbalance in dataset of classification problem etc. we looked the different eda methods like doing hypothesis, using box-plots to know the outliers , using heat maps to know the corelation among different features , matrix plot from this we can we get to know some nice clusters which are forming among the features together and their corelation with each other the problem is they give corelation between only two features at a time. next we saw about the missing values one there are three kinds of missing values mcar-missing completely at random, mar-missing at random(one of the sensor is faulty), mnar-missing not at random(like the co sensor is not designed to calculate the concentrations above 110). we can handle these through replacing the missing values using mean, median or mode where ever possible here is where the distribution of the particular feature is important to fill the missing values, we can also use ml model to particular feature to fill the missing values , backward and forward filling and interpolation techniques. outliers-finding can be done using iqr, probability of the data point lying within the 3 standard deviations from the mean of the feature, db scan for multivariate, we can also use t-sne plot to visualise the obtained outlier points of n-dimensions by projecting it on 2d plane. outlier-handling, dropping the column or row, data capping replacing outlier values with iqr upper and lower bounds is an example. if an outlier is true we can isolate it and treat it separately. mean if influenced by outliers but median is not","we discussed about how different methods represent the confusion matrix of the logistic regression, we can know columns by either going through the documentation of the particular python library or use the support values and the dataset columns. we looked the method data scientist's have come up with to work with the data, that is crisp-dm cross industry standard process for data mining there are six steps in this business understanding, data understanding, data preparation, modelling, evaluation and deployment in these data understanding and preparation are the key aspects. we learnt how important it is to do eda. we saw a mind map about what to do when we have different issues with the data mainly the problems are related to the data y and x values. we saw what techniques are used to tackle these problems like if there is class imbalance in dataset of classification problem etc. we looked the different eda methods like doing hypothesis, using box-plots to know the outliers , using heat maps to know the corelation among different features , matrix plot from this we can we get to know some nice clusters which are forming among the features together and their corelation with each other the problem is they give corelation between only two features at a time. next we saw about the missing values one there are three kinds of missing values mcar-missing completely at random, mar-missing at random(one of the sensor is faulty), mnar-missing not at random(like the co sensor is not designed to calculate the concentrations above 110). we can handle these through replacing the missing values using mean, median or mode where ever possible here is where the distribution of the particular feature is important to fill the missing values, we can also use ml model to particular feature to fill the missing values , backward and forward filling and interpolation techniques. outliers-finding can be done using iqr, probability of the data point lying within the 3 standard deviations from the mean of the feature, db scan for multivariate, we can also use t-sne plot to visualise the obtained outlier points of n-dimensions by projecting it on 2d plane. outlier-handling, dropping the column or row, data capping replacing outlier values with iqr upper and lower bounds is an example. if an outlier is true we can isolate it and treat it separately. mean if influenced by outliers but median is not",9,-14.185201,17.413677,9.190362,8.863513,"dataâ, analyse, analyses"
142,"the lecture introduced multiple linear regression, emphasizing the importance of multiple independent variables and the 80-20% training-test split to prevent overfitting. overfitting was illustrated with a graph, showing how high training accuracy with low test accuracy indicates poor generalization.

key regression statistics were covered in excel, including multiple r, adjusted 
ð‘…^2, and variance formulas (explaining why the denominator is nâˆ’1 due to degrees of freedom). it was clarified that linear regression does not always imply a straight line, as polynomial models can also fit within this framework.

in the python segment, q-q plots were introduced to check if data follows a particular distribution.","the lecture introduced multiple linear regression, emphasizing the importance of multiple independent variables and the 80-20% training-test split to prevent overfitting. overfitting was illustrated with a graph, showing how high training accuracy with low test accuracy indicates poor generalization. key regression statistics were covered in excel, including multiple r, adjusted ^2, and variance formulas (explaining why the denominator is n 1 due to degrees of freedom). it was clarified that linear regression does not always imply a straight line, as polynomial models can also fit within this framework. in the python segment, q-q plots were introduced to check if data follows a particular distribution.",5,17.029047,0.9085147,12.968703,4.9264026,"regression, statistical, statistics"
143,"feature encoding techniques

during this session, we reviewed several techniques used to transform categorical data into numeric representations, which is an essential step in getting data ready for machine learning models. the discussion began with the introduction to feature encoding techniques such as vectorization techniques and utilization of one-hot encoding, label encoding, integer encoding, binary encoding, frequency encoding, and target encoding.

one-hot encoding:
one-hot encoding converts categorical features into vector where each category has a separate column. one-hot encoding is particularly helpful when encoding the input features (x) in the case of multiclass or multilabel problems. one-hot encoding is less ideal for encoding target variables (y). one of the primary disadvantages of one-hot encoding is that it can cause the curse of dimensionality. for example, if you have a variable such as pincode with thousands of distinct values, applying one-hot encoding will lead to a dimensionality explosion, making the dataset sparse and computationally intensive.

label encoding vs. integer encoding:
label encoding gives each category a specific integer. the method is well-suited to encoding target variables (y), especially for classification tasks, but may not be suitable for input features where the model may mistake the numerical ordering as an ordinal relationship. on the other hand, integer encoding is used when the target variable is naturally ordinal instead of nominal, such that the order of the categories holds important meaning.

binary encoding:-
binary encoding, or pseudo one-hot encoding, offers a more compact representation compared to one-hot encoding. by converting categorical values into binary code, a few columns (for example, three columns) can represent multiple classes (up to eight classes in this example). this method helps mitigate the issue of high dimensionality while preserving the distinctiveness of each category.

frequency encoding:
frequency encoding substitutes each class with its frequency in the dataset. while it reduces the representation, it is not necessarily good for target variables because two classes can have the same frequencies, causing possible loss of useful information.

target encoding:
target encoding allocates to each category the target variable's mean for that category. target encoding can be especially useful when there is high correlation between the target variable and the categorical feature. however, caution needs to be exercised to prevent data leakage while training the model.

simplification strategies:
tackling complicated regression questions by converting them into classification questions using methods such as feature binning is common. also introduced briefly was converting text into numeric vectors through vectorization methods as a key step in handling unstructured data.

in general, the suitable encoding method is determined by the type of data, if the variable is utilized as a feature or target, and possibly the effect it may have on dimensionality. every approach has its strengths and weaknesses, and the proper choice is critical to constructing efficient and effective machine learning models.","feature encoding techniques during this session, we reviewed several techniques used to transform categorical data into numeric representations, which is an essential step in getting data ready for machine learning models. the discussion began with the introduction to feature encoding techniques such as vectorization techniques and utilization of one-hot encoding, label encoding, integer encoding, binary encoding, frequency encoding, and target encoding. one-hot encoding: one-hot encoding converts categorical features into vector where each category has a separate column. one-hot encoding is particularly helpful when encoding the input features (x) in the case of multiclass or multilabel problems. one-hot encoding is less ideal for encoding target variables (y). one of the primary disadvantages of one-hot encoding is that it can cause the curse of dimensionality. for example, if you have a variable such as pincode with thousands of distinct values, applying one-hot encoding will lead to a dimensionality explosion, making the dataset sparse and computationally intensive. label encoding vs. integer encoding: label encoding gives each category a specific integer. the method is well-suited to encoding target variables (y), especially for classification tasks, but may not be suitable for input features where the model may mistake the numerical ordering as an ordinal relationship. on the other hand, integer encoding is used when the target variable is naturally ordinal instead of nominal, such that the order of the categories holds important meaning. binary encoding:- binary encoding, or pseudo one-hot encoding, offers a more compact representation compared to one-hot encoding. by converting categorical values into binary code, a few columns (for example, three columns) can represent multiple classes (up to eight classes in this example). this method helps mitigate the issue of high dimensionality while preserving the distinctiveness of each category. frequency encoding: frequency encoding substitutes each class with its frequency in the dataset. while it reduces the representation, it is not necessarily good for target variables because two classes can have the same frequencies, causing possible loss of useful information. target encoding: target encoding allocates to each category the target variable's mean for that category. target encoding can be especially useful when there is high correlation between the target variable and the categorical feature. however, caution needs to be exercised to prevent data leakage while training the model. simplification strategies: tackling complicated regression questions by converting them into classification questions using methods such as feature binning is common. also introduced briefly was converting text into numeric vectors through vectorization methods as a key step in handling unstructured data. in general, the suitable encoding method is determined by the type of data, if the variable is utilized as a feature or target, and possibly the effect it may have on dimensionality. every approach has its strengths and weaknesses, and the proper choice is critical to constructing efficient and effective machine learning models.",3,-42.990177,3.7125378,0.12866704,6.353206,"categorical, categorization, categorise"
144,"learnt about basics in excel, followed along with sir, and learnt new terms in data science and analysis. also plotted the data graphically and got to see correlation between variables and different ways of analyzing data.","learnt about basics in excel, followed along with sir, and learnt new terms in data science and analysis. also plotted the data graphically and got to see correlation between variables and different ways of analyzing data.",6,-15.914776,30.665884,7.180721,9.529228,"summarizing, summarize, summarization"
145,"in today's session we initially discussed expectation algebra and how each data point in a sample has an expected value equal to the mean and the variance of each observation is equal to the population variance which can be replaced with sample variance if the number of observations is large. after that we moved on to formulating the boundary conditions and how can we use mathematics to deal with logistic regression. the boundary separating any two classes can be either linear or non-linear. we then saw how using matrices to compute the weights is a lot easier. closed form solutions exist for logistic regression which looks similar to the one we had for mlr. in logistic regression, in order to increase our classification accuracy we maximise likehlihood (or log likelihood) which just means that the probability of those data points which belong to class 1 should be closer to 1 and the ones belonging to zero should be closer to zero. also disussed various terms which are used to say about how our model is performing and its ability to classify the data into respective classes. we use accuracy, precision and recall and various other terms such as f1-value which is the harmonic mean of precision and recall which is indicative of how our model is performing.","in today's session we initially discussed expectation algebra and how each data point in a sample has an expected value equal to the mean and the variance of each observation is equal to the population variance which can be replaced with sample variance if the number of observations is large. after that we moved on to formulating the boundary conditions and how can we use mathematics to deal with logistic regression. the boundary separating any two classes can be either linear or non-linear. we then saw how using matrices to compute the weights is a lot easier. closed form solutions exist for logistic regression which looks similar to the one we had for mlr. in logistic regression, in order to increase our classification accuracy we maximise likehlihood (or log likelihood) which just means that the probability of those data points which belong to class 1 should be closer to 1 and the ones belonging to zero should be closer to zero. also disussed various terms which are used to say about how our model is performing and its ability to classify the data into respective classes. we use accuracy, precision and recall and various other terms such as f1-value which is the harmonic mean of precision and recall which is indicative of how our model is performing.",10,11.715081,-17.033573,9.077327,-1.3591119,"classifications, histograms, histogram"
146,"in this session, we covered key concepts related to dimensionality reduction, categorical data encoding, and classification modeling techniques.

we began with t-sne (t-distributed stochastic neighbor embedding), which is widely used for visualizing high-dimensional data. however, since t-sne is stochastic in nature, it produces different clusters each time it runs, making it unsuitable for building predictive models. in contrast, pca (principal component analysis) can be used for dimensionality reduction but is more suited for continuous numerical data rather than categorical data.

moving on to categorical data encoding, we discussed one-hot encoding, which converts categorical variables into binary vectors. for example, if a variable y belongs to categories red, blue, and green, one-hot encoding would represent them as:
red â†’ 100
blue â†’ 010
green â†’ 001
while one-hot encoding ensures that categorical variables are properly represented, it significantly increases the number of columns, leading to the curse of dimensionality, especially when dealing with high-cardinality categorical features. this can make models computationally expensive and difficult to train.

we then examined different types of classification problems:

multi-class classification, where the output belongs to one of many classes (e.g., classifying an image as either a cat, dog, or bird).
multi-label classification, where the output can belong to multiple categories simultaneously (e.g., tagging an image with labels like ""outdoor,"" ""sunny,"" and ""people"").
since one-hot encoding can sometimes be inefficient, we discussed alternative encoding techniques, such as:

binary encoding: a more compact representation for categorical variables.
frequency encoding: uses the count of each category's occurrences but is applied only to input variables (x), not the target variable (y).
target encoding (mean encoding): uses the average value of y for each category to encode the variable. this method is useful but may introduce data leakage if not handled properly.
lastly, we explored feature binning, a technique used when continuous features need to be converted into categorical variables. once transformed, the problem can be approached using a classification model instead of a regression model.","in this session, we covered key concepts related to dimensionality reduction, categorical data encoding, and classification modeling techniques. we began with t-sne (t-distributed stochastic neighbor embedding), which is widely used for visualizing high-dimensional data. however, since t-sne is stochastic in nature, it produces different clusters each time it runs, making it unsuitable for building predictive models. in contrast, pca (principal component analysis) can be used for dimensionality reduction but is more suited for continuous numerical data rather than categorical data. moving on to categorical data encoding, we discussed one-hot encoding, which converts categorical variables into binary vectors. for example, if a variable y belongs to categories red, blue, and green, one-hot encoding would represent them as: red 100 blue 010 green 001 while one-hot encoding ensures that categorical variables are properly represented, it significantly increases the number of columns, leading to the curse of dimensionality, especially when dealing with high-cardinality categorical features. this can make models computationally expensive and difficult to train. we then examined different types of classification problems: multi-class classification, where the output belongs to one of many classes (e.g., classifying an image as either a cat, dog, or bird). multi-label classification, where the output can belong to multiple categories simultaneously (e.g., tagging an image with labels like ""outdoor,"" ""sunny,"" and ""people""). since one-hot encoding can sometimes be inefficient, we discussed alternative encoding techniques, such as: binary encoding: a more compact representation for categorical variables. frequency encoding: uses the count of each category's occurrences but is applied only to input variables (x), not the target variable (y). target encoding (mean encoding): uses the average value of y for each category to encode the variable. this method is useful but may introduce data leakage if not handled properly. lastly, we explored feature binning, a technique used when continuous features need to be converted into categorical variables. once transformed, the problem can be approached using a classification model instead of a regression model.",3,-43.20036,2.100011,0.00011191728,6.3737044,"categorical, categorization, categorise"
147,"data preprocessing & handling missing data
filling missing data:
use ml models like regression.
fill gaps using forward/backward methods.
fill based on column data distribution.

finding outliers:
use probability and standard deviation.
use boxplots to detect unusual values.
dbscan method helps in detecting outliers.
outliers affect the mean but not the median.

handling outliers:
dropping: remove extreme values.
capping: replace outliers with upper/lower limits.
isolating: keep outliers separate for analysis.
exploratory data analysis (eda) & visualization
matrix plot: shows feature distributions and clusters.
box plot: helps find outliers.
pair plot: shows how features are related.
crisp-dm framework (data mining process)

steps
1. business understanding â€“ know the problem and goal.
2. data understanding â€“ explore the data.
3. data preparation â€“ clean and organize the data.


4. modeling â€“ build predictive models.


5. evaluation â€“ test and check model performance.


6. deployment â€“ use the model in real-world scenarios.





the process runs in cycles, meaning steps are repeated.

setting deadlines is important to avoid endless improvements.","data preprocessing & handling missing data filling missing data: use ml models like regression. fill gaps using forward/backward methods. fill based on column data distribution. finding outliers: use probability and standard deviation. use boxplots to detect unusual values. dbscan method helps in detecting outliers. outliers affect the mean but not the median. handling outliers: dropping: remove extreme values. capping: replace outliers with upper/lower limits. isolating: keep outliers separate for analysis. exploratory data analysis (eda) & visualization matrix plot: shows feature distributions and clusters. box plot: helps find outliers. pair plot: shows how features are related. crisp-dm framework (data mining process) steps 1. business understanding know the problem and goal. 2. data understanding explore the data. 3. data preparation clean and organize the data. 4. modeling build predictive models. 5. evaluation test and check model performance. 6. deployment use the model in real-world scenarios. the process runs in cycles, meaning steps are repeated. setting deadlines is important to avoid endless improvements.",9,-17.530163,18.33288,8.624252,8.573209,"dataâ, analyse, analyses"
148,"we resumed where we left off in the previous class, discussing the terms that emerged from our extension (data analysis toolpack). we then examined their terminologies and connections, the meaning of these values, their graphical interpretation, and conclusions along with any associated errors or uncertainties. we examined a few beta and beta 0 examples together with their various particular instance circumstances. among these were the p-value and its fundamental application. additionally, we examined multiple linear regression and how certain of the terms that appeared in our table were primarily pertinent to this type of analysis. additionally, we examined the definition of anova and the concepts it encompasses (f statistic and why it should be large).","we resumed where we left off in the previous class, discussing the terms that emerged from our extension (data analysis toolpack). we then examined their terminologies and connections, the meaning of these values, their graphical interpretation, and conclusions along with any associated errors or uncertainties. we examined a few beta and beta 0 examples together with their various particular instance circumstances. among these were the p-value and its fundamental application. additionally, we examined multiple linear regression and how certain of the terms that appeared in our table were primarily pertinent to this type of analysis. additionally, we examined the definition of anova and the concepts it encompasses (f statistic and why it should be large).",13,4.4718814,17.25602,12.608316,5.8901677,"classification, classifying, classifications"
149,"with a focus on data analysis techniques, this session begun by reviewing key points from the midsem exam. we covered topics such as density estimation through kde graphs, validating data ranges, addressing absent values, and utilizing box diagrams to detect anomalies which was to be done in the exam. data normalization needs were identified, and though the concept of oversampling cases of heart disease was considered, the extreme class imbalance foreclosed the possibility.  due to multicollinearity i.e a statistical concept where several independent variables in a model are correlated the variables were correlated just not visible to be. we computed r2 statistics between pairs of features and employed the variance inflation factor to resolve this. what we found was that only 6 features were in effect independent, with all the other features being linear combinations of these.
we spoke about several performance evaluation methods, such as the confusion matrix.  the curse of dimensionality is a phenomenon that occurs when analyzing data in high-dimensional spaces. it can impact the accuracy of models, the speed of algorithms, and the ability to distinguish between data points which was also talked about in the class.  increasing the dataset or using dimensionality reduction techniques to maximize feature selection are two potential solutions to this problem.","with a focus on data analysis techniques, this session begun by reviewing key points from the midsem exam. we covered topics such as density estimation through kde graphs, validating data ranges, addressing absent values, and utilizing box diagrams to detect anomalies which was to be done in the exam. data normalization needs were identified, and though the concept of oversampling cases of heart disease was considered, the extreme class imbalance foreclosed the possibility. due to multicollinearity i.e a statistical concept where several independent variables in a model are correlated the variables were correlated just not visible to be. we computed r2 statistics between pairs of features and employed the variance inflation factor to resolve this. what we found was that only 6 features were in effect independent, with all the other features being linear combinations of these. we spoke about several performance evaluation methods, such as the confusion matrix. the curse of dimensionality is a phenomenon that occurs when analyzing data in high-dimensional spaces. it can impact the accuracy of models, the speed of algorithms, and the ability to distinguish between data points which was also talked about in the class. increasing the dataset or using dimensionality reduction techniques to maximize feature selection are two potential solutions to this problem.",13,-7.954096,10.283249,9.222244,7.6270204,"classification, classifying, classifications"
150,"first we analysed pivot tables using pivot tables, we can use them to calculate averages, totals and counts. we analysed stats like mean, median and variance. we also analysed how to deal with class imbalances.","first we analysed pivot tables using pivot tables, we can use them to calculate averages, totals and counts. we analysed stats like mean, median and variance. we also analysed how to deal with class imbalances.",6,-13.694135,28.28471,7.1552896,10.05563,"summarizing, summarize, summarization"
151,"today's lecture discussed dimensionality reduction and feature encoding methods. t-sne is good for visualising high-dimensional data but not for model building because it can lose information, while pca retains variance and can be applied to modeling. in feature encoding for multiclass and multilabel problems, encoding must be done with care to avoid ambiguity. target (y-values) but not feature (x-values) are label encoded since x affects predictions. integer encoding is applied when there is a natural ordering of categorical values, whereas one-hot encoding adds dimensions, which may lead to the curse of dimensionality. frequency encoding substitutes labels with counts of occurrences but is inappropriate for y-values because of potential overlap between classes. target encoding utilizes statistical aggregates of the target variable and may enhance model performance in certain scenarios. these methods assist in managing categorical data effectively for machine learning models.","today's lecture discussed dimensionality reduction and feature encoding methods. t-sne is good for visualising high-dimensional data but not for model building because it can lose information, while pca retains variance and can be applied to modeling. in feature encoding for multiclass and multilabel problems, encoding must be done with care to avoid ambiguity. target (y-values) but not feature (x-values) are label encoded since x affects predictions. integer encoding is applied when there is a natural ordering of categorical values, whereas one-hot encoding adds dimensions, which may lead to the curse of dimensionality. frequency encoding substitutes labels with counts of occurrences but is inappropriate for y-values because of potential overlap between classes. target encoding utilizes statistical aggregates of the target variable and may enhance model performance in certain scenarios. these methods assist in managing categorical data effectively for machine learning models.",3,-42.721634,1.3618238,-0.049373426,6.361301,"categorical, categorization, categorise"
152,"today's class covered forward and backward feature selection. forward selection adds features one by one, while backward selection removes the least important ones, both aiming to improve model performance. we also discussed and compared machine learning models including random forest, knn, artificial neural networks, xgboost, etc. 
also that the life of a data scientist includes spending time with numbers, models, and comparisons. we also discussed what a neural network is: a model made up of layers of nodes (neurons), including input, hidden, and output layers, where hidden layers help detect patterns and improve predictions by adjusting connections between neurons. in regression, we discussed the idea that regression means moving towards mediocrity, implying that extreme values tend to average out over time. logistic regression, however, is not about fitting a line but rather about modeling probabilities using the logistic function. the logistic unit, based on the sigmoid function 1/(1+e^(-x)), acts as a switch by mapping input values between 0 and 1, making it suitable for classification tasks.","today's class covered forward and backward feature selection. forward selection adds features one by one, while backward selection removes the least important ones, both aiming to improve model performance. we also discussed and compared machine learning models including random forest, knn, artificial neural networks, xgboost, etc. also that the life of a data scientist includes spending time with numbers, models, and comparisons. we also discussed what a neural network is: a model made up of layers of nodes (neurons), including input, hidden, and output layers, where hidden layers help detect patterns and improve predictions by adjusting connections between neurons. in regression, we discussed the idea that regression means moving towards mediocrity, implying that extreme values tend to average out over time. logistic regression, however, is not about fitting a line but rather about modeling probabilities using the logistic function. the logistic unit, based on the sigmoid function 1/(1+e^(-x)), acts as a switch by mapping input values between 0 and 1, making it suitable for classification tasks.",0,-0.96595603,-5.8698893,8.965046,4.3326735,"models, feature, features"
153,"standard error : indicates how different the sample mean is likely to be from the population mean. standard error for normal distribution ïƒ/âˆšn (known variance of population), s/âˆšn (population variance unknown). normal and t distribution. if population standard deviation is known, errors will follow normal distribution. if population standard deviation is unknown and sample size is <30, they will follow a t distribution.
test statistics for normal (z stat) and t (t stat) distributions. 
concept of p-value and relation with confidence interval. statistical equivalence of 2 values in an interval. a regression is not obtained if in y = î²0 + î²1x, î²1 is statistically equivalent to 0. multiple linear regression and anova (analysis of variance) as a tool to compare statistical equivalence of multiple averages simultaneously. f - statistics = msr/mse","standard error : indicates how different the sample mean is likely to be from the population mean. standard error for normal distribution / n (known variance of population), s/ n (population variance unknown). normal and t distribution. if population standard deviation is known, errors will follow normal distribution. if population standard deviation is unknown and sample size is <30, they will follow a t distribution. test statistics for normal (z stat) and t (t stat) distributions. concept of p-value and relation with confidence interval. statistical equivalence of 2 values in an interval. a regression is not obtained if in y = 0 + 1x, 1 is statistically equivalent to 0. multiple linear regression and anova (analysis of variance) as a tool to compare statistical equivalence of multiple averages simultaneously. f - statistics = msr/mse",7,39.174377,4.09793,15.169307,2.9010994,"statistics, statistical, statisticsâ"
154,"topics introduced:
1. empirical equation:
   - general form: y= beta_0 + beta_1 * l 
   - briefly explained as a basis for understanding simple linear relationships.

2. statistical and machine learning models:
   -slr (simple linear regression): explained as a method to model relationships between a dependent variable and one independent variable.
   -mlr (multiple linear regression): generalization of slr to multiple independent variables.
   -logistic regression: introduced as a classification technique.
   - random forest: brief mention as a popular ensemble learning method.
   - k-means clustering: used for grouping data into clusters.
   - hierarchical clustering: another clustering method, organizing data into a hierarchy.

key concept discussed:
4 levels of measurement:
1. nominal:
  - classification 
   - data with categories, no inherent order.
   - example: gender (male, female, other).
2. ordinal:
  - classification 
   - data with categories that can be ranked.
   - example: educational levels (high school, undergraduate, graduate).
3. interval:
  -  regression
   - data with measurable differences but no true zero.
   - example: temperature in celsius.
4. ratio:
  -  regression
   - data with a true zero and meaningful ratios.
   - example: height, weight.

ml categories: briefly introduced the models into supervised (e.g., slr, mlr, logistic regression, random forest) and unsupervised (e.g., k-means, hierarchical clustering) learning categories.","topics introduced: 1. empirical equation: - general form: y= beta_0 + beta_1 * l - briefly explained as a basis for understanding simple linear relationships. 2. statistical and machine learning models: -slr (simple linear regression): explained as a method to model relationships between a dependent variable and one independent variable. -mlr (multiple linear regression): generalization of slr to multiple independent variables. -logistic regression: introduced as a classification technique. - random forest: brief mention as a popular ensemble learning method. - k-means clustering: used for grouping data into clusters. - hierarchical clustering: another clustering method, organizing data into a hierarchy. key concept discussed: 4 levels of measurement: 1. nominal: - classification - data with categories, no inherent order. - example: gender (male, female, other). 2. ordinal: - classification - data with categories that can be ranked. - example: educational levels (high school, undergraduate, graduate). 3. interval: - regression - data with measurable differences but no true zero. - example: temperature in celsius. 4. ratio: - regression - data with a true zero and meaningful ratios. - example: height, weight. ml categories: briefly introduced the models into supervised (e.g., slr, mlr, logistic regression, random forest) and unsupervised (e.g., k-means, hierarchical clustering) learning categories.",4,-20.257498,-17.942327,2.0684261,0.43730274,"classification, classifying, classifications"
155,"in this session, we walked through the real-world machine learning (ml) cycle, starting from getting data â†’ exploring it â†’ cleaning & preprocessing â†’ trying multiple models â†’ choosing the best one (or even a mix of models).
feature selection & model search
â€¢	grid search: tries all possible parameter combinations to find the best one.
â€¢	forward & backward elimination: adds or removes features step by step to improve performance.
understanding ml models
â€¢	linear regression: predicts outcomes using a linear relationship with features (parametric, allows delta-based analysis).
â€¢	random forest: a non-parametric model that works on decision trees; no direct delta-based analysis.
â€¢	neural networks & xgboost: examples of models where weights are learned. more layers = deep learning. too much data with fewer constraints can cause overfitting.
classification & logistic regression
â€¢	logistic regression: used when the outcome is a category (supervised learning). the different groups are called labels.","in this session, we walked through the real-world machine learning (ml) cycle, starting from getting data exploring it cleaning & preprocessing trying multiple models choosing the best one (or even a mix of models). feature selection & model search grid search: tries all possible parameter combinations to find the best one. forward & backward elimination: adds or removes features step by step to improve performance. understanding ml models linear regression: predicts outcomes using a linear relationship with features (parametric, allows delta-based analysis). random forest: a non-parametric model that works on decision trees; no direct delta-based analysis. neural networks & xgboost: examples of models where weights are learned. more layers = deep learning. too much data with fewer constraints can cause overfitting. classification & logistic regression logistic regression: used when the outcome is a category (supervised learning). the different groups are called labels.",0,-0.5822068,-5.6570973,9.347152,4.0652194,"models, feature, features"
156,"in todayâ€™s lecture we learnt about the implementation of simple linear regression on excel. first we calculated the coefficients of regression by applying formula. then we obtained the scatter plot of data points and the corresponding regression line. we also used direct excel data analysis tool to verify the obtained result. it also gave certain values like r square, adjusted r square, confidence intervals of calculated coefficients etc. we tried to understand the meaning of those parameters. we were also introduced to the concept of histograms and how it gives idea of frequency of values bin-wise. we can determine the distribution of data through histogram. we obtained the method to calculate r square which is dependent on sum of square of errors and residuals. r square lies between 0 and 1. for a good model r squared value is close to 1. for simple linear regression, r squared is simply the square of correlation coefficient between dependent and independent variable. at the end of the lecture, the discussion was centered around the confidence interval and how standard error is calculated.","in today s lecture we learnt about the implementation of simple linear regression on excel. first we calculated the coefficients of regression by applying formula. then we obtained the scatter plot of data points and the corresponding regression line. we also used direct excel data analysis tool to verify the obtained result. it also gave certain values like r square, adjusted r square, confidence intervals of calculated coefficients etc. we tried to understand the meaning of those parameters. we were also introduced to the concept of histograms and how it gives idea of frequency of values bin-wise. we can determine the distribution of data through histogram. we obtained the method to calculate r square which is dependent on sum of square of errors and residuals. r square lies between 0 and 1. for a good model r squared value is close to 1. for simple linear regression, r squared is simply the square of correlation coefficient between dependent and independent variable. at the end of the lecture, the discussion was centered around the confidence interval and how standard error is calculated.",5,20.157076,-2.5909781,13.89278,4.97082,"regression, statistical, statistics"
157,"attributes are count, mean, median, mode while operations are sum, product etc. attributes for population these are called parameters. attributes for samples are called statistics. 

in supervised learning, simple regression model is regression model where y (dependent variable) is modelled c+mx where x is independent variable. here c represents bias term which represents all unaccounted factors(variables). c and m are called estimates of population parameters.
model is made from seen data to predict unseen situations.as the sample size increases, prediction error decreases and statistics get better.

when we find closed form estimated parameters, we have zero confidence in them because they are just points. confidence increases when we find intervals for estimated parameters.

to find the estimates, we try to find parameters which minimises some defined function of ei= yi-yi^{hat} which is called loss functions.

some famous loss functions are:
1. âˆ‘ei : +ve and -ve terms gets cancelled which is not good
2. â âˆ‘|ei| : sphere of influence is diamond, it is biased
3. â âˆ‘ eiâ² : sphere of influence is circle which is not biased","attributes are count, mean, median, mode while operations are sum, product etc. attributes for population these are called parameters. attributes for samples are called statistics. in supervised learning, simple regression model is regression model where y (dependent variable) is modelled c+mx where x is independent variable. here c represents bias term which represents all unaccounted factors(variables). c and m are called estimates of population parameters. model is made from seen data to predict unseen situations.as the sample size increases, prediction error decreases and statistics get better. when we find closed form estimated parameters, we have zero confidence in them because they are just points. confidence increases when we find intervals for estimated parameters. to find the estimates, we try to find parameters which minimises some defined function of ei= yi-yi^{hat} which is called loss functions. some famous loss functions are: 1. ei : +ve and -ve terms gets cancelled which is not good 2. |ei| : sphere of influence is diamond, it is biased 3. ei : sphere of influence is circle which is not biased",1,37.528137,-9.212085,16.380274,3.9466734,"population, models, estimating"
158,"in today's class, sir explained vif(variance inflation factor) and pca(principal component analysis). from the plot of vif vs râ², we came to know that râ² values also came down if we mechanically tend to reduce the number of features excessively to reduce the vif, but this can lead to bad prediction model. thus there is always an trade-off between lowering of vif value and minimum râ² value you need for your model. higher vif or higher p-values suggest that the parameter/features' dependence is larger on other parameters, thus we can drop those high depending parameters. then we learned more about pcs (principal components); 1. pcs are perpendicular to each other, 2. number of pcs = number of dimensions in original dataset, 3. pc1 represents the highest variance, pc2 represents second highest variance and so on.. 
we will take number of pcs such that their cumulative variance fraction is >=0.95. then we discussed loadings of pcs and that the latter is the linear combination of variances. then we discussed interpretibility, real features (vif), non-real features (pca), and pros and cons of pca wrt to vif. 
at last, sir discussed t-sne (t-distributed stochastic neighbour encoding) which maps n-dimension distance probability (found using gaussian distribution) into 2-d or at max 3-d (now the probability if calculated using t-distribution). why t-distribution? because it can give probability of distant points in distribution as compared to gaussian distribution where probability of distant point in distribution is nearly zero. also t-distribution is helpful in another way as it further increases the distance between points when on mapped on 2-d space as compared to the distance on n-dimension.","in today's class, sir explained vif(variance inflation factor) and pca(principal component analysis). from the plot of vif vs r , we came to know that r values also came down if we mechanically tend to reduce the number of features excessively to reduce the vif, but this can lead to bad prediction model. thus there is always an trade-off between lowering of vif value and minimum r value you need for your model. higher vif or higher p-values suggest that the parameter/features' dependence is larger on other parameters, thus we can drop those high depending parameters. then we learned more about pcs (principal components); 1. pcs are perpendicular to each other, 2. number of pcs = number of dimensions in original dataset, 3. pc1 represents the highest variance, pc2 represents second highest variance and so on.. we will take number of pcs such that their cumulative variance fraction is >=0.95. then we discussed loadings of pcs and that the latter is the linear combination of variances. then we discussed interpretibility, real features (vif), non-real features (pca), and pros and cons of pca wrt to vif. at last, sir discussed t-sne (t-distributed stochastic neighbour encoding) which maps n-dimension distance probability (found using gaussian distribution) into 2-d or at max 3-d (now the probability if calculated using t-distribution). why t-distribution? because it can give probability of distant points in distribution as compared to gaussian distribution where probability of distant point in distribution is nearly zero. also t-distribution is helpful in another way as it further increases the distance between points when on mapped on 2-d space as compared to the distance on n-dimension.",11,-20.230135,1.4230219,10.418461,13.393194,"pca, heatmap, heatmaps"
159,"we began by using logistic regression and learned to compute the weights. the intention is to predict outcomes such that our predicted ones match the true targets as much as possible. if the actual outcome is 1, we want to maximize the probability of predicting 1, and if it's 0, we want to maximize the probability of predicting 0. we look at the overall objective across all training data. since working with products in probability calculations can be tricky, we take the logarithm to simplify things, turning them into sums instead. we evaluate how well our model performs by using a confusion matrix, which breaks down predictions into four categories: true negatives, false positives, false negatives, and true positives. from this, we define key performance metrics. accuracy tells us how many predictions we got right overall. precision measures how many of our predicted positives were actually correct, while recall checks how many actual positive cases we successfully identified. finally, the f1 score, which is the harmonic mean of precision and recall, gives a more balanced assessment, avoiding the misleading optimism that accuracy alone can sometimes create.","we began by using logistic regression and learned to compute the weights. the intention is to predict outcomes such that our predicted ones match the true targets as much as possible. if the actual outcome is 1, we want to maximize the probability of predicting 1, and if it's 0, we want to maximize the probability of predicting 0. we look at the overall objective across all training data. since working with products in probability calculations can be tricky, we take the logarithm to simplify things, turning them into sums instead. we evaluate how well our model performs by using a confusion matrix, which breaks down predictions into four categories: true negatives, false positives, false negatives, and true positives. from this, we define key performance metrics. accuracy tells us how many predictions we got right overall. precision measures how many of our predicted positives were actually correct, while recall checks how many actual positive cases we successfully identified. finally, the f1 score, which is the harmonic mean of precision and recall, gives a more balanced assessment, avoiding the misleading optimism that accuracy alone can sometimes create.",10,13.197141,-22.191442,8.6863,-1.599816,"classifications, histograms, histogram"
160,"in this lecture, we explored how to perform exploratory data analysis (eda) on multiple datasets to extract useful information.

we first worked with a dataset containing summaries submitted by students. using an excel pivot table, we calculated important statistics like the mean, maximum, minimum, and standard deviation of the number of characters in each summary. to gain more insight into the distribution, we plotted histograms and scatter plots, which helped us understand how the data was spread out. additionally, we analyzed the minimum number of characters in the summaries for each day and plotted it against the dates. we did the same for the maximum and average values and also tracked the number of students submitting summaries each day. moreover, we examined each student's submission patterns, such as the number of characters they submitted and whether they submitted summaries on specific days.

next, we analyzed a sensor dataset with observations for each date. using pivot tables, we counted the number of data points for each year, which helped us select which data to use for training. we also visualized the input and output columns using line graphs, which allowed us to observe how the output changed with different inputs. by applying principal component analysis (pca), we discovered that only 17 out of 240 columns were needed to capture most of the variance in the data, meaning these 17 columns would be the independent features to use for further analysis.

finally, we worked with transformer data to create a time-series dataset. we focused on understanding the patterns over time and identified anomalies in the data. for example, we noticed unexpected fluctuations during the night when no output was expected, indicating a problem with the system.","in this lecture, we explored how to perform exploratory data analysis (eda) on multiple datasets to extract useful information. we first worked with a dataset containing summaries submitted by students. using an excel pivot table, we calculated important statistics like the mean, maximum, minimum, and standard deviation of the number of characters in each summary. to gain more insight into the distribution, we plotted histograms and scatter plots, which helped us understand how the data was spread out. additionally, we analyzed the minimum number of characters in the summaries for each day and plotted it against the dates. we did the same for the maximum and average values and also tracked the number of students submitting summaries each day. moreover, we examined each student's submission patterns, such as the number of characters they submitted and whether they submitted summaries on specific days. next, we analyzed a sensor dataset with observations for each date. using pivot tables, we counted the number of data points for each year, which helped us select which data to use for training. we also visualized the input and output columns using line graphs, which allowed us to observe how the output changed with different inputs. by applying principal component analysis (pca), we discovered that only 17 out of 240 columns were needed to capture most of the variance in the data, meaning these 17 columns would be the independent features to use for further analysis. finally, we worked with transformer data to create a time-series dataset. we focused on understanding the patterns over time and identified anomalies in the data. for example, we noticed unexpected fluctuations during the night when no output was expected, indicating a problem with the system.",9,-10.124178,24.679419,7.8552113,10.243971,"dataâ, analyse, analyses"
161,"today's lecture covered population parameters, sample statistics, and simple linear regression with the equation y = bâ‚€ + bâ‚x, where bâ‚€ (intercept) and bâ‚ (slope) estimate population parameters. bias was defined as the effect of unaccounted variables.

we compared error methods like sum of absolute errors and sum of squared errors (sse), favoring sse for its sensitivity to outliers. formulas for bâ‚€ and bâ‚ were derived, and confidence intervals were introduced for more reliable parameter estimation. this formed the basis of regression modeling and error analysis.","today's lecture covered population parameters, sample statistics, and simple linear regression with the equation y = b + b x, where b (intercept) and b (slope) estimate population parameters. bias was defined as the effect of unaccounted variables. we compared error methods like sum of absolute errors and sum of squared errors (sse), favoring sse for its sensitivity to outliers. formulas for b and b were derived, and confidence intervals were introduced for more reliable parameter estimation. this formed the basis of regression modeling and error analysis.",1,30.518427,-10.534402,16.47815,4.328155,"population, models, estimating"
162,"we started by looking into what statistics is in simple terms. it is using sample to predict and understand the overall population. then the estimate that we make for a sample is a statistic and it is the best estimate that we obtain of the actual population value called the parameter(population parameter). then we started looking into linear regression. we started with the example of sales vs advertisement data that of a company. there was like an upward pattern, where the sales was increasing with the increase in advertisement. the objective was to find a curve that could give a function 'f' in y=f(x) for this problem. we tried a linear function, y = b0+b1x. we saw that the term b0 is called bias because, it is telling that there is a bias in the output variable that even without any advertisement, the sales was non zero. then we used minimizing residual sum method to find the values of b0 and b1. we also saw that mean of the input features and the output, both lie on the best fit curve.","we started by looking into what statistics is in simple terms. it is using sample to predict and understand the overall population. then the estimate that we make for a sample is a statistic and it is the best estimate that we obtain of the actual population value called the parameter(population parameter). then we started looking into linear regression. we started with the example of sales vs advertisement data that of a company. there was like an upward pattern, where the sales was increasing with the increase in advertisement. the objective was to find a curve that could give a function 'f' in y=f(x) for this problem. we tried a linear function, y = b0+b1x. we saw that the term b0 is called bias because, it is telling that there is a bias in the output variable that even without any advertisement, the sales was non zero. then we used minimizing residual sum method to find the values of b0 and b1. we also saw that mean of the input features and the output, both lie on the best fit curve.",1,30.055176,-4.4898624,15.867383,4.0522285,"population, models, estimating"
163,"in today's class we have learned about how to improve quality of result 1. improving quality of sample 2. using different model and choosing the best one . linear regression models the relationship between independent variables and an outcome as a linear combination but the model can be non linear also. each function can be computed from the taylor series . we also learned that if we add unnecessary parameters adusted r^2 value will decrease . models that can be categorized 
as  parametric and non parametric. parametric models are flexible and allow for delta analysis directly  ex- simple linear regression and family. observing how the label changes upon changing a particular feature. non parametric models examples - random forest, xg boost, k nearest neighbors etc. then we saw neural networks where computer have mapping similar to human brain consisting of many hidden layer if hidden layers are more 1 then it is a deep network.","in today's class we have learned about how to improve quality of result 1. improving quality of sample 2. using different model and choosing the best one . linear regression models the relationship between independent variables and an outcome as a linear combination but the model can be non linear also. each function can be computed from the taylor series . we also learned that if we add unnecessary parameters adusted r^2 value will decrease . models that can be categorized as parametric and non parametric. parametric models are flexible and allow for delta analysis directly ex- simple linear regression and family. observing how the label changes upon changing a particular feature. non parametric models examples - random forest, xg boost, k nearest neighbors etc. then we saw neural networks where computer have mapping similar to human brain consisting of many hidden layer if hidden layers are more 1 then it is a deep network.",0,2.8891954,-1.7383109,9.835642,4.1662183,"models, feature, features"
164,"in todayâ€™s class, i learnt about the key concepts related to classification and the importance of feature engineering in improving the performance of classifiers. if we engineer new features from the provided features it will be a better model and capture the dependencies. sometimes using the basic features cannot help in providing better results.

we also learnt the concept of standard error, which represents the standard deviation of the sampling distribution of the sample mean. it was explained that if the population standard deviation is known, the standard error can be derived.","in today s class, i learnt about the key concepts related to classification and the importance of feature engineering in improving the performance of classifiers. if we engineer new features from the provided features it will be a better model and capture the dependencies. sometimes using the basic features cannot help in providing better results. we also learnt the concept of standard error, which represents the standard deviation of the sampling distribution of the sample mean. it was explained that if the population standard deviation is known, the standard error can be derived.",7,30.302282,4.2461634,15.081694,3.1979964,"statistics, statistical, statisticsâ"
165,"in the lecture we learned about nominal, interval, ratio scales and how are they used in the field of data science. we also learned briefly about supervised and unsupervised learnings i.e. if the data is labelled or unlabelled. the prof also discussed about various ml models and telling their types and uses accordingly in a brief manner.","in the lecture we learned about nominal, interval, ratio scales and how are they used in the field of data science. we also learned briefly about supervised and unsupervised learnings i.e. if the data is labelled or unlabelled. the prof also discussed about various ml models and telling their types and uses accordingly in a brief manner.",4,-18.390581,-11.806581,2.206681,1.0093886,"classification, classifying, classifications"
166,"standard error : indicates how different the sample mean is likely to be from the population mean. standard error for normal distribution s/âˆšn. normal and t distribution. if population standard deviation is known, errors will follow normal distribution. if population standard deviation is unknown and sample size is <30, they will follow a t distribution.
test statistics for normal (z stat) and t (stat) distributions. 
concept of p-value and relation with confidence interval. statistical equivalence of 2 values in an interval. a regression is not obtained if in y = î²0 + î²1x, î²1 is statistically equivalent to 0. multiple linear regression and anova (analysis of variance) as a tool to compare statistical equivalence of multiple averages simultaneously. f - statistics = msr/mse","standard error : indicates how different the sample mean is likely to be from the population mean. standard error for normal distribution s/ n. normal and t distribution. if population standard deviation is known, errors will follow normal distribution. if population standard deviation is unknown and sample size is <30, they will follow a t distribution. test statistics for normal (z stat) and t (stat) distributions. concept of p-value and relation with confidence interval. statistical equivalence of 2 values in an interval. a regression is not obtained if in y = 0 + 1x, 1 is statistically equivalent to 0. multiple linear regression and anova (analysis of variance) as a tool to compare statistical equivalence of multiple averages simultaneously. f - statistics = msr/mse",7,39.184956,4.1043096,15.069504,2.8716145,"statistics, statistical, statisticsâ"
167,"the session started with a recap of the mid-semester exam, emphasizing problem-solving strategies and methods to address various kinds of questions efficiently. this served to reinforce important concepts and enhance analytical thinking for similar problems in the future.

next, the discussion covered the primary steps involved in exploratory data analysis (eda). this included data visualization techniques such as histograms and heat maps, which help in understanding data distributions and correlations. additionally, handling missing values was discussed, emphasizing methods like imputation or dropping missing data based on context.

one of the major challenges with the dataset was the under-sampling of the target class, ""heart diseases."" imbalance in this form can severely influence prediction accuracy, causing the model to become biased. the drawback of using an imbalanced dataset was discussed, as well as possible remedies in the form of resampling techniques or utilizing correct evaluation measures like precision-recall curves.

feature transformation and selection were also investigated. the conditions under which columns were to be dropped were described, such as when features were highly collinear or had minimal variance. feature scaling was highlighted as important, especially for models that are sensitive to feature magnitude, like support vector machines (svm) and logistic regression.

model selection was informed by data type and problem. tree models, svm, and logistic regression were the contenders as viable candidates. the pros and cons of each model were assessed based on the characteristics of the dataset, making an informed decision.

lastly, the curse of dimensionality was discussed, meaning the issues that arise from dealing with high-dimensional data, including overfitting and higher computational costs. the variance inflation factor (vif) was identified as a metric for the detection of multicollinearity to assist in feature selection and enhancing model performance.","the session started with a recap of the mid-semester exam, emphasizing problem-solving strategies and methods to address various kinds of questions efficiently. this served to reinforce important concepts and enhance analytical thinking for similar problems in the future. next, the discussion covered the primary steps involved in exploratory data analysis (eda). this included data visualization techniques such as histograms and heat maps, which help in understanding data distributions and correlations. additionally, handling missing values was discussed, emphasizing methods like imputation or dropping missing data based on context. one of the major challenges with the dataset was the under-sampling of the target class, ""heart diseases."" imbalance in this form can severely influence prediction accuracy, causing the model to become biased. the drawback of using an imbalanced dataset was discussed, as well as possible remedies in the form of resampling techniques or utilizing correct evaluation measures like precision-recall curves. feature transformation and selection were also investigated. the conditions under which columns were to be dropped were described, such as when features were highly collinear or had minimal variance. feature scaling was highlighted as important, especially for models that are sensitive to feature magnitude, like support vector machines (svm) and logistic regression. model selection was informed by data type and problem. tree models, svm, and logistic regression were the contenders as viable candidates. the pros and cons of each model were assessed based on the characteristics of the dataset, making an informed decision. lastly, the curse of dimensionality was discussed, meaning the issues that arise from dealing with high-dimensional data, including overfitting and higher computational costs. the variance inflation factor (vif) was identified as a metric for the detection of multicollinearity to assist in feature selection and enhancing model performance.",13,-12.329711,11.035508,8.833147,7.8302364,"classification, classifying, classifications"
168,"the lecture comprised of how do we get the empirical equation showing the relations between inputs and outputs of the dataset. now to measure this equations, there are four types of measurements which we can do. nominal,  which includes indentify gender where we also discussed a fundamental problem of giving one of the class a higher numeric value than other even though we don't want to. ordinal, which includes classifying according to ranks like grades or salary. interval, which includes data measured along any scale where we discussed that the same difference between two scales have different significance ( 2.5 feet and 5 feet ; 5â°c and 10â°c ). ratio, which can be said as quantitative version of interval scale as it has some zero value which can serve as a reference point. then we discussed about the difference between supervised and unsupervised learning which nothing but the absence of labels in case of unsupervised learning. then we vaguely moved through the process of how the empirical relation is obtained by first clustering the features and labelling them to gain the required function.","the lecture comprised of how do we get the empirical equation showing the relations between inputs and outputs of the dataset. now to measure this equations, there are four types of measurements which we can do. nominal, which includes indentify gender where we also discussed a fundamental problem of giving one of the class a higher numeric value than other even though we don't want to. ordinal, which includes classifying according to ranks like grades or salary. interval, which includes data measured along any scale where we discussed that the same difference between two scales have different significance ( 2.5 feet and 5 feet ; 5 c and 10 c ). ratio, which can be said as quantitative version of interval scale as it has some zero value which can serve as a reference point. then we discussed about the difference between supervised and unsupervised learning which nothing but the absence of labels in case of unsupervised learning. then we vaguely moved through the process of how the empirical relation is obtained by first clustering the features and labelling them to gain the required function.",4,-25.514109,-11.257229,1.4515489,0.4097707,"classification, classifying, classifications"
169,"today's class start with a question that we have only 1 sample (eg 30 observations) so, how to estimate population mean based only on 1 sample ? is it possible. one interesting thing that when errors become predictive then it is not a regression model. whatever the distribution of sample could be -> when we take their multiple sample then we always get normal distribution. let's suppose we have sample from a population then first step would be calculate the mean and(assume this mean is close to the population mean). if it is close to the population mean then sampling distribution of the mean is close normal distribution with mean and variance. the formula for calculating standard deviation of the sampling distribution of means is standard deviation of the population /root under number of observations in sample. further ahead we have learnt about the confidence interval which is basically the area under curve. for 95% confidence, the area under curve is 0.95 and for the observations that doesn't lie in this region is 0.025 each of side. one more important thing what we learnt that if number of observations is less than 30 then we do not get normal distribution and we use t distribution which is basically a continuous probability distribution that generalizes the standard normal distribution. the 95% confidence interval practically means that if we take 100 samples the mean of 95 of those samples will lies between that area. in the equation y=b0 + b1x if b1 is statistically equal to zero then we don't get any regression. if p- value < 0.05 then value of b1 is accepted and p-value also helps in the selection of the features. in the last minutes we also talked about the multiple linear regression which is a statistical technique that uses multiple independent variables to predict the value of a dependent variable and anova -> which is used to compare statistically equivalence of ""multiple averages"" simultaneously. the equation for mlr is y=b0 + b1x1 + b2x2 + b3x3 ....and if atleast on of coefficient is not zero then regression is possible. f-statistic = msr/mse.","today's class start with a question that we have only 1 sample (eg 30 observations) so, how to estimate population mean based only on 1 sample ? is it possible. one interesting thing that when errors become predictive then it is not a regression model. whatever the distribution of sample could be -> when we take their multiple sample then we always get normal distribution. let's suppose we have sample from a population then first step would be calculate the mean and(assume this mean is close to the population mean). if it is close to the population mean then sampling distribution of the mean is close normal distribution with mean and variance. the formula for calculating standard deviation of the sampling distribution of means is standard deviation of the population /root under number of observations in sample. further ahead we have learnt about the confidence interval which is basically the area under curve. for 95% confidence, the area under curve is 0.95 and for the observations that doesn't lie in this region is 0.025 each of side. one more important thing what we learnt that if number of observations is less than 30 then we do not get normal distribution and we use t distribution which is basically a continuous probability distribution that generalizes the standard normal distribution. the 95% confidence interval practically means that if we take 100 samples the mean of 95 of those samples will lies between that area. in the equation y=b0 + b1x if b1 is statistically equal to zero then we don't get any regression. if p- value < 0.05 then value of b1 is accepted and p-value also helps in the selection of the features. in the last minutes we also talked about the multiple linear regression which is a statistical technique that uses multiple independent variables to predict the value of a dependent variable and anova -> which is used to compare statistically equivalence of ""multiple averages"" simultaneously. the equation for mlr is y=b0 + b1x1 + b2x2 + b3x3 ....and if atleast on of coefficient is not zero then regression is possible. f-statistic = msr/mse.",7,37.41846,1.9842082,15.3114605,2.5788412,"statistics, statistical, statisticsâ"
170,"we began by looking at a closed-form solution in multiple linear regression, which proved to be impractical because of complexities of inversion of matrices and multicollinearity. at feature selection, we introduced removal of features where the p-value is greater than 0.05, meaning they contribute minimally to the model. we then looked into data training and testing, which for example, should be split 80-20 after exploratory data analysis. we also evaluated the râ² values of both the training and testing datasets in order to observe potential overfitting, and the model's generalization towards the population was assured. also discussed was the concept of adjusted râ², multiple r, and why it only has (n-1) degrees of freedom in tss.","we began by looking at a closed-form solution in multiple linear regression, which proved to be impractical because of complexities of inversion of matrices and multicollinearity. at feature selection, we introduced removal of features where the p-value is greater than 0.05, meaning they contribute minimally to the model. we then looked into data training and testing, which for example, should be split 80-20 after exploratory data analysis. we also evaluated the r values of both the training and testing datasets in order to observe potential overfitting, and the model's generalization towards the population was assured. also discussed was the concept of adjusted r , multiple r, and why it only has (n-1) degrees of freedom in tss.",2,8.864375,7.918205,11.324408,5.0836453,"regression, regressions, features"
171,"we were analyzing various numbers from the summary table that we got from the output of linear regression toolkit on our dataset. they are:
1. t stat: used in calculation of p value, calculated by normalizing sample statistic with zero as mean and standard error as standard deviation.
2. p value: kind of says how statistically close is to say this estimated coefficent is zero.if it is close to zero, one can conclude that the coefficient is statistically insignificant from zero, if the value is close to one, it is statistically significant to zero, meaning this variable has little to no effect in the prediction.
3. standard error and confidence intervals: any point estimate for the population parameter exactly has zero probability of being the real value, thus it is always best to assign a interval and say, with x% confidence i can say, any sample mean will lie in this interval. for this, we first assume that the standard deviation of the sample is standard deviation of the population and compute the standard error, which is std/square_root_of(n)[n being the number of observations in the sample]. for 95% confidence one level, mu-2*standard_error to mu+2*standard_error is the interval.

then we ended the class with a small discussion on multiple linear regression, and what anova(analysis of variance).","we were analyzing various numbers from the summary table that we got from the output of linear regression toolkit on our dataset. they are: 1. t stat: used in calculation of p value, calculated by normalizing sample statistic with zero as mean and standard error as standard deviation. 2. p value: kind of says how statistically close is to say this estimated coefficent is zero.if it is close to zero, one can conclude that the coefficient is statistically insignificant from zero, if the value is close to one, it is statistically significant to zero, meaning this variable has little to no effect in the prediction. 3. standard error and confidence intervals: any point estimate for the population parameter exactly has zero probability of being the real value, thus it is always best to assign a interval and say, with x% confidence i can say, any sample mean will lie in this interval. for this, we first assume that the standard deviation of the sample is standard deviation of the population and compute the standard error, which is std/square_root_of(n)[n being the number of observations in the sample]. for 95% confidence one level, mu-2*standard_error to mu+2*standard_error is the interval. then we ended the class with a small discussion on multiple linear regression, and what anova(analysis of variance).",7,35.252426,0.33481392,15.009765,2.8700645,"statistics, statistical, statisticsâ"
172,"we learnt about some machine learning techniques to fit data.
some techniques- include simple linear regression, multiple linear regression, logistic regression and random forest.
we studied levels of measurement like :
1.) nominal(just classification, not any ranking, also discrete)
2.) ordinal(it is ordered and discrete)
3.) interval(continous, absolute 0 doesn't mean absence of anything)
4.) ratio(continous, absolute 0 is defined by nature).
also nominal and ordinal are used in classification whereas interval and ratio are used in regression","we learnt about some machine learning techniques to fit data. some techniques- include simple linear regression, multiple linear regression, logistic regression and random forest. we studied levels of measurement like : 1.) nominal(just classification, not any ranking, also discrete) 2.) ordinal(it is ordered and discrete) 3.) interval(continous, absolute 0 doesn't mean absence of anything) 4.) ratio(continous, absolute 0 is defined by nature). also nominal and ordinal are used in classification whereas interval and ratio are used in regression",4,-22.30683,-16.286343,2.1364977,0.49396497,"classification, classifying, classifications"
173,"in today's class first we learn some basic features in excel like multiplication ,taking average of the whole row and performing some other functions also . we created terms such as x bar, y bar and some more terms and then we created a scatter plot using excel only. then we used a data analysis tool in excel and created a summary output of a linear regression model which showed different types of values. we then learn about sst, ssr and sse. we also learned about histogram and how to know the nature of distribution from this. then we learnt about gaussian normal distribution which is observed when output is dependent on a large number of unknown causes which are random. for slr, the coefficient of determination is exactly like same as the square of the correlation coefficient between x and y. the more near this coefficient is to 1 the better are the results .we then learned about positive correlation and negative correlation as well. we also learnt that if we take a large amount of samples each of same size and using each of that sample we calculate the mean so if the samples are good then the mean will like close to each other and if we collect all such means and create a frequency distribution or histogram the shape will be of gaussian normal distribution.","in today's class first we learn some basic features in excel like multiplication ,taking average of the whole row and performing some other functions also . we created terms such as x bar, y bar and some more terms and then we created a scatter plot using excel only. then we used a data analysis tool in excel and created a summary output of a linear regression model which showed different types of values. we then learn about sst, ssr and sse. we also learned about histogram and how to know the nature of distribution from this. then we learnt about gaussian normal distribution which is observed when output is dependent on a large number of unknown causes which are random. for slr, the coefficient of determination is exactly like same as the square of the correlation coefficient between x and y. the more near this coefficient is to 1 the better are the results .we then learned about positive correlation and negative correlation as well. we also learnt that if we take a large amount of samples each of same size and using each of that sample we calculate the mean so if the samples are good then the mean will like close to each other and if we collect all such means and create a frequency distribution or histogram the shape will be of gaussian normal distribution.",5,25.328697,-2.5587656,14.150145,4.5018535,"regression, statistical, statistics"
174,"started with a topic in correlation between signals. moved on to using heat maps to cluster features. mention of using varuance inflation factor to eliminate correlated variables. pca serves a similar task. caution to check with domain knowledge before using a certain result from vif.

interpretability. task of predicting and sensitivity analysis are part of interpretability. principal components do not allow for sensitivity analysis. it is recommended to use vif first and proceed with pca, if needed - when there are too many variables vif produces. normalize before pca.

t-sne helped in showing clusters in mnist. but failed to show any clusters with the midsem dataset.","started with a topic in correlation between signals. moved on to using heat maps to cluster features. mention of using varuance inflation factor to eliminate correlated variables. pca serves a similar task. caution to check with domain knowledge before using a certain result from vif. interpretability. task of predicting and sensitivity analysis are part of interpretability. principal components do not allow for sensitivity analysis. it is recommended to use vif first and proceed with pca, if needed - when there are too many variables vif produces. normalize before pca. t-sne helped in showing clusters in mnist. but failed to show any clusters with the midsem dataset.",11,-15.725478,5.295961,10.289144,13.024363,"pca, heatmap, heatmaps"
175,"predicting population from samples
our objective is to accurately predict population characteristics based on a representative sample. for instance, measuring the heights of all indian citizens by solely sampling primary school students would be an inadequate approach.
key concepts
â€¢	attributes: count, mode, mean, median, standard deviation, variance.
â€¢	operators: count, add, subtract, multiply, divide.
â€¢	levels of measurement: 
o	nominal level:
ï‚§	attributes: count, mode 
ï‚§	operators: count
o	ordinal level:
ï‚§	attributes: count, mode, median 
ï‚§	operators: count
o	interval level:
ï‚§	attributes: count, mode, mean, median, standard deviation, variance 
ï‚§	operators: count, add, subtract
o	ratio level:
ï‚§	attributes: count, mode, mean, median, standard deviation, variance 
ï‚§	operators: count, add, subtract, multiply, divide
these attributes and operators provide estimates of population qualities. however, we must also account for the inherent error in these estimates.
â€¢	population: provides parameters.
â€¢	sample: provides statistics.
linear regression
suppose we collect a substantial dataset and model it using the equation:
â€¢	y = b0 + b1x
â€¢	y: dependent variable, response variable, or label.
â€¢	x: independent variable, feature, or predictor.
this linear model, while simplistic, provides a foundational understanding.
when performing linear regression on multiple samples (s1, s2) from the same population, we obtain different models. consequently, the calculated values for b0 and b1 are estimates or statistics of the true population parameters (b0p and b1p).
confidence intervals
the true population parameters (b0p and b1p) are typically unknown. therefore, we estimate their range using confidence intervals.
â€¢	b0p lies within the interval: b0 - d to b0 + d
â€¢	a point interval has 0% confidence, while an infinite interval has 100% confidence.
model fit and error minimization
to determine the best-fit line, we compare actual y values (yi) with the model's predicted values (yi-hat). the difference (error, ei) is calculated.
â€¢	error minimization: 
o	manhattan error: summation of |ei|
o	euclidean error: summation of ei^2
minimizing euclidean error often leads to a closed-form solution for b0 and b1, as in linear regression. however, this is not always the case.
point estimates and confidence intervals
ultimately, b0 and b1 are point estimates. to enhance our understanding, we construct confidence intervals (e.g., 95%, 90%, 99%) within which the true population parameters (b0p and b1p) are likely to reside.","predicting population from samples our objective is to accurately predict population characteristics based on a representative sample. for instance, measuring the heights of all indian citizens by solely sampling primary school students would be an inadequate approach. key concepts attributes: count, mode, mean, median, standard deviation, variance. operators: count, add, subtract, multiply, divide. levels of measurement: o nominal level: attributes: count, mode operators: count o ordinal level: attributes: count, mode, median operators: count o interval level: attributes: count, mode, mean, median, standard deviation, variance operators: count, add, subtract o ratio level: attributes: count, mode, mean, median, standard deviation, variance operators: count, add, subtract, multiply, divide these attributes and operators provide estimates of population qualities. however, we must also account for the inherent error in these estimates. population: provides parameters. sample: provides statistics. linear regression suppose we collect a substantial dataset and model it using the equation: y = b0 + b1x y: dependent variable, response variable, or label. x: independent variable, feature, or predictor. this linear model, while simplistic, provides a foundational understanding. when performing linear regression on multiple samples (s1, s2) from the same population, we obtain different models. consequently, the calculated values for b0 and b1 are estimates or statistics of the true population parameters (b0p and b1p). confidence intervals the true population parameters (b0p and b1p) are typically unknown. therefore, we estimate their range using confidence intervals. b0p lies within the interval: b0 - d to b0 + d a point interval has 0% confidence, while an infinite interval has 100% confidence. model fit and error minimization to determine the best-fit line, we compare actual y values (yi) with the model's predicted values (yi-hat). the difference (error, ei) is calculated. error minimization: o manhattan error: summation of |ei| o euclidean error: summation of ei^2 minimizing euclidean error often leads to a closed-form solution for b0 and b1, as in linear regression. however, this is not always the case. point estimates and confidence intervals ultimately, b0 and b1 are point estimates. to enhance our understanding, we construct confidence intervals (e.g., 95%, 90%, 99%) within which the true population parameters (b0p and b1p) are likely to reside.",1,36.52916,-10.849884,16.55667,3.977349,"population, models, estimating"
176,"today's class dealt with the statistical principles underlying linear regression as well as the evaluation of the reliability of the regression coefficients. regression coefficients are estimates obtained from sample data and are not necessarily the population values. thus, how would one assess whether such estimates are reliable or not is using statistical concepts such as sampling distributions and confidence intervals, among others.the question explained that taking several representative samples from a population leads to a normal distribution formed by the means of the samples. that knowledge can then be used to obtain a standard error in expressing variability in regression coefficients. a topic covered is using confidence intervals in order to give some limits of expectation to how a true coefficient's value will be, as far as one is allowed to make estimations of those. if the confidence interval for a coefficient does not include zero, the coefficient is statistically significant and represents a valid model.
the p-value was defined as the probability that a regression coefficient is simply zero due to chance alone. a p-value less than 0.05 indicates that the coefficient is statistically significant and represents an important relationship between variables. sample size was discussed as being important. larger samples decrease variability, giving narrower confidence intervals and more dependable conclusions, whereas smaller samples increase uncertainty and make the intervals wider. the session also brought to the fore the fact that such statistical tools must be used to ensure that linear regression models are reliable and generalizable, representing true population relationships.","today's class dealt with the statistical principles underlying linear regression as well as the evaluation of the reliability of the regression coefficients. regression coefficients are estimates obtained from sample data and are not necessarily the population values. thus, how would one assess whether such estimates are reliable or not is using statistical concepts such as sampling distributions and confidence intervals, among others.the question explained that taking several representative samples from a population leads to a normal distribution formed by the means of the samples. that knowledge can then be used to obtain a standard error in expressing variability in regression coefficients. a topic covered is using confidence intervals in order to give some limits of expectation to how a true coefficient's value will be, as far as one is allowed to make estimations of those. if the confidence interval for a coefficient does not include zero, the coefficient is statistically significant and represents a valid model. the p-value was defined as the probability that a regression coefficient is simply zero due to chance alone. a p-value less than 0.05 indicates that the coefficient is statistically significant and represents an important relationship between variables. sample size was discussed as being important. larger samples decrease variability, giving narrower confidence intervals and more dependable conclusions, whereas smaller samples increase uncertainty and make the intervals wider. the session also brought to the fore the fact that such statistical tools must be used to ensure that linear regression models are reliable and generalizable, representing true population relationships.",7,27.069523,5.246067,14.247429,3.4347003,"statistics, statistical, statisticsâ"
177,"in today's session, we discussed attributes like count, mean, mode, median, standard deviation, variance, etc which serve as a parameter for population and statistics for sample data. based on the sample, we can only estimate the attributes of the population. depending on the observations (or features) we decide which machine learning technique will produce the best predictions with minimal error. linear regression equation ( y=ax+b) where b is the bias which represents the effect of the sum of all unknown features that we have not accounted for. here, in this case, 'a' and 'b' variables are the population parameter estimates. as we increase the sample size we get better estimates. the interval range of these statistic parameters ( a and b) is called the confidence interval. the higher the confidence, the higher the interval size, and the lower the confidence, the lower the interval size. the exact value of 'a' and 'b' are point estimates and their confidence is zero percent. the mean of observations lies on the line of linear regression. we minimize the sum of the square of error of each observation to get the best line equation for linear regression.","in today's session, we discussed attributes like count, mean, mode, median, standard deviation, variance, etc which serve as a parameter for population and statistics for sample data. based on the sample, we can only estimate the attributes of the population. depending on the observations (or features) we decide which machine learning technique will produce the best predictions with minimal error. linear regression equation ( y=ax+b) where b is the bias which represents the effect of the sum of all unknown features that we have not accounted for. here, in this case, 'a' and 'b' variables are the population parameter estimates. as we increase the sample size we get better estimates. the interval range of these statistic parameters ( a and b) is called the confidence interval. the higher the confidence, the higher the interval size, and the lower the confidence, the lower the interval size. the exact value of 'a' and 'b' are point estimates and their confidence is zero percent. the mean of observations lies on the line of linear regression. we minimize the sum of the square of error of each observation to get the best line equation for linear regression.",1,34.01188,-8.176067,16.581753,4.031295,"population, models, estimating"
178,"today, we started the class with the importance of domain knowledge, common sense so that we can detect that what is wrong in the data, what are the outliers. sometimes we encounter data which is mathematically possible but has no physical significance. in such cases, having the domain knowledge is very critical. we learnt about some industry processes for data mining. we came to know that how to clean and format data because as discussed we wont get direct data as we got in assignments till now. the real world data problems are very different and has too many outliers. we also talked about the hypothesis testing.","today, we started the class with the importance of domain knowledge, common sense so that we can detect that what is wrong in the data, what are the outliers. sometimes we encounter data which is mathematically possible but has no physical significance. in such cases, having the domain knowledge is very critical. we learnt about some industry processes for data mining. we came to know that how to clean and format data because as discussed we wont get direct data as we got in assignments till now. the real world data problems are very different and has too many outliers. we also talked about the hypothesis testing.",9,-8.1362915,19.69929,8.959048,9.234239,"dataâ, analyse, analyses"
179,"we began with logistic regression, which aims to maximize the likelihood of predicted outcomes to match targets. for t = 1, we maximize p, and for t = 0, we maximize 1 - p. because likelihood involves products, we take the logarithm to simplify calculations.  the confusion matrix contains:
- true negative (tn), false positive (fp), false negative (fn), and true positive (tp).

important performance metrics: 

- accuracy:  (tp + tn)/total events
- precision: correctly detected events among all detected
- recall: correctly identified instances of a class
- f1 score: harmonic mean of precision and recall for balanced evaluation","we began with logistic regression, which aims to maximize the likelihood of predicted outcomes to match targets. for t = 1, we maximize p, and for t = 0, we maximize 1 - p. because likelihood involves products, we take the logarithm to simplify calculations. the confusion matrix contains: - true negative (tn), false positive (fp), false negative (fn), and true positive (tp). important performance metrics: - accuracy: (tp + tn)/total events - precision: correctly detected events among all detected - recall: correctly identified instances of a class - f1 score: harmonic mean of precision and recall for balanced evaluation",10,14.493874,-21.826708,8.715845,-1.6974409,"classifications, histograms, histogram"
180,"receiver operating characteristic curve plots the true positive rate against the false positive rate for varying values of the threshold that is used to assign the class label based on the probabilities output by the machine learning model. the line with a slope of 1 represents no model or the output achieved by purely guessing the class label. the area under this curve is known as auc and it should be close to 1 for a good classifier, whereas it is 0.5 for the random guesses.
clustering is used when we just have the data points and no corresponding labels. k-means clustering initially assigns each point to one of the k classes randomly. then assuming these labels to be true, we find the mean or representative of the classes. and then we reassign points to classes based on their similarity (distance) to the means. this process is repeated until there are no reassignments. another method is hierarchical clustering where initially each point is its own cluster. then the closest points or clusters are joined / combined into one cluster. this process is repeated until only one cluster is left withh all the data points. then a dendrogram is used to represent this process, and any horizontal cut will define one possibility for the number of clusters. note that in k-means clustering, the number of clusters (k) needs to be given as an input to the machine learning model / algorithm.","receiver operating characteristic curve plots the true positive rate against the false positive rate for varying values of the threshold that is used to assign the class label based on the probabilities output by the machine learning model. the line with a slope of 1 represents no model or the output achieved by purely guessing the class label. the area under this curve is known as auc and it should be close to 1 for a good classifier, whereas it is 0.5 for the random guesses. clustering is used when we just have the data points and no corresponding labels. k-means clustering initially assigns each point to one of the k classes randomly. then assuming these labels to be true, we find the mean or representative of the classes. and then we reassign points to classes based on their similarity (distance) to the means. this process is repeated until there are no reassignments. another method is hierarchical clustering where initially each point is its own cluster. then the closest points or clusters are joined / combined into one cluster. this process is repeated until only one cluster is left withh all the data points. then a dendrogram is used to represent this process, and any horizontal cut will define one possibility for the number of clusters. note that in k-means clustering, the number of clusters (k) needs to be given as an input to the machine learning model / algorithm.",8,-5.5197134,-21.860945,6.266991,0.42034423,"classification, clusterings, classifying"
181,"todayâ€™s class began with a revision of the t-sne plot. then, we moved on to feature engineering, starting with feature encoding, which is essential when either dependent or independent variables are categorical. these variables must be encoded before being used in a machine learning model.  

we first discussed one-hot encodingwith an example where the dependent variable was categorical, while three features were numerical, and one was categorical (e.g., ""blue""). we then covered multiclass and multilabel problems and discussed the drawback of one-hot encodingâ€”it increases the number of columns, leading to the curse of dimensionality and making the dataset sparse.  

for nominal data, one-hot encoding can be used if the number of classes is small. however, for ordinal data, assigning specific weights to categories is necessary, so one-hot encoding is not recommended.  

next, we explored binary encoding, where categorical values are first converted into numerical values and then expressed in binary notation. we also covered frequency encoding and target encoding as alternative methods.  

towards the end, we started discussing llms (large language models)and how words are converted into numerical data or vectors for processing in machine learning models.","today s class began with a revision of the t-sne plot. then, we moved on to feature engineering, starting with feature encoding, which is essential when either dependent or independent variables are categorical. these variables must be encoded before being used in a machine learning model. we first discussed one-hot encodingwith an example where the dependent variable was categorical, while three features were numerical, and one was categorical (e.g., ""blue""). we then covered multiclass and multilabel problems and discussed the drawback of one-hot encoding it increases the number of columns, leading to the curse of dimensionality and making the dataset sparse. for nominal data, one-hot encoding can be used if the number of classes is small. however, for ordinal data, assigning specific weights to categories is necessary, so one-hot encoding is not recommended. next, we explored binary encoding, where categorical values are first converted into numerical values and then expressed in binary notation. we also covered frequency encoding and target encoding as alternative methods. towards the end, we started discussing llms (large language models)and how words are converted into numerical data or vectors for processing in machine learning models.",3,-41.15392,2.6788814,0.1104091,6.248586,"categorical, categorization, categorise"
182,"learned how to do linear regression in excel.
r^2 parameter named so because in the case of simple linear regression it is equal to the square of the sample correlation coefficient.
amount of information captured by the model is how much variation in the data can be explained by it.
sample means tend to a normal distribution by the central limit theorem.
the standard deviation of that distribution is the standard error and it is inversely proportional to the square root of the number of samples, thus more samples is better.","learned how to do linear regression in excel. r^2 parameter named so because in the case of simple linear regression it is equal to the square of the sample correlation coefficient. amount of information captured by the model is how much variation in the data can be explained by it. sample means tend to a normal distribution by the central limit theorem. the standard deviation of that distribution is the standard error and it is inversely proportional to the square root of the number of samples, thus more samples is better.",7,25.33727,-0.32546118,14.396555,3.8223035,"statistics, statistical, statisticsâ"
183,"the class started off with the realisation that till now, we were working with good tailored data without any issues. however in real life, we almost never get any such data without any problems. much of the real life data has a lot of problems which need to be understood in order to solve the problems in the data and get some meaningful results. for this, we perform eda or exploratory data analysis on our data. our data could come in various formats like text, some files or some databases with large amounts of data present, and we might be exposed to non-tabular and non-numeric data. so we started off with a process for data mining, known as crisp - dm i.e. cross industry standard process for data mining. it has 6 steps which run cyclically, which includes business understanding, data understanding, data preparation, modelling, evaluation and deployment of the model. business understanding involves defining business objectives, assess situations based on domain knowledge and understand the goals of the firm, while also understanding the constraints. also understand the success criteria and create a project plan as to how much time to allocate to different procedures. 
the major steps which we were focusing on were data understanding and data preparation. data understanding involves collection of data, describe and explore the data and also verify the quality of data which you have collected as to whether it has some useful insights to give or not. data preparation involves selection and cleaning of the data and construct the data into that which is good to work on. we further moved on to eda, which is a part of the data understanding step. 
eda involves performing some initial investigations on the data, and to gain some basic insights from it, and to spot anomalies, create and test hypothesis and check our assumptions. eda basically uses mathematical and visual statistical tools. we then went through a mind map pertaining to data problems. problems can be in the dependent or independent variable or both. we could have maybe no data about the dependent variable, or maybe insufficient or incorrect data, or maybe we could have too much data to handle. each of these problems have different solutions which involve sampling or generating data based on some theory. problems with independent variables involve having too many independent variables i.e. having multiple columns and having problems within the columns and also between columns. within columns we could have missing data, some distribution issues, heteroscadasticity i.e. varying variance across the data. between columns, we could have too few or too many features. 
eda is the first step in the data analysis pipeline, where we can get some insights like how features are distributed, what are the outliers, etc. a common eda technique involves plotting histograms and checking the distributions of each variable. then we plot box plots, which show us the amount of variability in the data, where the central line shows us the median value and the box around it shows the variations. anything outside the main boxes may be considered as outliers. we also plot some correlation heat maps to understand correlations between different variables. matrix plot is a 2-feature-each plot plotted in a matrix formation, which are used to find valuable insights, but are limited to two features at a time. some line plots can show some trends in the data, which can be useful to make certain predictions. 
we then moved on to handling missing values. there are multiple types of missing data. mcar - missing completely at random, mar - missing at random or mnar - missing not at random. mcar has completely random data points missing, mar has some relationship between the missing data points which could be due to resource, method of measurement, etc. mnar says that unobserved values are itself responsible for the data being missing. now when we have identified missing data, we could maybe ignore these missing points. but many algorithms donâ€™t know how to handle missing data, and information may be lost. we could also delete all the missing values, which could lead to data loss, but it is the easiest to do. another option could be to replace the missing values with some data statistic like the mean or the median. mode could be used for categorical data. sometimes instead of the data mean, we could use some of the values close to the missing data point in order to fill the place. we also have multi variate approaches, where we observe the other columns as well in order to fill our missing value. knn takes the mean of the nearest neighbour and fits the mean of the neighbouring values, while mice fits a linear predictor in order to fill in values based on a trend. time series data is special because we can use temporal judgement to fill the data. we can use interpolation methods to fill in points or use simple moving averages as well. 
we then talked about outliers which are data points that are signifcantly different from the rest of the observations. some algorithms are not very sensitive to outliers, while some are quite sensitive. we discussed about quartiles and the inter quartile range, which can be used to detect outliers. standard deviation can also be used for normally distributed data, where any data point beyond 3 standard deviations from the mean, can be classified as an outlier. for multi variate data, we have dbscan which is density based spatial clustering of applications with noise, where outliers are points which are not classified into any broad clusters of high density points. outliers also can be dealt with in many ways like removing them from the dataset, etc. true outliers are data points which are extreme values but they are not erreneous. for such outliers, we could make a separate set out of these and deal with the normal observations and the outliers separately.  for outliers, we use the median and not the mean as the mean is influenced by the outliers, while the median is not. also, for calculating the median, we sort the data first so that we get the correct metric. 
not all techniques are useful for all kinds of data. hence domain knowledge becomes important so as to know what techniques are best for us.","the class started off with the realisation that till now, we were working with good tailored data without any issues. however in real life, we almost never get any such data without any problems. much of the real life data has a lot of problems which need to be understood in order to solve the problems in the data and get some meaningful results. for this, we perform eda or exploratory data analysis on our data. our data could come in various formats like text, some files or some databases with large amounts of data present, and we might be exposed to non-tabular and non-numeric data. so we started off with a process for data mining, known as crisp - dm i.e. cross industry standard process for data mining. it has 6 steps which run cyclically, which includes business understanding, data understanding, data preparation, modelling, evaluation and deployment of the model. business understanding involves defining business objectives, assess situations based on domain knowledge and understand the goals of the firm, while also understanding the constraints. also understand the success criteria and create a project plan as to how much time to allocate to different procedures. the major steps which we were focusing on were data understanding and data preparation. data understanding involves collection of data, describe and explore the data and also verify the quality of data which you have collected as to whether it has some useful insights to give or not. data preparation involves selection and cleaning of the data and construct the data into that which is good to work on. we further moved on to eda, which is a part of the data understanding step. eda involves performing some initial investigations on the data, and to gain some basic insights from it, and to spot anomalies, create and test hypothesis and check our assumptions. eda basically uses mathematical and visual statistical tools. we then went through a mind map pertaining to data problems. problems can be in the dependent or independent variable or both. we could have maybe no data about the dependent variable, or maybe insufficient or incorrect data, or maybe we could have too much data to handle. each of these problems have different solutions which involve sampling or generating data based on some theory. problems with independent variables involve having too many independent variables i.e. having multiple columns and having problems within the columns and also between columns. within columns we could have missing data, some distribution issues, heteroscadasticity i.e. varying variance across the data. between columns, we could have too few or too many features. eda is the first step in the data analysis pipeline, where we can get some insights like how features are distributed, what are the outliers, etc. a common eda technique involves plotting histograms and checking the distributions of each variable. then we plot box plots, which show us the amount of variability in the data, where the central line shows us the median value and the box around it shows the variations. anything outside the main boxes may be considered as outliers. we also plot some correlation heat maps to understand correlations between different variables. matrix plot is a 2-feature-each plot plotted in a matrix formation, which are used to find valuable insights, but are limited to two features at a time. some line plots can show some trends in the data, which can be useful to make certain predictions. we then moved on to handling missing values. there are multiple types of missing data. mcar - missing completely at random, mar - missing at random or mnar - missing not at random. mcar has completely random data points missing, mar has some relationship between the missing data points which could be due to resource, method of measurement, etc. mnar says that unobserved values are itself responsible for the data being missing. now when we have identified missing data, we could maybe ignore these missing points. but many algorithms don t know how to handle missing data, and information may be lost. we could also delete all the missing values, which could lead to data loss, but it is the easiest to do. another option could be to replace the missing values with some data statistic like the mean or the median. mode could be used for categorical data. sometimes instead of the data mean, we could use some of the values close to the missing data point in order to fill the place. we also have multi variate approaches, where we observe the other columns as well in order to fill our missing value. knn takes the mean of the nearest neighbour and fits the mean of the neighbouring values, while mice fits a linear predictor in order to fill in values based on a trend. time series data is special because we can use temporal judgement to fill the data. we can use interpolation methods to fill in points or use simple moving averages as well. we then talked about outliers which are data points that are signifcantly different from the rest of the observations. some algorithms are not very sensitive to outliers, while some are quite sensitive. we discussed about quartiles and the inter quartile range, which can be used to detect outliers. standard deviation can also be used for normally distributed data, where any data point beyond 3 standard deviations from the mean, can be classified as an outlier. for multi variate data, we have dbscan which is density based spatial clustering of applications with noise, where outliers are points which are not classified into any broad clusters of high density points. outliers also can be dealt with in many ways like removing them from the dataset, etc. true outliers are data points which are extreme values but they are not erreneous. for such outliers, we could make a separate set out of these and deal with the normal observations and the outliers separately. for outliers, we use the median and not the mean as the mean is influenced by the outliers, while the median is not. also, for calculating the median, we sort the data first so that we get the correct metric. not all techniques are useful for all kinds of data. hence domain knowledge becomes important so as to know what techniques are best for us.",9,-13.153341,18.637028,9.130784,9.075683,"dataâ, analyse, analyses"
184,"in today's class will learn how to calculate population mean from sample mean while considering multiple samples and single sample, then we learnt about confidence intervals for estimating a particular population mean and know how to calculate the probability of a particular value of population mean to lie in a certain interval with a particular probability. 

standard error of the mean is given by 
î¼= ïƒ/âˆšn

the means of multiple samples form a normal distribution and their mean is the estimated mean of population.
the probability is calculated in terms of p value.

we also calculated the value of beta 0 and beta 1 for a linear regression model. 

in last we learnt about anova table, which is used to determine if there are statistically significant differences between the means of three or more samples. 
in next lecture multiple regression model will be covered.","in today's class will learn how to calculate population mean from sample mean while considering multiple samples and single sample, then we learnt about confidence intervals for estimating a particular population mean and know how to calculate the probability of a particular value of population mean to lie in a certain interval with a particular probability. standard error of the mean is given by = / n the means of multiple samples form a normal distribution and their mean is the estimated mean of population. the probability is calculated in terms of p value. we also calculated the value of beta 0 and beta 1 for a linear regression model. in last we learnt about anova table, which is used to determine if there are statistically significant differences between the means of three or more samples. in next lecture multiple regression model will be covered.",7,35.667385,4.6541195,15.079226,2.4238143,"statistics, statistical, statisticsâ"
185,"in multiple linear regression, a direct mathematical solution exists but is not practical due to difficulties in handling large matrices and multicollinearity. to check how well a model works, data is split into 80% for training and 20% for testing. overfitting happens when a model performs well on training data but poorly on new data. linear regression means using a linear combination of variables, not always a straight-line fit. parametric models (slr, mlr) use p-values, while non-parametric models (decision trees, random forest) rely on metrics like râ² and rmse. we also discussed python libraries like sklearn and statsmodels.api","in multiple linear regression, a direct mathematical solution exists but is not practical due to difficulties in handling large matrices and multicollinearity. to check how well a model works, data is split into 80% for training and 20% for testing. overfitting happens when a model performs well on training data but poorly on new data. linear regression means using a linear combination of variables, not always a straight-line fit. parametric models (slr, mlr) use p-values, while non-parametric models (decision trees, random forest) rely on metrics like r and rmse. we also discussed python libraries like sklearn and statsmodels.api",0,5.7958326,-0.290256,10.265246,4.373472,"models, feature, features"
186,"todays session started with the summary comparision and heat map where one summary was completely different than the others and it was found out that it was another class's summary. from this we conclude that we can catch malpractises by simply looking at the data. then sir taught the confusion matrix a tool that evaluates classification models by presenting actual and predicted values in a two-way matrix. it was followed by eda. business understanding, data understanding, data preparation, modeling, assessment, and deployment are the six essential phases that make up exploratory data analysis (eda).
ta named shubham used visualizations to show a diabetes prediction model during an eda session. . regression, multivariate data imputation, replacing by mean/median , and deleting  were the methods discussed for dealing with missing data.
changes in the price of nvidia's stock demonstrated how steep drops could be mistakenly categorized as anomalies. high-dimensional data visualization was proposed using t-sne. 
session concluded with eda being taught and basically sir told us how we could be fooled by looking at plots and why thorough eda is imp.","todays session started with the summary comparision and heat map where one summary was completely different than the others and it was found out that it was another class's summary. from this we conclude that we can catch malpractises by simply looking at the data. then sir taught the confusion matrix a tool that evaluates classification models by presenting actual and predicted values in a two-way matrix. it was followed by eda. business understanding, data understanding, data preparation, modeling, assessment, and deployment are the six essential phases that make up exploratory data analysis (eda). ta named shubham used visualizations to show a diabetes prediction model during an eda session. . regression, multivariate data imputation, replacing by mean/median , and deleting were the methods discussed for dealing with missing data. changes in the price of nvidia's stock demonstrated how steep drops could be mistakenly categorized as anomalies. high-dimensional data visualization was proposed using t-sne. session concluded with eda being taught and basically sir told us how we could be fooled by looking at plots and why thorough eda is imp.",9,-11.7995405,15.503519,8.470605,9.139468,"dataâ, analyse, analyses"
187,"sir started by talking about y = f(x). he first talked about old and new methods of data analysis. for this he took the example of finding the temperature difference between the two ends of a pipe using the flow rate of the fluid flowing inside the pipe. y was the temperature difference denoted by delta-t and x was the flow rate.

in the old methods we used to get the equation relating y = f(x) like dt = l23, while in the new methods we get data points and obtain a plot of the curve which relates temperature difference with the corresponding flow rate.

there are 4 level of measurements: 

1. nominal (discrete) - no ordering in this level of measurements , no calculated value associated with it. ml category - classification 

2. ordinal (discrete) - they have a sequence and associating a number to nominal and ordinal is not correct. ml category - classification  
we have to encode the names as vectors like dog [ 1 0 0 0 ] , cat [ 0 0 0 1 ] 

3. interval(continuous)-for eg 5 and 10 degree celsius like we don't have, since the concept of reference matters. in case of this , we have 0 has arbitrary defined . ml category - regression 

4. ratio (continuous) - since the concept of reference matters. in case of this , we have 0 has arbitrary defined. ml category - regression 

when we have both labels and features then - supervised learning
with only features we have then - unsupervised learning 

y = f(x) y - label and x - features 

(monthly purchases)  = f(salary, family size, ...........) 

types :
 
1. simple linear regression 
2. multiple regression
3. polynomial regression 
4. random forest 
5. multiple regression 

y = f(x)

no labels  -- ""unsupervised learning""
	           --  k-mean clustering 
	           --  hierarchical clustering

we have population inside which we have sample and no matter how large the size of data is it is always taken as sample . 

larger the size of the population more accurate prediction .

then he talked about unsupervised learning in which we don't know the value of labels associated with features .","sir started by talking about y = f(x). he first talked about old and new methods of data analysis. for this he took the example of finding the temperature difference between the two ends of a pipe using the flow rate of the fluid flowing inside the pipe. y was the temperature difference denoted by delta-t and x was the flow rate. in the old methods we used to get the equation relating y = f(x) like dt = l23, while in the new methods we get data points and obtain a plot of the curve which relates temperature difference with the corresponding flow rate. there are 4 level of measurements: 1. nominal (discrete) - no ordering in this level of measurements , no calculated value associated with it. ml category - classification 2. ordinal (discrete) - they have a sequence and associating a number to nominal and ordinal is not correct. ml category - classification we have to encode the names as vectors like dog [ 1 0 0 0 ] , cat [ 0 0 0 1 ] 3. interval(continuous)-for eg 5 and 10 degree celsius like we don't have, since the concept of reference matters. in case of this , we have 0 has arbitrary defined . ml category - regression 4. ratio (continuous) - since the concept of reference matters. in case of this , we have 0 has arbitrary defined. ml category - regression when we have both labels and features then - supervised learning with only features we have then - unsupervised learning y = f(x) y - label and x - features (monthly purchases) = f(salary, family size, ...........) types : 1. simple linear regression 2. multiple regression 3. polynomial regression 4. random forest 5. multiple regression y = f(x) no labels -- ""unsupervised learning"" -- k-mean clustering -- hierarchical clustering we have population inside which we have sample and no matter how large the size of data is it is always taken as sample . larger the size of the population more accurate prediction . then he talked about unsupervised learning in which we don't know the value of labels associated with features .",4,-26.015587,-13.071803,1.6536808,0.24528714,"classification, classifying, classifications"
188,"a basic introduction to excel was given . multiple functionalities of excel such as calculation of estimated parameter , means , and errors were discussed . scatter plot of error and given data were plotted . regression line was also superimposed on the scatter plot of the data. if the error has some sort of trend in it, it means that the regression line is not a good fit for the data as it failed to capture the pattern of the data. outcome dependent on large numbers of unknown causes (random) the distribution observed is ""gaussian normal"" distribution. formula for the coefficient of determination was derived r^2 = 1- sse/sst . for the case of slr the coefficient of determination is exactly the same the square of correlation coefficient of x and y . this doesn't hold true for mlr . r^2 is the measure how much variation of data is captured , its value lies between 0 and 1.","a basic introduction to excel was given . multiple functionalities of excel such as calculation of estimated parameter , means , and errors were discussed . scatter plot of error and given data were plotted . regression line was also superimposed on the scatter plot of the data. if the error has some sort of trend in it, it means that the regression line is not a good fit for the data as it failed to capture the pattern of the data. outcome dependent on large numbers of unknown causes (random) the distribution observed is ""gaussian normal"" distribution. formula for the coefficient of determination was derived r^2 = 1- sse/sst . for the case of slr the coefficient of determination is exactly the same the square of correlation coefficient of x and y . this doesn't hold true for mlr . r^2 is the measure how much variation of data is captured , its value lies between 0 and 1.",5,23.58401,-3.4034266,14.164394,4.605112,"regression, statistical, statistics"
189,"in today's session, the main focus was on understanding the working of simple linear regression. for this, we used ms excel and data analysis toolpack. we started by finding the terms like x_bar, y_bar, xy_bar, xsq_bar, xbar_sq for the linear regression equation. using these values, we found the values of 'a' and 'b'. these newly found values of 'a' and 'b' will be used in the slr equation: y^ = ax + b. 
we also calculated the error from the data. we represented it first as a scatter plot and then as a histogram. through scatter plot, we noticed that the errors were randomly spread throughout the x-axis. but after plotting errors on histogram, our opinion changed. 
when the output depends on a large number of unknown causes (which are completely random) the distribution should be a gaussian normal distribution. but while plotting a histogram for the errors we did not obtain a gaussian distribution. this suggests that our model of linear regression fails into considering some trends in the data. 
later we spent some time, understanding the concept of variance and correlation coefficient using the data analysis toolkit. we learned about the coefficient of determination (r^2).","in today's session, the main focus was on understanding the working of simple linear regression. for this, we used ms excel and data analysis toolpack. we started by finding the terms like x_bar, y_bar, xy_bar, xsq_bar, xbar_sq for the linear regression equation. using these values, we found the values of 'a' and 'b'. these newly found values of 'a' and 'b' will be used in the slr equation: y^ = ax + b. we also calculated the error from the data. we represented it first as a scatter plot and then as a histogram. through scatter plot, we noticed that the errors were randomly spread throughout the x-axis. but after plotting errors on histogram, our opinion changed. when the output depends on a large number of unknown causes (which are completely random) the distribution should be a gaussian normal distribution. but while plotting a histogram for the errors we did not obtain a gaussian distribution. this suggests that our model of linear regression fails into considering some trends in the data. later we spent some time, understanding the concept of variance and correlation coefficient using the data analysis toolkit. we learned about the coefficient of determination (r^2).",5,22.004065,-4.4771147,14.191599,4.953077,"regression, statistical, statistics"
190,"the lecture started with some recap about the closed form solutions that we discussed during the multiple linear regression which can't be use in actual world as it deal with the inversion of matrix and also the fact that it also includes the multi collinearity.  if p> 0.05 we remove that feature . then we discussed about train and test data today, then we saw r2 values of training. we need our model to learn and represent population. we also discussed that why there is n-1 and not n in the formula of tss","the lecture started with some recap about the closed form solutions that we discussed during the multiple linear regression which can't be use in actual world as it deal with the inversion of matrix and also the fact that it also includes the multi collinearity. if p> 0.05 we remove that feature . then we discussed about train and test data today, then we saw r2 values of training. we need our model to learn and represent population. we also discussed that why there is n-1 and not n in the formula of tss",2,7.451027,9.628128,11.453178,5.042754,"regression, regressions, features"
191,"in data science, data is divided into 4 different categories according to nature of measurement as follows

levels of measurements : 

1. nominal : there is no ordering, only categorization is there. we can do frequency distribution on this data. it is discrete. example : gender, colour.
2. ordinal : there is inherent ordering in data. it is discrete. example : grades

in data science, it is advised not to assign numerical values to nominal and ordinal quantities because it gives them inherent value and order. for example : man : 1, woman : 0 looks like man has more value than woman which is non nonsensical.
we use one hot encoding to encode these data types.

3. interval : it is continuous. values have ordering and meaning. reference point has arbitrary definition. example : temperature.

4. ratio : it is continuous. reference point is defined. example : height, salary.


nominal and ordinal label types fall under categorization while interval and ratio fall under regression. 

sometimes labels are not available for data which goes under unsupervised learning while other is supervised learning.","in data science, data is divided into 4 different categories according to nature of measurement as follows levels of measurements : 1. nominal : there is no ordering, only categorization is there. we can do frequency distribution on this data. it is discrete. example : gender, colour. 2. ordinal : there is inherent ordering in data. it is discrete. example : grades in data science, it is advised not to assign numerical values to nominal and ordinal quantities because it gives them inherent value and order. for example : man : 1, woman : 0 looks like man has more value than woman which is non nonsensical. we use one hot encoding to encode these data types. 3. interval : it is continuous. values have ordering and meaning. reference point has arbitrary definition. example : temperature. 4. ratio : it is continuous. reference point is defined. example : height, salary. nominal and ordinal label types fall under categorization while interval and ratio fall under regression. sometimes labels are not available for data which goes under unsupervised learning while other is supervised learning.",4,-27.108826,-16.991762,1.3391671,0.1711566,"classification, classifying, classifications"
192,"the lecture discusses two main ways to improve the results of a particular model:

improving the sample: this can be done by making the sample more representative of the data or by increasing the size of the sample. a more representative sample ensures that the model reflects the real-world situation better, while increasing the size of the sample generally helps the model generalize better.

improving the method: this refers to improving the model itself or adjusting hyperparameters, among other things. one example discussed is linear regression, where the outcome is expressed as a linear combination of independent variables. however, to accommodate different types of functions, we can create new independent variables by applying nonlinear transformations to the original variables. for example, the model can be expressed as:
y = î²â‚€ + î²â‚ xâ‚ + î²â‚‚ xâ‚‚ + î²â‚ƒ xâ‚ƒ
where xâ‚‚ = sin(xâ‚) and xâ‚ƒ = xâ‚â².

applying such transformations can make the model more complex, but it also causes the p-value to increase and the adjusted r-squared to decrease, indicating that the model is becoming more unstable. a technique for selecting the right features is backward feature engineering, where we start with all features and remove the least significant ones based on p-values. similarly, there are forward feature engineering and mixed methods to determine the most appropriate set of features to use in a linear regression model.

the lecture then moves on to supervised learning methods, which are generally used to solve various types of problems. for these problems, we gather the data and apply multiple methods to it, eventually selecting the one that gives the best results. sometimes, more than one method might be combined to achieve better performance, such as by averaging the results or using some other approach to integrate the outputs. for example, in the case presented in the slides, the random forest method yields the best results. however, even in such cases, a linear regression model might still be chosen if the priority is expandability, as linear models are often easier to modify and apply to other data.

next, the lecture covers classification, which is another type of supervised learning, where the goal is to predict categorical labels for data. the labels could be binary (yes/no, 0/1) or have multiple categories (such as classifying animals as mammals, reptiles, etc.). the simplest method for classification is logistic regression. in logistic regression, the goal is to separate the space into regions, each corresponding to a different label. in the simplest case, the model can be written as:
a = wâ‚ xâ‚ + wâ‚‚ xâ‚‚ + wâ‚ƒ xâ‚ƒ + v
where a is a linear combination of the input features. to convert this result into a binary label (0 or 1), we apply the sigmoid function:
sigmoid(a) = 1 / (1 + e^(-a))
this function maps the linear combination of features to a value between 0 and 1, which can then be interpreted as the probability of the data belonging to one of the two classes.","the lecture discusses two main ways to improve the results of a particular model: improving the sample: this can be done by making the sample more representative of the data or by increasing the size of the sample. a more representative sample ensures that the model reflects the real-world situation better, while increasing the size of the sample generally helps the model generalize better. improving the method: this refers to improving the model itself or adjusting hyperparameters, among other things. one example discussed is linear regression, where the outcome is expressed as a linear combination of independent variables. however, to accommodate different types of functions, we can create new independent variables by applying nonlinear transformations to the original variables. for example, the model can be expressed as: y = + x + x + x where x = sin(x ) and x = x . applying such transformations can make the model more complex, but it also causes the p-value to increase and the adjusted r-squared to decrease, indicating that the model is becoming more unstable. a technique for selecting the right features is backward feature engineering, where we start with all features and remove the least significant ones based on p-values. similarly, there are forward feature engineering and mixed methods to determine the most appropriate set of features to use in a linear regression model. the lecture then moves on to supervised learning methods, which are generally used to solve various types of problems. for these problems, we gather the data and apply multiple methods to it, eventually selecting the one that gives the best results. sometimes, more than one method might be combined to achieve better performance, such as by averaging the results or using some other approach to integrate the outputs. for example, in the case presented in the slides, the random forest method yields the best results. however, even in such cases, a linear regression model might still be chosen if the priority is expandability, as linear models are often easier to modify and apply to other data. next, the lecture covers classification, which is another type of supervised learning, where the goal is to predict categorical labels for data. the labels could be binary (yes/no, 0/1) or have multiple categories (such as classifying animals as mammals, reptiles, etc.). the simplest method for classification is logistic regression. in logistic regression, the goal is to separate the space into regions, each corresponding to a different label. in the simplest case, the model can be written as: a = w x + w x + w x + v where a is a linear combination of the input features. to convert this result into a binary label (0 or 1), we apply the sigmoid function: sigmoid(a) = 1 / (1 + e^(-a)) this function maps the linear combination of features to a value between 0 and 1, which can then be interpreted as the probability of the data belonging to one of the two classes.",0,3.3076885,-4.4339952,9.5911875,4.056963,"models, feature, features"
193,"he session focused on real-world data challenges and the significance of exploratory data analysis (eda). the crisp-dm process was emphasized, particularly the stages of data understanding and preparation. eda plays a crucial role in uncovering insights, detecting anomalies, and testing hypotheses.

common issues with independent variables, such as an excessive number of features or missing data, were examined. various eda techniques, including histograms, box plots, and heatmaps, were utilized to visualize data distributions, variability, and correlations.

different approaches to handling missing valuesâ€”categorized as mcar, mar, and mnarâ€”were discussed. strategies such as deletion and imputation methods (mean, median, mode, knn, and mice) were explored.

outlier detection methods, including quartiles, iqr, and standard deviation, were analyzed, along with strategies for handling them. the median was highlighted as a reliable measure for outlier analysis. selecting appropriate techniques requires a strong understanding of domain knowledge.","he session focused on real-world data challenges and the significance of exploratory data analysis (eda). the crisp-dm process was emphasized, particularly the stages of data understanding and preparation. eda plays a crucial role in uncovering insights, detecting anomalies, and testing hypotheses. common issues with independent variables, such as an excessive number of features or missing data, were examined. various eda techniques, including histograms, box plots, and heatmaps, were utilized to visualize data distributions, variability, and correlations. different approaches to handling missing values categorized as mcar, mar, and mnar were discussed. strategies such as deletion and imputation methods (mean, median, mode, knn, and mice) were explored. outlier detection methods, including quartiles, iqr, and standard deviation, were analyzed, along with strategies for handling them. the median was highlighted as a reliable measure for outlier analysis. selecting appropriate techniques requires a strong understanding of domain knowledge.",9,-19.485054,20.371693,8.121873,8.685696,"dataâ, analyse, analyses"
194,"summary:

- *histogram: a frequency chart where the height represents the frequency, and the horizontal interval is the class width.

- random data nature: errors in perfectly random data follow a gaussian distribution. if errors show a pattern, the model fails to capture it.

- *key terms:  
  - **ssr (sum of squares for regression)**: variations explained by the regression model.  
  - **sse (sum of squares for error)**: variations due to random errors.  
  - **sst (total sum of squares)**: sst = ssr + sse.  
  - **râ² (coefficient of determination)**: râ² = ssr/sst, representing the squared correlation between x and y.

- *central limit theorem:  
  - calculated means (máµ¢) tend to have a **normal distribution**.  
  - this distribution is called the **sampling distribution of the sample mean**.  
  - *properties:  
    1. the expected value (mean) of the sampling distribution is close to the population mean.  
    2. the standard deviation of the sampling distribution (standard error, sâ‚“ì„) is related to the population's standard deviation (ïƒ) as follows:  
       sâ‚“ì„ âˆ ïƒ.","summary: - *histogram: a frequency chart where the height represents the frequency, and the horizontal interval is the class width. - random data nature: errors in perfectly random data follow a gaussian distribution. if errors show a pattern, the model fails to capture it. - *key terms: - **ssr (sum of squares for regression)**: variations explained by the regression model. - **sse (sum of squares for error)**: variations due to random errors. - **sst (total sum of squares)**: sst = ssr + sse. - **r (coefficient of determination)**: r = ssr/sst, representing the squared correlation between x and y. - *central limit theorem: - calculated means (m ) tend to have a **normal distribution**. - this distribution is called the **sampling distribution of the sample mean**. - *properties: 1. the expected value (mean) of the sampling distribution is close to the population mean. 2. the standard deviation of the sampling distribution (standard error, s ) is related to the population's standard deviation ( ) as follows: s .",5,27.013643,-2.8081272,14.411895,4.2102833,"regression, statistical, statistics"
195,"in today's lecture, we learnt what is meant by saying that a sample is statistically significant  and statistically equal to zero (when the sample/observation and zero both lie in sane c.i.), also learnt the distinction between statistically significant and 'by chance'. then we shifted to mlr, it deals with more than one independent variable known as features. to analyse texts as data, texts can be embedded in a form of vector (these vector essentially contains features), process used here is known as feature engineering. later on we saw how the mlr dataset looks like and how to operate it on excel. dataset can be written in form of equations which then can be converted into matrices. the objective function for the mlr also remains the same i.e, to minimize the sse. after that, we discussed mlr gradient descent process, like how this method actually works,etc. solvers take an observation and try to minimize the objective function. then we saw a mlr dataset with 1 dependent and 5 independent variables. we removed 3 independent variables because of their high p-values(variable with highest p value was removed first) one by one in order to fit the dataset at its best. we finally look at what f-value is, it is = msr/mse, we cannot inference anything for a particular model from it's own f-value. but we can compare f-values of 2 models and the model with high f-value fits the data better than the other. also, râ² increases with the increase in number of independent variables and vice-versa.","in today's lecture, we learnt what is meant by saying that a sample is statistically significant and statistically equal to zero (when the sample/observation and zero both lie in sane c.i.), also learnt the distinction between statistically significant and 'by chance'. then we shifted to mlr, it deals with more than one independent variable known as features. to analyse texts as data, texts can be embedded in a form of vector (these vector essentially contains features), process used here is known as feature engineering. later on we saw how the mlr dataset looks like and how to operate it on excel. dataset can be written in form of equations which then can be converted into matrices. the objective function for the mlr also remains the same i.e, to minimize the sse. after that, we discussed mlr gradient descent process, like how this method actually works,etc. solvers take an observation and try to minimize the objective function. then we saw a mlr dataset with 1 dependent and 5 independent variables. we removed 3 independent variables because of their high p-values(variable with highest p value was removed first) one by one in order to fit the dataset at its best. we finally look at what f-value is, it is = msr/mse, we cannot inference anything for a particular model from it's own f-value. but we can compare f-values of 2 models and the model with high f-value fits the data better than the other. also, r increases with the increase in number of independent variables and vice-versa.",2,12.726074,6.0880523,12.066197,4.176264,"regression, regressions, features"
196,"key concepts:
1. population and sample:
   - population: the entire group of interest.
   - sample: a subset of the population used to infer properties of the population.

2. parameters and statistics:
   - parameters: characteristics of a population.
   - statistics: calculated measures from a sample that estimate population parameters.

3. measures of central tendency and dispersion:
   - mean, median, mode: describe the center of data.
   - standard deviation, variance: measure the spread of data.
   - basic operations (e.g., count, add, subtract, multiply, divide) were applied to both population and sample data.

4. dependent and independent variables:
   - dependent variable y: the outcome we are trying to predict or explain.
   - independent variable (x): the predictor or input variable influencing y.

5. simple linear regression (slr):
   - regression line equation:  y = beta_0 + beta_1 x .
   - estimation of coefficients beta_0 and beta_1:
     - calculated from sample data as 'point estimates' of population parameters.
     - recognized that point estimates may not exactly match population parameters but are close approximations.
     - interval estimates: used to state a confidence range (e.g., 95%) where population parameters likely lie.

6. goal of slr:
   - minimize sse (sum of squared errors):
     - sse measures the total deviation of observed values from the predicted line.
   - brief derivation: covered the mathematical basis for minimizing sse to find optimal coefficients.","key concepts: 1. population and sample: - population: the entire group of interest. - sample: a subset of the population used to infer properties of the population. 2. parameters and statistics: - parameters: characteristics of a population. - statistics: calculated measures from a sample that estimate population parameters. 3. measures of central tendency and dispersion: - mean, median, mode: describe the center of data. - standard deviation, variance: measure the spread of data. - basic operations (e.g., count, add, subtract, multiply, divide) were applied to both population and sample data. 4. dependent and independent variables: - dependent variable y: the outcome we are trying to predict or explain. - independent variable (x): the predictor or input variable influencing y. 5. simple linear regression (slr): - regression line equation: y = beta_0 + beta_1 x . - estimation of coefficients beta_0 and beta_1: - calculated from sample data as 'point estimates' of population parameters. - recognized that point estimates may not exactly match population parameters but are close approximations. - interval estimates: used to state a confidence range (e.g., 95%) where population parameters likely lie. 6. goal of slr: - minimize sse (sum of squared errors): - sse measures the total deviation of observed values from the predicted line. - brief derivation: covered the mathematical basis for minimizing sse to find optimal coefficients.",1,30.446293,-8.1631,16.248135,4.246621,"population, models, estimating"
197,"discussed the  distinct difference between sample and population and why sample should be a good even representative of the population , also mentioned different attributes  associated with sample and population . discussed the difference between statistics and parameters  like  statistics is calculated using sample and parameters using population.briefly introduced the linear regression model and how and why we use mean squared error as the error model to minimise.sir also discussed about confidence interval and how it is generally 90,95,99%
and is used in estimation.","discussed the distinct difference between sample and population and why sample should be a good even representative of the population , also mentioned different attributes associated with sample and population . discussed the difference between statistics and parameters like statistics is calculated using sample and parameters using population.briefly introduced the linear regression model and how and why we use mean squared error as the error model to minimise.sir also discussed about confidence interval and how it is generally 90,95,99% and is used in estimation.",1,32.10667,-2.1913772,15.466848,3.4500546,"population, models, estimating"
198,"todayâ€™s session focus mainly on exploratory data analysis but before that we learnt that how to differentiate between actual and predicted values in confusion matrices. crisp-dm which stands for cross industry process for data mining in these 6 steps run cyclically 
1) business understanding in which we do many steps including assessing situation (assumption and constraints), risk.
2)data understanding in these we collect initial data, explore data and also verify data quality. 
3) data preparation in these we select data, clean data, construct data, integrate and format data. after this we can infer which is dependent and independent variables. last three steps involved modelling, evaluation and deployment.
exploratory data analysis is an approach used in statistics and data science to analyze and investigate data sets. it involves statistical graphs and data visualization methods to visually represent the data. next we see that what are the problems associated with dependent and independent variables. in case of dependent variables problems such as not availability(remedy is that use unsupervised learning and create labels), incorrect, insufficient data( it includes very few observations or data imbalances) or too much data. and in  independent variables there can be problems within the column or between the column itself. heteroskedastic is that different variance exists throughout the entire datasets range. further ahead we explore types of missing data in the formats - 
1) missing completely at random 
2) missing at random 
3) missing not at random. so what do we do - 1) let them be n/a 
2) delete all instances with missing values 
3) or replace n/a values with a data statistics like mean or mode. mode value helps in categorical data. and multivariate data imputation can also be done. further we explore outliers which are the data points that differ significantly from the rest of data points. most of the models does not know how to handles this, but there are some models which can tackle this problem. how to detect these outliers we can do these either by univariate or multivariate methods. we calculate quartiles which divide a dataset into four equal parts, providing insights into data distribution. 
 1. first quartile (q1) â€“ 25% (lower quartile) - it represents the 25th percentile.
 2. second quartile (q2) â€“ it represents 50th percentile and median of the entire dataset.
 3. third quartile (q3) â€“ 75% (upper quartile)
 it represents the 75th percentile, meaning 75% of the data falls below this value.
standard deviation methods is similar. in case of multivariate we have dbscan which stands for density-based spatial clustering of applications with noise. next we see that how to handles the outliers we can do these by data trimming and data capping. values that lies in the extremes arenâ€™t erroneous but they are called true outliers. example - stock prices can drastically shoot up or down, extreme weather events. one important note is that means are influenced by the outliers, median are not influenced by outliers. to get median data should be sorted first. combination of univariate columns many results in error.","today s session focus mainly on exploratory data analysis but before that we learnt that how to differentiate between actual and predicted values in confusion matrices. crisp-dm which stands for cross industry process for data mining in these 6 steps run cyclically 1) business understanding in which we do many steps including assessing situation (assumption and constraints), risk. 2)data understanding in these we collect initial data, explore data and also verify data quality. 3) data preparation in these we select data, clean data, construct data, integrate and format data. after this we can infer which is dependent and independent variables. last three steps involved modelling, evaluation and deployment. exploratory data analysis is an approach used in statistics and data science to analyze and investigate data sets. it involves statistical graphs and data visualization methods to visually represent the data. next we see that what are the problems associated with dependent and independent variables. in case of dependent variables problems such as not availability(remedy is that use unsupervised learning and create labels), incorrect, insufficient data( it includes very few observations or data imbalances) or too much data. and in independent variables there can be problems within the column or between the column itself. heteroskedastic is that different variance exists throughout the entire datasets range. further ahead we explore types of missing data in the formats - 1) missing completely at random 2) missing at random 3) missing not at random. so what do we do - 1) let them be n/a 2) delete all instances with missing values 3) or replace n/a values with a data statistics like mean or mode. mode value helps in categorical data. and multivariate data imputation can also be done. further we explore outliers which are the data points that differ significantly from the rest of data points. most of the models does not know how to handles this, but there are some models which can tackle this problem. how to detect these outliers we can do these either by univariate or multivariate methods. we calculate quartiles which divide a dataset into four equal parts, providing insights into data distribution. 1. first quartile (q1) 25% (lower quartile) - it represents the 25th percentile. 2. second quartile (q2) it represents 50th percentile and median of the entire dataset. 3. third quartile (q3) 75% (upper quartile) it represents the 75th percentile, meaning 75% of the data falls below this value. standard deviation methods is similar. in case of multivariate we have dbscan which stands for density-based spatial clustering of applications with noise. next we see that how to handles the outliers we can do these by data trimming and data capping. values that lies in the extremes aren t erroneous but they are called true outliers. example - stock prices can drastically shoot up or down, extreme weather events. one important note is that means are influenced by the outliers, median are not influenced by outliers. to get median data should be sorted first. combination of univariate columns many results in error.",9,-13.337839,18.382421,9.28029,9.053692,"dataâ, analyse, analyses"
199,"n this class, we explored various aspects of feature engineering and its significance in improving model performance. we then delved into multiple linear regression (mlr), discussing how adding more terms can impact the model's ð‘…2
value and p-value, helping to assess the goodness of fit and statistical significance of predictors. moving beyond mlr, we examined different types of supervised learning techniques, including random forest, which is an ensemble method known for its robustness and ability to handle complex data patterns. we also covered neural networks for regression, highlighting their ability to capture non-linear relationships in data. lastly, we studied logistic regression, a fundamental classification algorithm used for binary and multi-class classification problems. this session provided a comprehensive understanding of various regression and supervised learning techniques, equipping us with valuable insights into predictive modeling.","n this class, we explored various aspects of feature engineering and its significance in improving model performance. we then delved into multiple linear regression (mlr), discussing how adding more terms can impact the model's 2 value and p-value, helping to assess the goodness of fit and statistical significance of predictors. moving beyond mlr, we examined different types of supervised learning techniques, including random forest, which is an ensemble method known for its robustness and ability to handle complex data patterns. we also covered neural networks for regression, highlighting their ability to capture non-linear relationships in data. lastly, we studied logistic regression, a fundamental classification algorithm used for binary and multi-class classification problems. this session provided a comprehensive understanding of various regression and supervised learning techniques, equipping us with valuable insights into predictive modeling.",13,-4.5978546,-5.733,8.798849,4.9612617,"classification, classifying, classifications"
200,"todayâ€™s class start with a discussion on how do we eliminate noise from dataset. noise makes it difficult to find a trend in data therefore it is becomes a necessary part of exploratory data analysis. we learn one method named â€œsimple moving averageâ€ in which we consider a window around every data point and average the values. window width can be varied to adjust the level of smoothing. higher window size makes more smoothing. this method is also used for filling up missing values, replacing outliers.
another method is â€œexponential moving averageâ€ which works better on time series data. one good practice is that first removes the outliers then apply moving averages method. next we learnt about the standardization and normalization of datasets. linear regression model is immune to the data scaling but many models such as gradient descent method is not. clustering algorithm based on euclidean distance will greatly influence by scaling the data. the normalization makes value lie between [0,1] using formula such as x=[x-x(min)/ x(max)-x(min)] and the standardization creates standard normal distribution. normalization does not change the shape of distribution of the data. we use log transformation when data is heteroscadascity. next we discuss about data imbalance which happens when certain instances of a class might show up more frequency than others. example rare disease diagnostic, forgery. we can overcome this by either under sampling the majority class or over sampling the minority class. we can do this with the help of smote tool.","today s class start with a discussion on how do we eliminate noise from dataset. noise makes it difficult to find a trend in data therefore it is becomes a necessary part of exploratory data analysis. we learn one method named simple moving average in which we consider a window around every data point and average the values. window width can be varied to adjust the level of smoothing. higher window size makes more smoothing. this method is also used for filling up missing values, replacing outliers. another method is exponential moving average which works better on time series data. one good practice is that first removes the outliers then apply moving averages method. next we learnt about the standardization and normalization of datasets. linear regression model is immune to the data scaling but many models such as gradient descent method is not. clustering algorithm based on euclidean distance will greatly influence by scaling the data. the normalization makes value lie between [0,1] using formula such as x=[x-x(min)/ x(max)-x(min)] and the standardization creates standard normal distribution. normalization does not change the shape of distribution of the data. we use log transformation when data is heteroscadascity. next we discuss about data imbalance which happens when certain instances of a class might show up more frequency than others. example rare disease diagnostic, forgery. we can overcome this by either under sampling the majority class or over sampling the minority class. we can do this with the help of smote tool.",9,-20.450314,10.882953,10.140413,10.087796,"dataâ, analyse, analyses"
201,"in summary, forward feature engineering and backward feature engineering are two
techniques used in machine learning for selecting relevant features to include in a model.
forward feature engineering starts with an empty feature set and iteratively adds one
feature at a time based on their performance, while backward feature engineering starts
with a complete set of features and removes features one by one until the model
performance reaches a peak. both techniques have their advantages and disadvantages
and can be used in combination to optimize the feature selection process. the resulting regression method is known
as polynomial regression - since
polynomial terms are introduced as
independent variable to handle non-linearity
in y. in general, introducing additional x variables
to improve the performance of ml methods
is known as feature engineering","in summary, forward feature engineering and backward feature engineering are two techniques used in machine learning for selecting relevant features to include in a model. forward feature engineering starts with an empty feature set and iteratively adds one feature at a time based on their performance, while backward feature engineering starts with a complete set of features and removes features one by one until the model performance reaches a peak. both techniques have their advantages and disadvantages and can be used in combination to optimize the feature selection process. the resulting regression method is known as polynomial regression - since polynomial terms are introduced as independent variable to handle non-linearity in y. in general, introducing additional x variables to improve the performance of ml methods is known as feature engineering",0,0.8842222,-7.6456294,9.662531,3.9818585,"models, feature, features"
202,"we explored neural networks for classification on playground.tensorflow.org, focusing on performance metrics like precision, accuracy, recall, and f1-score. we analyzed true positive (tp) and false positive (fp) rates, noting that accuracy is calculated as:

ï¿¼

we examined classification challenges using two overlapping distributions, highlighting misclassification in the intersection region. the quality of a classifier was discussed using roc curves, where a sharper curve indicates a better classifier, while a 45â° line represents a random classifier. auc (area under the roc curve) was introduced as a measure of classifier performance, with auc = 1 being ideal and auc = 0.5 meaning no classification ability.

we also started exploring unsupervised learning, introducing clustering techniques like k-means clustering and hierarchical clustering, where labels are not required.","we explored neural networks for classification on playground.tensorflow.org, focusing on performance metrics like precision, accuracy, recall, and f1-score. we analyzed true positive (tp) and false positive (fp) rates, noting that accuracy is calculated as: we examined classification challenges using two overlapping distributions, highlighting misclassification in the intersection region. the quality of a classifier was discussed using roc curves, where a sharper curve indicates a better classifier, while a 45 line represents a random classifier. auc (area under the roc curve) was introduced as a measure of classifier performance, with auc = 1 being ideal and auc = 0.5 meaning no classification ability. we also started exploring unsupervised learning, introducing clustering techniques like k-means clustering and hierarchical clustering, where labels are not required.",8,-1.7273049,-19.518553,6.8530264,0.5302005,"classification, clusterings, classifying"
203,"in today's class (12/2/25)
we started with understanding the matrix where we first implemented the code and through the algorithm what's the confusion matrix understanding the values of true positive true negative false positive falls negative then precision where we discussed that we should not depend on the accuracy by the model which is inbuilt in the libraries. 
we also had a short discussion about the neural networks i that was shown by the sir on playground.tensorflow.org where we learn about various boundaries over fitting features edition of neuron sedition of players and how does it affect the output of our model. 
then we started discussing about the region of conversions curve to understand a good classifier and a bad classifier where we also use the website and understood when that the features were added it became a very good classifier the region of convergence should be steep in the starting and then saturating the area under curve of roc curve is also a metric when it reaches the value 1, it is a very good classifier while when approximately 0.5, it's very bad. a 45 degrees slope of roc is a very bad classifier. next we went to understand data imbalance which is caused due to lack of data which cannot be easily separated which is usually understood and frauds in finances and other examples to. the other two matrix at we discussed a recall and support. 
next we started understanding the on un-supervising algorithm clustering where the output y in y=f(x) is also not given. this form of learning is used when we have to cluster all the input and analyse the groups terror than giving some significance earlier.
the example used here was of e-commerce website analysing about good buyers, bad buyers, etc.
the last part discuss was two types of clustering k-means clustering (pre-determining the number of clusters and than it will form the points in the group based on centroid distance of all points) and hierarchial clustering (which uses euclidean distance to match the points and based on our set threshold, a point to cut we get the numberâ ofâ clusters)","in today's class (12/2/25) we started with understanding the matrix where we first implemented the code and through the algorithm what's the confusion matrix understanding the values of true positive true negative false positive falls negative then precision where we discussed that we should not depend on the accuracy by the model which is inbuilt in the libraries. we also had a short discussion about the neural networks i that was shown by the sir on playground.tensorflow.org where we learn about various boundaries over fitting features edition of neuron sedition of players and how does it affect the output of our model. then we started discussing about the region of conversions curve to understand a good classifier and a bad classifier where we also use the website and understood when that the features were added it became a very good classifier the region of convergence should be steep in the starting and then saturating the area under curve of roc curve is also a metric when it reaches the value 1, it is a very good classifier while when approximately 0.5, it's very bad. a 45 degrees slope of roc is a very bad classifier. next we went to understand data imbalance which is caused due to lack of data which cannot be easily separated which is usually understood and frauds in finances and other examples to. the other two matrix at we discussed a recall and support. next we started understanding the on un-supervising algorithm clustering where the output y in y=f(x) is also not given. this form of learning is used when we have to cluster all the input and analyse the groups terror than giving some significance earlier. the example used here was of e-commerce website analysing about good buyers, bad buyers, etc. the last part discuss was two types of clustering k-means clustering (pre-determining the number of clusters and than it will form the points in the group based on centroid distance of all points) and hierarchial clustering (which uses euclidean distance to match the points and based on our set threshold, a point to cut we get the number of clusters)",8,-2.339514,-20.698177,6.5810285,0.32132676,"classification, clusterings, classifying"
204,"at first, we discussed a few concepts from the previous class, which were wrongly interpreted. one of these was that the p-value (95% confidence interval) represents the area under the curve, which lies outside the interval, from both the sides. so, it is correct to say that for p-value<0.05, the coefficients are statistically significant and must be included in the regression model. or we can say that the area under the curve, outside the interval on either of the sides must be less than 0.025. the other misconception was about the solution for the mlr model. solving the equations can give us a closed-form solution in theory, but practically, it is difficult to achieve. hence, we look for numerical methods to get approximate solutions, which are not closed form. finding such exact solutions is not practically possible as it involves calculating lots of matrix inversions. these calculations are difficult as the size of these matrices is very large. also, many times, there is multicollinearity in the independent variables. this means that these â€˜independentâ€™ variables are not really independent. if these problems are not addressed, then it can lead us to an unstable solution, which may not work out for a longer time and will eventually have to be reformed, to correctly predict the newly evolving data.
after this, we went further to discuss how exactly the procedure is carried out, to analyze the data and develop models that correctly explains it. whatever sample we chose, should never be used completely to train our model. a part of it (80% or 10%) should be reserved as the â€˜test dataâ€™ and can be used to test our model later. how much part of the data is reserved for testing depends on the size of the data. if we have large data then we can keep 20% for testing and rest for training the model. but if the data size is small, then maybe we will have to use more than 90% to train the model. if we keep on decreasing the data for training the model, at a certain point it would be insufficient to train the model and this small amount of data may not as well represent the population well.
 after we prepare a model based on our training data and use the test data to check whether this model correctly predicts the outcome, we can get two different types of metrics.
first is the training metrics. this includes the error metrics associated with the training data. next, we have test metrics, which includes the error metrics associated with the test data. some metrics are relevant to both of these (like r2), while some are specific to each of these. so, if r2 of both the sets match, we can fairly conclude that the model is good enough. however, if we have some model which overfits the training data (r2=1) then this model will not be good for the test data. the example which we discussed in class was that a curve which passes through all the points of the training data does not fit the test data well. but a best fit line for the training data is a better model for the test data. we then talked about another metric i.e. multiple r. it gives us an idea about the correlation between y and various independent variables. 
adjusted r2 is another metric which we discussed. adjusted r2 penalizes addition of excessive independent variables. it keeps on decreasing if we add more and more variables which do not improve the model.
so, it gives an idea of how effective the addition of new variables is.
as we predicted the y values using the mlr model and plotted these against the corresponding x values, we got a best fitting curve. so, linear regression will not always give as a straight line. it just states that the y is a linear combination of the independent variables and y may also be a non- linear. we had a discussion about parametric and non- parametric models. slr and mlr are defined as parametric models as they involve parameters ( intercept, beta1, beta2â€¦). these parameters have p -value associated with them. 
random forests and decision tree are examples of non-parametric models. they don't involve any parameters and hence don't have any p-values associated with them.
so, while comparing between models from these two categories, we may use r2 and rmse error metrics instead of p-values. these metrics are common to both of these models. at the end of the class, we learnt how we can use the python libraries like scikit-learn and statsmodel to perform these regressions and give us the values of these various metrics.
scikit-learn doesn't return us some metrics, so we used another library which is statsmodel, in which we used ols (ordinary least square algorithm). this gave us a variety of different metrics- including kurtosis, skewness, aic, bic, t-statistics, p-values, and results for few tests for normality and correlation between residuals, like durbin watson, omnibus test, jarque-bera test.","at first, we discussed a few concepts from the previous class, which were wrongly interpreted. one of these was that the p-value (95% confidence interval) represents the area under the curve, which lies outside the interval, from both the sides. so, it is correct to say that for p-value<0.05, the coefficients are statistically significant and must be included in the regression model. or we can say that the area under the curve, outside the interval on either of the sides must be less than 0.025. the other misconception was about the solution for the mlr model. solving the equations can give us a closed-form solution in theory, but practically, it is difficult to achieve. hence, we look for numerical methods to get approximate solutions, which are not closed form. finding such exact solutions is not practically possible as it involves calculating lots of matrix inversions. these calculations are difficult as the size of these matrices is very large. also, many times, there is multicollinearity in the independent variables. this means that these independent variables are not really independent. if these problems are not addressed, then it can lead us to an unstable solution, which may not work out for a longer time and will eventually have to be reformed, to correctly predict the newly evolving data. after this, we went further to discuss how exactly the procedure is carried out, to analyze the data and develop models that correctly explains it. whatever sample we chose, should never be used completely to train our model. a part of it (80% or 10%) should be reserved as the test data and can be used to test our model later. how much part of the data is reserved for testing depends on the size of the data. if we have large data then we can keep 20% for testing and rest for training the model. but if the data size is small, then maybe we will have to use more than 90% to train the model. if we keep on decreasing the data for training the model, at a certain point it would be insufficient to train the model and this small amount of data may not as well represent the population well. after we prepare a model based on our training data and use the test data to check whether this model correctly predicts the outcome, we can get two different types of metrics. first is the training metrics. this includes the error metrics associated with the training data. next, we have test metrics, which includes the error metrics associated with the test data. some metrics are relevant to both of these (like r2), while some are specific to each of these. so, if r2 of both the sets match, we can fairly conclude that the model is good enough. however, if we have some model which overfits the training data (r2=1) then this model will not be good for the test data. the example which we discussed in class was that a curve which passes through all the points of the training data does not fit the test data well. but a best fit line for the training data is a better model for the test data. we then talked about another metric i.e. multiple r. it gives us an idea about the correlation between y and various independent variables. adjusted r2 is another metric which we discussed. adjusted r2 penalizes addition of excessive independent variables. it keeps on decreasing if we add more and more variables which do not improve the model. so, it gives an idea of how effective the addition of new variables is. as we predicted the y values using the mlr model and plotted these against the corresponding x values, we got a best fitting curve. so, linear regression will not always give as a straight line. it just states that the y is a linear combination of the independent variables and y may also be a non- linear. we had a discussion about parametric and non- parametric models. slr and mlr are defined as parametric models as they involve parameters ( intercept, beta1, beta2 ). these parameters have p -value associated with them. random forests and decision tree are examples of non-parametric models. they don't involve any parameters and hence don't have any p-values associated with them. so, while comparing between models from these two categories, we may use r2 and rmse error metrics instead of p-values. these metrics are common to both of these models. at the end of the class, we learnt how we can use the python libraries like scikit-learn and statsmodel to perform these regressions and give us the values of these various metrics. scikit-learn doesn't return us some metrics, so we used another library which is statsmodel, in which we used ols (ordinary least square algorithm). this gave us a variety of different metrics- including kurtosis, skewness, aic, bic, t-statistics, p-values, and results for few tests for normality and correlation between residuals, like durbin watson, omnibus test, jarque-bera test.",2,7.3804364,2.1092844,10.859181,4.8154316,"regression, regressions, features"
205,"in today's class , we discussed about how we can estimate the mean of the population from a sample. we discussed many terms in today's class such as standard error which is used to give a confidence interval used to give an estimated range of values between which the population mean can be. various levels of confidence levels can be decided as per the need like 90%, 95% etc. then we discussed how p value can be used to approximate whether our estimate is good or not. a low p value indicates that our estimate is good as this depicts that it lies more away from the zero. because the more closer a in y=ax+b is y to zero it will not be called as a regression model. p value can be calculated using hypothesis testing. also the total varinace is the sum of variance in the regresion model and the variance in error which is normally distributed.","in today's class , we discussed about how we can estimate the mean of the population from a sample. we discussed many terms in today's class such as standard error which is used to give a confidence interval used to give an estimated range of values between which the population mean can be. various levels of confidence levels can be decided as per the need like 90%, 95% etc. then we discussed how p value can be used to approximate whether our estimate is good or not. a low p value indicates that our estimate is good as this depicts that it lies more away from the zero. because the more closer a in y=ax+b is y to zero it will not be called as a regression model. p value can be calculated using hypothesis testing. also the total varinace is the sum of variance in the regresion model and the variance in error which is normally distributed.",7,35.997227,-0.5301448,15.29351,2.8111284,"statistics, statistical, statisticsâ"
206,"today's class covered a diverse range of analytical and problem-solving concepts. we began with exploratory data analysis (eda), learning how to uncover patterns, detect anomalies, and summarize key insights using visualizations and statistical techniques. moving on, we explored pivot tables, a powerful tool for dynamically summarizing and analyzing large datasets, especially useful in business and financial contexts. next, we tackled an optimization problem in a chemical plant, focusing on maximizing efficiency and minimizing costs through mathematical modeling and problem-solving techniques. finally, we delved into a case study on transformer failure, analyzing the causes of failures and exploring predictive maintenance strategies to enhance reliability and prevent breakdowns. the session provided valuable insights into data-driven decision-making and practical applications across industries.","today's class covered a diverse range of analytical and problem-solving concepts. we began with exploratory data analysis (eda), learning how to uncover patterns, detect anomalies, and summarize key insights using visualizations and statistical techniques. moving on, we explored pivot tables, a powerful tool for dynamically summarizing and analyzing large datasets, especially useful in business and financial contexts. next, we tackled an optimization problem in a chemical plant, focusing on maximizing efficiency and minimizing costs through mathematical modeling and problem-solving techniques. finally, we delved into a case study on transformer failure, analyzing the causes of failures and exploring predictive maintenance strategies to enhance reliability and prevent breakdowns. the session provided valuable insights into data-driven decision-making and practical applications across industries.",6,-18.471085,26.166815,7.452542,9.347417,"summarizing, summarize, summarization"
207,"in today's class, we began with summarizing the learnings from the previous class. for y = b0 + b1x; we saw that b1 is more important than b0 because with b0 being zero, we at least have b1 which gives a slope for the regression. we discussed what statistically same and statistically significant means. we studied what happens when zero is the interval depending on which we can say if two values a and b are distinct or obtained by chance. if zero lies in the interval, then the values obtained are just by chance. as the models are similar to one another in the interval; the values obtained for a and b could have been zero. later, sir showed the class his interesting plot from class data. he used nlp and heatmaps to find the similarity in student's submissions. later, the class proceeded with multiple linear regression. in multiple linear regression, we have more than one independent variable. to extract information from text, we need to create feature out of it. this technique that deals with features is feature engineering. we need to create additional features from the data, because sometimes we may be given data about 'x', but we need information of its derivative. so, we create a feature of its derivative. 
we then studied the mlr gradient descent method and how it correlates the beta values, y-values, x-values and errors. from the data analysis toolpack, we studied the impact of different independent variables, on the f-value, p-value and r-sqaure terms. we notice that some independent variables do not contribute significantly to the regression process and hence we can omit them.","in today's class, we began with summarizing the learnings from the previous class. for y = b0 + b1x; we saw that b1 is more important than b0 because with b0 being zero, we at least have b1 which gives a slope for the regression. we discussed what statistically same and statistically significant means. we studied what happens when zero is the interval depending on which we can say if two values a and b are distinct or obtained by chance. if zero lies in the interval, then the values obtained are just by chance. as the models are similar to one another in the interval; the values obtained for a and b could have been zero. later, sir showed the class his interesting plot from class data. he used nlp and heatmaps to find the similarity in student's submissions. later, the class proceeded with multiple linear regression. in multiple linear regression, we have more than one independent variable. to extract information from text, we need to create feature out of it. this technique that deals with features is feature engineering. we need to create additional features from the data, because sometimes we may be given data about 'x', but we need information of its derivative. so, we create a feature of its derivative. we then studied the mlr gradient descent method and how it correlates the beta values, y-values, x-values and errors. from the data analysis toolpack, we studied the impact of different independent variables, on the f-value, p-value and r-sqaure terms. we notice that some independent variables do not contribute significantly to the regression process and hence we can omit them.",2,15.773009,8.150864,12.387956,4.182697,"regression, regressions, features"
208,"today's session focused on principal component analysis (pca) as a feature reduction technique, emphasizing its role in dimensionality reduction while retaining maximum variance. the discussion began with the importance of removing correlated factors using variance inflation factor (vif) before applying pca. key concepts included the identification of principal components (pcs), ensuring that they capture the highest variance in data, and how pca effectively reduces dimensionality. the session also touched on the use of the elbow method to determine the optimal number of pcs for a dataset. further, the applications of pca were explored, including its role in visualization (eda), prediction models, and understanding data structure. a comparison was made between normal regression models and pc regression, demonstrating how pca-transformed variables can enhance interpretability and model efficiency. the session concluded with a discussion on the significance of pca in real-world data analysis and its integration into predictive modeling workflows.","today's session focused on principal component analysis (pca) as a feature reduction technique, emphasizing its role in dimensionality reduction while retaining maximum variance. the discussion began with the importance of removing correlated factors using variance inflation factor (vif) before applying pca. key concepts included the identification of principal components (pcs), ensuring that they capture the highest variance in data, and how pca effectively reduces dimensionality. the session also touched on the use of the elbow method to determine the optimal number of pcs for a dataset. further, the applications of pca were explored, including its role in visualization (eda), prediction models, and understanding data structure. a comparison was made between normal regression models and pc regression, demonstrating how pca-transformed variables can enhance interpretability and model efficiency. the session concluded with a discussion on the significance of pca in real-world data analysis and its integration into predictive modeling workflows.",11,-12.736722,2.3047588,10.024473,12.607563,"pca, heatmap, heatmaps"
209,"in today's class we discussed about principal component analysis (pca). it is used as a feature reduction technique . we discussed  the importance of removing correlated factors using variance inflation factor (vif) before applying pca. the session covered how principal components (pcs) work, focusing on picking the ones that explain the most variation in the data. pca is mainly used to shrink the number of features while keeping the important information. we also looked at the elbow method, which helps decide how many pcs to keep for the best results. the session went over where pca is useful, like in data visualization (eda), predictive models, and understanding patterns in data. there was also a comparison between standard regression and pc regression, showing how transforming variables with pca can make models clearer and more efficient. we also talked about why pca is such a big deal in data analysis and how it fits into predictive modeling.","in today's class we discussed about principal component analysis (pca). it is used as a feature reduction technique . we discussed the importance of removing correlated factors using variance inflation factor (vif) before applying pca. the session covered how principal components (pcs) work, focusing on picking the ones that explain the most variation in the data. pca is mainly used to shrink the number of features while keeping the important information. we also looked at the elbow method, which helps decide how many pcs to keep for the best results. the session went over where pca is useful, like in data visualization (eda), predictive models, and understanding patterns in data. there was also a comparison between standard regression and pc regression, showing how transforming variables with pca can make models clearer and more efficient. we also talked about why pca is such a big deal in data analysis and how it fits into predictive modeling.",11,-13.247482,3.0528376,10.094688,12.722983,"pca, heatmap, heatmaps"
210,"today we discussed about logistic regression. it is used to solve classification problems basically to distinguish between two or more classes. the sigmoid function gives value between 0 and 1 which is essentially the probability.
so basically we intend to get the probabilities of all the classes and the class with highest probability is the the class to which our test case belong to. we also talked about softmax function and clusters.
we also discussed the mathematical interpretations of logit function.",today we discussed about logistic regression. it is used to solve classification problems basically to distinguish between two or more classes. the sigmoid function gives value between 0 and 1 which is essentially the probability. so basically we intend to get the probabilities of all the classes and the class with highest probability is the the class to which our test case belong to. we also talked about softmax function and clusters. we also discussed the mathematical interpretations of logit function.,10,8.801821,-18.09159,8.950834,-1.4424729,"classifications, histograms, histogram"
211,"how to deal with bad data? crispdm asa a cyclical processes to deal with bad data sets. business understanding, data understanding, data preparation, modelling, evaluation and deployment are the six strps involved in crispdm. missing values, outliers, heteroscadiscity and deviation from assumed distribution are bad data points in a coloumn. too many featured, too few features, feature scaling problems. 

a few observations were made about the features in pima indian diabetes dataset with box distribution, histograms, heat maps, and scatter plots.
in a pollutants dataset, three types of missing values were taught. missing compmetely at random, missing at random and missing not at random. 

dbscan is a clustering method is similar to kmeans clustering and hierarchial clustering. a case of a few outliers lying on the border of the protection of a features using 2dtsne was seen. dealing with outliers changes with the domain from which the data/feature belongs.

hw: why is median preffered over mean when identifying outliers?

caution! not every eda methods needs yo be applied on every dataset.","how to deal with bad data? crispdm asa a cyclical processes to deal with bad data sets. business understanding, data understanding, data preparation, modelling, evaluation and deployment are the six strps involved in crispdm. missing values, outliers, heteroscadiscity and deviation from assumed distribution are bad data points in a coloumn. too many featured, too few features, feature scaling problems. a few observations were made about the features in pima indian diabetes dataset with box distribution, histograms, heat maps, and scatter plots. in a pollutants dataset, three types of missing values were taught. missing compmetely at random, missing at random and missing not at random. dbscan is a clustering method is similar to kmeans clustering and hierarchial clustering. a case of a few outliers lying on the border of the protection of a features using 2dtsne was seen. dealing with outliers changes with the domain from which the data/feature belongs. hw: why is median preffered over mean when identifying outliers? caution! not every eda methods needs yo be applied on every dataset.",9,-15.71032,17.318258,9.090915,8.747136,"dataâ, analyse, analyses"
212,"we started with an explanation of population and sample. a sample should be a good representative of the population. we train our model on the sample and predict or interpret for the whole population. we have to predict the parameters of the population using the sample's statistics. these statistics include count, mean, mode, standard error, median, standard deviation, and variance.

in y = f(x), y is the dependent variable, and x is the independent variable. for simple linear regression, the form of the equation is y = b0 + b1x, where our job is to predict b0 and b1 to best fit our data. for different samples, the values of b0 and b1 may vary, which is why we introduce confidence intervals.

point estimates give a single value for a population parameter but may not be accurate due to variation in samples. interval estimates provide a range where the true population value is likely to be, with a certain level of confidence. for example, we can say we are 95% confident that our b0 or b1 lies within this interval.","we started with an explanation of population and sample. a sample should be a good representative of the population. we train our model on the sample and predict or interpret for the whole population. we have to predict the parameters of the population using the sample's statistics. these statistics include count, mean, mode, standard error, median, standard deviation, and variance. in y = f(x), y is the dependent variable, and x is the independent variable. for simple linear regression, the form of the equation is y = b0 + b1x, where our job is to predict b0 and b1 to best fit our data. for different samples, the values of b0 and b1 may vary, which is why we introduce confidence intervals. point estimates give a single value for a population parameter but may not be accurate due to variation in samples. interval estimates provide a range where the true population value is likely to be, with a certain level of confidence. for example, we can say we are 95% confident that our b0 or b1 lies within this interval.",1,34.87454,-5.22744,16.187643,3.5762799,"population, models, estimating"
213,"we first discussed how to improve the quality of our results. there are three ways to achieve this: improving the sample, selecting the best model, and fine-tuning the chosen model. then, we moved from simple linear regression (slr) to multiple linear regression (mlr). we can transition from slr to mlr by transforming some features into functions of themselves. this increases the p-value of other features, potentially making their coefficients zero. additionally, the adjusted râ² increases in mlr because we are adding more features to our data.  

then, we analyzed a dataset containing both linear and non-linear trends. among the four models we tested, random forest, knn, and ann effectively captured the trend.
then, we analyzed r-squared and mean squared error (mse) for our models. we observed that xgboost outperformed random forest. knn, being less complex, provided better results, making it a suitable choice for our dataset.  

next, we moved on to classification. the first method we explored was logistic regression, where we predict class labels. we also briefly discussed the sigmoid function, weights, and bias in logistic regression.","we first discussed how to improve the quality of our results. there are three ways to achieve this: improving the sample, selecting the best model, and fine-tuning the chosen model. then, we moved from simple linear regression (slr) to multiple linear regression (mlr). we can transition from slr to mlr by transforming some features into functions of themselves. this increases the p-value of other features, potentially making their coefficients zero. additionally, the adjusted r increases in mlr because we are adding more features to our data. then, we analyzed a dataset containing both linear and non-linear trends. among the four models we tested, random forest, knn, and ann effectively captured the trend. then, we analyzed r-squared and mean squared error (mse) for our models. we observed that xgboost outperformed random forest. knn, being less complex, provided better results, making it a suitable choice for our dataset. next, we moved on to classification. the first method we explored was logistic regression, where we predict class labels. we also briefly discussed the sigmoid function, weights, and bias in logistic regression.",0,-1.1493464,-3.6271615,9.243369,4.5084352,"models, feature, features"
214,"in todayâ€™s class we visit a website named â€œplayground.tensorflow.orgâ€ in which we play among the number of layers or neurons and features to see what kind of classification does it do. next we see some codes related to logistic regression, we see that we canâ€™t rely on just score or accuracy of the model. we have to compare the elements of confusion matrices to see whether it is good model or not. in the code we see two new terms 1) true positive rate - it is calculated as tp / (tp + fn), where tp is the number of true positive and fn is the number of false negative instances.
2)false positive rate - it is calculated as fp/fp+tn, where fp is the number of false positives and tn is the number of true negatives.
next we learnt about the receiver operating characteristics curve (roc) - it tell us that when does the classifier start detecting incorrect false positive. it refers the quality of classifier. for a good classifier the curve should be straight, if the curve gets flatter then it meant that the classifier is randomly classifying without any mathematical function.
we would like a classifier that discriminates between the classes even if it is overlapping. the other part of roc curve is area under curve(auc), it should be close to 1 for a good classifier. if it is close to 0.5 then it is not a good classifier. next we see an example of an imbalanced datasets where we calculate precision, recall value for class 3. other than the precision and recall, support represents the number of actual occurrences of a class in a datasets. examples of imbalance datasets is fraudulent or scams. interesting things is that we can convert regression problem into classification problem by creating bins but vice versa is not possible as it cause data losses. we see the unsupervised method of classification which is â€œclusteringâ€. k-means and hierarchical clusterings are two which we discuss. in k-means we have to specify the number of clusters but in hierarchical clustering we donâ€™t have to specify such things. in hierarchical clustering every data points is assigned to its own unique cluster.","in today s class we visit a website named playground.tensorflow.org in which we play among the number of layers or neurons and features to see what kind of classification does it do. next we see some codes related to logistic regression, we see that we can t rely on just score or accuracy of the model. we have to compare the elements of confusion matrices to see whether it is good model or not. in the code we see two new terms 1) true positive rate - it is calculated as tp / (tp + fn), where tp is the number of true positive and fn is the number of false negative instances. 2)false positive rate - it is calculated as fp/fp+tn, where fp is the number of false positives and tn is the number of true negatives. next we learnt about the receiver operating characteristics curve (roc) - it tell us that when does the classifier start detecting incorrect false positive. it refers the quality of classifier. for a good classifier the curve should be straight, if the curve gets flatter then it meant that the classifier is randomly classifying without any mathematical function. we would like a classifier that discriminates between the classes even if it is overlapping. the other part of roc curve is area under curve(auc), it should be close to 1 for a good classifier. if it is close to 0.5 then it is not a good classifier. next we see an example of an imbalanced datasets where we calculate precision, recall value for class 3. other than the precision and recall, support represents the number of actual occurrences of a class in a datasets. examples of imbalance datasets is fraudulent or scams. interesting things is that we can convert regression problem into classification problem by creating bins but vice versa is not possible as it cause data losses. we see the unsupervised method of classification which is clustering . k-means and hierarchical clusterings are two which we discuss. in k-means we have to specify the number of clusters but in hierarchical clustering we don t have to specify such things. in hierarchical clustering every data points is assigned to its own unique cluster.",8,0.24971816,-19.58732,6.954005,0.34248176,"classification, clusterings, classifying"
215,"the lecture started with the ""expectation algebra"" concept of how, for one sample of points, the expectation of any point is the sample's mean. we had seen the derivation of standard deviation of the sample mean which is expressed in terms of population standard deviation.
in classification, we have seen that using two different models causes more problems at the intersection points.
then we started with ""logistic unit and logistic regression"". the logistic unit predicts the probability that whether data belongs to class 1 or class 0. it doesn't predict the class. the probability function used here is the sigmoid function, which gives the probability that given data point x belongs to class y. if p(y/x) is greater than 0.5 then  x belongs to class y. then we see how the weight matrix is calculated in logistic regression. we formulated a likelihood function so that on maximizing likelihood we get the desired weight matrix. however, maximizing is not a stable method so we introduced a negative sign in likelihood and now minimize the negated likelihood to get weight matrix.
at last, we saw the ""confusion matrix"" and ""quality metrics"" like accuracy = (true positive + true negative)/total, precision = of all events you detected, how many are detected correctly, recall = of the specific class, how many you able to detect correctly, f1 score which is the harmonic mean of precision and recall.
at the end, we have a ta session discussing errors in exercise 1.","the lecture started with the ""expectation algebra"" concept of how, for one sample of points, the expectation of any point is the sample's mean. we had seen the derivation of standard deviation of the sample mean which is expressed in terms of population standard deviation. in classification, we have seen that using two different models causes more problems at the intersection points. then we started with ""logistic unit and logistic regression"". the logistic unit predicts the probability that whether data belongs to class 1 or class 0. it doesn't predict the class. the probability function used here is the sigmoid function, which gives the probability that given data point x belongs to class y. if p(y/x) is greater than 0.5 then x belongs to class y. then we see how the weight matrix is calculated in logistic regression. we formulated a likelihood function so that on maximizing likelihood we get the desired weight matrix. however, maximizing is not a stable method so we introduced a negative sign in likelihood and now minimize the negated likelihood to get weight matrix. at last, we saw the ""confusion matrix"" and ""quality metrics"" like accuracy = (true positive + true negative)/total, precision = of all events you detected, how many are detected correctly, recall = of the specific class, how many you able to detect correctly, f1 score which is the harmonic mean of precision and recall. at the end, we have a ta session discussing errors in exercise 1.",10,11.9348955,-18.628633,8.978064,-1.4372721,"classifications, histograms, histogram"
216,"we started by going over the closed-form solution in multiple linear regression, which isnâ€™t very practical because it requires matrix inversion and also runs into issues with multicollinearity. then we talked about feature selection, where we remove features with a p-value greater than 0.05 because they donâ€™t really add value to the model. after that, we got into training and testing data. usually, we split the data 80-20%, doing it randomly after exploratory data analysis. we also checked whether the r-squared values for both the training and testing datasets are close to each other, since if they arenâ€™t, we might be dealing with overfitting. the goal is for the model to generalize well and represent the population. finally, we covered adjusted r-squared, multiple r, and why thereâ€™s only n-1 degrees of freedom in the total sum of squares (tss).","we started by going over the closed-form solution in multiple linear regression, which isn t very practical because it requires matrix inversion and also runs into issues with multicollinearity. then we talked about feature selection, where we remove features with a p-value greater than 0.05 because they don t really add value to the model. after that, we got into training and testing data. usually, we split the data 80-20%, doing it randomly after exploratory data analysis. we also checked whether the r-squared values for both the training and testing datasets are close to each other, since if they aren t, we might be dealing with overfitting. the goal is for the model to generalize well and represent the population. finally, we covered adjusted r-squared, multiple r, and why there s only n-1 degrees of freedom in the total sum of squares (tss).",2,8.854967,7.999603,11.248548,4.9359293,"regression, regressions, features"
217,"machine learning is the application of statistics in different ways like linear regression, logistic regression, random forest, etc. there are 4 levels of measurement: - 1) nominal type: it is a discrete type of measurement for example gender and color. in this measurement, there is no ordering defined. it differentiates the characteristics of something that you are measuring. 2) ordinal type: in this type, there is a sequence or order. it is also a discrete type. example: grades. 3) interval type: it is a continuous type. example: temperature. only the difference matters in the case of intervals. in temperature zero has an arbitrary definition. 4) ratio type: it is also a continuous type. it's all about existence. for example: height, weight, salary, etc. 
nominal and ordinal types are used in classification whereas interval and ratio types are used in regression problems.
""supervised learning"" is learning from data where we know features as well as labels.
""unsupervised learning"" is learning from data where there is no label. its input data consists of only features.
there is whole data which is called ""population"" data whereas the part of that whole data is called ""sample"" data.","machine learning is the application of statistics in different ways like linear regression, logistic regression, random forest, etc. there are 4 levels of measurement: - 1) nominal type: it is a discrete type of measurement for example gender and color. in this measurement, there is no ordering defined. it differentiates the characteristics of something that you are measuring. 2) ordinal type: in this type, there is a sequence or order. it is also a discrete type. example: grades. 3) interval type: it is a continuous type. example: temperature. only the difference matters in the case of intervals. in temperature zero has an arbitrary definition. 4) ratio type: it is also a continuous type. it's all about existence. for example: height, weight, salary, etc. nominal and ordinal types are used in classification whereas interval and ratio types are used in regression problems. ""supervised learning"" is learning from data where we know features as well as labels. ""unsupervised learning"" is learning from data where there is no label. its input data consists of only features. there is whole data which is called ""population"" data whereas the part of that whole data is called ""sample"" data.",4,-26.15492,-16.288061,1.4105366,0.18631314,"classification, classifying, classifications"
218,"we continued in this lecture defining the various result tools. like p-value, t-stats, upper and lower confidence interval. first we took a small sample and tried to find the confidence whether it is taken from the population or not? with the help of mean.
since we dont know the population standard deviation we take that of the sample to calculate the standard error and plot the normal distribution. in our todays class we just try to focus on to how much extent the sample can represent the population or whether the correct sample is taken with how much probability. we defined various steps to do that. as i discussed above. also if the number of observation is less than 30 we use t-distribtion. it is more wider and shorter as compared to a normal distribution. 95% confidence interval. what does that mean? if you take 100 samples the mean of 95 of those samples would lie between the range values defined at those confidence interval. t-stat=(x-u)/std dev.

what is p-value? it tells you how likely beta1 =0 or what is the chance the data does not have a regression model. because in excel if the p-value is very low that means mean of the data is statistically greatly different than 0.(for slr it is beta1). if it is >=0.05 then we can not say 95% confidently that it follows regression.

in mlr p-value helps you in feature selection.at last we touched upon anova it is used to compare statistical equivalence of ""multiple averages simultaneouly"". anova gives you the f-statistics= msr/mse. so if f-stat is larger that means variance is better explained by the regression.","we continued in this lecture defining the various result tools. like p-value, t-stats, upper and lower confidence interval. first we took a small sample and tried to find the confidence whether it is taken from the population or not? with the help of mean. since we dont know the population standard deviation we take that of the sample to calculate the standard error and plot the normal distribution. in our todays class we just try to focus on to how much extent the sample can represent the population or whether the correct sample is taken with how much probability. we defined various steps to do that. as i discussed above. also if the number of observation is less than 30 we use t-distribtion. it is more wider and shorter as compared to a normal distribution. 95% confidence interval. what does that mean? if you take 100 samples the mean of 95 of those samples would lie between the range values defined at those confidence interval. t-stat=(x-u)/std dev. what is p-value? it tells you how likely beta1 =0 or what is the chance the data does not have a regression model. because in excel if the p-value is very low that means mean of the data is statistically greatly different than 0.(for slr it is beta1). if it is >=0.05 then we can not say 95% confidently that it follows regression. in mlr p-value helps you in feature selection.at last we touched upon anova it is used to compare statistical equivalence of ""multiple averages simultaneouly"". anova gives you the f-statistics= msr/mse. so if f-stat is larger that means variance is better explained by the regression.",7,34.984283,1.6504133,15.009226,2.8346975,"statistics, statistical, statisticsâ"
219,"today's class was spent on revisiting the midsem exam in which we saw data insight analysis using kde plots, range checks, missing value imputation, and box plots for outliers. we determined that normalization was needed and touched upon a recommendation to oversample heart disease, which was not carried out as there was extreme class imbalance.
we went back to the midsem correlation heat map, which at first glance seemed to have no significant relationships. but this was deceptive because of multicollinearity. to correct for this, we calculated râ² values between all pairs of features and applied variance inflation factor (vif) to conclude that only six features were actually independent, and others could be represented as linear combinations of these.
the lecture also touched on performance measures, the confusion matrix, and ta feedback on the e3 exercise. we wrapped up with a discussion of the curse of dimensionality, which occurs when there are too many features compared to available data, causing overfitting, sparsity growth, computational expense, and complexity. the solutions are either to grow data volume or to shrink feature dimensions.","today's class was spent on revisiting the midsem exam in which we saw data insight analysis using kde plots, range checks, missing value imputation, and box plots for outliers. we determined that normalization was needed and touched upon a recommendation to oversample heart disease, which was not carried out as there was extreme class imbalance. we went back to the midsem correlation heat map, which at first glance seemed to have no significant relationships. but this was deceptive because of multicollinearity. to correct for this, we calculated r values between all pairs of features and applied variance inflation factor (vif) to conclude that only six features were actually independent, and others could be represented as linear combinations of these. the lecture also touched on performance measures, the confusion matrix, and ta feedback on the e3 exercise. we wrapped up with a discussion of the curse of dimensionality, which occurs when there are too many features compared to available data, causing overfitting, sparsity growth, computational expense, and complexity. the solutions are either to grow data volume or to shrink feature dimensions.",9,-7.23789,10.6865635,9.4151535,7.711164,"dataâ, analyse, analyses"
220,"a confusion matrix is a fundamental tool for evaluating a classifier's performance, displaying the counts of true positives (tp), false positives (fp), true negatives (tn), and false negatives (fn). it helps compute key performance metrics such as precision (tp / (tp + fp)), which measures correctness among positive predictions, and recall (tp / (tp + fn)), indicating how well the model identifies actual positives. the f1-score, the harmonic mean of precision and recall, balances both metrics.

data imbalance, where one class significantly outnumbers others, can distort model performance, often requiring techniques like resampling (oversampling or undersampling) to balance the dataset.

in unsupervised learning, clustering is used to group similar data points. k-means clustering partitions data into k clusters using centroids, while hierarchical clustering builds a nested hierarchy of clusters based on similarity. these methods are widely used for segmentation, anomaly detection, and pattern discovery in large datasets. evaluating classifier quality and understanding clustering techniques are crucial in machine learning applications.","a confusion matrix is a fundamental tool for evaluating a classifier's performance, displaying the counts of true positives (tp), false positives (fp), true negatives (tn), and false negatives (fn). it helps compute key performance metrics such as precision (tp / (tp + fp)), which measures correctness among positive predictions, and recall (tp / (tp + fn)), indicating how well the model identifies actual positives. the f1-score, the harmonic mean of precision and recall, balances both metrics. data imbalance, where one class significantly outnumbers others, can distort model performance, often requiring techniques like resampling (oversampling or undersampling) to balance the dataset. in unsupervised learning, clustering is used to group similar data points. k-means clustering partitions data into k clusters using centroids, while hierarchical clustering builds a nested hierarchy of clusters based on similarity. these methods are widely used for segmentation, anomaly detection, and pattern discovery in large datasets. evaluating classifier quality and understanding clustering techniques are crucial in machine learning applications.",8,-2.1796677,-17.53139,6.6838303,0.7299234,"classification, clusterings, classifying"
221,"discussed about the plan for the next remaining 10 lectures and about group project, learnt about function encoding in the start of class. saw an example of converting y(red, blue, green) with three column to be encoded to a function f(x) with three variables y1, y2, y3. there are two types of problems multiclass and multilabel, based on that we can have the approach ahead. then we saw binary encoding which results in a very compact encoding. we learnt that conversion in detail, how to get the answer. frequency encoding is in which we replace the category values with it's frequency it occurs in dataset. in target encoding, all the r values in the column are replaced by average calculated score above 2.5 in this case.","discussed about the plan for the next remaining 10 lectures and about group project, learnt about function encoding in the start of class. saw an example of converting y(red, blue, green) with three column to be encoded to a function f(x) with three variables y1, y2, y3. there are two types of problems multiclass and multilabel, based on that we can have the approach ahead. then we saw binary encoding which results in a very compact encoding. we learnt that conversion in detail, how to get the answer. frequency encoding is in which we replace the category values with it's frequency it occurs in dataset. in target encoding, all the r values in the column are replaced by average calculated score above 2.5 in this case.",3,-41.162693,6.9458404,0.44654322,6.038962,"categorical, categorization, categorise"
222,"we learned about the metrics used to check how good a linear regression model is, like sse,râ²,etc. 
we also have a hands on excel .
the data we use is just a sample , so the values we calculate for the model, like a and b, are only guesses about the real values for the entire population.
goal  is to create a model that works well for data outside the sample.
idea of sampling distributions and confidence intervals. if we take many small samples from the population and find their means, those means would form a normal distribution, even if the population data isnâ€™t normally distributed. this is called the central limit theorem (clt). the bigger samples give more accurate results, while smaller samples create more uncertainty.","we learned about the metrics used to check how good a linear regression model is, like sse,r ,etc. we also have a hands on excel . the data we use is just a sample , so the values we calculate for the model, like a and b, are only guesses about the real values for the entire population. goal is to create a model that works well for data outside the sample. idea of sampling distributions and confidence intervals. if we take many small samples from the population and find their means, those means would form a normal distribution, even if the population data isn t normally distributed. this is called the central limit theorem (clt). the bigger samples give more accurate results, while smaller samples create more uncertainty.",7,30.180916,0.41760522,14.936338,3.0470216,"statistics, statistical, statisticsâ"
223,"mostly we discussed midsem. we discussed different appraoches to the questions. we saw we can do eda and predicting ailment and in the next part we also saw what can be the reason. the main is different distirbution as kde plots were different. also we saw whether to impute the data or remove the rows as number of missing values were very small if compared to the data. we discussed different aspects of midsem question in detail and we understood that the data need to be well understandable to solve. we also started next topic : curse of dimensionality and about dimensionality reduction . feature selection ,increasing data or dropping features are some ways to reduce dimensions.also we saw variance equation factor which helps in seeing essential independent features.","mostly we discussed midsem. we discussed different appraoches to the questions. we saw we can do eda and predicting ailment and in the next part we also saw what can be the reason. the main is different distirbution as kde plots were different. also we saw whether to impute the data or remove the rows as number of missing values were very small if compared to the data. we discussed different aspects of midsem question in detail and we understood that the data need to be well understandable to solve. we also started next topic : curse of dimensionality and about dimensionality reduction . feature selection ,increasing data or dropping features are some ways to reduce dimensions.also we saw variance equation factor which helps in seeing essential independent features.",11,-5.257169,8.470191,9.468507,7.085938,"pca, heatmap, heatmaps"
224,"in today's class first to discuss some topics from the previous class like the confidence interval and what happens if you lies in the confidence interval .then in today's class we started with multiple linear language regression ,mlr which deals with more than one independent variable. then we learn about embedding vector like we do the example of a single photo and then that photo can be divided into many different kinds of features which together are called emitting vector. then we learned that feature engineering is taking basic data and ensuring that these features are represented on the final set we also learned about how these changes with time and the term give me to jack was liberation whose unit is hertz. what mlr is basically is that use the values of all x and corresponding values of their y to create beta(s). then we learned about mlr gradient descent which is when we have k dimensional hyper surface and when we move along the hyper surface we try to find out the minima and after continuously moving we will come to that minima and resonate about that minimum point which will be the minima.
in the later half of the lecture we worked upon some data values in which we had 5 different features - five values of x. we tried various analysis of that like we dropped the feature with highest p value. ideally the p value should be less than 0.05.the f value should be as iarge as possible because it is msr / mse and msr should be as large as possible and mse should be very small. then we dropped other features as well according to their p value and analysed the data.","in today's class first to discuss some topics from the previous class like the confidence interval and what happens if you lies in the confidence interval .then in today's class we started with multiple linear language regression ,mlr which deals with more than one independent variable. then we learn about embedding vector like we do the example of a single photo and then that photo can be divided into many different kinds of features which together are called emitting vector. then we learned that feature engineering is taking basic data and ensuring that these features are represented on the final set we also learned about how these changes with time and the term give me to jack was liberation whose unit is hertz. what mlr is basically is that use the values of all x and corresponding values of their y to create beta(s). then we learned about mlr gradient descent which is when we have k dimensional hyper surface and when we move along the hyper surface we try to find out the minima and after continuously moving we will come to that minima and resonate about that minimum point which will be the minima. in the later half of the lecture we worked upon some data values in which we had 5 different features - five values of x. we tried various analysis of that like we dropped the feature with highest p value. ideally the p value should be less than 0.05.the f value should be as iarge as possible because it is msr / mse and msr should be as large as possible and mse should be very small. then we dropped other features as well according to their p value and analysed the data.",2,13.852239,8.30365,12.301338,4.069847,"regression, regressions, features"
225,"in today's lecture, sir explained how to do exploratory data analysis (eda). he used excel and pivot tables to do so. we look at different things like mean, median, min, max, stdev, histogram, box plot, summary statistics, 
 scatter plot from the pivot table in order to get an idea about session summary data set. using this we were able to get some outliers whose summary's character values were significantly higher than others. then we also did the same eda for different dataset related to chemical plant where there was a data filled in 241 columns on daily basis for around 6 years. there were also some missing entries/outliers. the report on this data based showed that all that 241 columns can be more or less replaced with 17 pcas. at last tas discussed about our e2-submissions and gave us their insights about that.","in today's lecture, sir explained how to do exploratory data analysis (eda). he used excel and pivot tables to do so. we look at different things like mean, median, min, max, stdev, histogram, box plot, summary statistics, scatter plot from the pivot table in order to get an idea about session summary data set. using this we were able to get some outliers whose summary's character values were significantly higher than others. then we also did the same eda for different dataset related to chemical plant where there was a data filled in 241 columns on daily basis for around 6 years. there were also some missing entries/outliers. the report on this data based showed that all that 241 columns can be more or less replaced with 17 pcas. at last tas discussed about our e2-submissions and gave us their insights about that.",6,-10.493467,26.28783,7.773913,10.275719,"summarizing, summarize, summarization"
226,"sir started the class with discussion on improving the quality of results and the ways to do it one of them was to use grids for parameters and try out every point in the grid. later we moved on to continue the discussion on multiple linear regression with a topic of using taylor series to capture non linearity in datasets by seeing an example of a dataset whose error scatter plot looked like a sine wave. here we looked a back elimination of irrelevant features based on p values. here sir briefly introduced overfitting. further sir discussed the difference between forward and backward feature engineering. moving on sir gave us the difference between parametric and non parametric models and their uses by looking at various models like k nearest neighbours, linear regression, random forest, neural networks, etc. later we compared different matric values like r-squared, mse, etc for all the models to compare them. sir explained in detail the interpretation of the mse and r-squared metrics both while comparing the models amongst themselves and evaluating a model within itself. further we moved on classification of nominal and ordinal variables which began with logistic regression and further discussing the sigmoid function which acts like a switch between zero and one","sir started the class with discussion on improving the quality of results and the ways to do it one of them was to use grids for parameters and try out every point in the grid. later we moved on to continue the discussion on multiple linear regression with a topic of using taylor series to capture non linearity in datasets by seeing an example of a dataset whose error scatter plot looked like a sine wave. here we looked a back elimination of irrelevant features based on p values. here sir briefly introduced overfitting. further sir discussed the difference between forward and backward feature engineering. moving on sir gave us the difference between parametric and non parametric models and their uses by looking at various models like k nearest neighbours, linear regression, random forest, neural networks, etc. later we compared different matric values like r-squared, mse, etc for all the models to compare them. sir explained in detail the interpretation of the mse and r-squared metrics both while comparing the models amongst themselves and evaluating a model within itself. further we moved on classification of nominal and ordinal variables which began with logistic regression and further discussing the sigmoid function which acts like a switch between zero and one",0,-2.64554,-1.8617424,9.245233,4.571201,"models, feature, features"
227,"today in class, proff started by discussing about heatmaps which is used to help visualize relationships between multiple parameters. then we discussed about variance inflation factor(vif) which is used to detect multicollinearity. the higher value of vif for a particular feature suggest that it should be removed from the model in order to improve the model's performance. then we discussed about pca ( principal component analysis) is based on singular value decomposition of matrix and transforms correlated features into orthogonal principal components. pca can reduce dimensionality visualisation and prediction but does not take sensitivity into account which makes normalisation neccessary.","today in class, proff started by discussing about heatmaps which is used to help visualize relationships between multiple parameters. then we discussed about variance inflation factor(vif) which is used to detect multicollinearity. the higher value of vif for a particular feature suggest that it should be removed from the model in order to improve the model's performance. then we discussed about pca ( principal component analysis) is based on singular value decomposition of matrix and transforms correlated features into orthogonal principal components. pca can reduce dimensionality visualisation and prediction but does not take sensitivity into account which makes normalisation neccessary.",11,-15.025359,2.125614,10.120286,12.835867,"pca, heatmap, heatmaps"
228,"linear regression:-
linear regression does not mean that the outcome itself is linear. it means that the outcome is expressed as a linear combination of independent variables.


taylor series expansion:-
a mathematical technique used to express a function as a series expansion.

if errors exhibit specific trends (e.g., sinusoidal patterns), we can introduce new features:
uâ‚ = xâ‚
uâ‚‚ = xâ‚â² (polynomial feature)
uâ‚ƒ = xâ‚â³
uâ‚„ = sin(xâ‚)
generalized as: uâ‚™ = f(xâ‚, ...)

feature selection:-

backward feature elimination:-
eliminates features based on p-values, removing the ones with the highest p-values (typically > 0.05).


forward feature engineering:-
starts with an empty feature set and progressively adds important features to build a more refined model.


backward feature engineering:-
begins with a full feature set and eliminates irrelevant or redundant features.


building a good model:-
a good model should:
avoid excessive unnecessary variables.
prevent overfitting.
ensure generalization.


model selection and explainability:-
we learned about exploratory data analysis (eda) and how to select the best model for a given problem.

parametric vs. non-parametric models:-
parametric models allow for ""what-if"" simulations and interpretability.
non-parametric models are more flexible but may lack explainability.

classification models:-

when predicting classes of categorical variables, we use classification models such as:
logistic regression â†’ determines class probabilities.decision boundaries defines regions where data points belong to a specific class.

we also learnt some very basic introduction to neural network and knn","linear regression:- linear regression does not mean that the outcome itself is linear. it means that the outcome is expressed as a linear combination of independent variables. taylor series expansion:- a mathematical technique used to express a function as a series expansion. if errors exhibit specific trends (e.g., sinusoidal patterns), we can introduce new features: u = x u = x (polynomial feature) u = x u = sin(x ) generalized as: u = f(x , ...) feature selection:- backward feature elimination:- eliminates features based on p-values, removing the ones with the highest p-values (typically > 0.05). forward feature engineering:- starts with an empty feature set and progressively adds important features to build a more refined model. backward feature engineering:- begins with a full feature set and eliminates irrelevant or redundant features. building a good model:- a good model should: avoid excessive unnecessary variables. prevent overfitting. ensure generalization. model selection and explainability:- we learned about exploratory data analysis (eda) and how to select the best model for a given problem. parametric vs. non-parametric models:- parametric models allow for ""what-if"" simulations and interpretability. non-parametric models are more flexible but may lack explainability. classification models:- when predicting classes of categorical variables, we use classification models such as: logistic regression determines class probabilities.decision boundaries defines regions where data points belong to a specific class. we also learnt some very basic introduction to neural network and knn",0,2.4024644,-2.433048,9.769947,4.12924,"models, feature, features"
229,"in today's class we mainly discussed what is exploratory data analysis and how we perform it. for this we ran through an example of data and with the help of pivot table, we tried to get as many insights from the data as we can by observing the box plots, scatter plots, histograms etc. of the data and making conclusions from these plots. by observing the number. of characters submitted by each student in the session summaries for our course, we saw that the maximum value is far away from the mean than the minimum. from this we tried to see whether the distribution is skewed or there are some outliers in the data by observing the box plots and we saw that the data did have some outliers. from this we discussed whether these outliers meaning no. the characters submitted are really constant for a particular student or does this show the jerkiness of a student where on some days he may have submitted more in the lecture summary. this data can be called as time series data. then we also saw the data for a chemical process and saw that for some years the minimum value of some variable was zero and as we dug deep into this problem we saw that the reason behind it being zero was some absent data. like this we also tried to gain some other insights also. in the end, feedback for exercise 2 was provided by the tas.","in today's class we mainly discussed what is exploratory data analysis and how we perform it. for this we ran through an example of data and with the help of pivot table, we tried to get as many insights from the data as we can by observing the box plots, scatter plots, histograms etc. of the data and making conclusions from these plots. by observing the number. of characters submitted by each student in the session summaries for our course, we saw that the maximum value is far away from the mean than the minimum. from this we tried to see whether the distribution is skewed or there are some outliers in the data by observing the box plots and we saw that the data did have some outliers. from this we discussed whether these outliers meaning no. the characters submitted are really constant for a particular student or does this show the jerkiness of a student where on some days he may have submitted more in the lecture summary. this data can be called as time series data. then we also saw the data for a chemical process and saw that for some years the minimum value of some variable was zero and as we dug deep into this problem we saw that the reason behind it being zero was some absent data. like this we also tried to gain some other insights also. in the end, feedback for exercise 2 was provided by the tas.",9,-8.952712,23.287558,8.082702,10.069968,"dataâ, analyse, analyses"
230,in today's lecture we first learnt about feature encoding and we learnt about multi class problem which we use when one expected output of many classes and multi level problem that is expecting output that suggest all possible labels. then we learnt about different types of including. first one is label encoding when independent variable is categorical in nature and if you want to do label based classification we can do this for y but we have to be careful on x side and we have to look for labels. then comes integer encoding which is when numbers assigned will mean something and when there is some original variable. then one hot encoding in which we increase number of columns hence inviting the curse of dimensionality it leads to sparseness of data we should not use this for ordinal variables we can only use this for variables with nominal levels of measurement. zen binary encoding which is like for three columns we can use 8 levels and we use binary system in this then comes frequency encoding that is the category values are replaced by its frequency in the column. in target encoding we replace the values by their average. then we learnt about feature bending that is when continuous features need to be converted into categorical features for example when height and weight we can categorise them like underweight under height and so on. then comes text processing which includes the methods to convert text data into vectors for machine learning like by making dictionaries,in today's lecture we first learnt about feature encoding and we learnt about multi class problem which we use when one expected output of many classes and multi level problem that is expecting output that suggest all possible labels. then we learnt about different types of including. first one is label encoding when independent variable is categorical in nature and if you want to do label based classification we can do this for y but we have to be careful on x side and we have to look for labels. then comes integer encoding which is when numbers assigned will mean something and when there is some original variable. then one hot encoding in which we increase number of columns hence inviting the curse of dimensionality it leads to sparseness of data we should not use this for ordinal variables we can only use this for variables with nominal levels of measurement. zen binary encoding which is like for three columns we can use 8 levels and we use binary system in this then comes frequency encoding that is the category values are replaced by its frequency in the column. in target encoding we replace the values by their average. then we learnt about feature bending that is when continuous features need to be converted into categorical features for example when height and weight we can categorise them like underweight under height and so on. then comes text processing which includes the methods to convert text data into vectors for machine learning like by making dictionaries,3,-44.491554,3.3154445,0.16334689,6.5448003,"categorical, categorization, categorise"
231,"feature encoding. when either the dependent variable or some of the independent variables are categorical then, they have to be appropriately encoded prior to being used for training ml models. the project will be related to assessment of exercises which we have done. label encoding, one hot encoding, binary encoding, integer encoding, frequency encoding, target encoding. multiclass problem- mnist dataset- there are 10 classes(0-9), multilabel problem- there are multiple labels associated with a single object, a image with both cat and dog. how we encode a variable depends on domain knowledge. label encoding- take categorical variables and assign numerical values to these values-this for nominal variable. we can use label encoding to encode output variable, but we should avoid using it on input variable. integer encoding is for ordinal variable- the numbers carry a value and have a meaning. one hot encoding- converting the output into a vector of dimension of number of classes. this increases the number of columns in data and introduces the curse of dimensionality. they choice of encoding depends on number of classes. one hot encoding can be used for nominal variables and classes are not too many. binary encoding- it is just a different notation of one hot encoding. we are converting vectors in one hot encoding into binary values(pseudo one hot encoding).  frequency encoding- the category values are replaced with its frequency in the column. class takes on value of occurance we have to check whether two classes have same frequency. target encoding- we collect all y values corresponding to a particular class take their average and use this to represnt x in input. this is used to encode the input variables. for encoding input variables, we can use one hot encoding generally. for output variable encoding we can use other techniques. eda is important as if we are not able to capture what data is properly we will not be able to perform the further processes correctly.  we need to think multiple times before applying a particular encoding. feature binning- many times we want to convert a continous problem to a discrete problem. we will divide the continous variable into bins and then assign categories to this. this now becomes a classification problem. it can be used for several reasons including problem simplification, reducing the impact of outliers and noise in the data, handling non linear relationships. how to process text data- natural language processing(nlp)- examples of the manner in which statistical processing can generate deterministic output- code generation. how to convert text into numbers so that analysis is useful. method1- drop the common words(stop words); convert to lower case. - create a dictionary. -express the document using the dictionary. in a sentence we cant know exact meaning until we know the context in which it is being used as each word might have multiple uses.","feature encoding. when either the dependent variable or some of the independent variables are categorical then, they have to be appropriately encoded prior to being used for training ml models. the project will be related to assessment of exercises which we have done. label encoding, one hot encoding, binary encoding, integer encoding, frequency encoding, target encoding. multiclass problem- mnist dataset- there are 10 classes(0-9), multilabel problem- there are multiple labels associated with a single object, a image with both cat and dog. how we encode a variable depends on domain knowledge. label encoding- take categorical variables and assign numerical values to these values-this for nominal variable. we can use label encoding to encode output variable, but we should avoid using it on input variable. integer encoding is for ordinal variable- the numbers carry a value and have a meaning. one hot encoding- converting the output into a vector of dimension of number of classes. this increases the number of columns in data and introduces the curse of dimensionality. they choice of encoding depends on number of classes. one hot encoding can be used for nominal variables and classes are not too many. binary encoding- it is just a different notation of one hot encoding. we are converting vectors in one hot encoding into binary values(pseudo one hot encoding). frequency encoding- the category values are replaced with its frequency in the column. class takes on value of occurance we have to check whether two classes have same frequency. target encoding- we collect all y values corresponding to a particular class take their average and use this to represnt x in input. this is used to encode the input variables. for encoding input variables, we can use one hot encoding generally. for output variable encoding we can use other techniques. eda is important as if we are not able to capture what data is properly we will not be able to perform the further processes correctly. we need to think multiple times before applying a particular encoding. feature binning- many times we want to convert a continous problem to a discrete problem. we will divide the continous variable into bins and then assign categories to this. this now becomes a classification problem. it can be used for several reasons including problem simplification, reducing the impact of outliers and noise in the data, handling non linear relationships. how to process text data- natural language processing(nlp)- examples of the manner in which statistical processing can generate deterministic output- code generation. how to convert text into numbers so that analysis is useful. method1- drop the common words(stop words); convert to lower case. - create a dictionary. -express the document using the dictionary. in a sentence we cant know exact meaning until we know the context in which it is being used as each word might have multiple uses.",3,-44.502193,4.161085,0.23687826,6.553008,"categorical, categorization, categorise"
232,"the lecture continued on the lines of logistic regression and classification ideas, from where we left off. we started off by discussing the error metrics and the confusion matrix. then we were given a demonstration on some sample data on â€˜playground.tensorflow.orgâ€™, which allowed us to play around with data and neural networks. it allowed us to add features to our classification problem. then we moved on to some code for logistic regression. we understood that the score function for a logistic regression model library gives us the accuracy, which we should not believe. we also plotted the confusion matrix for the data, and understood the true positives and true negatives. we then studied the reciever operating characteristics (roc) curve. the roc curve rose vertically at the start and then became horizontal. this should us that for the given dataset, our model first detected all the true positives before starting to detect false positives. ideally, the roc curve is usually shaped in a crescent shape. the roc curve indicates the quality of the classifier, where a sharper roc curve indicates a better model. the logistic_regression function returns the predicted class for an observation, or the probability value associated with it. hence, by changing the threshold between the classes, we can change the quality of the model and check it using the roc curve. the 45 degree line on the roc curve tells us that there is no classification between the points. hence, the flatter the roc curve for our classifier, the worse is our classification model. we also measure the area under the roc curve and call it as auc. for a good classifier, this value is close to 1. the worst classifier is the one with the roc curve the same as the 45 degrees line. hence the worst case auc value is 0.5. the points on the roc curve correspond to different threshold values for the classification. 
we then moved on to data having more than 2 classes, and we realised that sometimes, due to lack of anomalous data, we might not be able to use our classifier. in the entire system of the classifier, some classes may have very low f1-score, which means that they are under-represented and the classifier canâ€™t be relied on for classification of data into that class. each class has itâ€™s own roc curve, and hence a classifier can be good for one class, while being equally bad for another, which can be seen using the roc curve of that class for our classifier. 
then we moved on to clustering, which was a part of unsupervised learning. in this case, our data does not have any labels, and we need to assign labels to the data ourselves based on some characteristics. this assignment becomes a part of eda. we then studied about some clustering algorithms like hierarchical clustering and k-means clustering. for k-means clustering, we need to give the number of clusters as the input, which is difficult to find out just based on visualisation of data. however, in hierarchical clustering, we plot a dendogram and then based on that, we can decide the number of clusters we need. 
in k-means clustering, we first specify the number of desired clusters. then the algorithm starts randomly assigning points to any cluster, and then we find the centroid of each cluster. then we find the distance of each point in a cluster to the centroid of all the clusters. then we reassign each point based on the centroid they are closest to. this process repeats until each point is closest to the centroid of its cluster than any other cluster. it is an o(n^2) algorithm. in this algorithm, the initial assignment also changes our final results. hence, we run the algorithm multiple times and check that for each point, which set was it clustered into multiple times, and hence we decide the final cluster it belongs to. 
hierarchical clustering does not require any initial cluster number. it results in a dendogram, which can help us decide our degree of clustering. it starts by assigning each point as a different cluster. it then gets grouped with other clusters and we get the dendogram. the number of clusters can be decided by where we observe the dendogram. the pair wise distance between each pair of clusters is calculated, and those clusters are grouped, whose distance is less than all the other pairwise distances of those clusters. this goes on till we have just one cluster, giving us the dendogram, and the number of clusters is decided by where we cut the graph.","the lecture continued on the lines of logistic regression and classification ideas, from where we left off. we started off by discussing the error metrics and the confusion matrix. then we were given a demonstration on some sample data on playground.tensorflow.org , which allowed us to play around with data and neural networks. it allowed us to add features to our classification problem. then we moved on to some code for logistic regression. we understood that the score function for a logistic regression model library gives us the accuracy, which we should not believe. we also plotted the confusion matrix for the data, and understood the true positives and true negatives. we then studied the reciever operating characteristics (roc) curve. the roc curve rose vertically at the start and then became horizontal. this should us that for the given dataset, our model first detected all the true positives before starting to detect false positives. ideally, the roc curve is usually shaped in a crescent shape. the roc curve indicates the quality of the classifier, where a sharper roc curve indicates a better model. the logistic_regression function returns the predicted class for an observation, or the probability value associated with it. hence, by changing the threshold between the classes, we can change the quality of the model and check it using the roc curve. the 45 degree line on the roc curve tells us that there is no classification between the points. hence, the flatter the roc curve for our classifier, the worse is our classification model. we also measure the area under the roc curve and call it as auc. for a good classifier, this value is close to 1. the worst classifier is the one with the roc curve the same as the 45 degrees line. hence the worst case auc value is 0.5. the points on the roc curve correspond to different threshold values for the classification. we then moved on to data having more than 2 classes, and we realised that sometimes, due to lack of anomalous data, we might not be able to use our classifier. in the entire system of the classifier, some classes may have very low f1-score, which means that they are under-represented and the classifier can t be relied on for classification of data into that class. each class has it s own roc curve, and hence a classifier can be good for one class, while being equally bad for another, which can be seen using the roc curve of that class for our classifier. then we moved on to clustering, which was a part of unsupervised learning. in this case, our data does not have any labels, and we need to assign labels to the data ourselves based on some characteristics. this assignment becomes a part of eda. we then studied about some clustering algorithms like hierarchical clustering and k-means clustering. for k-means clustering, we need to give the number of clusters as the input, which is difficult to find out just based on visualisation of data. however, in hierarchical clustering, we plot a dendogram and then based on that, we can decide the number of clusters we need. in k-means clustering, we first specify the number of desired clusters. then the algorithm starts randomly assigning points to any cluster, and then we find the centroid of each cluster. then we find the distance of each point in a cluster to the centroid of all the clusters. then we reassign each point based on the centroid they are closest to. this process repeats until each point is closest to the centroid of its cluster than any other cluster. it is an o(n^2) algorithm. in this algorithm, the initial assignment also changes our final results. hence, we run the algorithm multiple times and check that for each point, which set was it clustered into multiple times, and hence we decide the final cluster it belongs to. hierarchical clustering does not require any initial cluster number. it results in a dendogram, which can help us decide our degree of clustering. it starts by assigning each point as a different cluster. it then gets grouped with other clusters and we get the dendogram. the number of clusters can be decided by where we observe the dendogram. the pair wise distance between each pair of clusters is calculated, and those clusters are grouped, whose distance is less than all the other pairwise distances of those clusters. this goes on till we have just one cluster, giving us the dendogram, and the number of clusters is decided by where we cut the graph.",8,-3.9361691,-21.140898,6.291726,0.33183062,"classification, clusterings, classifying"
233,"1. we learned closed form solution in multiple linear regression .
2. in feature selection we remove the feature if it's  p value is greater than 0.05  
3. we use 80-20 random split b/w test and train data . 
4. overfit can happen if r2 values of test and train data are not close enough .
5. we learned about adjusted r2 and want a model to represent population and also about 
n-1 degree of freedom in tss",1. we learned closed form solution in multiple linear regression . 2. in feature selection we remove the feature if it's p value is greater than 0.05 3. we use 80-20 random split b/w test and train data . 4. overfit can happen if r2 values of test and train data are not close enough . 5. we learned about adjusted r2 and want a model to represent population and also about n-1 degree of freedom in tss,2,10.016544,10.8834095,11.429417,5.037189,"regression, regressions, features"
234,"the following topics were discussed in today's class:-
1. linear regression with higher order terms: linear regression is just the linear combination of many variables, those variables could be higher powers of the a single independent variable. thus increasing the complexity of the model and its capability to generalize patterns. those higher powers of the same variables are examples of engineered features, this regression with higher order terms is called polynomial regression. we also learnt that based on the problem in hand, one can use more than one models for the same problem.
2. forward and backward feature engineering:  in forward feature engineering the independent variables are chosen one by one with proper analysis. in the other case, all the available variables are added into the model and the redundant ones are removed based on the p-values
3. parametric and non parametric models: in parametric models, the form of the function is assumed with a number of parameters. the ultimate aim of the problem then becomes finding optimal parameters minimizing loss function. these models ability is limited as they can't capture intricate patterns, causing underfitting issues. in case of non-parameteric models, any assumption is not made about the function and the data itself is used to make the prediction. these models have greater complexity and capture patterns better, but have the issue of overfitting. examples like random forest, x-boost, k-nn
4. neural networks: a model having multiple layers connected to each other communicating with each other to finally give a output. if the number of layers is more than one, then it is called deep learning.
5. introduction to classification problem: the objective of this problem is to come with a decision boundary that best separates the given classes. little bit about the sigmoid function was also discussed.","the following topics were discussed in today's class:- 1. linear regression with higher order terms: linear regression is just the linear combination of many variables, those variables could be higher powers of the a single independent variable. thus increasing the complexity of the model and its capability to generalize patterns. those higher powers of the same variables are examples of engineered features, this regression with higher order terms is called polynomial regression. we also learnt that based on the problem in hand, one can use more than one models for the same problem. 2. forward and backward feature engineering: in forward feature engineering the independent variables are chosen one by one with proper analysis. in the other case, all the available variables are added into the model and the redundant ones are removed based on the p-values 3. parametric and non parametric models: in parametric models, the form of the function is assumed with a number of parameters. the ultimate aim of the problem then becomes finding optimal parameters minimizing loss function. these models ability is limited as they can't capture intricate patterns, causing underfitting issues. in case of non-parameteric models, any assumption is not made about the function and the data itself is used to make the prediction. these models have greater complexity and capture patterns better, but have the issue of overfitting. examples like random forest, x-boost, k-nn 4. neural networks: a model having multiple layers connected to each other communicating with each other to finally give a output. if the number of layers is more than one, then it is called deep learning. 5. introduction to classification problem: the objective of this problem is to come with a decision boundary that best separates the given classes. little bit about the sigmoid function was also discussed.",0,1.463028,-1.9292126,9.690787,4.1913657,"models, feature, features"
235,"sir talked about closed form solutions in mlr. 
this is not feasible due to two reasons
(1) multi colinearity
(2) matrix inversion
if p value is bigger than 0.05 then values are strategically similar to 0 and don't help much so these feature values are deleted.

then sir said that we should never use the entire data for taining our model. after cleaning our data (which would be most of the task that we would do in data science) we should split the the data into two parts - training data and testing data. now the question arises what should be the ratio of volume of the two parts? it should be 80-20. we can keep 50-50 but that won't be enough to train our model and this much data won't be enough to represent the entire population.
how to split data? there are functions in python which do this. sir also said that people who are not familiar with python should start working on it.
then sir talked about overfitting of data and gave us example of the worst case of over fiting.

if r-square values are near then over fitting issue won't occur. i am talking about the r-square values of training and testing metrics. sir also gave a formula for adjusted r which is mentioned in notes which has degree of freedom and tss and rss. i don't know much about the notation because in the last semester (in ai/ds) the notation was a bit different so of you are reading this please check notation.
sir also talked about multiple r = square root of r-square.",sir talked about closed form solutions in mlr. this is not feasible due to two reasons (1) multi colinearity (2) matrix inversion if p value is bigger than 0.05 then values are strategically similar to 0 and don't help much so these feature values are deleted. then sir said that we should never use the entire data for taining our model. after cleaning our data (which would be most of the task that we would do in data science) we should split the the data into two parts - training data and testing data. now the question arises what should be the ratio of volume of the two parts? it should be 80-20. we can keep 50-50 but that won't be enough to train our model and this much data won't be enough to represent the entire population. how to split data? there are functions in python which do this. sir also said that people who are not familiar with python should start working on it. then sir talked about overfitting of data and gave us example of the worst case of over fiting. if r-square values are near then over fitting issue won't occur. i am talking about the r-square values of training and testing metrics. sir also gave a formula for adjusted r which is mentioned in notes which has degree of freedom and tss and rss. i don't know much about the notation because in the last semester (in ai/ds) the notation was a bit different so of you are reading this please check notation. sir also talked about multiple r = square root of r-square.,2,4.739155,6.704524,10.702576,5.1872854,"regression, regressions, features"
236,"we discussed population parameters and sample statistics such as mean, median, and variance. in simple linear regression, we introduced the equation y = bâ‚€ + bâ‚x, where bâ‚€(intercept) and bâ‚(slope) are sample estimates of population parameters. bias was defined as the influence of unaccounted variables in the model.  
we explored error calculation methods like sum of absolute errors and sum of squared errors (sse), with sse preferred for its sensitivity to large deviations. different samples yield different regression lines, so evaluating errors is crucial to determine accuracy.  
we derived formulas to compute bâ‚€ and bâ‚and discussed confidence intervals for estimating population parameters, which offer better reliability compared to point estimates. this established the basis of regression modeling and error analysis.","we discussed population parameters and sample statistics such as mean, median, and variance. in simple linear regression, we introduced the equation y = b + b x, where b (intercept) and b (slope) are sample estimates of population parameters. bias was defined as the influence of unaccounted variables in the model. we explored error calculation methods like sum of absolute errors and sum of squared errors (sse), with sse preferred for its sensitivity to large deviations. different samples yield different regression lines, so evaluating errors is crucial to determine accuracy. we derived formulas to compute b and b and discussed confidence intervals for estimating population parameters, which offer better reliability compared to point estimates. this established the basis of regression modeling and error analysis.",1,30.614523,-10.406203,16.406502,4.2159185,"population, models, estimating"
237,"today's lecture began with variance inflation factor (vif), where we explored how vif varies with râ² and examined its graphical representation. we learned that the factor with the highest vif is the most dependent, so we remove it iteratively until all remaining factors have a vif below the threshold.

next, we covered principal component analysis (pca), another feature reduction method. key observations included:

the number of principal components equals the original data dimensions, but we select the most important ones.
principal components are orthogonal to each other.
the principal axis that captures the most variance in the data is considered the most significant.
each principal component has loadings with the original features.
we then discussed the use cases of pca, including:

prediction models â€“ pca can be used for improving model performance but is not suitable for ""what-if"" scenarios, where retaining actual features is crucial.
exploratory data analysis (eda) â€“ understanding data structure and reducing dimensions for visualization.
to illustrate pca, we applied it to a 28ã—28 pixel dataset used for predicting handwritten numbers.

finally, we introduced t-sne (t-distributed stochastic neighbor embedding) and compared normal distribution vs. t-distribution","today's lecture began with variance inflation factor (vif), where we explored how vif varies with r and examined its graphical representation. we learned that the factor with the highest vif is the most dependent, so we remove it iteratively until all remaining factors have a vif below the threshold. next, we covered principal component analysis (pca), another feature reduction method. key observations included: the number of principal components equals the original data dimensions, but we select the most important ones. principal components are orthogonal to each other. the principal axis that captures the most variance in the data is considered the most significant. each principal component has loadings with the original features. we then discussed the use cases of pca, including: prediction models pca can be used for improving model performance but is not suitable for ""what-if"" scenarios, where retaining actual features is crucial. exploratory data analysis (eda) understanding data structure and reducing dimensions for visualization. to illustrate pca, we applied it to a 28 28 pixel dataset used for predicting handwritten numbers. finally, we introduced t-sne (t-distributed stochastic neighbor embedding) and compared normal distribution vs. t-distribution",11,-16.135937,2.3703136,10.240303,12.942932,"pca, heatmap, heatmaps"
238,"the session began with the brief of logistic regression and revision of gradient descent , sir also introduced confusion matrix and discussed terms like recall, f1 score , sir then discussed roc curve ( tpr vs fpr ) and how each class have their own roc curve. sir also discussed clustering mainly k means clustering then introduced hierarchical clustering in the end.","the session began with the brief of logistic regression and revision of gradient descent , sir also introduced confusion matrix and discussed terms like recall, f1 score , sir then discussed roc curve ( tpr vs fpr ) and how each class have their own roc curve. sir also discussed clustering mainly k means clustering then introduced hierarchical clustering in the end.",8,-6.8517013,-16.954744,6.2451324,0.72968596,"classification, clusterings, classifying"
239,"learned about pca, which helps reduce data size while keeping important patterns.
before using pca, it's necessary to remove similar information using vif.
principal components (pcs) were introduced, which hold the most important variations in the data.
pca helps in simplifying data by keeping only the essential parts.
the elbow method was explained as a way to decide how many pcs to keep.
discussed where pca is useful, such as in data visualization, making predictions, and understanding data patterns.
compared normal regression with pca-based regression, showing how pca makes models easier to understand and more effective.
ended with how pca is applied in real-world data projects and its role in predictive analysis.","learned about pca, which helps reduce data size while keeping important patterns. before using pca, it's necessary to remove similar information using vif. principal components (pcs) were introduced, which hold the most important variations in the data. pca helps in simplifying data by keeping only the essential parts. the elbow method was explained as a way to decide how many pcs to keep. discussed where pca is useful, such as in data visualization, making predictions, and understanding data patterns. compared normal regression with pca-based regression, showing how pca makes models easier to understand and more effective. ended with how pca is applied in real-world data projects and its role in predictive analysis.",11,-13.413357,3.2907352,10.104769,12.731281,"pca, heatmap, heatmaps"
240,"eda was the primary topic of discussion. two datasets from different contexts were chosen and eda was done on them. they are as follows
1. our summary submissions: we used pivot tables to explore various aspects of the data like average character count for every class submission, max, min characters of the submission. then box plot was used to visualize the outliers present in the dataset. scatter plot and histogram to visualize the distribution of characters count.
2. data from a chemical factory setting: similar eda practice was presented.
finally the discussion ended with the analysis of our submissions of assignment 2.","eda was the primary topic of discussion. two datasets from different contexts were chosen and eda was done on them. they are as follows 1. our summary submissions: we used pivot tables to explore various aspects of the data like average character count for every class submission, max, min characters of the submission. then box plot was used to visualize the outliers present in the dataset. scatter plot and histogram to visualize the distribution of characters count. 2. data from a chemical factory setting: similar eda practice was presented. finally the discussion ended with the analysis of our submissions of assignment 2.",6,-9.80213,27.982327,7.61153,9.9397335,"summarizing, summarize, summarization"
241,"in the starting, we gave a look at both parts of the problem statement and understood that chatgpt is good for the coding part but not for designing the solution. we started off with the eda and made scatter plots and histograms. by looking into it, we realized the need for normalisation. then to check outliers, we made boxplots. it was very surprising to see that there were no outliers in any boxplot. to understand the representation of each ailment, we made a bar graph or pie chart. we came to know that heart diseases are very under-represented. as a result, we can say that we can't give our analysis on heart diseases because undersampling leads to loss of information while oversampling will not be accurate, so it is better to drop heart disease ailment.
we then made a heatmap to understand the correlations between parameters. we used random forest model for regression. by looking into the confusion matrix, we saw it gave a nice result except for heart disease.
we then made kde plots instead of histograms.
then we had a look at the curse of dimensionality. it leads to increased complexity and cost of computation. we can reduce it by reducing dimensions, feature selections, increasing the amount of data, etc.","in the starting, we gave a look at both parts of the problem statement and understood that chatgpt is good for the coding part but not for designing the solution. we started off with the eda and made scatter plots and histograms. by looking into it, we realized the need for normalisation. then to check outliers, we made boxplots. it was very surprising to see that there were no outliers in any boxplot. to understand the representation of each ailment, we made a bar graph or pie chart. we came to know that heart diseases are very under-represented. as a result, we can say that we can't give our analysis on heart diseases because undersampling leads to loss of information while oversampling will not be accurate, so it is better to drop heart disease ailment. we then made a heatmap to understand the correlations between parameters. we used random forest model for regression. by looking into the confusion matrix, we saw it gave a nice result except for heart disease. we then made kde plots instead of histograms. then we had a look at the curse of dimensionality. it leads to increased complexity and cost of computation. we can reduce it by reducing dimensions, feature selections, increasing the amount of data, etc.",9,-5.1994414,15.099976,9.546326,8.15417,"dataâ, analyse, analyses"
242,"in this lecture we started by looking at how results were used to be predicted earlier, without any ml models. people used to collect data, through experiments and observations and then by using those x and corresponding y values they used to manually fit the data into some curve/relation which could then be used to predict future values.
but with the advent of ml algorithms, the need to manually determine relations has vanished and we can use various available ml models to fit relations among collected data and the model can predict any future values fed to it. some of these algorithms include- linear and multiple regression, random forests, etc.
next, we studied about the 4 levels/scales of measurement- nominal, ordinal, interval and ratio.
the nominal level categorizes the labels qualitatively into groups, which donâ€™t have any specific order. we can use frequency distributions to analyze this class of labels. also, since in nominal level, we do not consider ordering, it is incorrect to represent the values in terms of single numerical values. instead, we should use a vector which will have only one value as â€˜1â€™ and others â€˜0â€™ depending on which category the label y belongs to.
next, we talked about ordinal level which is associated with a specific sequence. the example we considered that of grades. grades have a specific order of importance but the interval between these is not fixed/defined.
the interval level consists of values that have equal intervals but the absolute zero for the measurements is not defined. in this level also, order matters. we discussed the example of temperature in celsius or fahrenheit scales. in these scales the 0 value is not absolute. so, we can just compare the difference between two values of temperature and comment on which one is hotter, but we cannot comment on the ratio of temperatures.
ratio class has an absolute zero defined and at this value it means that there is complete absence of that measurable. we considered example of height and weight here. so, heights of two persons can be compared and we can comment about the ratio of their heights. (eg. we can say a person with height of 5 ft is twice as tall as the one with height of 2.5ft.)
nominal and ordinal levels consist of discrete data, whereas interval and ratio levels contain continuous data.
next, we talked about supervised and unsupervised learning models:
supervised leaning models are the ones in which we feed the complete data, consisting of both the features and labels, based on which the ml model determines the future values. it uses classification or regression techniques for the same.
in unsupervised learning, only the x values are given based on the various x values (features) the machine learns by itself and makes clusters of values with similar features and assigns labels to each, after which it can predict futures values, based on the determined relations.
we use k-means clustering and hierarchical clustering algorithms for the same.
so, the ml models try to determine the function â€˜fâ€™ which relates y and x as y=f(x), based on available data.
we can always collect a sample of the entire population of data to develop these models.","in this lecture we started by looking at how results were used to be predicted earlier, without any ml models. people used to collect data, through experiments and observations and then by using those x and corresponding y values they used to manually fit the data into some curve/relation which could then be used to predict future values. but with the advent of ml algorithms, the need to manually determine relations has vanished and we can use various available ml models to fit relations among collected data and the model can predict any future values fed to it. some of these algorithms include- linear and multiple regression, random forests, etc. next, we studied about the 4 levels/scales of measurement- nominal, ordinal, interval and ratio. the nominal level categorizes the labels qualitatively into groups, which don t have any specific order. we can use frequency distributions to analyze this class of labels. also, since in nominal level, we do not consider ordering, it is incorrect to represent the values in terms of single numerical values. instead, we should use a vector which will have only one value as 1 and others 0 depending on which category the label y belongs to. next, we talked about ordinal level which is associated with a specific sequence. the example we considered that of grades. grades have a specific order of importance but the interval between these is not fixed/defined. the interval level consists of values that have equal intervals but the absolute zero for the measurements is not defined. in this level also, order matters. we discussed the example of temperature in celsius or fahrenheit scales. in these scales the 0 value is not absolute. so, we can just compare the difference between two values of temperature and comment on which one is hotter, but we cannot comment on the ratio of temperatures. ratio class has an absolute zero defined and at this value it means that there is complete absence of that measurable. we considered example of height and weight here. so, heights of two persons can be compared and we can comment about the ratio of their heights. (eg. we can say a person with height of 5 ft is twice as tall as the one with height of 2.5ft.) nominal and ordinal levels consist of discrete data, whereas interval and ratio levels contain continuous data. next, we talked about supervised and unsupervised learning models: supervised leaning models are the ones in which we feed the complete data, consisting of both the features and labels, based on which the ml model determines the future values. it uses classification or regression techniques for the same. in unsupervised learning, only the x values are given based on the various x values (features) the machine learns by itself and makes clusters of values with similar features and assigns labels to each, after which it can predict futures values, based on the determined relations. we use k-means clustering and hierarchical clustering algorithms for the same. so, the ml models try to determine the function f which relates y and x as y=f(x), based on available data. we can always collect a sample of the entire population of data to develop these models.",4,-26.895813,-14.354215,1.5633318,0.45207685,"classification, classifying, classifications"
243,"summary

1. linear regression demo in excel: demonstration of linear regression in excel, including visualization and interpretation of results.  

2. histogram & frequency distribution: introduction to histograms as graphical representations of data distribution and frequency distributions to analyze occurrences of values. discussed types of distributions for truly random data.  

3. regression coefficients calculation: regression coefficients (î²â‚€ and î²â‚) were calculated using closed-form expressions rather than iterative methods.  

4. scatter plot of original data: created scatter plots to visualize data points, identify trends, and assess relationships before applying regression models.  

5. error metrics: calculated key error metrics to evaluate model performance, including sse (sum of squared errors), mse (mean squared error), rmse (root mean squared error), and mae (mean absolute error).  

6. example of line force-fitted on non-linear data: demonstrated an example where a linear model was applied to non-linear data, highlighting issues of poor fit.  

7. scatter plot of error values: visualized error values (residuals), noting that a good model should produce randomly distributed residuals without patterns.  

8. criteria for a good model: a good model is one that explains most of the variation in the data and minimizes error.  

9. total variation in dataset (sst): explained sst (total sum of squares) as a measure of total data variability, with components ssr (sum of squares regression) and sse (sum of squares error). the equation sst = ssr + sse + 0 was discussed.  

10. coefficient of determination (râ²): introduced the formula râ² = 1 - (sse/sst), which indicates the proportion of variance explained by the model in slr.  

11. standard deviation: defined as a measure of dispersion and variability within the dataset relative to the mean.  

12. central limit theorem (clt): explained how the sampling distribution of the sample mean approaches normality as sample size increases, which is crucial for statistical inference.","summary 1. linear regression demo in excel: demonstration of linear regression in excel, including visualization and interpretation of results. 2. histogram & frequency distribution: introduction to histograms as graphical representations of data distribution and frequency distributions to analyze occurrences of values. discussed types of distributions for truly random data. 3. regression coefficients calculation: regression coefficients ( and ) were calculated using closed-form expressions rather than iterative methods. 4. scatter plot of original data: created scatter plots to visualize data points, identify trends, and assess relationships before applying regression models. 5. error metrics: calculated key error metrics to evaluate model performance, including sse (sum of squared errors), mse (mean squared error), rmse (root mean squared error), and mae (mean absolute error). 6. example of line force-fitted on non-linear data: demonstrated an example where a linear model was applied to non-linear data, highlighting issues of poor fit. 7. scatter plot of error values: visualized error values (residuals), noting that a good model should produce randomly distributed residuals without patterns. 8. criteria for a good model: a good model is one that explains most of the variation in the data and minimizes error. 9. total variation in dataset (sst): explained sst (total sum of squares) as a measure of total data variability, with components ssr (sum of squares regression) and sse (sum of squares error). the equation sst = ssr + sse + 0 was discussed. 10. coefficient of determination (r ): introduced the formula r = 1 - (sse/sst), which indicates the proportion of variance explained by the model in slr. 11. standard deviation: defined as a measure of dispersion and variability within the dataset relative to the mean. 12. central limit theorem (clt): explained how the sampling distribution of the sample mean approaches normality as sample size increases, which is crucial for statistical inference.",5,22.20554,-5.82858,14.432793,4.7193484,"regression, statistical, statistics"
244,"we resumed (mainly examining various statistical conditions) from where we left off in the previous class, discussing the terms emerging from our extension (data analysis toolpack), and then explored their interconnections and terminologies, what these values signify, their graphical representation, and conclusions associated with certain errors or uncertainties that accompany them.
we examined several instances of beta and beta 0, along with their varying specific case conditions.
we also examined multiple linear regression and noted that several of the terms in our table were primarily significant for this type of regression. we also examined the meaning of the term anova and the related concepts it encompasses (f statistic and the reason for its significance, to be continued in the next session).","we resumed (mainly examining various statistical conditions) from where we left off in the previous class, discussing the terms emerging from our extension (data analysis toolpack), and then explored their interconnections and terminologies, what these values signify, their graphical representation, and conclusions associated with certain errors or uncertainties that accompany them. we examined several instances of beta and beta 0, along with their varying specific case conditions. we also examined multiple linear regression and noted that several of the terms in our table were primarily significant for this type of regression. we also examined the meaning of the term anova and the related concepts it encompasses (f statistic and the reason for its significance, to be continued in the next session).",13,4.485668,17.146183,12.613638,5.924703,"classification, classifying, classifications"
245,"discussed bird's eye view of next month. then we started feature encoding. no matter what the form of y be, when passed to algorithm, y should be numerical. 1st method to change categorical data is one hot encoding. which is just creating additional columns. dealt with multi class( like showing 8) where expected output is one of many classes. 2nd is multi label(a same picture having a dog and cat), output suggest all possible artifacts of the outcome. next approach we saw is label encoding where suppose red is 0 green is 1, need to remember that y doesn't not get effected by these values. but x can't be label encoded it will affect it's values and will affect the entire algorithm. next was integer encoding if suppose like grades, there is inherent order in it so can directly increaseencode it to that. in one hot encoding we  number of columns just pave way for curse of dimensionality, data becomes sparse. binary encoding convert in binary form then .next up was target encoding like averaging of the category values. to convert continuous problem into discrete we can use feature binning. we lastly saw how text data is getting converted to categories.","discussed bird's eye view of next month. then we started feature encoding. no matter what the form of y be, when passed to algorithm, y should be numerical. 1st method to change categorical data is one hot encoding. which is just creating additional columns. dealt with multi class( like showing 8) where expected output is one of many classes. 2nd is multi label(a same picture having a dog and cat), output suggest all possible artifacts of the outcome. next approach we saw is label encoding where suppose red is 0 green is 1, need to remember that y doesn't not get effected by these values. but x can't be label encoded it will affect it's values and will affect the entire algorithm. next was integer encoding if suppose like grades, there is inherent order in it so can directly increaseencode it to that. in one hot encoding we number of columns just pave way for curse of dimensionality, data becomes sparse. binary encoding convert in binary form then .next up was target encoding like averaging of the category values. to convert continuous problem into discrete we can use feature binning. we lastly saw how text data is getting converted to categories.",3,-43.228107,6.260866,0.3519875,6.149076,"categorical, categorization, categorise"
246,"class notes summary:

time-series data analysis:

use time-series analysis when data is insufficient for long-range predictions.

small samples may follow a uniform distribution rather than a gaussian distribution.


outcome distribution:

with many unknown variables, the outcome distribution typically approaches a gaussian/normal distribution.


regression analysis:

error decomposition:


sst: total sum of squares

sse: error sum of squares

ssr: regression sum of squares


coefficient of determination ():



correlation:

correlation coefficient (r):

high : strong linear relationship.

low : weak linear relationship.


examples: positive and negative correlation scenarios.


visualization notes:

use histograms for small samples.

scatter plots for correlation analysis.

normal distribution curves for gaussian outcomes.","class notes summary: time-series data analysis: use time-series analysis when data is insufficient for long-range predictions. small samples may follow a uniform distribution rather than a gaussian distribution. outcome distribution: with many unknown variables, the outcome distribution typically approaches a gaussian/normal distribution. regression analysis: error decomposition: sst: total sum of squares sse: error sum of squares ssr: regression sum of squares coefficient of determination (): correlation: correlation coefficient (r): high : strong linear relationship. low : weak linear relationship. examples: positive and negative correlation scenarios. visualization notes: use histograms for small samples. scatter plots for correlation analysis. normal distribution curves for gaussian outcomes.",5,23.237509,-6.720593,14.318812,4.7169747,"regression, statistical, statistics"
247,"in this class first we did the recap for the p-value and the statistics and later we continued where we left off. the mlr. we focused on how various features get selected. like for the output value depends on how many independent variable. selection of appropriate feature from the data is called feature engineering. suppose for eg we are given a data of vibration wrt time. and we are interested in rate of change of freq instead of vibration. feature engineering deals with ensuring the data contains the rate of change also.

mlr continuation:
  here also like slr our objective is to minimize the square of errors. we use matrix terms to simplify the equations and then take the derivative of the cost function. but since mlr may contain different type of errors. mlr is not considered as a closed form. so we cant fix a soln instead we take a random point (random coefficient  values i mean wrt the cost function graph) and use gradient descent to approach the minima after a lot of iterations. same as newton raphson method.

then we moved onto the analysis. f-stats. the greater the value the better the model performed. it is msr/mse  (variance explained by regression/variance not explained roughly). mse,sse are useful in optimization whereas rmse, mae are relatable and helpful in interpretation, explanation. although the p value is not 0 . coefficient is very much different from 0. but if 0 lies in the 95% confidence interval we are unsure. there is not stability and the coefficient analysis is not statistically significant. 

i still didnt understand the last part of the lecture about dropping the feature with maximum p-value. and how it gave a better results and why did we want to bring one of the feature coeff. p-value close to 0.05.","in this class first we did the recap for the p-value and the statistics and later we continued where we left off. the mlr. we focused on how various features get selected. like for the output value depends on how many independent variable. selection of appropriate feature from the data is called feature engineering. suppose for eg we are given a data of vibration wrt time. and we are interested in rate of change of freq instead of vibration. feature engineering deals with ensuring the data contains the rate of change also. mlr continuation: here also like slr our objective is to minimize the square of errors. we use matrix terms to simplify the equations and then take the derivative of the cost function. but since mlr may contain different type of errors. mlr is not considered as a closed form. so we cant fix a soln instead we take a random point (random coefficient values i mean wrt the cost function graph) and use gradient descent to approach the minima after a lot of iterations. same as newton raphson method. then we moved onto the analysis. f-stats. the greater the value the better the model performed. it is msr/mse (variance explained by regression/variance not explained roughly). mse,sse are useful in optimization whereas rmse, mae are relatable and helpful in interpretation, explanation. although the p value is not 0 . coefficient is very much different from 0. but if 0 lies in the 95% confidence interval we are unsure. there is not stability and the coefficient analysis is not statistically significant. i still didnt understand the last part of the lecture about dropping the feature with maximum p-value. and how it gave a better results and why did we want to bring one of the feature coeff. p-value close to 0.05.",2,14.236379,6.056069,12.484269,4.0474906,"regression, regressions, features"
248,"we started from the formulae of a and b which we had derived in the previous class. then using excel or any spreadsheet, we found out the values of a and b manually, then showed our result (y cap), errors, y using scatter plots, lines, histograms and other representations. we also looked at the errors arising in our y cap predictions. then using the extensions and data analysis paks (toolkits) we learnt how these tasks can be performed directly without manual effort, saving time. we then looked at some terminologies appearing in the table provided by our extension tool and learnt their significance, like confidence intervals and our examples or the sample size significance. we looked at mean variance, other such moments for a brief time, and then proceeded to discussions regarding importance of taking reasonable assumptions","we started from the formulae of a and b which we had derived in the previous class. then using excel or any spreadsheet, we found out the values of a and b manually, then showed our result (y cap), errors, y using scatter plots, lines, histograms and other representations. we also looked at the errors arising in our y cap predictions. then using the extensions and data analysis paks (toolkits) we learnt how these tasks can be performed directly without manual effort, saving time. we then looked at some terminologies appearing in the table provided by our extension tool and learnt their significance, like confidence intervals and our examples or the sample size significance. we looked at mean variance, other such moments for a brief time, and then proceeded to discussions regarding importance of taking reasonable assumptions",6,15.868166,-3.6270182,13.630657,5.467515,"summarizing, summarize, summarization"
249,"we first compared about the estimators being the statistics of the population. later we used excel to perform linear reg with the help of the formula provided in prev and class and performing the linear reg with the help of xlminer toolpack. later we talked about the ei errors in the regression. commenting on the error whether it is random or uniform. usually when the model explains the data variance successfully the errors should be close to a random. how should we comment on randomly distributed errors of regression? it should follow a gaussian distribution. then we can say our model knows all the parameters which can be the reason for the variation of the data. there were several results statistics like r^2 anova which given when we used xlminer. so like r^2 shows how much variation of the data does the particular model explains. we later moved on the proof of r^2 which is derived from sst,ssr,sse. r^2 was ssr/sst. r^2 is also called coef of determination. but why square? for the case of slr, the coeff of determination is the sq(correlation between x&y). since correlation coeff=> r. we call coeff of determination as r^2 (but only for slr(simple linear regression)). what is correlation coef. : as x changes wrt to its mean. how y changes wrt its mean. positive and negative correlation coeff shows how y changes wrt x (positively or negatively.) higher the value of r^2 higher explanation the model gives on the data variance. later at last we touched upon the standard error.","we first compared about the estimators being the statistics of the population. later we used excel to perform linear reg with the help of the formula provided in prev and class and performing the linear reg with the help of xlminer toolpack. later we talked about the ei errors in the regression. commenting on the error whether it is random or uniform. usually when the model explains the data variance successfully the errors should be close to a random. how should we comment on randomly distributed errors of regression? it should follow a gaussian distribution. then we can say our model knows all the parameters which can be the reason for the variation of the data. there were several results statistics like r^2 anova which given when we used xlminer. so like r^2 shows how much variation of the data does the particular model explains. we later moved on the proof of r^2 which is derived from sst,ssr,sse. r^2 was ssr/sst. r^2 is also called coef of determination. but why square? for the case of slr, the coeff of determination is the sq(correlation between x&y). since correlation coeff=> r. we call coeff of determination as r^2 (but only for slr(simple linear regression)). what is correlation coef. : as x changes wrt to its mean. how y changes wrt its mean. positive and negative correlation coeff shows how y changes wrt x (positively or negatively.) higher the value of r^2 higher explanation the model gives on the data variance. later at last we touched upon the standard error.",5,23.060425,-0.4546408,13.877623,4.3078256,"regression, statistical, statistics"
250,"if sample space is limited we use time series analysis.all samples don't follow gaussian distribution some follow uniform distribution if size is small.for a large sample variables tend to follow gaussian distribution, then sir discussed regression analysis and then broke down the error as sst=ssr+sse and introduced r-squared value(râ²)= 1-sse/sst, also introduced correlation coefficient(r) and if it's high that means both variables are more closely related and vice-versa, and if -ve r value indicates that both variables are negatively dependent on each other.","if sample space is limited we use time series analysis.all samples don't follow gaussian distribution some follow uniform distribution if size is small.for a large sample variables tend to follow gaussian distribution, then sir discussed regression analysis and then broke down the error as sst=ssr+sse and introduced r-squared value(r )= 1-sse/sst, also introduced correlation coefficient(r) and if it's high that means both variables are more closely related and vice-versa, and if -ve r value indicates that both variables are negatively dependent on each other.",5,25.597048,-5.3171597,14.030457,4.4576073,"regression, statistical, statistics"
251,"in todays lecture, we started with the continuation of the previous lectures discussion on population and sample. population has its parameters and sample has statistics. we want to estimate the parameters based on the statistics. we then moved our discussion towards coming up with a model from a dataset. depending upon the type of scatter plot of the data, we might want to fit a different model. suppose there is a dataset where all points seem to be more or less unaffected by the parameters and more like scattered in a particular circular region, we might want to find its point estimate. if the 2 parameters seem to be linearly varying with each other, as in the example of marketing expense vs revenue, we might want to fit a line to it. now the question arises - how do we fit the best line? for that, we use simple linear regression. suppose our lines predicts a y hat for a given x and the real value corresponding to that x is y, we define error e = y hat - y. we want our line to be such that this error is minimum. we do so by minimising the sum of square of this errors. we finally arrived at direct formulas for the slope and intercept of the equation.","in todays lecture, we started with the continuation of the previous lectures discussion on population and sample. population has its parameters and sample has statistics. we want to estimate the parameters based on the statistics. we then moved our discussion towards coming up with a model from a dataset. depending upon the type of scatter plot of the data, we might want to fit a different model. suppose there is a dataset where all points seem to be more or less unaffected by the parameters and more like scattered in a particular circular region, we might want to find its point estimate. if the 2 parameters seem to be linearly varying with each other, as in the example of marketing expense vs revenue, we might want to fit a line to it. now the question arises - how do we fit the best line? for that, we use simple linear regression. suppose our lines predicts a y hat for a given x and the real value corresponding to that x is y, we define error e = y hat - y. we want our line to be such that this error is minimum. we do so by minimising the sum of square of this errors. we finally arrived at direct formulas for the slope and intercept of the equation.",1,28.009722,-10.191167,16.0697,4.0773625,"population, models, estimating"
252,"when working with data...we almost never have access to the entire populationâ€”just a sample, which we call a dataset, but when training a model, we donâ€™t use the whole dataset....instead, we randomly split it...about 80% for training and 20% for testing.....the test set helps us compare different models and see how well they perform on new data.

multiple r (the square root of r^2) tells us how strongly the independent and dependent variables are related.....in linear regression, we can have multiple input variables (features) and even multiple outputs, which means we often solve the problem using matrices.

when analyzing variance, we want to know how much each independent variable contributes. the concept of degrees of freedom comes into play hereâ€”since we use the sample mean, we effectively lose one degree of freedom. because both ssr (sum of squares for regression) and sst (total sum of squares) involve the mean, we adjust for this in the formula for adjusted râ²:

adjusted r^2 = 1 - ((ssr/(n-k-1))/(sst/(n-1)))

quantile-quantile (qq) plot tells us how close distribution is to normal distribution, ideal plot is y=x.","when working with data...we almost never have access to the entire population just a sample, which we call a dataset, but when training a model, we don t use the whole dataset....instead, we randomly split it...about 80% for training and 20% for testing.....the test set helps us compare different models and see how well they perform on new data. multiple r (the square root of r^2) tells us how strongly the independent and dependent variables are related.....in linear regression, we can have multiple input variables (features) and even multiple outputs, which means we often solve the problem using matrices. when analyzing variance, we want to know how much each independent variable contributes. the concept of degrees of freedom comes into play here since we use the sample mean, we effectively lose one degree of freedom. because both ssr (sum of squares for regression) and sst (total sum of squares) involve the mean, we adjust for this in the formula for adjusted r : adjusted r^2 = 1 - ((ssr/(n-k-1))/(sst/(n-1))) quantile-quantile (qq) plot tells us how close distribution is to normal distribution, ideal plot is y=x.",2,14.015874,1.5537643,11.96766,4.714725,"regression, regressions, features"
253,"sir started the classes by discussing about confidence intervals and revisited â€˜area under the curveâ€™. then he talked about statistically significant and statistically similar values.
then he presented the session summary analysis and he was showing us the various methods how he analyses the summaries, one of them was â€˜common bag of words. he then told us that there were 9 suspicious submissions and 2 of them were almost alike.
then he talked a bit about the e1 submissions and then told us that those who are not comfortable with python should make efforts. for this he has uploaded a few things on moodle.
then he started talking about multiple linear regression. he said that before being able to process images or body of text through machine learning first of all, we need to convert them into a vector ïƒ  [x1, x2â€¦, xk].
for example, sales would be a function of the features such as age, earning, location, family size, etc.
then he talked about feature engineering. feature engineering means improving data by choosing or creating useful information to help a machine learning model work better.
then he said that features can be measured in hertz.","sir started the classes by discussing about confidence intervals and revisited area under the curve . then he talked about statistically significant and statistically similar values. then he presented the session summary analysis and he was showing us the various methods how he analyses the summaries, one of them was common bag of words. he then told us that there were 9 suspicious submissions and 2 of them were almost alike. then he talked a bit about the e1 submissions and then told us that those who are not comfortable with python should make efforts. for this he has uploaded a few things on moodle. then he started talking about multiple linear regression. he said that before being able to process images or body of text through machine learning first of all, we need to convert them into a vector [x1, x2 , xk]. for example, sales would be a function of the features such as age, earning, location, family size, etc. then he talked about feature engineering. feature engineering means improving data by choosing or creating useful information to help a machine learning model work better. then he said that features can be measured in hertz.",13,-0.34944445,5.7430773,9.459925,5.6926975,"classification, classifying, classifications"
254,"the key concept discussed in the class was ""the 4 levels of measurement in statistics"", which are:
1. nominal (example, gender)
2. ordinal (example, levels of education)
3. interval (example, temperature)
4. ratio (example: height, weight)

the problem of encoding, and a possible solution (one-hot-encoding)

the difference between the population and the data sample.
the problem faced in the early days regarding insufficient data, and how the situation has changed now.
discussed about the power required in train the state-of-the-art llms, and compared it with the monthly power consumption of a small town.

a brief introduction to statistical and ml models like slr (simple linear regression), logistic regression, and unsupervised learning including clustering algorithms like k-means.","the key concept discussed in the class was ""the 4 levels of measurement in statistics"", which are: 1. nominal (example, gender) 2. ordinal (example, levels of education) 3. interval (example, temperature) 4. ratio (example: height, weight) the problem of encoding, and a possible solution (one-hot-encoding) the difference between the population and the data sample. the problem faced in the early days regarding insufficient data, and how the situation has changed now. discussed about the power required in train the state-of-the-art llms, and compared it with the monthly power consumption of a small town. a brief introduction to statistical and ml models like slr (simple linear regression), logistic regression, and unsupervised learning including clustering algorithms like k-means.",4,-20.864134,-14.511214,2.0428267,0.52807677,"classification, classifying, classifications"
255,"in today's lecture, the discussion was on feature engineering and encoding. depending upon the type of measurement (nominal/ordinal/interval/ratio) the inputs has to concerted into appropriate numbers that machine can understand (i.e., there is a need for encoding). we learnt different types of encoding methods. 
one-hot encoding: this splits the main dependent variable y into more than one variable which are independent of each other. but this method invites the curse of dimensionality (as it explodes into many number of columns) and gets into trouble in case of multiclass problem & multilabel problem; and hence should be used only in case of nominal data with less number of columns. 
after that sir explained label encoding and integer encoding. in integer encoding when we want to categorise things based on integers, there is an inherit sense of ordering where as in case of label encoding, numbers only represents different categories, it does not mean any kind of ordering. 
then sir explained binary encoding: binary encoding also accounts for combination of variables like (1,1,0) etc. unlike one-hot encoding where such combination tend to create (multilabel) problems. frequency encoding and target encoding were also explained along with this methods.
at last, sir explained feature binning, when we have low value of râ² (all the variance is not possibly explained by the regression model), in such cases me form bins and create classification model on them,  thus even if the regression model fails to predict the values effectively,  classification model works well in this case with high accuracy, precision,  recall, and f1 scores. towards the end of the lecture, we had and short introduction about how to process text data. how we can store it (use a number for each word or use an array to store that number; how the meaning of word can change with respect to its surrounding, etc. and how yo process text considering all this things).","in today's lecture, the discussion was on feature engineering and encoding. depending upon the type of measurement (nominal/ordinal/interval/ratio) the inputs has to concerted into appropriate numbers that machine can understand (i.e., there is a need for encoding). we learnt different types of encoding methods. one-hot encoding: this splits the main dependent variable y into more than one variable which are independent of each other. but this method invites the curse of dimensionality (as it explodes into many number of columns) and gets into trouble in case of multiclass problem & multilabel problem; and hence should be used only in case of nominal data with less number of columns. after that sir explained label encoding and integer encoding. in integer encoding when we want to categorise things based on integers, there is an inherit sense of ordering where as in case of label encoding, numbers only represents different categories, it does not mean any kind of ordering. then sir explained binary encoding: binary encoding also accounts for combination of variables like (1,1,0) etc. unlike one-hot encoding where such combination tend to create (multilabel) problems. frequency encoding and target encoding were also explained along with this methods. at last, sir explained feature binning, when we have low value of r (all the variance is not possibly explained by the regression model), in such cases me form bins and create classification model on them, thus even if the regression model fails to predict the values effectively, classification model works well in this case with high accuracy, precision, recall, and f1 scores. towards the end of the lecture, we had and short introduction about how to process text data. how we can store it (use a number for each word or use an array to store that number; how the meaning of word can change with respect to its surrounding, etc. and how yo process text considering all this things).",3,-43.86404,3.8460102,0.13817824,6.453427,"categorical, categorization, categorise"
256,"in todayâ€™s lecture we saw some more methods to reduce the dimensionality of data, inorder to solve data problems associated with large dimensions. 
in the last class, we had talked about vif analysis. we continued the discussion on it. 
in vif analysis, we start eliminating the features one by one, with the ones having large vif values. we continue the process until we get a set of features which have vif values, smaller than the desired threshold, which can be 10/5. we eliminate the features based on our domain knowledge.
depending on the threshold set, we can have different combination and numbers of features selected for the regression model. in such a case, we should evaluate the metrics for each of the model and then by comparing these, we can decide which one to use.
the next method we saw was that of pca (principal component analysis). it helps to reduce the dimensionality of the problem by creating orthogonal principal components. each of the principal component is a combination of the original features, with the weights called as â€˜loadingsâ€™. the loadings tell us the importance of each feature. so, instead of using too many features, we are combining them into a principal component and using this to predict the response variable. the number of principal components is equal to that of the original dimensions. however, we can decide to take up two/three of these pcs suitable to our model. the first pc explains the maximum variance in the data, the next explains some amount of the remaining variance. this continues until all the principal components explain the complete variance in the data.
the cumulative variance explained by all the pcs is given by the elbow diagram, which is asymptotic to 1. the elbow diagram tells us how many pcs would be needed to explain a certain amount of variance in the data.
among the two methods, vif is generally preferred first because of interpretability. x1,x2,x3 etc.  represent the actual physical features, whereas the pcs, which are a combination of these are just mathematical parameters. 
so, pcs are most suitable for prediction analysis but they cannot be used for â€˜what-if analysisâ€™ or â€˜delta analysisâ€™. hence, through pcs we achieve the goal of reducing the dimensions, however we cannot do any sensitivity analysis.
so, normally vif is done first and if still there are many features left, we perform pca.
the advantages of using pca is that it helps in dimension reduction (data reduction), predictive analysis, visualization (understanding of the structure of the data). so, this is a part of eda and can be used to determine the number of clusters (k) in k-means clustering.
another disadvantage of pca is that it is sensitive to data scale. hence, normalization (if required) must be done before doing pca.
next, we talked about the t-sne plots. 
the t-sne plots also reduce the large number of dimensions in the data into 2-3 dimensions which can be easily visualized. this is done on the basis of the distance between the points in the n-dimensional space. through this method, we lose the exactness of the data, but instead we get the information about the relative closeness of the points. 
t-sne creates a probability distribution for each of the point, which contains the probability of closeness of that point from every other point. 
gaussian normal distribution is used in higher dimensional space, but in t-sne, as the name suggests, t-distribution is used. t-distribution increases the distance between dissimilar points. so, in n-dimensional space, if the probability of two points being close is very high, they are clustered together in the t-sne plots.
all these methods help in reducing the number of dimensions in the data set.","in today s lecture we saw some more methods to reduce the dimensionality of data, inorder to solve data problems associated with large dimensions. in the last class, we had talked about vif analysis. we continued the discussion on it. in vif analysis, we start eliminating the features one by one, with the ones having large vif values. we continue the process until we get a set of features which have vif values, smaller than the desired threshold, which can be 10/5. we eliminate the features based on our domain knowledge. depending on the threshold set, we can have different combination and numbers of features selected for the regression model. in such a case, we should evaluate the metrics for each of the model and then by comparing these, we can decide which one to use. the next method we saw was that of pca (principal component analysis). it helps to reduce the dimensionality of the problem by creating orthogonal principal components. each of the principal component is a combination of the original features, with the weights called as loadings . the loadings tell us the importance of each feature. so, instead of using too many features, we are combining them into a principal component and using this to predict the response variable. the number of principal components is equal to that of the original dimensions. however, we can decide to take up two/three of these pcs suitable to our model. the first pc explains the maximum variance in the data, the next explains some amount of the remaining variance. this continues until all the principal components explain the complete variance in the data. the cumulative variance explained by all the pcs is given by the elbow diagram, which is asymptotic to 1. the elbow diagram tells us how many pcs would be needed to explain a certain amount of variance in the data. among the two methods, vif is generally preferred first because of interpretability. x1,x2,x3 etc. represent the actual physical features, whereas the pcs, which are a combination of these are just mathematical parameters. so, pcs are most suitable for prediction analysis but they cannot be used for what-if analysis or delta analysis . hence, through pcs we achieve the goal of reducing the dimensions, however we cannot do any sensitivity analysis. so, normally vif is done first and if still there are many features left, we perform pca. the advantages of using pca is that it helps in dimension reduction (data reduction), predictive analysis, visualization (understanding of the structure of the data). so, this is a part of eda and can be used to determine the number of clusters (k) in k-means clustering. another disadvantage of pca is that it is sensitive to data scale. hence, normalization (if required) must be done before doing pca. next, we talked about the t-sne plots. the t-sne plots also reduce the large number of dimensions in the data into 2-3 dimensions which can be easily visualized. this is done on the basis of the distance between the points in the n-dimensional space. through this method, we lose the exactness of the data, but instead we get the information about the relative closeness of the points. t-sne creates a probability distribution for each of the point, which contains the probability of closeness of that point from every other point. gaussian normal distribution is used in higher dimensional space, but in t-sne, as the name suggests, t-distribution is used. t-distribution increases the distance between dissimilar points. so, in n-dimensional space, if the probability of two points being close is very high, they are clustered together in the t-sne plots. all these methods help in reducing the number of dimensions in the data set.",11,-18.734747,2.4109197,10.448361,13.405713,"pca, heatmap, heatmaps"
257,"sir started the class by taking in questions submitted via tha form futher logistic units were discussed. starting with conditional probability as it is the outcome of a logistic unit which is in turn linked with a class. probability of the predicted outcomes and the known outcomes which are know from the dataset are compared further sir linked the probability of y given x with a binomal distribution. here we are interested in maximizing probability of the predicted if the label predicted matches that from the dataset for which we define likelyhood which is the product of probabilities. a log likelyhood is considered because dealing with product is difficult for optimization and as minima are more stable and easier to compute so now we consider negative of the log likelyhood which is to be minimized with respect to the weights. the optimisation is carried out by gradient descent. also a case were the boundary may be non linear was considered where we were encouraged to use polynomials and other non linear functions with emphasis on preventing overfitting while maintaining generalization which is done by understanding the nature of the population. also it was explained how every regression method can be used for classification. futher confusion matrix were introduced here the difference between false negatives and false positives was discussed. here the concept of data imbalance was discussed and its interpretation with accuracy, precision which is defined as how many of the event you have detected how many are correct and recall which is defined as how many of the class events how many of those have you detected. class ended with a discussion on submissions in exercise 1","sir started the class by taking in questions submitted via tha form futher logistic units were discussed. starting with conditional probability as it is the outcome of a logistic unit which is in turn linked with a class. probability of the predicted outcomes and the known outcomes which are know from the dataset are compared further sir linked the probability of y given x with a binomal distribution. here we are interested in maximizing probability of the predicted if the label predicted matches that from the dataset for which we define likelyhood which is the product of probabilities. a log likelyhood is considered because dealing with product is difficult for optimization and as minima are more stable and easier to compute so now we consider negative of the log likelyhood which is to be minimized with respect to the weights. the optimisation is carried out by gradient descent. also a case were the boundary may be non linear was considered where we were encouraged to use polynomials and other non linear functions with emphasis on preventing overfitting while maintaining generalization which is done by understanding the nature of the population. also it was explained how every regression method can be used for classification. futher confusion matrix were introduced here the difference between false negatives and false positives was discussed. here the concept of data imbalance was discussed and its interpretation with accuracy, precision which is defined as how many of the event you have detected how many are correct and recall which is defined as how many of the class events how many of those have you detected. class ended with a discussion on submissions in exercise 1",10,10.814367,-19.08158,8.984921,-1.50913,"classifications, histograms, histogram"
258,"today's lecture was very interactive since we planned next ten lectures and briefly discussed group project. we started by diving into function encoding, which we demonstrated using an example. here, we used to transform one function with three color variables (red, blue, green) into one single function with variables y1, y2, and y3. we became familiar with two major types of problems: multiclass and multilabel with different solutions each.

we studied more about binary encoding, which is a very space-efficient way of representing data. how to convert data through this process was discussed in depth so that it would be understood how to get to the answer. we also studied frequency encoding, in which values of categories are substituted by how often they occur in the dataset. target encoding was also mentioned, where every value in a column is mapped to the average score computed for a given condition, like scores greater than 2.5.

we also learned about some other encoding methods such as label encoding, one-hot encoding, and image encoding. we had a hands-on example of how to do feature binning through an example involving a random scatter of data. and lastly, we briefly discussed test data processing, wrapping up a thorough session with a series of key topics that deal with data encoding and processing.","today's lecture was very interactive since we planned next ten lectures and briefly discussed group project. we started by diving into function encoding, which we demonstrated using an example. here, we used to transform one function with three color variables (red, blue, green) into one single function with variables y1, y2, and y3. we became familiar with two major types of problems: multiclass and multilabel with different solutions each. we studied more about binary encoding, which is a very space-efficient way of representing data. how to convert data through this process was discussed in depth so that it would be understood how to get to the answer. we also studied frequency encoding, in which values of categories are substituted by how often they occur in the dataset. target encoding was also mentioned, where every value in a column is mapped to the average score computed for a given condition, like scores greater than 2.5. we also learned about some other encoding methods such as label encoding, one-hot encoding, and image encoding. we had a hands-on example of how to do feature binning through an example involving a random scatter of data. and lastly, we briefly discussed test data processing, wrapping up a thorough session with a series of key topics that deal with data encoding and processing.",3,-40.874504,5.9586005,0.39989057,6.0754547,"categorical, categorization, categorise"
259,"in class, we discussed pivot tables in excel, including their features such as calculating minimum, maximum, and average values. we then explored various plots, including box plots and histograms, to identify outliers visually. this led to a discussion on exploratory data analysis , illustrated through various examples, including one involving a chemical factory. lastly, the tas reviewed common mistakes from assignment 2 and provided feedback, emphasizing the importance of presenting the report more effectively.","in class, we discussed pivot tables in excel, including their features such as calculating minimum, maximum, and average values. we then explored various plots, including box plots and histograms, to identify outliers visually. this led to a discussion on exploratory data analysis , illustrated through various examples, including one involving a chemical factory. lastly, the tas reviewed common mistakes from assignment 2 and provided feedback, emphasizing the importance of presenting the report more effectively.",6,-9.29544,27.318491,7.602747,10.084745,"summarizing, summarize, summarization"
260,"1. major focus of todayâ€™s lecture was on sampling distribution, how from a sample of population, one can estimate sampling distribution.
2. the mean of distribution being the expected value of the sample mean and the variance of distribution being sigma/sqrt(n) 
3. once estimated one can then analyse the data and can find the confidence interval which is the percentage of area i.e. 95% interval holds area of 0.95
4. when number of observations is less than 30, t distribution holds with its own defined formula and t statistics is applied
5. then we move to concept of p value, that is the possibility that a sample is being selected from a different distribution altogether
6. we then analysed the chances where linear regression is not possible that is when beta 1 is statistically 0
7. same for multiple linear regression, any one is non zero simultaneously.","1. major focus of today s lecture was on sampling distribution, how from a sample of population, one can estimate sampling distribution. 2. the mean of distribution being the expected value of the sample mean and the variance of distribution being sigma/sqrt(n) 3. once estimated one can then analyse the data and can find the confidence interval which is the percentage of area i.e. 95% interval holds area of 0.95 4. when number of observations is less than 30, t distribution holds with its own defined formula and t statistics is applied 5. then we move to concept of p value, that is the possibility that a sample is being selected from a different distribution altogether 6. we then analysed the chances where linear regression is not possible that is when beta 1 is statistically 0 7. same for multiple linear regression, any one is non zero simultaneously.",7,39.017464,1.8020055,15.397673,2.4521177,"statistics, statistical, statisticsâ"
261,"today we discussed about simple linear regression mainly but we also covered population and sample. usually to predict something about the population we use sample data but how close is this prediction to the actual value depends upon how good the method used for collecting data was. and discussing upon this we shifted towards the visualization of the data and then deciding which model is better to predict the unseen data. for a linearly distributed points we can use point model, or some circular model and it will give some results but they will not be close to the actual value therefore it is necessary that we visualize and then decide which model. as sir mentioned math is blind it will throw out numbers for the data points when applied but it's us who are responsible to choose the right model. we plotted a example graph which was linearly distributed and thus thought to use simple linear regression. from here we saw math behind linear regression and what mean ,median, mode and frequency actually mean for the data and how important it is .","today we discussed about simple linear regression mainly but we also covered population and sample. usually to predict something about the population we use sample data but how close is this prediction to the actual value depends upon how good the method used for collecting data was. and discussing upon this we shifted towards the visualization of the data and then deciding which model is better to predict the unseen data. for a linearly distributed points we can use point model, or some circular model and it will give some results but they will not be close to the actual value therefore it is necessary that we visualize and then decide which model. as sir mentioned math is blind it will throw out numbers for the data points when applied but it's us who are responsible to choose the right model. we plotted a example graph which was linearly distributed and thus thought to use simple linear regression. from here we saw math behind linear regression and what mean ,median, mode and frequency actually mean for the data and how important it is .",2,9.149056,-1.5672952,10.532889,4.4327645,"regression, regressions, features"
262,"at the start of class, we derived the closed form solution for mlr and showed that they are possible only in theory and not in practice. later, sir showed his data analysis on previous class's summary. this time the heatmap was more red than the previous one and only two similar submissions. sir showed us the two similar submission and we notice how even the text were written differently, ai tools could figure out that they are similar in meaning and understanding. 
we moved on to the topic of sample. we learned that if we have a sample, we should not entirely use it to train a model. we should always split the sample in two parts: one for training the model and the other for testing the model. this splitting, however, depends on the quantity of available data and the model. 
we noticed that even if we separate the sample in two parts, it is possible that the testing data may not give predictions with similar accuracy like that of the training data. this depends on the r-square values for the training and testing data. if the r-square values are nearby, only then will we get desired results. we also studied that r-square value for the training data is not desired to be one, because it will not be compatible to the testing data otherwise. 
we understood the terminology for r-square, adjusted r-square and multiple r. we how using (n-1) in calculated adjusted r-square would not be a biased estimator. 
in mlr model, the outcome of the linear regression need not to be linear. it would be the linear combination of independent variables. 
we moved on to the python-based approach for data analysis. we used two new modules: scklearn and statsmodel for analysing data. scklearn do not give parametric values liek r-square, p-value for its model. these values can be obtained in statsmodel. 
we also learned how the previous observation that 'p-value less than 0.5 is desired' can lead us in trouble during hypothesis testing.","at the start of class, we derived the closed form solution for mlr and showed that they are possible only in theory and not in practice. later, sir showed his data analysis on previous class's summary. this time the heatmap was more red than the previous one and only two similar submissions. sir showed us the two similar submission and we notice how even the text were written differently, ai tools could figure out that they are similar in meaning and understanding. we moved on to the topic of sample. we learned that if we have a sample, we should not entirely use it to train a model. we should always split the sample in two parts: one for training the model and the other for testing the model. this splitting, however, depends on the quantity of available data and the model. we noticed that even if we separate the sample in two parts, it is possible that the testing data may not give predictions with similar accuracy like that of the training data. this depends on the r-square values for the training and testing data. if the r-square values are nearby, only then will we get desired results. we also studied that r-square value for the training data is not desired to be one, because it will not be compatible to the testing data otherwise. we understood the terminology for r-square, adjusted r-square and multiple r. we how using (n-1) in calculated adjusted r-square would not be a biased estimator. in mlr model, the outcome of the linear regression need not to be linear. it would be the linear combination of independent variables. we moved on to the python-based approach for data analysis. we used two new modules: scklearn and statsmodel for analysing data. scklearn do not give parametric values liek r-square, p-value for its model. these values can be obtained in statsmodel. we also learned how the previous observation that 'p-value less than 0.5 is desired' can lead us in trouble during hypothesis testing.",2,10.113903,3.7539463,11.037963,4.921545,"regression, regressions, features"
263,"we started the lecture by discussing a few questions from the previous class. sir showed us how we can find the standard error of the sample means just by using a single sample and then we discussed a few questions related to frequency distribution charts. the number of bins that need to be considered for correctly interpreting the â€˜goodnessâ€™ of the model depends on how we want to view the distribution. if we want a rough idea then we can use lesser number of bins. also we can use various formulae to find out the number of bins, depending on the size of the data. 
we then discussed some problems with fitting two different models to the same data. if we use two different models then it becomes difficult to find out which model we should use at the boundary. also, there maybe some discontinuity at the boundary and it may be difficult, in some cases to clearly define the boundary.
next we started with the topic of classification. we continued with the logistic regression model and studied about the sigmoid function. the sigmoid function returns a probability value, p which is the probability that the point belongs to a certain class. if the point is far away from the inflection point then it is clear which class it will belong to. but if it is close to the inflection point, then we can find the probabilities and depending on the probability values we classify the objects in different classes.
if p represents the probability of the object belonging to class 1 then if p > 0.5, we can say that the object belongs to class 1 otherwise it belongs to class 0.
our goal is to find the weights such that the difference between the predicted values and the true values is minimized.
if t is the actual value and p is the probability of t=1 then if t is actually 1 then we want p to be as close to 1 as possible. on the other hand, if t=0 then we want p to be as close to 0 as possible.
rather than maximizing any function we want to minimise it because maxima is unstable.
after fitting a model to the data, we can use a confusion matrix to assess the quality of the model.
we then discussed a few metrics related to classification. accuracy is one of the metrics that we discussed.
precision is yet another metric which is defined as the ratio of number of events correctly detected to the total number of events detected.
however accuracy can give us â€˜ false hopesâ€™. in situations wherein we have a large number of data points belonging to one class and a very few from the other then accuracy can be very high in such a situation because it is biased towards the data set with large number of points.
recall is defined as- how many events of a particular class you were able to detect correctly.
f value is the harmonic mean of recall and precision and it is better than accuracy and it doesn't give false hopes like accuracy. so it is a better metric to judge to goodness of the model.","we started the lecture by discussing a few questions from the previous class. sir showed us how we can find the standard error of the sample means just by using a single sample and then we discussed a few questions related to frequency distribution charts. the number of bins that need to be considered for correctly interpreting the goodness of the model depends on how we want to view the distribution. if we want a rough idea then we can use lesser number of bins. also we can use various formulae to find out the number of bins, depending on the size of the data. we then discussed some problems with fitting two different models to the same data. if we use two different models then it becomes difficult to find out which model we should use at the boundary. also, there maybe some discontinuity at the boundary and it may be difficult, in some cases to clearly define the boundary. next we started with the topic of classification. we continued with the logistic regression model and studied about the sigmoid function. the sigmoid function returns a probability value, p which is the probability that the point belongs to a certain class. if the point is far away from the inflection point then it is clear which class it will belong to. but if it is close to the inflection point, then we can find the probabilities and depending on the probability values we classify the objects in different classes. if p represents the probability of the object belonging to class 1 then if p > 0.5, we can say that the object belongs to class 1 otherwise it belongs to class 0. our goal is to find the weights such that the difference between the predicted values and the true values is minimized. if t is the actual value and p is the probability of t=1 then if t is actually 1 then we want p to be as close to 1 as possible. on the other hand, if t=0 then we want p to be as close to 0 as possible. rather than maximizing any function we want to minimise it because maxima is unstable. after fitting a model to the data, we can use a confusion matrix to assess the quality of the model. we then discussed a few metrics related to classification. accuracy is one of the metrics that we discussed. precision is yet another metric which is defined as the ratio of number of events correctly detected to the total number of events detected. however accuracy can give us false hopes . in situations wherein we have a large number of data points belonging to one class and a very few from the other then accuracy can be very high in such a situation because it is biased towards the data set with large number of points. recall is defined as- how many events of a particular class you were able to detect correctly. f value is the harmonic mean of recall and precision and it is better than accuracy and it doesn't give false hopes like accuracy. so it is a better metric to judge to goodness of the model.",10,12.359067,-17.70797,9.029728,-1.3645284,"classifications, histograms, histogram"
264,"during this session, we explored key concepts in model evaluation and clustering techniques. we began with understanding the roc curve, a graphical tool for assessing classifier performance by plotting the true positive rate (tpr) against the false positive rate (fpr). tpr (or sensitivity/recall) is defined as the ratio of correctly classified positives to the total actual positives, while fpr is the ratio of negatives incorrectly classified as positive to the total actual negatives. by varying the decision threshold, different tpr and fpr values are obtained, which form the roc curve. a perfect classifier achieves an immediate vertical jump to (0,1) with an auc of 1.0, whereas a poor modelâ€™s roc curve follows the diagonal with an auc near 0.5.
additionally, the auc, which ranges from 0 to 1, summarizes model performance and assists in comparing models. however, limitations exist, especially with imbalanced datasets and the need for selecting a proper decision threshold.

we got to know about a tool ('neural network playground') , where we can vary the parameters and visualize the classification and regression with neural networks.

we also discussed converting regression problems into classification tasks by discretizing the continuous target variable. this method simplifies the problem by grouping continuous values into categories but involves trade-offs, including loss of detail. domain knowledge is essential for choosing proper bin thresholds.

then shifted focus to clustering methods. k-means clustering organizes data into groups based on similarity by assigning points to centroids and iteratively updating these centroids. hierarchical clustering, in contrast, builds a dendrogramâ€”a tree-like diagram that shows how clusters merge over various dissimilarity levels. the vertical height in the dendrogram represents the distance at which clusters merge. in particular, complete linkage defines cluster distance by the maximum distance between any pair of points, yielding compact, well-separated clusters.","during this session, we explored key concepts in model evaluation and clustering techniques. we began with understanding the roc curve, a graphical tool for assessing classifier performance by plotting the true positive rate (tpr) against the false positive rate (fpr). tpr (or sensitivity/recall) is defined as the ratio of correctly classified positives to the total actual positives, while fpr is the ratio of negatives incorrectly classified as positive to the total actual negatives. by varying the decision threshold, different tpr and fpr values are obtained, which form the roc curve. a perfect classifier achieves an immediate vertical jump to (0,1) with an auc of 1.0, whereas a poor model s roc curve follows the diagonal with an auc near 0.5. additionally, the auc, which ranges from 0 to 1, summarizes model performance and assists in comparing models. however, limitations exist, especially with imbalanced datasets and the need for selecting a proper decision threshold. we got to know about a tool ('neural network playground') , where we can vary the parameters and visualize the classification and regression with neural networks. we also discussed converting regression problems into classification tasks by discretizing the continuous target variable. this method simplifies the problem by grouping continuous values into categories but involves trade-offs, including loss of detail. domain knowledge is essential for choosing proper bin thresholds. then shifted focus to clustering methods. k-means clustering organizes data into groups based on similarity by assigning points to centroids and iteratively updating these centroids. hierarchical clustering, in contrast, builds a dendrogram a tree-like diagram that shows how clusters merge over various dissimilarity levels. the vertical height in the dendrogram represents the distance at which clusters merge. in particular, complete linkage defines cluster distance by the maximum distance between any pair of points, yielding compact, well-separated clusters.",8,-4.523636,-19.38118,6.45461,0.61035997,"classification, clusterings, classifying"
265,"explained different levels of measurement like ordinary , nominal , interval and ratios .also explained different models of machine learning for regression and classification.gave a introduction on the difference in supervised and unsupervised learning like when labels are known , it is supervised when they are unknown it is unsupervised.","explained different levels of measurement like ordinary , nominal , interval and ratios .also explained different models of machine learning for regression and classification.gave a introduction on the difference in supervised and unsupervised learning like when labels are known , it is supervised when they are unknown it is unsupervised.",4,-17.707863,-13.10627,2.2578695,0.97048956,"classification, classifying, classifications"
266,"todayâ€™s session mainly based on exploratory data analysis. it involves exploring the data to identify patterns, trends, outliers and other features that may be unexpected. it often utilises summary statistics such as mean, median, mode and skewness to describe the central tendency and distribution of the data. we saw 3 examples of eda which is performed on different domains or industries data. the first one is a case of chemical plant in which we have 241 columns. the second one is case of transformer device which is also a case of imbalanced dataset. the third one is summary dataset. through summary datasets we learnt about the â€œpivot tableâ€ in excel which helps in summarising and analysing data in easy and simple manner without writing any code. it also helps in summarising large sets of data. quality of model depends on exploratory data analysis. by performing eda we gain a better understanding of the data and address these data problems before proceeding with further analysis or modelling.","today s session mainly based on exploratory data analysis. it involves exploring the data to identify patterns, trends, outliers and other features that may be unexpected. it often utilises summary statistics such as mean, median, mode and skewness to describe the central tendency and distribution of the data. we saw 3 examples of eda which is performed on different domains or industries data. the first one is a case of chemical plant in which we have 241 columns. the second one is case of transformer device which is also a case of imbalanced dataset. the third one is summary dataset. through summary datasets we learnt about the pivot table in excel which helps in summarising and analysing data in easy and simple manner without writing any code. it also helps in summarising large sets of data. quality of model depends on exploratory data analysis. by performing eda we gain a better understanding of the data and address these data problems before proceeding with further analysis or modelling.",9,-15.497613,21.825178,8.138557,9.173558,"dataâ, analyse, analyses"
267,"in todayâ€™s class, we explored several important statistical concepts. we started by learning how to estimate the population mean using sample means, both when dealing with a single sample and multiple samples. we then moved on to confidence intervals, which are used to estimate a population mean within a specific range and determine the probability that a particular value of the population mean lies within that interval.
we also discussed the standard error of the mean, which measures the variability of sample means and is calculated as:
standardâ errorâ (se)=s/âˆšn
here, s is the population standard deviation, and n is the sample size. we observed that when we take multiple samples, the means of these samples form a normal distribution, with their mean serving as the best estimate for the population mean. probabilities in these contexts are often expressed in terms of the p-value, which indicates the likelihood of observing a result as extreme as the one we obtained, assuming the null hypothesis is true.
then we studied the anova (analysis of variance) table, which is used to assess whether there are statistically significant differences among the means of three or more groups. this is a key tool for comparing group means in experimental data.

in the next lecture, weâ€™ll delve into multiple regression models, where weâ€™ll learn how to handle situations involving more than one independent variable.","in today s class, we explored several important statistical concepts. we started by learning how to estimate the population mean using sample means, both when dealing with a single sample and multiple samples. we then moved on to confidence intervals, which are used to estimate a population mean within a specific range and determine the probability that a particular value of the population mean lies within that interval. we also discussed the standard error of the mean, which measures the variability of sample means and is calculated as: standard error (se)=s/ n here, s is the population standard deviation, and n is the sample size. we observed that when we take multiple samples, the means of these samples form a normal distribution, with their mean serving as the best estimate for the population mean. probabilities in these contexts are often expressed in terms of the p-value, which indicates the likelihood of observing a result as extreme as the one we obtained, assuming the null hypothesis is true. then we studied the anova (analysis of variance) table, which is used to assess whether there are statistically significant differences among the means of three or more groups. this is a key tool for comparing group means in experimental data. in the next lecture, we ll delve into multiple regression models, where we ll learn how to handle situations involving more than one independent variable.",7,34.71373,4.7752337,14.9156685,2.4958212,"statistics, statistical, statisticsâ"
268,"today we learnt about four  scales nominal, ordinal,interval ratio discussed examples to understand each on of them in detail. and also learnt that for discrete scales like nominal and ordinal we have to use classification ml models and for continuous scales we have to use regression scales. sometimes label and features both are given in that case we can use supervised learning models when only features are given we can use unsupervised learning methods like means and hierarchical clustering.","today we learnt about four scales nominal, ordinal,interval ratio discussed examples to understand each on of them in detail. and also learnt that for discrete scales like nominal and ordinal we have to use classification ml models and for continuous scales we have to use regression scales. sometimes label and features both are given in that case we can use supervised learning models when only features are given we can use unsupervised learning methods like means and hierarchical clustering.",4,-18.135368,-11.552119,2.1636443,1.0054928,"classification, classifying, classifications"
269,"we discussed doubts submitted in the google form, like how is the standard deviation of the sample  related with the variance and the number of data points in our sample (n) which came out to be s=sigma/root n
we moved to our theory and looked at logistic regression, how multiple variables can affect the result of our logistic regression, and how to set the weights of these multiple parameters in the training data set, so that the training data set best represents the actual sample, without much overfitting. 
later we looked at the quality metrics which can be used to assess our results of our model. here we looked at the confusion matrix and the four types of entries which can be made which are false positive, false negative, true negative and true positive. we then looked at the 
different calculations which can be made on these entry values to gauge and arrive at a certain conclusion, like accuracy, and so on
at the end, the tas of our course had come and they discussed the exercise 1, what was missing in the spreadsheet and report, and how it should be organised and presented well. one important thing was that read the documentation of the spreadsheet functions and how they actually act, for example the kurt (kurtosis) function.","we discussed doubts submitted in the google form, like how is the standard deviation of the sample related with the variance and the number of data points in our sample (n) which came out to be s=sigma/root n we moved to our theory and looked at logistic regression, how multiple variables can affect the result of our logistic regression, and how to set the weights of these multiple parameters in the training data set, so that the training data set best represents the actual sample, without much overfitting. later we looked at the quality metrics which can be used to assess our results of our model. here we looked at the confusion matrix and the four types of entries which can be made which are false positive, false negative, true negative and true positive. we then looked at the different calculations which can be made on these entry values to gauge and arrive at a certain conclusion, like accuracy, and so on at the end, the tas of our course had come and they discussed the exercise 1, what was missing in the spreadsheet and report, and how it should be organised and presented well. one important thing was that read the documentation of the spreadsheet functions and how they actually act, for example the kurt (kurtosis) function.",13,5.449015,-11.659348,9.783034,4.9945455,"classification, classifying, classifications"
270,"in todays class (7/3/25)
we started the lecture with discussion on vif and understood the justification about the threshold values and its use on identifying the independent features.
next, we started principal component analysis (pca), which helps to reduce the dimensions of data to help get the understanding the maximum variance, where pc1 gives max variance, pc2 has next maximum and moving on which we understood with an example 2-d linear dataset. reduced to 1d aligning the axis to the line. each principal component is the linear combination of the original features which are known as loaders. vif should be conducted first to eliminate multi-collinearity and pca since affects interpretability as features are the combination of multiple features. 
next we used t-sne helping to visualize data in 2d, based on t-distribution using gradient descent algorithm making it slow and inconsistent. it calculates probability distribution for all points which are the nearest neighbors for each and every point. it can be very sueful tool for classification and clustering","in todays class (7/3/25) we started the lecture with discussion on vif and understood the justification about the threshold values and its use on identifying the independent features. next, we started principal component analysis (pca), which helps to reduce the dimensions of data to help get the understanding the maximum variance, where pc1 gives max variance, pc2 has next maximum and moving on which we understood with an example 2-d linear dataset. reduced to 1d aligning the axis to the line. each principal component is the linear combination of the original features which are known as loaders. vif should be conducted first to eliminate multi-collinearity and pca since affects interpretability as features are the combination of multiple features. next we used t-sne helping to visualize data in 2d, based on t-distribution using gradient descent algorithm making it slow and inconsistent. it calculates probability distribution for all points which are the nearest neighbors for each and every point. it can be very sueful tool for classification and clustering",11,-18.58752,1.1061708,10.357467,13.2729,"pca, heatmap, heatmaps"
271,"in order to improve results, improve samples by improving quality of samples or by increasing the quantity. we can also improve method by using multiple methods and then selecting best based on your knowledge of metrics used. grid search is one of the ways to select best methods where we use n dimensional grid with all possible combinations of parameters. 
in polynomial regression we use powers of features based on domain knowledge and data exploration. we can use forward or backward feature selection methods. in forward we start with empty set of features and we keep adding features to improve the results. in backward process, we start with all features and reduce them to reach better results. 
neural networks is used in ml. it has hidden layers and weights connecting the neurons.","in order to improve results, improve samples by improving quality of samples or by increasing the quantity. we can also improve method by using multiple methods and then selecting best based on your knowledge of metrics used. grid search is one of the ways to select best methods where we use n dimensional grid with all possible combinations of parameters. in polynomial regression we use powers of features based on domain knowledge and data exploration. we can use forward or backward feature selection methods. in forward we start with empty set of features and we keep adding features to improve the results. in backward process, we start with all features and reduce them to reach better results. neural networks is used in ml. it has hidden layers and weights connecting the neurons.",0,1.5017594,-6.4295106,9.515712,4.0206914,"models, feature, features"
272,"we began by discussing how to improve the quality of a modelâ€™s results. there are three key approaches:  
1. improving the sample â€“ this can be done by either increasing the sample size or enhancing the quality of the sample.  
2. improving the method â€“ using multiple methods and selecting the best one.  
3. fine-tuning the method â€“ properly adjusting and optimizing the selected method.  

we then discussed polynomial regression followed by feature engineering, which consists of:  
- forward selection â€“ adding features one by one.  
- backward selection â€“ starting with all features and removing the ones that arenâ€™t needed.  

after that, we outlined the modeling workflow:  
1. collect data  
2. process data  
3. obtain clean data  
4. apply various models and compare metrics  
5. select the best model  

we also emphasized that if a single model can predict data effectively, it is preferable to using multiple models. we then experimented with different models like linear regression, svm regression, and random forest. the random forest model gave the best results, but we noted that it is a non-parametric model, making it less suitable for tasks like delta analysis (which requires parametric models like linear regression).  

we then introduced neural networks, explaining that they are parametric models as they involve weights. a neural network consists of:  
- input layer  
- hidden layers (where transformations occur: y = f(wixi))  
- output layer  

as the number of hidden layers increases, it transitions into deep learning. we also examined model evaluation using râ² and mse:  
- if training and test râ²/mse are similar, the model is well-fitted.  
- if training error is significantly lower than test error, the model is overfitting.  

lastly, we started with classification, which is used for nominal or ordinal data. in supervised learning, both x (input) and y (output) are available, while regression aims to pull data points back to their mean values. we then covered logistic regression, which is used for classification tasks. it helps define decision boundaries between different classes using the sigmoid function.","we began by discussing how to improve the quality of a model s results. there are three key approaches: 1. improving the sample this can be done by either increasing the sample size or enhancing the quality of the sample. 2. improving the method using multiple methods and selecting the best one. 3. fine-tuning the method properly adjusting and optimizing the selected method. we then discussed polynomial regression followed by feature engineering, which consists of: - forward selection adding features one by one. - backward selection starting with all features and removing the ones that aren t needed. after that, we outlined the modeling workflow: 1. collect data 2. process data 3. obtain clean data 4. apply various models and compare metrics 5. select the best model we also emphasized that if a single model can predict data effectively, it is preferable to using multiple models. we then experimented with different models like linear regression, svm regression, and random forest. the random forest model gave the best results, but we noted that it is a non-parametric model, making it less suitable for tasks like delta analysis (which requires parametric models like linear regression). we then introduced neural networks, explaining that they are parametric models as they involve weights. a neural network consists of: - input layer - hidden layers (where transformations occur: y = f(wixi)) - output layer as the number of hidden layers increases, it transitions into deep learning. we also examined model evaluation using r and mse: - if training and test r /mse are similar, the model is well-fitted. - if training error is significantly lower than test error, the model is overfitting. lastly, we started with classification, which is used for nominal or ordinal data. in supervised learning, both x (input) and y (output) are available, while regression aims to pull data points back to their mean values. we then covered logistic regression, which is used for classification tasks. it helps define decision boundaries between different classes using the sigmoid function.",0,-0.076411456,-4.5168433,9.351179,4.2319803,"models, feature, features"
273,"the class started off by discussing the midsemester examination. we realised that ai tools do provide the right guidance, but if asked for complete solutions, they do suggest something completely off. we discussed that the test data for the prediction model had a completely different distribution as compared to the training data. we also had very few samples for one class which made our model unreliable for predicting that class. we concluded that the understanding of the data is very important for us to make efficient use of ai tools. we then moved forward and discussed about the class performance in assignment e3, and filled a survey about the midsem exam experience. 
till now, we had discussed problems within the same column. now we move on to problems across columns. we started discussion about a concept known as the curse of dimensionality. it describes the problems which occurs when the number of features in the dataset increases significantly. in that case, we get increased sparcity, increased complexity, increased computational cost and distance distortions. if data becomes sparse in an n dimensional space, the distance between the points increases a lot, and hence pattern detection in daya becomes difficult. to deal with this issue, we should try to decrease the dimensionality, or maybe we should select the most relevant features or use regularisation. we discussed about variance inflation factor, where a feature x_i is tried to be expressed as a function of the other features x_j (j not equal to i) and then the r^2 value of this model, is used to calculate the vif for a given feature. vif is given by 1 / (1 - r^2), which tells us that as the value of r^2 for a given feature expression increases, the vif also increases. we then say that if the vif for a given feature exceeds 10, the feature can be expressed as a linear combination of other features, and hence that feature is eliminated. this procedure is done for all such features which have larger vifs. one feature having a large vif is removed and then the procedure is re-run. on doing this repeatedly, we tend to reduce the dimension and also filter out the most independent features.","the class started off by discussing the midsemester examination. we realised that ai tools do provide the right guidance, but if asked for complete solutions, they do suggest something completely off. we discussed that the test data for the prediction model had a completely different distribution as compared to the training data. we also had very few samples for one class which made our model unreliable for predicting that class. we concluded that the understanding of the data is very important for us to make efficient use of ai tools. we then moved forward and discussed about the class performance in assignment e3, and filled a survey about the midsem exam experience. till now, we had discussed problems within the same column. now we move on to problems across columns. we started discussion about a concept known as the curse of dimensionality. it describes the problems which occurs when the number of features in the dataset increases significantly. in that case, we get increased sparcity, increased complexity, increased computational cost and distance distortions. if data becomes sparse in an n dimensional space, the distance between the points increases a lot, and hence pattern detection in daya becomes difficult. to deal with this issue, we should try to decrease the dimensionality, or maybe we should select the most relevant features or use regularisation. we discussed about variance inflation factor, where a feature x_i is tried to be expressed as a function of the other features x_j (j not equal to i) and then the r^2 value of this model, is used to calculate the vif for a given feature. vif is given by 1 / (1 - r^2), which tells us that as the value of r^2 for a given feature expression increases, the vif also increases. we then say that if the vif for a given feature exceeds 10, the feature can be expressed as a linear combination of other features, and hence that feature is eliminated. this procedure is done for all such features which have larger vifs. one feature having a large vif is removed and then the procedure is re-run. on doing this repeatedly, we tend to reduce the dimension and also filter out the most independent features.",0,-5.569022,6.5680213,9.414792,6.927229,"models, feature, features"
274,"in todays class (12/3/25)
the class started with understanding differences between t-sne and pca, where t-sne can't be used while changing dimensions (projected features accounts into data loss and thus model can't be strained on) whereas pca is consistent (principal components contains into all the data while reducing dimensions eliminating the problem of data loss)
next we discussed about what will be the structure of last 10 classes and exercises. accounting the remaining class into feature engineering, big data and cloud computing, exercise 5-6 will be normal. 7 will comprise of project with due 13th april while 8-9-10 will be based in big data and cloud computing.
later, the discussion entered into the feature encoding, we initiated the discussion the importance of numerical value of y while passing for any algorithm/ model. there are two methods of which one is one hot encoding ( basically including creation of additional columns) dealing with multi-class objectives and other is multi-label classification where the output suggests the possible outcomes labelling each of them. 
later we delved into the discussion of label encoding where we understood the effects of x and y. followed the discussion on integer encoding where we used the examples of grades where there is a specific order in the output which can be directly implied into the code.
finally after discussing about one hot encoding ( number of columns help us to overcome the curse of dimensionality), binary encoding ( conversion of outputs into the binary form), and target encoding ( includes averaging the category values), we dived into the discussion of feature binning which is used for conversion of continuous problem into the discrete cases and ended the class understanding forming categories from the text data.","in todays class (12/3/25) the class started with understanding differences between t-sne and pca, where t-sne can't be used while changing dimensions (projected features accounts into data loss and thus model can't be strained on) whereas pca is consistent (principal components contains into all the data while reducing dimensions eliminating the problem of data loss) next we discussed about what will be the structure of last 10 classes and exercises. accounting the remaining class into feature engineering, big data and cloud computing, exercise 5-6 will be normal. 7 will comprise of project with due 13th april while 8-9-10 will be based in big data and cloud computing. later, the discussion entered into the feature encoding, we initiated the discussion the importance of numerical value of y while passing for any algorithm/ model. there are two methods of which one is one hot encoding ( basically including creation of additional columns) dealing with multi-class objectives and other is multi-label classification where the output suggests the possible outcomes labelling each of them. later we delved into the discussion of label encoding where we understood the effects of x and y. followed the discussion on integer encoding where we used the examples of grades where there is a specific order in the output which can be directly implied into the code. finally after discussing about one hot encoding ( number of columns help us to overcome the curse of dimensionality), binary encoding ( conversion of outputs into the binary form), and target encoding ( includes averaging the category values), we dived into the discussion of feature binning which is used for conversion of continuous problem into the discrete cases and ended the class understanding forming categories from the text data.",3,-39.828373,3.9466305,0.2078319,6.250567,"categorical, categorization, categorise"
275,"today we learned about multiple linear regression on independent variables. we learned about training, test data and what are the conditions for feeding data in machine learning model. we discussed that 80 % of data should be used to train and rest should be used to test the model.
we then learned about confidence interval and training matrix.
then we learned about importance of r^2 and square root of it and how do we see the values for them as goot fit or bad fir
then we moved to python where sir used many libraries  such as skewness and kurtosis statistics.","today we learned about multiple linear regression on independent variables. we learned about training, test data and what are the conditions for feeding data in machine learning model. we discussed that 80 % of data should be used to train and rest should be used to test the model. we then learned about confidence interval and training matrix. then we learned about importance of r^2 and square root of it and how do we see the values for them as goot fit or bad fir then we moved to python where sir used many libraries such as skewness and kurtosis statistics.",13,4.7480965,2.7649202,10.267928,5.0433893,"classification, classifying, classifications"
276,"today in class we continued discussing multiple linear regression (mlr) which is a statistical method used to model the relationship between one dependent variable (y) and multiple independent variables (x1,x2,...,xn). the general equation is y=beta0â€‹+beta1â€‹x1â€‹+beta2â€‹x2â€‹+...+betanâ€‹xnâ€‹+epsilon. the goal is to find the best-fit line (or hyperplane) that minimizes the difference between the predicted and actual values. we showed how it can be derived using simple calculus and how it yields an answer identical to gradient descent. gradient descent is an iterative optimization algorithm used to minimize the cost function, typically mean squared error (mse). j(beta)= sum(beta(xiâ€‹)âˆ’yiâ€‹)**2). several metrics evaluate the performance of a multiple linear regression model such as the r squared, adjusted r squared, condition number and f statistic. we also touched upon backward and forward feature engineering.","today in class we continued discussing multiple linear regression (mlr) which is a statistical method used to model the relationship between one dependent variable (y) and multiple independent variables (x1,x2,...,xn). the general equation is y=beta0 +beta1 x1 +beta2 x2 +...+betan xn +epsilon. the goal is to find the best-fit line (or hyperplane) that minimizes the difference between the predicted and actual values. we showed how it can be derived using simple calculus and how it yields an answer identical to gradient descent. gradient descent is an iterative optimization algorithm used to minimize the cost function, typically mean squared error (mse). j(beta)= sum(beta(xi ) yi )**2). several metrics evaluate the performance of a multiple linear regression model such as the r squared, adjusted r squared, condition number and f statistic. we also touched upon backward and forward feature engineering.",2,17.32407,3.5637171,12.754443,4.35564,"regression, regressions, features"
277,"to improve our results, we focused on three key areas: refining the data, enhancing our methods, and fine-tuning those methods. hereâ€™s how we approached it:

we started with multiple linear regression (mlr) for simple, linear relationships. however, when dealing with more complex, nonlinear data, we introduced features like \(x^2\) and \(\sin(x)\) to capture the patterns. this approach evolved into polynomial regression. initially, adding more features improved the model, but over time, it started to perform poorly. to address this, we kept only the most significant features based on their p-values.

we also experimented with feature selection techniques, such as forward and backward selection. the main challenge was avoiding overfittingâ€”where the model performs well on training data but poorly on new data. instead of using separate models for different types of relationships, we considered combining them into a single model. random forest emerged as a strong candidate for this approach.

next, we explored both parametric and non-parametric methods, along with delta analysis. we also briefly introduced neural networks and deep learning, which use multiple layers to uncover complex patterns in data. finally, we shifted our focus to classification, comparing traditional regression with logistic regression. we discussed the role of weights and introduced the sigmoid function, which is central to logistic regression due to its s-shaped curve.","to improve our results, we focused on three key areas: refining the data, enhancing our methods, and fine-tuning those methods. here s how we approached it: we started with multiple linear regression (mlr) for simple, linear relationships. however, when dealing with more complex, nonlinear data, we introduced features like \(x^2\) and \(\sin(x)\) to capture the patterns. this approach evolved into polynomial regression. initially, adding more features improved the model, but over time, it started to perform poorly. to address this, we kept only the most significant features based on their p-values. we also experimented with feature selection techniques, such as forward and backward selection. the main challenge was avoiding overfitting where the model performs well on training data but poorly on new data. instead of using separate models for different types of relationships, we considered combining them into a single model. random forest emerged as a strong candidate for this approach. next, we explored both parametric and non-parametric methods, along with delta analysis. we also briefly introduced neural networks and deep learning, which use multiple layers to uncover complex patterns in data. finally, we shifted our focus to classification, comparing traditional regression with logistic regression. we discussed the role of weights and introduced the sigmoid function, which is central to logistic regression due to its s-shaped curve.",0,-1.5024391,-3.8770208,9.051841,4.4428897,"models, feature, features"
278,"in today's session we learnt about various levels of measurement which are nominal, ordinal, interval and ratio and their proper definitions and usage. learnt about how can we use these levels of measurement while solving a classification (nominal or ordinal level of measurement) or a regression problem (interval or ratio level of measurement). also how can we use numbers to represent nominal data using one-hot encoding which is just making the numbers as switches so that the data just takes value of 1 at the class it belongs to and 0 everywhere else. learnt a little about supervised and unsupervised learning which is basically the classification of a machine learning problem based on if we know the label 'y' or not. various techniques/algorithms we use for supervised or unsupervised learning were also discussed briefly. at the end of the class we were taught how we only know the data of a sample of a population even after so many technological advancements and we make predictions of the population based on the sample. also how the function y=f(x) has an inherent error because we don't have access to entire population data.","in today's session we learnt about various levels of measurement which are nominal, ordinal, interval and ratio and their proper definitions and usage. learnt about how can we use these levels of measurement while solving a classification (nominal or ordinal level of measurement) or a regression problem (interval or ratio level of measurement). also how can we use numbers to represent nominal data using one-hot encoding which is just making the numbers as switches so that the data just takes value of 1 at the class it belongs to and 0 everywhere else. learnt a little about supervised and unsupervised learning which is basically the classification of a machine learning problem based on if we know the label 'y' or not. various techniques/algorithms we use for supervised or unsupervised learning were also discussed briefly. at the end of the class we were taught how we only know the data of a sample of a population even after so many technological advancements and we make predictions of the population based on the sample. also how the function y=f(x) has an inherent error because we don't have access to entire population data.",4,-23.97546,-11.367493,1.5578998,0.6004764,"classification, classifying, classifications"
279,"we started with vif, variance inflation factor( effect of all the axes {except xi} on xi). if greater vif-> r^2 is close to 1.{heavy dependence of xi on rest of them} no problem if we eliminate that feature. progressively, keep on eliminating features one by one {on the basis of higher vif for feature reduction}.

pca-> the max number of pc's are equal to the number of dimensions of the dataset. whenever we do pca, always pc1 is something which explains the maximum variation in the data. pc2 explans the 2nd max amount of variation in the data. pc helps you reduce the dimensionality of the problem. each pc can be linearly dependent on all of the real dimensions.

so what should we do first pca or vif?
vif, because it reduces the real dimension which could have been incorporated in the pca without eliminating them. later after real dimension elimination we get the real picture and the pca would be so beneficial to us for dimension reduction.

it helps us in data reduction, prediction model, visualization (part of eda{exploratory data analysis})

t-sne (t-distributed stochastic neighbour encoding) for each point it calculates the probability distribution of distance of all other points wrt that point.

at higher dimension --> gaussian normal distribution
2-3 dimension-> t-distribution{transformation} helps by enhancing the distinction between dissimilar points.","we started with vif, variance inflation factor( effect of all the axes {except xi} on xi). if greater vif-> r^2 is close to 1.{heavy dependence of xi on rest of them} no problem if we eliminate that feature. progressively, keep on eliminating features one by one {on the basis of higher vif for feature reduction}. pca-> the max number of pc's are equal to the number of dimensions of the dataset. whenever we do pca, always pc1 is something which explains the maximum variation in the data. pc2 explans the 2nd max amount of variation in the data. pc helps you reduce the dimensionality of the problem. each pc can be linearly dependent on all of the real dimensions. so what should we do first pca or vif? vif, because it reduces the real dimension which could have been incorporated in the pca without eliminating them. later after real dimension elimination we get the real picture and the pca would be so beneficial to us for dimension reduction. it helps us in data reduction, prediction model, visualization (part of eda{exploratory data analysis}) t-sne (t-distributed stochastic neighbour encoding) for each point it calculates the probability distribution of distance of all other points wrt that point. at higher dimension --> gaussian normal distribution 2-3 dimension-> t-distribution{transformation} helps by enhancing the distinction between dissimilar points.",11,-19.933891,2.1928782,10.478036,13.464669,"pca, heatmap, heatmaps"
280,"today's lecture started off with a discussion about the difference between population and sample. sample is a small subset representing the entire population because gathering data for the entire population can be very difficult. so the sample should be a good representative of the population, and we should be able to predict the population's behaviour from the sample. there are various statistical attributes of data which can be calculated. some of them are frequency, mean, median, mode, std. deviation, variance, etc. if these attributes are calculated for the sample, they are called as statistics, whereas if they are estimated for the population, they are called the parameters. so our main focus with data science and ml is to estimate the parameters from the statistics. 
then we moved on to simple linear regression, where we have only one influencing or independent variable steering our predictions. our model is basically of the form y = ax + b, where x is the independent variable (predictor, feature), y is the dependent variable (response variable, label) and b is the bias term. the bias term accounts for all the unknown variables influencing our predictions. so as and when we bring more and more influencing variables into our model, the bias term keeps on reducing. 
one special thing that sir mentioned today was that a model can be as simple as a point. even a point predictor can be a model, however it may not be relevant for any practical usage and may not represent our data well. 
so a and b are the statistics of the population parameters. we also defined a confidence interval, which is the interval of values within which the parameters lie. the larger the confidence interval, the more confident we can be of the parameter value lying in that interval. then we also talked about minimising the error between our predictions and the actual data value at a given point. we said that the error can be formulated in various ways, but the most optimum way is to consider the sum of the squared errors, as that is something which is least influenced by the sign and the direction of fluctuation, and hence should be the best choice for minimisation. upon doing minimisation, we calculated the values of the statistics a and b. however these were closed form solution which were basically point estimates of the parameters. these estimates might not give us any confidence about the actual parameter values, hence we also need to find a possible interval within which the value of the parameters can exist with a very high probability.","today's lecture started off with a discussion about the difference between population and sample. sample is a small subset representing the entire population because gathering data for the entire population can be very difficult. so the sample should be a good representative of the population, and we should be able to predict the population's behaviour from the sample. there are various statistical attributes of data which can be calculated. some of them are frequency, mean, median, mode, std. deviation, variance, etc. if these attributes are calculated for the sample, they are called as statistics, whereas if they are estimated for the population, they are called the parameters. so our main focus with data science and ml is to estimate the parameters from the statistics. then we moved on to simple linear regression, where we have only one influencing or independent variable steering our predictions. our model is basically of the form y = ax + b, where x is the independent variable (predictor, feature), y is the dependent variable (response variable, label) and b is the bias term. the bias term accounts for all the unknown variables influencing our predictions. so as and when we bring more and more influencing variables into our model, the bias term keeps on reducing. one special thing that sir mentioned today was that a model can be as simple as a point. even a point predictor can be a model, however it may not be relevant for any practical usage and may not represent our data well. so a and b are the statistics of the population parameters. we also defined a confidence interval, which is the interval of values within which the parameters lie. the larger the confidence interval, the more confident we can be of the parameter value lying in that interval. then we also talked about minimising the error between our predictions and the actual data value at a given point. we said that the error can be formulated in various ways, but the most optimum way is to consider the sum of the squared errors, as that is something which is least influenced by the sign and the direction of fluctuation, and hence should be the best choice for minimisation. upon doing minimisation, we calculated the values of the statistics a and b. however these were closed form solution which were basically point estimates of the parameters. these estimates might not give us any confidence about the actual parameter values, hence we also need to find a possible interval within which the value of the parameters can exist with a very high probability.",1,34.383286,-7.3995357,16.31731,3.801199,"population, models, estimating"
281,"first we were assigned python tutorials to complete. then we wee shown how to infer statistical data based on similiarity using llms. then we studied mutiple regression model in which the dependent variables were the features and multiple parameters such as r,p,f value then we tested these metrics on a dataset","first we were assigned python tutorials to complete. then we wee shown how to infer statistical data based on similiarity using llms. then we studied mutiple regression model in which the dependent variables were the features and multiple parameters such as r,p,f value then we tested these metrics on a dataset",13,-7.36188,-3.6059203,9.710953,5.3078346,"classification, classifying, classifications"
282,"class started with a recap of previous class. one of the inherent assumption of closed form of linear regression is errors are normally distributed. if that is violated we have to take care of it. whatever be the distribution of sample, when we take multiple samples and calculate the mean of the samples, we will always get normal distribution of means of samples. lets suppose we have only one sample with 30 observations. 
step1: calculate mean of the sample.(here we assume that the calculated mean is close to the population mean.) 
[if we have many sample all these means would form a normal distribution(u,s/root(n)).] step2: get the sample std dev and assume it to be close to samples std deviation 
step3: calculate sigma/root(n), we can describe the normal distribution. 
we want to get interval within which the population mean is likely to lie.
using excel, python and other tools we can create data with a particular distribution.
after this sir went explain statistics and explained estimation of parameter using statistics. then sir explained about the gaussian normal distribution. if the number of observations is less than 30, we wont get a normal distribution. we would get a t- distribution. so generally when the number of observations are less than 30, we must use t distribution. we started with a sample and we are able to say with x% confidence that population mean lies in an interval. 95% confidence interval- ""if you take 100 samples, the mean of 95 of those samples will lie in this interval. t=(x-u)/std.dev
p value: we have a sample and we calculated 95% confidence interval. we get a new set of observations - sample 2 and sample 3. the mean of sample 2 lies in the confidence interval whereas mean of sample 3 does not lie in confidence interval. we can say that sample 2 belongs to the population and sample 3 does not belong to population. 
when a=0 (y=ax+b) becomes zero, we can't use linear regression.  we find the distribution of a. we have a calculated value of a. we find the confidence interval. if zero lies in this confidence interval, we cant have linear regression.  (statistically all values in confidence interval are similar). p value is the area under curve between x=b1 and infinity *2. p value is determined by the position of b1. for the regression to be nice, p value has to be less than 0.05.(for b1 to lie outside the confidence interval, p value has to be less than 0.05). b1 should lie outside confidence interval because we need 0 and b1 to be statistically different.","class started with a recap of previous class. one of the inherent assumption of closed form of linear regression is errors are normally distributed. if that is violated we have to take care of it. whatever be the distribution of sample, when we take multiple samples and calculate the mean of the samples, we will always get normal distribution of means of samples. lets suppose we have only one sample with 30 observations. step1: calculate mean of the sample.(here we assume that the calculated mean is close to the population mean.) [if we have many sample all these means would form a normal distribution(u,s/root(n)).] step2: get the sample std dev and assume it to be close to samples std deviation step3: calculate sigma/root(n), we can describe the normal distribution. we want to get interval within which the population mean is likely to lie. using excel, python and other tools we can create data with a particular distribution. after this sir went explain statistics and explained estimation of parameter using statistics. then sir explained about the gaussian normal distribution. if the number of observations is less than 30, we wont get a normal distribution. we would get a t- distribution. so generally when the number of observations are less than 30, we must use t distribution. we started with a sample and we are able to say with x% confidence that population mean lies in an interval. 95% confidence interval- ""if you take 100 samples, the mean of 95 of those samples will lie in this interval. t=(x-u)/std.dev p value: we have a sample and we calculated 95% confidence interval. we get a new set of observations - sample 2 and sample 3. the mean of sample 2 lies in the confidence interval whereas mean of sample 3 does not lie in confidence interval. we can say that sample 2 belongs to the population and sample 3 does not belong to population. when a=0 (y=ax+b) becomes zero, we can't use linear regression. we find the distribution of a. we have a calculated value of a. we find the confidence interval. if zero lies in this confidence interval, we cant have linear regression. (statistically all values in confidence interval are similar). p value is the area under curve between x=b1 and infinity *2. p value is determined by the position of b1. for the regression to be nice, p value has to be less than 0.05.(for b1 to lie outside the confidence interval, p value has to be less than 0.05). b1 should lie outside confidence interval because we need 0 and b1 to be statistically different.",7,37.179756,2.2835498,15.3762665,2.444375,"statistics, statistical, statisticsâ"
283,started with interactive session on a website to see how the model classifies different data types. then we went for the python code for logistic regression. accuracy as a score is not that correct.  dealt with concepts of tpr fpr using the roc curve as to when false positives started to come. we plotter two distribution and interpretated situation when they starter to overlap. these actually depends on 2 factors: distribution of observations and efficiency of classifier algorithm. area under auc in a roc curve. thebetter the model means auc close to 1 worst case af 45 degree line. support in classification report is how observations support this. started new topic of clustering it is an unsupervised learning. 2 types k means and hierarchical. dendrogram gives an idea how much clusters should we make. in k means we pre allot clusters tp data points then updates based on distance from centroid. in hierarchical each point is itself a unique cluster. now depends where we cut the distance and find number of clusters. we always take maximal inter cluster distance.,started with interactive session on a website to see how the model classifies different data types. then we went for the python code for logistic regression. accuracy as a score is not that correct. dealt with concepts of tpr fpr using the roc curve as to when false positives started to come. we plotter two distribution and interpretated situation when they starter to overlap. these actually depends on 2 factors: distribution of observations and efficiency of classifier algorithm. area under auc in a roc curve. thebetter the model means auc close to 1 worst case af 45 degree line. support in classification report is how observations support this. started new topic of clustering it is an unsupervised learning. 2 types k means and hierarchical. dendrogram gives an idea how much clusters should we make. in k means we pre allot clusters tp data points then updates based on distance from centroid. in hierarchical each point is itself a unique cluster. now depends where we cut the distance and find number of clusters. we always take maximal inter cluster distance.,8,-3.7114775,-20.276823,6.332155,0.4025415,"classification, clusterings, classifying"
284,"data scaling, linear regr3ssions is affected by it. gradiaent descent is immune to it. methods to deal with it. normalize all samples to range between 0 and 1.

clustering. on running clustering through a normalized data we obtained an errorneous result with linear or parallel boundaries. the original data in aspect ratio 1 is spread over a narrow range of y values. when normalized, the clustering improved. because clustering makes use of euclidian distance, scaling or transforming data can improve or worsen classification.

normalization techniques. standardization is to turn a disttibution into one that resembles a standard normal distribution. boxcox transformation turns distributions far from normal into close to normal distributions. (x^lambda-1)/lambda. log transformation takes care of heteroscadacity. because euclidian distance 

caution against confusion matrix being transposed. data imbalance and clustering.

dealing with data imbalance.
case of exoplamets and timeseries flux data. overtsampling is the idea of duplicating certain values when under represented. smote performs linear sampling with two hyperparamaeters, number of samples and smote%.","data scaling, linear regr3ssions is affected by it. gradiaent descent is immune to it. methods to deal with it. normalize all samples to range between 0 and 1. clustering. on running clustering through a normalized data we obtained an errorneous result with linear or parallel boundaries. the original data in aspect ratio 1 is spread over a narrow range of y values. when normalized, the clustering improved. because clustering makes use of euclidian distance, scaling or transforming data can improve or worsen classification. normalization techniques. standardization is to turn a disttibution into one that resembles a standard normal distribution. boxcox transformation turns distributions far from normal into close to normal distributions. (x^lambda-1)/lambda. log transformation takes care of heteroscadacity. because euclidian distance caution against confusion matrix being transposed. data imbalance and clustering. dealing with data imbalance. case of exoplamets and timeseries flux data. overtsampling is the idea of duplicating certain values when under represented. smote performs linear sampling with two hyperparamaeters, number of samples and smote%.",11,-20.967436,9.528025,10.192286,10.150711,"pca, heatmap, heatmaps"
285,"this was our first hands on lecture in which we used excel to understand simple linear regression. we first loaded the dataset in excel and crated its scatter plot to identify that the dataset followed a somewhat linear variation. we then calculated various parameters like x*y, x^2, denominator which we used to find out the simple linear regression line parameters beta_0 and beta_1. we then plotted the scatter plot and histogram of the residual terms in slr. we saw that the histogram was kind of uniform rather than a a normally distributed which meant that the errors were not random. we then looked at sst = ssr + sse where sst gives measure of total variation, ssr is variation explained by regression model and sse is variation not explained by regression model. we define coefficient of determination r^2 which determines how well the variation is explained by regression model. in slr, this r = r where r is correlation coefficient which is a measure of how good the dependent variable varies wrt its mean compared to with in dependent variable wrt its mean.","this was our first hands on lecture in which we used excel to understand simple linear regression. we first loaded the dataset in excel and crated its scatter plot to identify that the dataset followed a somewhat linear variation. we then calculated various parameters like x*y, x^2, denominator which we used to find out the simple linear regression line parameters beta_0 and beta_1. we then plotted the scatter plot and histogram of the residual terms in slr. we saw that the histogram was kind of uniform rather than a a normally distributed which meant that the errors were not random. we then looked at sst = ssr + sse where sst gives measure of total variation, ssr is variation explained by regression model and sse is variation not explained by regression model. we define coefficient of determination r^2 which determines how well the variation is explained by regression model. in slr, this r = r where r is correlation coefficient which is a measure of how good the dependent variable varies wrt its mean compared to with in dependent variable wrt its mean.",5,21.182617,-0.40304783,13.605761,4.7036967,"regression, statistical, statistics"
286,"class started with pivot table. dependinv on the level of details want, we are going more levels deeper. we saw conversion of histogram to box plot in excel to have an idea about outliers. most common submission lie in a band. we used line diagram for dates and total characters/avg characters. mainly we learnt about exploratory data analysis, and infer from there trends and analysis we can derive. we can check for anamolies.
we did another case study on the plant with 241 columns. manipulating to get desired analysis. then finally we had discussion on e2 assignment","class started with pivot table. dependinv on the level of details want, we are going more levels deeper. we saw conversion of histogram to box plot in excel to have an idea about outliers. most common submission lie in a band. we used line diagram for dates and total characters/avg characters. mainly we learnt about exploratory data analysis, and infer from there trends and analysis we can derive. we can check for anamolies. we did another case study on the plant with 241 columns. manipulating to get desired analysis. then finally we had discussion on e2 assignment",6,-11.372799,27.56209,7.4902134,10.009644,"summarizing, summarize, summarization"
287,"in today's session, we first see that we can improve the quality of results in 3 ways as: we can improve the sample by increasing the quantity of sample and size of the sample, improve the method by using multiple methods and select the best one, fine tune or properly use the method. then we see that in linear regression, outcome is expressed as a linear combination of independent variables. then we see that if the error plot is not random or follows a pattern, forcing a line to model to the data gives improper results. hence, we need non-linear independent variables so that mlr will predict the desired non-linear y. for ex, to get model for sin(x1) we take first independent variable to be x1, the other can be introduce as x2=x1*x1 and further x3=x1*x1*x1 and so on. the resulting method is polynomial regression, introducing such new x to improve models in ml is called as feature engineering. you can add features but based on f-statistic and adjusted r^2 you can cancel some features which are needed to be eliminated for better regression model. then we saw the comparison of backward v/s forward feature engineering. then we applied different models on the same model, compare them and check which of the model best fits the data. then we at last started learning about classification, seeing about logistic regression in which outcome is a classifier.","in today's session, we first see that we can improve the quality of results in 3 ways as: we can improve the sample by increasing the quantity of sample and size of the sample, improve the method by using multiple methods and select the best one, fine tune or properly use the method. then we see that in linear regression, outcome is expressed as a linear combination of independent variables. then we see that if the error plot is not random or follows a pattern, forcing a line to model to the data gives improper results. hence, we need non-linear independent variables so that mlr will predict the desired non-linear y. for ex, to get model for sin(x1) we take first independent variable to be x1, the other can be introduce as x2=x1*x1 and further x3=x1*x1*x1 and so on. the resulting method is polynomial regression, introducing such new x to improve models in ml is called as feature engineering. you can add features but based on f-statistic and adjusted r^2 you can cancel some features which are needed to be eliminated for better regression model. then we saw the comparison of backward v/s forward feature engineering. then we applied different models on the same model, compare them and check which of the model best fits the data. then we at last started learning about classification, seeing about logistic regression in which outcome is a classifier.",0,4.0682282,-4.3407164,9.624159,4.258986,"models, feature, features"
288,"the lecture covered some basic ideas on machine learning and statistics. machine learning is essentially finding a relationship between features-inputs and labels-outputs. to achieve this, methods like linear regression, logistic regression, random forests, k-means clustering, and hierarchical clustering are used. these methods help solve problems like classification-grouping things-and regression-predicting values.
the lecture also introduced levels of measurement, which describe how data is categorized:
nominal data is applied to name things without any kind of order, such as color or gender.
ordinal data is ordered, but the difference between them is not consistent, like grades: a, b, c. encoding the ordinal data using numbers can lead to problems since it may suggest a difference that does not exist. using vectors will help to avoid this.
interval data is used for measurement, like temperature, where zero does not represent nothing.
ratio data includes things like height or weight, where zero means nothing, and you can compare values meaningfully (e.g., 10 kg is twice as heavy as 5 kg).
in machine learning, if the label is nominal, the problem is classification. if the label is interval or ratio, it is regression. machine learning can be supervised or unsupervised. in supervised learning, labels are already given, while in unsupervised learning, there are no labels. clustering methods are used in unsupervised learning to group data by similarity, and labels can be assigned later.
the last discussion about the difference between population and sample occurred. population refers to the entire dataset, but there can be a size that may be computationally expensive to analyze; thus, we often use a sample which is actually a smaller, manageable part of the data.","the lecture covered some basic ideas on machine learning and statistics. machine learning is essentially finding a relationship between features-inputs and labels-outputs. to achieve this, methods like linear regression, logistic regression, random forests, k-means clustering, and hierarchical clustering are used. these methods help solve problems like classification-grouping things-and regression-predicting values. the lecture also introduced levels of measurement, which describe how data is categorized: nominal data is applied to name things without any kind of order, such as color or gender. ordinal data is ordered, but the difference between them is not consistent, like grades: a, b, c. encoding the ordinal data using numbers can lead to problems since it may suggest a difference that does not exist. using vectors will help to avoid this. interval data is used for measurement, like temperature, where zero does not represent nothing. ratio data includes things like height or weight, where zero means nothing, and you can compare values meaningfully (e.g., 10 kg is twice as heavy as 5 kg). in machine learning, if the label is nominal, the problem is classification. if the label is interval or ratio, it is regression. machine learning can be supervised or unsupervised. in supervised learning, labels are already given, while in unsupervised learning, there are no labels. clustering methods are used in unsupervised learning to group data by similarity, and labels can be assigned later. the last discussion about the difference between population and sample occurred. population refers to the entire dataset, but there can be a size that may be computationally expensive to analyze; thus, we often use a sample which is actually a smaller, manageable part of the data.",4,-24.938925,-15.185337,1.7190385,0.49382463,"classification, classifying, classifications"
289,"we learnt:
-samples should be representative
-a narrow definition of the population may allow us to sample the whole population
-for every sample we can find the following three for the data:
   1. level of measurement- nominal/ordinal/interval/ratio
   2. attributes- count(frequency), mean, median, mode...
   3. operations- count, add, subtract, multiply...
-""we estimate a parameter for a population and we calculate a statistic for a sample.""
-finally, ""we wish to estimate the parameters from the calculated statistics.""
-from the scatter plot of a given data we decide if linear model is a good fit
-for the same data we might even use a single point as the model, but that is a naive model
-slr terms- dependent/independent variables; bias
-bias occurs due to the other variables' effect on the dependent variable
-the model that we create is particular to the specific sample that was used to create it, and so each sample of a population yields a different model
-the credibilty of a model then depends on the sample-taker's job
-we calculate b0 and b1 using the sample and based on that we find an interval where we are confident that the population parameter lies
-point estimate- zero confidence
-as we increase the interval size for the population b0/b1- the confidence increases
-criteria used to find the best fit line- minimise the sse
-mathematics of slr","we learnt: -samples should be representative -a narrow definition of the population may allow us to sample the whole population -for every sample we can find the following three for the data: 1. level of measurement- nominal/ordinal/interval/ratio 2. attributes- count(frequency), mean, median, mode... 3. operations- count, add, subtract, multiply... -""we estimate a parameter for a population and we calculate a statistic for a sample."" -finally, ""we wish to estimate the parameters from the calculated statistics."" -from the scatter plot of a given data we decide if linear model is a good fit -for the same data we might even use a single point as the model, but that is a naive model -slr terms- dependent/independent variables; bias -bias occurs due to the other variables' effect on the dependent variable -the model that we create is particular to the specific sample that was used to create it, and so each sample of a population yields a different model -the credibilty of a model then depends on the sample-taker's job -we calculate b0 and b1 using the sample and based on that we find an interval where we are confident that the population parameter lies -point estimate- zero confidence -as we increase the interval size for the population b0/b1- the confidence increases -criteria used to find the best fit line- minimise the sse -mathematics of slr",1,33.683643,-5.826865,16.17263,3.651495,"population, models, estimating"
290,"discussed true/false positives and negatives
f1 value = balance of precision and recall, used confusion matrix too
recall (correct ones out of real class)
accuracy (how many right ones you got) and precision (correct ones out of predicted)
aim was setting weights to make predictions match actual results (reduce errors)
target can be either predicting class or chance (like coin flip probabilities)
we looked at p and 1-p for all examples
if chance > 0.5, it goes to one class, else other one
features can be more; started with 3 features, answer was 0 or 1
logistic doesn't say the exact class but tells the chance of belonging to one class or another","discussed true/false positives and negatives f1 value = balance of precision and recall, used confusion matrix too recall (correct ones out of real class) accuracy (how many right ones you got) and precision (correct ones out of predicted) aim was setting weights to make predictions match actual results (reduce errors) target can be either predicting class or chance (like coin flip probabilities) we looked at p and 1-p for all examples if chance > 0.5, it goes to one class, else other one features can be more; started with 3 features, answer was 0 or 1 logistic doesn't say the exact class but tells the chance of belonging to one class or another",10,11.936236,-21.250566,8.75859,-1.5068458,"classifications, histograms, histogram"
291,"simple linear regression is a statistical method used to predict the value of a dependent variable based on one independent variable by fitting a straight line. if there are more than one variable it is called multiple linear regression. sample data represents a subset of the population used for analysis, while population data includes the entire dataset of interest. we use our sample to estimates the parameters of the population by beta 0 and beta 1. different sample can result in different model with different accuracy .the best-fit line minimizes the sum of squared differences between observed and predicted values. it provides insights into the relationship between variables, helping in prediction and trend analysis.","simple linear regression is a statistical method used to predict the value of a dependent variable based on one independent variable by fitting a straight line. if there are more than one variable it is called multiple linear regression. sample data represents a subset of the population used for analysis, while population data includes the entire dataset of interest. we use our sample to estimates the parameters of the population by beta 0 and beta 1. different sample can result in different model with different accuracy .the best-fit line minimizes the sum of squared differences between observed and predicted values. it provides insights into the relationship between variables, helping in prediction and trend analysis.",1,28.729666,-6.3329625,15.453788,4.457319,"population, models, estimating"
292,"we started the lecture with a quick recap of the concepts from previous class- including regression coefficients and discussed about the sampling distribution of mean (histogram plots of the sample). we learnt that whatever be the distribution of the population, the sampling distribution of the sample mean will always be normally distributed. we donâ€™t have the opportunity to take multiple samples. we can practically take a single sample, which can have multiple observations. so, using these â€˜statisticsâ€™ values, we have to estimate the population parameters. so, while taking up a sample, we assume that it is a â€˜goodâ€™ representative of the population.
so, our first step towards estimating the population parameters, is to calculate the mean of sample, by assuming that it is close to population mean.
then we find out the sample standard distribution, assuming that it is close to population standard distribution. so, our ultimate goal is to find out an interval around the calculated value of sample mean, within which we can say with certain confidence level that our population mean would lie. for our sample observations, we make different categories/â€™binsâ€™ in which we divide the data values. then we plot a histogram (frequency distribution graph) for these bins. we observe that as we start decreasing the width of the class, the curve becomes smoother and smoother. this curve has a gaussian normal distribution.
if we divide the frequency values for various bins, by the total frequency, we get the probability that a certain data point will lie within that bin. the area under the curve within that interval, gives us the value of probability. 
so, when we say there is a 95% confidence interval, by it we mean that- if you take 100 samples, then 95 out of these would lie in that interval. also, the area under the curve within this interval would be 0.95. these plots are called â€˜probability density function(pdf)â€™.
if the number of observations is less than 30, then the distribution becomes a â€˜tâ€™ distribution, rather than a normal distribution.
the normal(or t) distribution is centered around the mean, and its tails extend to infinity on both the sides. for an interval, say (a,b), any value within this is â€˜not statistically differentâ€™ from the mean. but any value out of this interval can be said to be â€˜statistically differentâ€™ from the mean values. it may also come from an entirely different population. 
so, if we want to check whether a model is truly a slr model, we can check whether the coefficient of x, i.e. 'a' is statistically different from 0 or not. if it is, then we may conclude that the model is appropriate.
we consider that 0 is the at the centre of the distribution and then define an interval for 95% confidence, we want the value of the regression coefficient to lie outside this, so as to make our regression model valid.
twice the area under the curve from the point, which corresponds to the regression coefficient, to infinity is termed as the â€˜p-valueâ€™. so, for a 95% confidence interval, we want our p-value to be less than 0.05.
we can use this for multiple linear regression as well. those independent variables that have corresponding p-values, greater than 0.05(95% confidence) can be neglected in the regression expression.
lastly, we talked about anova- analysis of variance, which is a tool that tells you what is the statistic probability that at least one of the coefficients in mlr is non zero.","we started the lecture with a quick recap of the concepts from previous class- including regression coefficients and discussed about the sampling distribution of mean (histogram plots of the sample). we learnt that whatever be the distribution of the population, the sampling distribution of the sample mean will always be normally distributed. we don t have the opportunity to take multiple samples. we can practically take a single sample, which can have multiple observations. so, using these statistics values, we have to estimate the population parameters. so, while taking up a sample, we assume that it is a good representative of the population. so, our first step towards estimating the population parameters, is to calculate the mean of sample, by assuming that it is close to population mean. then we find out the sample standard distribution, assuming that it is close to population standard distribution. so, our ultimate goal is to find out an interval around the calculated value of sample mean, within which we can say with certain confidence level that our population mean would lie. for our sample observations, we make different categories/ bins in which we divide the data values. then we plot a histogram (frequency distribution graph) for these bins. we observe that as we start decreasing the width of the class, the curve becomes smoother and smoother. this curve has a gaussian normal distribution. if we divide the frequency values for various bins, by the total frequency, we get the probability that a certain data point will lie within that bin. the area under the curve within that interval, gives us the value of probability. so, when we say there is a 95% confidence interval, by it we mean that- if you take 100 samples, then 95 out of these would lie in that interval. also, the area under the curve within this interval would be 0.95. these plots are called probability density function(pdf) . if the number of observations is less than 30, then the distribution becomes a t distribution, rather than a normal distribution. the normal(or t) distribution is centered around the mean, and its tails extend to infinity on both the sides. for an interval, say (a,b), any value within this is not statistically different from the mean. but any value out of this interval can be said to be statistically different from the mean values. it may also come from an entirely different population. so, if we want to check whether a model is truly a slr model, we can check whether the coefficient of x, i.e. 'a' is statistically different from 0 or not. if it is, then we may conclude that the model is appropriate. we consider that 0 is the at the centre of the distribution and then define an interval for 95% confidence, we want the value of the regression coefficient to lie outside this, so as to make our regression model valid. twice the area under the curve from the point, which corresponds to the regression coefficient, to infinity is termed as the p-value . so, for a 95% confidence interval, we want our p-value to be less than 0.05. we can use this for multiple linear regression as well. those independent variables that have corresponding p-values, greater than 0.05(95% confidence) can be neglected in the regression expression. lastly, we talked about anova- analysis of variance, which is a tool that tells you what is the statistic probability that at least one of the coefficients in mlr is non zero.",7,37.29734,1.323418,15.360534,2.4974434,"statistics, statistical, statisticsâ"
293,we started session with discussion of exploring data basically eda (not easy) - problem solving skill and communication skill is also important. data is tabulated in multiple tables and sometimes merge multiple files into one table. data mining standard and rules are there (sop) also we have to see how to deploy model (at the end of sem nearly). when to say model is success : acceptance criteria also when to stop exit criteria. don't use deep learning when need is not there. goal need to be defined what is to be applied regression or clustering. problems with dependent variables and independent variables. correct the data if the data is incorrect or imbalance. modification of data if skewed apply transformations. -statistical problems. also one ta ( a student) explained about eda and a project: 800 datapoints with main issue if person is diabetes or not.. checking their distribution of data. feature correlation and outcomes are also important. different plot like scatter plot and some clusters plot. also different methods used in the project were discussed. univariate and multivariate data discussion. which methods to be used judiciously ?,we started session with discussion of exploring data basically eda (not easy) - problem solving skill and communication skill is also important. data is tabulated in multiple tables and sometimes merge multiple files into one table. data mining standard and rules are there (sop) also we have to see how to deploy model (at the end of sem nearly). when to say model is success : acceptance criteria also when to stop exit criteria. don't use deep learning when need is not there. goal need to be defined what is to be applied regression or clustering. problems with dependent variables and independent variables. correct the data if the data is incorrect or imbalance. modification of data if skewed apply transformations. -statistical problems. also one ta ( a student) explained about eda and a project: 800 datapoints with main issue if person is diabetes or not.. checking their distribution of data. feature correlation and outcomes are also important. different plot like scatter plot and some clusters plot. also different methods used in the project were discussed. univariate and multivariate data discussion. which methods to be used judiciously ?,9,-21.661158,17.735346,8.603888,8.901662,"dataâ, analyse, analyses"
294,"to improve the results you can either improve the sample or improve the method used. improving the sample could mean increasing the quality of the sample or the quantity. when it comes to improving the method, you can either fine-tune or better understand the method or use multiple and select the best one based on your knowledge about the metrics. one technique used to improve the method is call grid search where you form an n-dimensional grid with all the possible value combinations of the n parameters of the method, and then evaluate which one is the best.
polynomial regression is a type of linear regression where we add additional features derived from the given features, and they are respectively, the features raised to an exponent, eg.- x1 -> x1^2, x1^3, etc. more generally, we can use domain knowledge and insights gained from exploratory data analysis to engineer even better features.
there are two extremes to the process of feature selection. in forward feature selection, we start with an empty set of selected features, and then based on our knowledge, keep on adding features to the dataset in the hopes of improving the results. in backward feature selection, we start with all the features we can think of and then start reducing them by some metric (eg. p-value).
in parametric methods (such as linear regression), we can perform delta analysis, which is answering questions like how much does the output change with a slight change in a particular feature. this can be somehow achieved in non-parametric models also, but is much easier in parametric ones.
in real life after spending 80% of the time doing something with the data that does not include 'fitting a model', we actually need to fit many models and then decide which amongst them is the best based on various factors, some of which are: interpretability (whether we can make sense of what the model is doing), maintainability (the model will have to be recreated when 'data drift' {some change(s) to the trend in the data that occurs with time} is observed, hence if one model can fit the whole data, it is better), and of course the understanding and feel for numbers and error metrics.
a neural network is a model where we have hidden layers that are connected to input and output and amongst themselves using links that have an associated weights. the output function can still be expressed as w_i*x_i. more layers (called deep neural networks) add more flexibility / provide more degrees of freedom to the neural network, but in turn it becomes extremely 'data hungry'. the latest neural network models such as chatgpt have billions of parameters and are trained on internet-scale data, but the task that it performs is just: given a few characters / words, what is the most likely next character / word.
logistic regression is classification period.
need to predict the boundaries that separate the different classes. in case of overlap, we look for minimizing the number of mis-classifications. define an equation for the boundary and then based on the output, assign a class label. one such function that can do this is the sigmoid function (s(a) = 1/(1+e^(-a))).","to improve the results you can either improve the sample or improve the method used. improving the sample could mean increasing the quality of the sample or the quantity. when it comes to improving the method, you can either fine-tune or better understand the method or use multiple and select the best one based on your knowledge about the metrics. one technique used to improve the method is call grid search where you form an n-dimensional grid with all the possible value combinations of the n parameters of the method, and then evaluate which one is the best. polynomial regression is a type of linear regression where we add additional features derived from the given features, and they are respectively, the features raised to an exponent, eg.- x1 -> x1^2, x1^3, etc. more generally, we can use domain knowledge and insights gained from exploratory data analysis to engineer even better features. there are two extremes to the process of feature selection. in forward feature selection, we start with an empty set of selected features, and then based on our knowledge, keep on adding features to the dataset in the hopes of improving the results. in backward feature selection, we start with all the features we can think of and then start reducing them by some metric (eg. p-value). in parametric methods (such as linear regression), we can perform delta analysis, which is answering questions like how much does the output change with a slight change in a particular feature. this can be somehow achieved in non-parametric models also, but is much easier in parametric ones. in real life after spending 80% of the time doing something with the data that does not include 'fitting a model', we actually need to fit many models and then decide which amongst them is the best based on various factors, some of which are: interpretability (whether we can make sense of what the model is doing), maintainability (the model will have to be recreated when 'data drift' {some change(s) to the trend in the data that occurs with time} is observed, hence if one model can fit the whole data, it is better), and of course the understanding and feel for numbers and error metrics. a neural network is a model where we have hidden layers that are connected to input and output and amongst themselves using links that have an associated weights. the output function can still be expressed as w_i*x_i. more layers (called deep neural networks) add more flexibility / provide more degrees of freedom to the neural network, but in turn it becomes extremely 'data hungry'. the latest neural network models such as chatgpt have billions of parameters and are trained on internet-scale data, but the task that it performs is just: given a few characters / words, what is the most likely next character / word. logistic regression is classification period. need to predict the boundaries that separate the different classes. in case of overlap, we look for minimizing the number of mis-classifications. define an equation for the boundary and then based on the output, assign a class label. one such function that can do this is the sigmoid function (s(a) = 1/(1+e^(-a))).",0,1.6896472,-4.6824136,9.534692,4.037636,"models, feature, features"
295,"we started with the discussion of non ideal data. unlike the data with which we have dealt till now, real life data most of the times contains anomalies like outliners and missing points. to prepare the data we follow a process named, exploratory data analysis (eda). the main idea behind eda is to identify anomalies, formulate and test hypotheses, and validate assumptions.
for visualization we use, histograms, correlation heatmaps,  line plots
to handle missing data we end to go with one of ignoring, deleting missing values or imputation that is calculating them back via some models. 
and lastly looked at outliers and how to deal with them. there are few algorithms using which we can find outliers and then we distinguished between true outliers, which are extreme but valid values, and erroneous outliers. and depending upon what we decide we can either remove them or do a separate analysis.","we started with the discussion of non ideal data. unlike the data with which we have dealt till now, real life data most of the times contains anomalies like outliners and missing points. to prepare the data we follow a process named, exploratory data analysis (eda). the main idea behind eda is to identify anomalies, formulate and test hypotheses, and validate assumptions. for visualization we use, histograms, correlation heatmaps, line plots to handle missing data we end to go with one of ignoring, deleting missing values or imputation that is calculating them back via some models. and lastly looked at outliers and how to deal with them. there are few algorithms using which we can find outliers and then we distinguished between true outliers, which are extreme but valid values, and erroneous outliers. and depending upon what we decide we can either remove them or do a separate analysis.",9,-12.216514,20.485065,8.81321,8.875725,"dataâ, analyse, analyses"
296,"discussed about the plan for the next remaining 10 lectures and about group project, learnt about function encoding in the start of class. saw an example of converting y(red, blue, green) with three column to be encoded to a function f(x) with three variables y1, y2, y3. there are two types of problems multiclass and multilabel, based on that we can have the approach ahead. then we saw binary encoding which results in a very compact encoding. we learnt that conversion in detail, how to get the answer. frequency encoding is in which we replace the category values with it's frequency it occurs in dataset. in target encoding, all the r values in the column are replaced by average calculated score above 2.5 in this case. other methods are like label encoding, one hot encoding, image encoding, etc. then we saw feature binning with an example of random spread of data. how to process test data.","discussed about the plan for the next remaining 10 lectures and about group project, learnt about function encoding in the start of class. saw an example of converting y(red, blue, green) with three column to be encoded to a function f(x) with three variables y1, y2, y3. there are two types of problems multiclass and multilabel, based on that we can have the approach ahead. then we saw binary encoding which results in a very compact encoding. we learnt that conversion in detail, how to get the answer. frequency encoding is in which we replace the category values with it's frequency it occurs in dataset. in target encoding, all the r values in the column are replaced by average calculated score above 2.5 in this case. other methods are like label encoding, one hot encoding, image encoding, etc. then we saw feature binning with an example of random spread of data. how to process test data.",3,-41.347397,6.6830173,0.43549198,6.029539,"categorical, categorization, categorise"
297,"today we reviewed the midsem and reviewed the questions such as kde plots, scaling. heat map didnt give much of a indication of the of so we had to find other metrics such as r^2.
then we were taught about the curse of  dimensionality","today we reviewed the midsem and reviewed the questions such as kde plots, scaling. heat map didnt give much of a indication of the of so we had to find other metrics such as r^2. then we were taught about the curse of dimensionality",13,-4.930242,11.088708,9.457012,7.6662655,"classification, classifying, classifications"
298,"we looked at principal component analysis (pca) and variance inflation factor (vif), how they differ and their functions of telling us about the correlation between features, eliminating redundant variables to increase the computation efficiency, and reduce itâ€™s cost. we also looked at t-sne (t-distributed stochastic neighbor embedding) and how it works. we studied this through analysing our mid semester paper","we looked at principal component analysis (pca) and variance inflation factor (vif), how they differ and their functions of telling us about the correlation between features, eliminating redundant variables to increase the computation efficiency, and reduce it s cost. we also looked at t-sne (t-distributed stochastic neighbor embedding) and how it works. we studied this through analysing our mid semester paper",11,-14.396586,0.3641831,10.093716,12.696839,"pca, heatmap, heatmaps"
299,"the basic idea of the session was to introduce the concept of deriving an empirical function which relates input and output. there are various models for classification and regression tasks. some models include simple linear regression, multiple linear regression, logistic regression, random forest etc.â€¨we got to know that machine learning is an application of statistics. â€¨there are four levels of measurement-â€¨1) nominal, which is discrete with no ordering.â€¨ 2) ordinal, which is discrete with inherent ordering in its values. â€¨3) interval, which is continuous but we cannot express the values in terms of ratios. for example, for temperature we canâ€™t say the body at temperature 10 degree celsius is twice as hot as 5 degree celsius. this is due to arbitrary zero reference.
4) ratio, which is continuous. ratios can be defined for them like height, weightâ€¨
we also got to know brief about supervised and semi-supervised learning. 
labels are provided in supervised learning, we just have to obtain the function. â€¨labels are not available for semi supervised learning which we can obtain using clustering methods.â€¨at the end we got to know that we donâ€™t have the information of entire population. we work on samples which is important statistical method.","the basic idea of the session was to introduce the concept of deriving an empirical function which relates input and output. there are various models for classification and regression tasks. some models include simple linear regression, multiple linear regression, logistic regression, random forest etc. we got to know that machine learning is an application of statistics. there are four levels of measurement- 1) nominal, which is discrete with no ordering. 2) ordinal, which is discrete with inherent ordering in its values. 3) interval, which is continuous but we cannot express the values in terms of ratios. for example, for temperature we can t say the body at temperature 10 degree celsius is twice as hot as 5 degree celsius. this is due to arbitrary zero reference. 4) ratio, which is continuous. ratios can be defined for them like height, weight we also got to know brief about supervised and semi-supervised learning. labels are provided in supervised learning, we just have to obtain the function. labels are not available for semi supervised learning which we can obtain using clustering methods. at the end we got to know that we don t have the information of entire population. we work on samples which is important statistical method.",4,-25.994198,-13.695665,1.6142414,0.35469273,"classification, classifying, classifications"
300,"recap:
emphasis on:
all the values in the ci are not really distinct values

what is the probability of getting d(a value far from the .95 ci)- really small
this is statistically significant

why are we worried about zero
-if beta1 is statistically similar to 0 then we are in trouble -> model not appropriate

if i am getting a non zero value in the ci arund 0 still not good

if i calculate a value if beta1 and it lies in the ci around 0-> still similar to 0
---------
mlr:
so far dealt with one independent variable
mlr- more than 1

relevant examples:
photos- pixels- x1, x2, x3, x4â€¦.
body of text need to be converted to a vector before being processed
â€œembedding vectorâ€
if i want to deal with sales processes- (age, earning, location, family size)- x1,x2, x3â€¦. x_k- features

techniques to select/create features- feature engineering

let us say you want to detect vibrations (x1) which is measured in terms of hertz

we measure the frequency, but it is the rate of change of frequency that we are more interested in

dx1/dt - doesnâ€™t exist as measured value- but we have to calculate it-  this is called feature engineering

in case of slr- we calculated coefficients by minimisation of ei_sq- we had closed form solution
but now we must use numerical solutions for mlr as we dont have a closed form solution for b0 b1 b2â€¦
using...
gradient descent method (it is similar in principle to newton raphson)
â€œn-dimensional hypersurfaceâ€
capturing all equations (yi = b0i + b1ix + â€¦) in a matrix
y = x.b + e -> a very compact notation

e_t.e is divided by 2m for convenience 

the gradient descent process
for all future ml models we learn from now on we wont have a closed form solution, so we will start at a random point

solvers- take an objective function- and try to max or min it
f value- variance by regression/variance by random error
how do we evaluate the value of error metrics like mse, rmse etc- we either do it in the context of its own model or in comparison to another

where are the metrics used-
sse mse- optimisation
rmse mae- interpretation

we need to check if errors are random even after the r^2 values comes out to be good enough

which of the variables are more impactful- which cause more change in the value of y when changed by the same amount

but
check if they are statistically significant- p value- can they be called statistically distinct from 0?

so on the basis of p value we start dropping variables-  â€œfeature selectionâ€
we start at the highest p value and start eliminating until we are satisfied (p value < 0.05)

remember- more the number of variables in consideration- r^2 is bound to increase","recap: emphasis on: all the values in the ci are not really distinct values what is the probability of getting d(a value far from the .95 ci)- really small this is statistically significant why are we worried about zero -if beta1 is statistically similar to 0 then we are in trouble -> model not appropriate if i am getting a non zero value in the ci arund 0 still not good if i calculate a value if beta1 and it lies in the ci around 0-> still similar to 0 --------- mlr: so far dealt with one independent variable mlr- more than 1 relevant examples: photos- pixels- x1, x2, x3, x4 . body of text need to be converted to a vector before being processed embedding vector if i want to deal with sales processes- (age, earning, location, family size)- x1,x2, x3 . x_k- features techniques to select/create features- feature engineering let us say you want to detect vibrations (x1) which is measured in terms of hertz we measure the frequency, but it is the rate of change of frequency that we are more interested in dx1/dt - doesn t exist as measured value- but we have to calculate it- this is called feature engineering in case of slr- we calculated coefficients by minimisation of ei_sq- we had closed form solution but now we must use numerical solutions for mlr as we dont have a closed form solution for b0 b1 b2 using... gradient descent method (it is similar in principle to newton raphson) n-dimensional hypersurface capturing all equations (yi = b0i + b1ix + ) in a matrix y = x.b + e -> a very compact notation e_t.e is divided by 2m for convenience the gradient descent process for all future ml models we learn from now on we wont have a closed form solution, so we will start at a random point solvers- take an objective function- and try to max or min it f value- variance by regression/variance by random error how do we evaluate the value of error metrics like mse, rmse etc- we either do it in the context of its own model or in comparison to another where are the metrics used- sse mse- optimisation rmse mae- interpretation we need to check if errors are random even after the r^2 values comes out to be good enough which of the variables are more impactful- which cause more change in the value of y when changed by the same amount but check if they are statistically significant- p value- can they be called statistically distinct from 0? so on the basis of p value we start dropping variables- feature selection we start at the highest p value and start eliminating until we are satisfied (p value < 0.05) remember- more the number of variables in consideration- r^2 is bound to increase",2,14.849473,6.174621,12.553402,3.9978106,"regression, regressions, features"
301,"we began by reading through the summaries and looking at the trends in the data. from the graph, we observed that as the sessions progressed, fewer people were submitting summaries, but the average word count in each summary was increasing. then, we looked at a case where the function was like a sine curve. we started with a single feature, x1. then, we produced additional features like x1^2, x1^3, x1^4 with polynomial regression. this allowed us to calculate a p-value. to enhance the model, we added another feature, sin(x1), and found that the new p-value calculated was smaller than before, so this sine-based feature was adding to some extent. although addition of too many features beyond a certain point may reduce the model's effectiveness, this is reflected in a drop in the adjusted r^2 value. as long as a single model can capture the data well, it is always better than using multiple models unnecessarily. we also covered some significant parametric methods, particularly neural networks. these models have an input layer that accepts features and computational layers that transform the data. when these computations are spread across several layers, it is known as deep learning. the input layer is connected to the computational layers through links, which represent the model's degrees of freedom. however, increasing these degrees of freedom too much can lead to overfitting, making the model less generalizable to new data.","we began by reading through the summaries and looking at the trends in the data. from the graph, we observed that as the sessions progressed, fewer people were submitting summaries, but the average word count in each summary was increasing. then, we looked at a case where the function was like a sine curve. we started with a single feature, x1. then, we produced additional features like x1^2, x1^3, x1^4 with polynomial regression. this allowed us to calculate a p-value. to enhance the model, we added another feature, sin(x1), and found that the new p-value calculated was smaller than before, so this sine-based feature was adding to some extent. although addition of too many features beyond a certain point may reduce the model's effectiveness, this is reflected in a drop in the adjusted r^2 value. as long as a single model can capture the data well, it is always better than using multiple models unnecessarily. we also covered some significant parametric methods, particularly neural networks. these models have an input layer that accepts features and computational layers that transform the data. when these computations are spread across several layers, it is known as deep learning. the input layer is connected to the computational layers through links, which represent the model's degrees of freedom. however, increasing these degrees of freedom too much can lead to overfitting, making the model less generalizable to new data.",0,-2.7053049,1.4657297,8.931832,3.9575536,"models, feature, features"
302,"we started by discussing the meaning of statistical significance. we discussed that if our value of beta lies within the 95% confidence interval, it means that our value depends on the sample we chose. if we chose another sample, the value could be different. so, we arrived at the conclusion that the value is not statistically significant; it is by chance. and if 0 is present in our interval, then beta can be zero by chance. however, if our value is outside that 95% interval, it means it is not by chanceâ€”it is statistically significant. for that, our p-value must be very low, allowing us to reject the null hypothesis that beta is zero. this applies to linear regression.

then we examined multiple linear regression (mlr), where we discussed different statistical parameters obtained using a spreadsheet. we analyzed the significance of each feature by looking at its coefficient. we also learned about f-statistics, which is the variance of error due to the regression model divided by the variance of error due to the error term. finally, we used solver to minimize our loss function, which we formulated using matrices.","we started by discussing the meaning of statistical significance. we discussed that if our value of beta lies within the 95% confidence interval, it means that our value depends on the sample we chose. if we chose another sample, the value could be different. so, we arrived at the conclusion that the value is not statistically significant; it is by chance. and if 0 is present in our interval, then beta can be zero by chance. however, if our value is outside that 95% interval, it means it is not by chance it is statistically significant. for that, our p-value must be very low, allowing us to reject the null hypothesis that beta is zero. this applies to linear regression. then we examined multiple linear regression (mlr), where we discussed different statistical parameters obtained using a spreadsheet. we analyzed the significance of each feature by looking at its coefficient. we also learned about f-statistics, which is the variance of error due to the regression model divided by the variance of error due to the error term. finally, we used solver to minimize our loss function, which we formulated using matrices.",7,34.56111,-0.16795714,14.739471,3.0872471,"statistics, statistical, statisticsâ"
303,"today's session covered variance inflation factor (vif) and the curse of dimensionality in machine learning. vif helps detect multicollinearity in regression models, where a high value indicates strong correlation between features, potentially affecting model stability. the curse of dimensionality describes challenges in high-dimensional data, such as increased sparsity, higher model complexity, computational costs, and distorted distance metrics, making pattern recognition harder. these concepts emphasize the need for careful feature selection and dimensionality reduction to improve model performance.","today's session covered variance inflation factor (vif) and the curse of dimensionality in machine learning. vif helps detect multicollinearity in regression models, where a high value indicates strong correlation between features, potentially affecting model stability. the curse of dimensionality describes challenges in high-dimensional data, such as increased sparsity, higher model complexity, computational costs, and distorted distance metrics, making pattern recognition harder. these concepts emphasize the need for careful feature selection and dimensionality reduction to improve model performance.",13,-8.688878,6.8105145,9.206899,7.3841567,"classification, classifying, classifications"
304,"in today's session we went forward with more statistics.
whatever the distribution of the population, the sampling distribution of the mean will always be a normal distribution.

problem: you have only one sample (eg. 30 observations). in real life, many times it is not possible to get multiple samples. we need to estimate the population mean based only on s1.

for this we are assuming that the sample is representative of the population.

step 1: calculate the sample mean. (assume this mean is close to the population mean)
now sampling distribution of the mean => normal(mu, sigma).
s(xbar) = sigma/sqrt(x) = s/sqrt(x)      
{sigma: population standard deviation ; s: sample standard deviation}

step 2: calculate the sample standard deviation and assume it to be close to population standard deviation.

we want to get the interval within which the population mean is likely to lie.

also, if the number of observations is less than 30 then instead of normal the sampling distribution follows t-distribution.

95% confidence interval:
if you take 100 samples, the mean of 95 of those samples will lie between the lower and upper limits of the interval.

then we discussed about t-value = (x - mu)/standard deviation

p-value:
y = b0 + b1*x
if b1 is statistically equal to zero, then we do not have a regression.
if b1 lies in the 95% confidence interval of normal distribution with mean=0 and sigma=same as that of the sampling distribution of b1 then it is statistically equal to zero.

p-value should be less than 0.05.

multiple linear regression:
y = b0 + b1*x1 + b2*x2 +...+bk*xk

anova: is used to compare statistical equivalence of ""multiple averages"" simultaneously.
 
=> f-statistic = msr/mse    (ideally this value should be large).","in today's session we went forward with more statistics. whatever the distribution of the population, the sampling distribution of the mean will always be a normal distribution. problem: you have only one sample (eg. 30 observations). in real life, many times it is not possible to get multiple samples. we need to estimate the population mean based only on s1. for this we are assuming that the sample is representative of the population. step 1: calculate the sample mean. (assume this mean is close to the population mean) now sampling distribution of the mean => normal(mu, sigma). s(xbar) = sigma/sqrt(x) = s/sqrt(x) {sigma: population standard deviation ; s: sample standard deviation} step 2: calculate the sample standard deviation and assume it to be close to population standard deviation. we want to get the interval within which the population mean is likely to lie. also, if the number of observations is less than 30 then instead of normal the sampling distribution follows t-distribution. 95% confidence interval: if you take 100 samples, the mean of 95 of those samples will lie between the lower and upper limits of the interval. then we discussed about t-value = (x - mu)/standard deviation p-value: y = b0 + b1*x if b1 is statistically equal to zero, then we do not have a regression. if b1 lies in the 95% confidence interval of normal distribution with mean=0 and sigma=same as that of the sampling distribution of b1 then it is statistically equal to zero. p-value should be less than 0.05. multiple linear regression: y = b0 + b1*x1 + b2*x2 +...+bk*xk anova: is used to compare statistical equivalence of ""multiple averages"" simultaneously. => f-statistic = msr/mse (ideally this value should be large).",7,37.816067,2.6722972,15.433517,2.4648223,"statistics, statistical, statisticsâ"
305,"we started our discussion with the classification evaluation metrics. then, we visited a website to visualize different data distributions and their classification boundaries by adjusting nodes and features for a neural network. after that, we moved to the notebook for logistic regression, where we loaded the data and converted it into a dataframe. we analyzed the classification performance using different evaluation metrics, as accuracy alone is not sufficient to validate our model's performance. we also looked at other evaluation metrics such as the confusion matrix, f1 score, recall, and accuracy. 
next, we examined the roc curve, which shows how many false positives our classifier detects before identifying true positives. the area under the roc curve (auc-roc) indicates the quality of our classifier. if the auc-roc value is less than or equal to 0.5, the classifier performs no better than a random guess.
this was for binary classification. we then moved on to multiclass classifiers, where we examined the classification evaluation scores and the confusion matrix for each class. we also looked at the auc-roc curve for each class and compared the performance of our classifier across different classes.
we started discussing clustering methods, which are unsupervised machine learning techniques. first, we explored the hierarchical clustering method. we plotted a dendrogram to analyze the closeness of data points and determine how to divide them into clusters. then, we moved on to the k-means clustering method, where we discussed how k-means assigns data points to clusters.","we started our discussion with the classification evaluation metrics. then, we visited a website to visualize different data distributions and their classification boundaries by adjusting nodes and features for a neural network. after that, we moved to the notebook for logistic regression, where we loaded the data and converted it into a dataframe. we analyzed the classification performance using different evaluation metrics, as accuracy alone is not sufficient to validate our model's performance. we also looked at other evaluation metrics such as the confusion matrix, f1 score, recall, and accuracy. next, we examined the roc curve, which shows how many false positives our classifier detects before identifying true positives. the area under the roc curve (auc-roc) indicates the quality of our classifier. if the auc-roc value is less than or equal to 0.5, the classifier performs no better than a random guess. this was for binary classification. we then moved on to multiclass classifiers, where we examined the classification evaluation scores and the confusion matrix for each class. we also looked at the auc-roc curve for each class and compared the performance of our classifier across different classes. we started discussing clustering methods, which are unsupervised machine learning techniques. first, we explored the hierarchical clustering method. we plotted a dendrogram to analyze the closeness of data points and determine how to divide them into clusters. then, we moved on to the k-means clustering method, where we discussed how k-means assigns data points to clusters.",8,-3.2022414,-18.623371,6.6436343,0.65272105,"classification, clusterings, classifying"
306,"in todayâ€™s session, we covered key statistical concepts and techniques used in data analysis and machine learning:

95% confidence interval & interpretation â€“ we discussed the 95% confidence interval for î²â‚ (beta1), which helps us understand the range in which the true value of a predictorâ€™s coefficient is likely to fall, with 95% confidence. this means if we repeat the sampling process multiple times, 95 out of 100 times, the coefficient will lie within this interval.

multiple linear regression â€“ this technique extends simple linear regression by using multiple independent variables to predict a dependent variable. it allows us to analyze the combined effect of multiple factors on an outcome, making it more powerful for real-world data analysis.

feature engineering â€“ we learned how to improve model performance by transforming raw data into meaningful features. this includes techniques like encoding categorical variables, scaling numerical data, and creating interaction terms to enhance predictive accuracy.","in today s session, we covered key statistical concepts and techniques used in data analysis and machine learning: 95% confidence interval & interpretation we discussed the 95% confidence interval for (beta1), which helps us understand the range in which the true value of a predictor s coefficient is likely to fall, with 95% confidence. this means if we repeat the sampling process multiple times, 95 out of 100 times, the coefficient will lie within this interval. multiple linear regression this technique extends simple linear regression by using multiple independent variables to predict a dependent variable. it allows us to analyze the combined effect of multiple factors on an outcome, making it more powerful for real-world data analysis. feature engineering we learned how to improve model performance by transforming raw data into meaningful features. this includes techniques like encoding categorical variables, scaling numerical data, and creating interaction terms to enhance predictive accuracy.",13,21.580627,6.217548,12.7457905,5.119302,"classification, classifying, classifications"
307,"we examined the summaries and saw that the average word count was rising over time, despite a decline in submissions. next, we modified a sine-like function for polynomial regression by adding characteristics like x1â², x1â³, and x1â´. the p-value decreased when a sine characteristic was added, demonstrating its importance. a single, high-performing model is better, though, as adding too many characteristics without making the model better might reduce the adjusted r2 score.

we also talked about neural networks: deep learning represents numerous levels, an input layer links to one or more layers of computation, and overfitting can occur when the number of degrees of freedom is increased.","we examined the summaries and saw that the average word count was rising over time, despite a decline in submissions. next, we modified a sine-like function for polynomial regression by adding characteristics like x1 , x1 , and x1 . the p-value decreased when a sine characteristic was added, demonstrating its importance. a single, high-performing model is better, though, as adding too many characteristics without making the model better might reduce the adjusted r2 score. we also talked about neural networks: deep learning represents numerous levels, an input layer links to one or more layers of computation, and overfitting can occur when the number of degrees of freedom is increased.",0,-3.5298667,1.994032,8.822265,3.8542528,"models, feature, features"
308,"in today's lecture, we built on our previous discussion about the distinction between a population and a sample. additionally, we delved deeper into simple linear regression (slr), exploring its concepts and applications. below is a detailed summary of the topics covered.

1. sample vs population
    1. sample is a part of the population which we use to predict the parameters of population.
    2. it should be a good representation of the population otherwise parameter estimates would be way off the mark.
    
    then we defined some formal terms for population and sample.
    
    properties of samples are called â€œstatisticsâ€
    
    properties of samples are called â€œparametersâ€
    
    we use statistics to estimate parameters.
    
2. then we delved into what operations can be applied on data captured via different scales of measurement
    1. nominal / ordinal: you can perform operations like counting, addition, subtraction
    2. interval / ratio: you can perform operations like counting, addition, subtraction, mean, median, mode, standard deviation, variance etc.
3. simple linear regression (slr)
    1. when we have one independent variable(x) and one dependent variable(y)
    2. independent variable(x): feature, predictor
    3. dependent variable(y): response variable, label
    4. we had a dataset of 100 points, plotted a scatter plot and observed that y is linearly dependent on x and decided to use slr (a line of form: a + bx) to solve the problem.
    5. reason for this offset â€˜aâ€™ is the unaccounted features which affects the target variable. we can also say that magnitude of bias = sum of all unaccounted feature in the system
    6. we discussed how parameter estimates vary depending on sample variability. to address this, we need to use a sample set that accurately represents the population, or our parameter estimates will be inaccurate.
    7. the parameter estimates (a, b) for sure will not be same as true population parameter. thus we use the concept of confidence interval where we find the probability of true population parameter in some range around the parameter estimates.
    8. finally, we explored how to find the best-fit straight line by examining various loss functions including manhattan distance, euclidean distance, and modular distance. we discussed where each technique is most applicable. for line fitting, we used euclidean distance and reviewed the derivation for estimating parameters (a, b). in the case of simple linear regression, we discovered a closed-form solution for these parameters.","in today's lecture, we built on our previous discussion about the distinction between a population and a sample. additionally, we delved deeper into simple linear regression (slr), exploring its concepts and applications. below is a detailed summary of the topics covered. 1. sample vs population 1. sample is a part of the population which we use to predict the parameters of population. 2. it should be a good representation of the population otherwise parameter estimates would be way off the mark. then we defined some formal terms for population and sample. properties of samples are called statistics properties of samples are called parameters we use statistics to estimate parameters. 2. then we delved into what operations can be applied on data captured via different scales of measurement 1. nominal / ordinal: you can perform operations like counting, addition, subtraction 2. interval / ratio: you can perform operations like counting, addition, subtraction, mean, median, mode, standard deviation, variance etc. 3. simple linear regression (slr) 1. when we have one independent variable(x) and one dependent variable(y) 2. independent variable(x): feature, predictor 3. dependent variable(y): response variable, label 4. we had a dataset of 100 points, plotted a scatter plot and observed that y is linearly dependent on x and decided to use slr (a line of form: a + bx) to solve the problem. 5. reason for this offset a is the unaccounted features which affects the target variable. we can also say that magnitude of bias = sum of all unaccounted feature in the system 6. we discussed how parameter estimates vary depending on sample variability. to address this, we need to use a sample set that accurately represents the population, or our parameter estimates will be inaccurate. 7. the parameter estimates (a, b) for sure will not be same as true population parameter. thus we use the concept of confidence interval where we find the probability of true population parameter in some range around the parameter estimates. 8. finally, we explored how to find the best-fit straight line by examining various loss functions including manhattan distance, euclidean distance, and modular distance. we discussed where each technique is most applicable. for line fitting, we used euclidean distance and reviewed the derivation for estimating parameters (a, b). in the case of simple linear regression, we discovered a closed-form solution for these parameters.",1,32.945198,-6.5659328,16.246574,3.8875687,"population, models, estimating"
309,"we have learned why not to use whole sample data for training itself so basically if we have a large dataset we can use 80,20% for smaller datasets can use 90,10% in a randomized way from these 2 sets we get training matrix, testing matrix upon comparing the r^2 value if they are close to each other it is a general model and a fairly good data if separated by each other then it is a overfit situation.we also learned that with increase no. of variables there is more scope for the variance to be captured and improve r^2 then we saw why n-1 term in sse in unbiased as we know mean  total unknowns drops by 1then we have a handsome experience of libraries like sklearn on jupyter notebook.","we have learned why not to use whole sample data for training itself so basically if we have a large dataset we can use 80,20% for smaller datasets can use 90,10% in a randomized way from these 2 sets we get training matrix, testing matrix upon comparing the r^2 value if they are close to each other it is a general model and a fairly good data if separated by each other then it is a overfit situation.we also learned that with increase no. of variables there is more scope for the variance to be captured and improve r^2 then we saw why n-1 term in sse in unbiased as we know mean total unknowns drops by 1then we have a handsome experience of libraries like sklearn on jupyter notebook.",2,7.197081,5.329729,10.783466,4.9434905,"regression, regressions, features"
310,"in today's lecture, we learnt about how to find regression coefficients beta0 and beta1 in excel from a data given in csv file using simple excel formulas of average, square, etc. after calculating beta0 and beta1, we calculated y_cap(estimated parameter) and e_i(error = yi-yi_cap), also plotted scatter plot for xi vs yi and fitted the best fit linear line in it using excel. also plotted scattered plot for error e_i. then plotted a histogram for the errors. if the error histogram looks like random or normal distribution then only our estimation or linear regression is useful, else if error histogram looks somewhat different from random distribution then the estimation using linear regression is said to be not good enough. we also calculated the statistics for the regression using the add-in named data analysis toolpak on excel for that given data which directly gives us all the things and coefficients for simple linear regression. sum of square of errors (sse) was also discussed, that best fit line can also be obtained by minimizing sse. at last we also came to know that how correlation coefficient (r) and coefficient of determination(râ²) are related i.e, (r)â²=(râ²), and, what are the formulas of r and râ²,etc.","in today's lecture, we learnt about how to find regression coefficients beta0 and beta1 in excel from a data given in csv file using simple excel formulas of average, square, etc. after calculating beta0 and beta1, we calculated y_cap(estimated parameter) and e_i(error = yi-yi_cap), also plotted scatter plot for xi vs yi and fitted the best fit linear line in it using excel. also plotted scattered plot for error e_i. then plotted a histogram for the errors. if the error histogram looks like random or normal distribution then only our estimation or linear regression is useful, else if error histogram looks somewhat different from random distribution then the estimation using linear regression is said to be not good enough. we also calculated the statistics for the regression using the add-in named data analysis toolpak on excel for that given data which directly gives us all the things and coefficients for simple linear regression. sum of square of errors (sse) was also discussed, that best fit line can also be obtained by minimizing sse. at last we also came to know that how correlation coefficient (r) and coefficient of determination(r ) are related i.e, (r) =(r ), and, what are the formulas of r and r ,etc.",5,19.533815,-3.6405385,14.051975,5.098512,"regression, statistical, statistics"
311,in today's lecture we started transitioning from excel to python. we came across a python library called sklearn or scikit-learn which provides us a lot of machine learning algorithms on our dataset efficiently. we also learnt that error scores on it's own are not that good indicator for example if we just take squared errors it is not a good identifier of the fit of the model as more the data points more will be the error hence we should use mean of squared errors or infact root of mean of squared errors which will be a better indicator. also that r2 score is also not perfect every time and we need to incorporate number of features and number of data points by which we calculate the adjusted r2 score which is a better indicator. after that we fit a linear regression model on a dataset resulting plot of which didn't showed up as straight line meaning linear regression doesn't just mean a line it can be a polynomial function too. we also touched the topic of train and test data that whenever working on a dataset we should always split it into training and test data preferably in 80:20 ratio. we also discussed that the model efficiency also depends upon the dataset we have like it is possible that on a problem today a linear model might be a good model but after some time when the data points change completely some other model say decision trees might outperform.,in today's lecture we started transitioning from excel to python. we came across a python library called sklearn or scikit-learn which provides us a lot of machine learning algorithms on our dataset efficiently. we also learnt that error scores on it's own are not that good indicator for example if we just take squared errors it is not a good identifier of the fit of the model as more the data points more will be the error hence we should use mean of squared errors or infact root of mean of squared errors which will be a better indicator. also that r2 score is also not perfect every time and we need to incorporate number of features and number of data points by which we calculate the adjusted r2 score which is a better indicator. after that we fit a linear regression model on a dataset resulting plot of which didn't showed up as straight line meaning linear regression doesn't just mean a line it can be a polynomial function too. we also touched the topic of train and test data that whenever working on a dataset we should always split it into training and test data preferably in 80:20 ratio. we also discussed that the model efficiency also depends upon the dataset we have like it is possible that on a problem today a linear model might be a good model but after some time when the data points change completely some other model say decision trees might outperform.,0,4.8553357,-0.16528633,10.287361,4.464185,"models, feature, features"
312,"we learned multiple linear regression where there are more that one features. feature selection is important for better models and can be done using domain knowledge.
error metric like rmse and mae have same unit as data so they can be interpreted easily while others like mse are helpful in optimisation.
mlr do have a closed form solution but it is expensive to calculate so we use techniques like gradient descent to find approximate solutions to the precision we want. we start from random solution and make updates to the solution.",we learned multiple linear regression where there are more that one features. feature selection is important for better models and can be done using domain knowledge. error metric like rmse and mae have same unit as data so they can be interpreted easily while others like mse are helpful in optimisation. mlr do have a closed form solution but it is expensive to calculate so we use techniques like gradient descent to find approximate solutions to the precision we want. we start from random solution and make updates to the solution.,0,3.918824,-7.273086,9.730718,4.1145396,"models, feature, features"
313,"today we started the discussion with feature encoding methods. when either of the dependent or independent variables are categorical we have to use feature encoding techniques to convert them into numerical forms with which we can work with. 
one hot encoding method creates columns which are equal to the number of unique categorical labels in the original data and fills each of these columns with respective true(1) and false(0)  values. this method should be used very cautiously as it increases the dimensionality of the data significantly and becomes cumbersome to work with. for one hot encoding treated data, the classification models should be tree based or neural network based as logistic regression cannot handle such data. to overcome the curse of dimensionality of one hot encoding we can follow another method which is similar to onh but uses bits instead of true or false values to encode the categories for example for categories of weather related data like hot, very hot the encodings can be 001 and 011 respectively. other most widely used data encoding method is label encoding in this integers are allotted to different categories present in the columns this is best suited for nominal scale entries of the columns as in this method there is no ordering of the integer values that is for example red:1 and blue:2 does not mean that blue is greater than red and its also important to note that this method works well for y and using this method for encoding x features must be avoided. integer encoding is another type of encoding which is very similar to label encoding but the only difference is the integer values have a sense of ordering and thus is suitable for ordinal scale valued columns. frequency encoding uses the number of occurrences of  particular category in a column and sets as the label for that particular category should only be used for features and not target variables, because there might be possible that two different categories might have same frequency but this issue some how doesn't affect the feature variables and thus can be used. target encoding this also used for encoding the feature variables where for a particular category all the corresponding y values are noted and then average of these noted values is set as the category label. we then discussed feature binning where the features with continuous values in discretised by making bins or categories so that the regression problem can be converted into classification problem , such conversion might be very useful when the actual data variance is very high and the r squared value of the regression model is very low. began discussion about processing text and how the text data is converted into numerical data which can be used to do some useful analysis. the common stop words are removed and dictionary of all the other words is made and a vector is associated to each sentence of the document based on whether the given word from the dictionary made is present in that particular sentence or not.","today we started the discussion with feature encoding methods. when either of the dependent or independent variables are categorical we have to use feature encoding techniques to convert them into numerical forms with which we can work with. one hot encoding method creates columns which are equal to the number of unique categorical labels in the original data and fills each of these columns with respective true(1) and false(0) values. this method should be used very cautiously as it increases the dimensionality of the data significantly and becomes cumbersome to work with. for one hot encoding treated data, the classification models should be tree based or neural network based as logistic regression cannot handle such data. to overcome the curse of dimensionality of one hot encoding we can follow another method which is similar to onh but uses bits instead of true or false values to encode the categories for example for categories of weather related data like hot, very hot the encodings can be 001 and 011 respectively. other most widely used data encoding method is label encoding in this integers are allotted to different categories present in the columns this is best suited for nominal scale entries of the columns as in this method there is no ordering of the integer values that is for example red:1 and blue:2 does not mean that blue is greater than red and its also important to note that this method works well for y and using this method for encoding x features must be avoided. integer encoding is another type of encoding which is very similar to label encoding but the only difference is the integer values have a sense of ordering and thus is suitable for ordinal scale valued columns. frequency encoding uses the number of occurrences of particular category in a column and sets as the label for that particular category should only be used for features and not target variables, because there might be possible that two different categories might have same frequency but this issue some how doesn't affect the feature variables and thus can be used. target encoding this also used for encoding the feature variables where for a particular category all the corresponding y values are noted and then average of these noted values is set as the category label. we then discussed feature binning where the features with continuous values in discretised by making bins or categories so that the regression problem can be converted into classification problem , such conversion might be very useful when the actual data variance is very high and the r squared value of the regression model is very low. began discussion about processing text and how the text data is converted into numerical data which can be used to do some useful analysis. the common stop words are removed and dictionary of all the other words is made and a vector is associated to each sentence of the document based on whether the given word from the dictionary made is present in that particular sentence or not.",3,-43.70188,2.9461772,-0.021809366,6.459658,"categorical, categorization, categorise"
314,"sir started the class with a heat map of features extracted from summaries and pointed out an outlier which was a segway into data understanding and preparation from raw data. sir discussed the crisp-dm operation procedure for working with data. further discussed exploratory data analysis which is an approach used in stats to analyse and investigate datasets with the goal of summarising their main characteristics and trends. sir then discussed a mind map of the approach over data analysis. one of the tas took over the lecture and explained exploratory data analysis via an example of a sample from adiabatic population. bhaiya started with plotting histograms and box plots to get a gist of data and further discussed matrix plot showing inter feature relations. here it shows correlation between only two features but when a feature depends on more than one other features, this relationship isn't recognised. then a dataset on temperature variation with time was discussed and sir explained how missing data problem can be fixed by following trends from the past. further a ganga water quality database followed by air quality data of powai were discussed to show missing data. methods like using mean, mode of rest of the data in place of missing values can be used to avoid dropping those observations. more sophisticated methods like use of knn, mice, interpolation to determine the missing values. further we moved on to discuss outliers in the dataset by considering a simple linear regression model. here different methods for finding outliers and eliminating them were discussed like quantiles here emphasis was put on why median is a better measure to put out outliers. the class ended on how these methods are to be judiciously used depending upon the context of data being analysied","sir started the class with a heat map of features extracted from summaries and pointed out an outlier which was a segway into data understanding and preparation from raw data. sir discussed the crisp-dm operation procedure for working with data. further discussed exploratory data analysis which is an approach used in stats to analyse and investigate datasets with the goal of summarising their main characteristics and trends. sir then discussed a mind map of the approach over data analysis. one of the tas took over the lecture and explained exploratory data analysis via an example of a sample from adiabatic population. bhaiya started with plotting histograms and box plots to get a gist of data and further discussed matrix plot showing inter feature relations. here it shows correlation between only two features but when a feature depends on more than one other features, this relationship isn't recognised. then a dataset on temperature variation with time was discussed and sir explained how missing data problem can be fixed by following trends from the past. further a ganga water quality database followed by air quality data of powai were discussed to show missing data. methods like using mean, mode of rest of the data in place of missing values can be used to avoid dropping those observations. more sophisticated methods like use of knn, mice, interpolation to determine the missing values. further we moved on to discuss outliers in the dataset by considering a simple linear regression model. here different methods for finding outliers and eliminating them were discussed like quantiles here emphasis was put on why median is a better measure to put out outliers. the class ended on how these methods are to be judiciously used depending upon the context of data being analysied",9,-13.670699,15.057589,8.864827,8.511533,"dataâ, analyse, analyses"
315,"in today's session we we started with pivot tables and how to use them to draw inferences from the data using min, max, std dev etc. it's a necesssary part of the exploratory data analysis. then we saw how to see how many outliers we have and how they vary within the data by using line plots, histograms, scatter plots and box plots. the decision boundary which is usually between q1-1.5iqr and q3+1.5iqr can be changed according to the data and its just a common practice to use 1.5 and we could have higher or lower number than that according to the data we have. if in our data we have large difference between mean and median, the mean is severely affected by the outliers and hence it indicates that the outliers exist in the data. outliers are not always insignificant and they may be telling crucial exceptions that may be occuring in our system and its useful that way in telling us about the potential failures in sensors and other things. later we discussed two industry projects where eda was heavily utilised in getting insights from the data and raising necessary questions which would later be utilised to solve the data on a large scale.","in today's session we we started with pivot tables and how to use them to draw inferences from the data using min, max, std dev etc. it's a necesssary part of the exploratory data analysis. then we saw how to see how many outliers we have and how they vary within the data by using line plots, histograms, scatter plots and box plots. the decision boundary which is usually between q1-1.5iqr and q3+1.5iqr can be changed according to the data and its just a common practice to use 1.5 and we could have higher or lower number than that according to the data we have. if in our data we have large difference between mean and median, the mean is severely affected by the outliers and hence it indicates that the outliers exist in the data. outliers are not always insignificant and they may be telling crucial exceptions that may be occuring in our system and its useful that way in telling us about the potential failures in sensors and other things. later we discussed two industry projects where eda was heavily utilised in getting insights from the data and raising necessary questions which would later be utilised to solve the data on a large scale.",9,-11.124129,19.188026,9.110063,9.191809,"dataâ, analyse, analyses"
316,"we studied the following statistical concepts: population and sample means alongside confidence intervals, which led us to understanding the standard error of means, written as ðœ‡ = ðœž/âˆšð‘›. from these, we understood that the normal distribution is applicable wherein the average of multiple sample means is equal to the population mean. in these analysis, the p-value has been a key variable for assessing the value of probabilities and guiding feature selection. we determined the intercept, slope as well as their respective coefficients which explain the relationship between the variablesâ€™ relationship. moreover, the f-statistic which addresses variance that has been accounted for was alongside the anova table â€“ in relation to the significance testing of average group differences. also, the lack of significance stems from the misunderstanding of the basic principles. this therefore bridges the gap between statistical concepts and how to prepare for the next class which involves multiple regression models.","we studied the following statistical concepts: population and sample means alongside confidence intervals, which led us to understanding the standard error of means, written as = / . from these, we understood that the normal distribution is applicable wherein the average of multiple sample means is equal to the population mean. in these analysis, the p-value has been a key variable for assessing the value of probabilities and guiding feature selection. we determined the intercept, slope as well as their respective coefficients which explain the relationship between the variables relationship. moreover, the f-statistic which addresses variance that has been accounted for was alongside the anova table in relation to the significance testing of average group differences. also, the lack of significance stems from the misunderstanding of the basic principles. this therefore bridges the gap between statistical concepts and how to prepare for the next class which involves multiple regression models.",7,32.912556,5.0530133,14.705307,2.6401844,"statistics, statistical, statisticsâ"
317,"we explored three ways to enhance model performance: improving the sample, choosing better methods, and fine-tuning models. polynomial regression and feature engineering were introduced, covering forward and backward selection for optimizing features.

the modeling workflow was outlined: data collection, preprocessing, model selection, and evaluation. among linear regression, svm regression, and random forest, random forest performed best but was noted as unsuitable for parametric tasks like delta analysis.

neural networks were introduced as parametric models with input, hidden, and output layers, transitioning into deep learning as layers increase. model evaluation used râ² and mse to detect overfitting.

finally, we discussed classification, distinguishing supervised learning (where both input and output data are known) and logistic regression, which defines decision boundaries using the sigmoid function.","we explored three ways to enhance model performance: improving the sample, choosing better methods, and fine-tuning models. polynomial regression and feature engineering were introduced, covering forward and backward selection for optimizing features. the modeling workflow was outlined: data collection, preprocessing, model selection, and evaluation. among linear regression, svm regression, and random forest, random forest performed best but was noted as unsuitable for parametric tasks like delta analysis. neural networks were introduced as parametric models with input, hidden, and output layers, transitioning into deep learning as layers increase. model evaluation used r and mse to detect overfitting. finally, we discussed classification, distinguishing supervised learning (where both input and output data are known) and logistic regression, which defines decision boundaries using the sigmoid function.",0,-2.088829,-5.6835303,8.903991,4.5172877,"models, feature, features"
318,"in this lecture, we continued our discussion on eda. we learnt how to create pivot tables in excel, which summarize the entire data into a table, whose entries can be chosen by us. this will help us create a summary of the data and understand the various trends and patterns in the data. using the pivot table entries, one can then create plots to analyze the distribution/ pattern in the data. this can be used to predict any desired value in future. the example which we discussed in class was that of the average number of characters in the summary on any particular day. this can be useful to predict the approximate number of characters in the summary of any day in the future.
we can use various plots including- box plots, scatter plots, histograms, as well as descriptive statistics function in excel to determine the outliers in the data set. any of these can be used to reach out to a conclusion regarding the outliers. also, it is not always correct to completely disregard the outliers, as they may also reveal certain important problems, patterns in the data, which cannot be ignored.
eda includes all these tasks of cleaning, comprehending, analyzing the data and extracting valuable insights from it, which can be further used to build a model. 
apart from this, we also looked at two case studies. one was regarding the measurements of numerous different parameters related to a process in chemical plant, with the measurements collected on daily basis. we created a pivot table which included the various parameters in the columns and the dates in the rows. then we added the values of- count of the total measurements made in that year, the average, max, min of these measurements for each of the parameter. we observed in some cases, the min value of the parameter dropped to 0, which cannot be practically possible, as we cannot have 0 pressure or temperature. the problem occurred due to the missing values of these parameters on some days.
the next case study was about the creation of energy using solar radiation. the oil and water temperatures were recorded at different time intervals, and the data was available. various line plots were created to analyze the variation in these temperatures at different times of the day and further on different days.
correlation maps were also made to understand the correlation between the various independent variables. there were certain patterns in the temperatures, from birdâ€™s eye point of view. however, more distinct patterns were observed within these large variations, when looked through a zoomed in perspective, suggesting hour-to-hour as well as daily variations in the temperatures. all these plots could be analyzed, to help us understand how we can separate the noise from the signals, identify the outliers, and use them to predict the values of the output variable in the future.","in this lecture, we continued our discussion on eda. we learnt how to create pivot tables in excel, which summarize the entire data into a table, whose entries can be chosen by us. this will help us create a summary of the data and understand the various trends and patterns in the data. using the pivot table entries, one can then create plots to analyze the distribution/ pattern in the data. this can be used to predict any desired value in future. the example which we discussed in class was that of the average number of characters in the summary on any particular day. this can be useful to predict the approximate number of characters in the summary of any day in the future. we can use various plots including- box plots, scatter plots, histograms, as well as descriptive statistics function in excel to determine the outliers in the data set. any of these can be used to reach out to a conclusion regarding the outliers. also, it is not always correct to completely disregard the outliers, as they may also reveal certain important problems, patterns in the data, which cannot be ignored. eda includes all these tasks of cleaning, comprehending, analyzing the data and extracting valuable insights from it, which can be further used to build a model. apart from this, we also looked at two case studies. one was regarding the measurements of numerous different parameters related to a process in chemical plant, with the measurements collected on daily basis. we created a pivot table which included the various parameters in the columns and the dates in the rows. then we added the values of- count of the total measurements made in that year, the average, max, min of these measurements for each of the parameter. we observed in some cases, the min value of the parameter dropped to 0, which cannot be practically possible, as we cannot have 0 pressure or temperature. the problem occurred due to the missing values of these parameters on some days. the next case study was about the creation of energy using solar radiation. the oil and water temperatures were recorded at different time intervals, and the data was available. various line plots were created to analyze the variation in these temperatures at different times of the day and further on different days. correlation maps were also made to understand the correlation between the various independent variables. there were certain patterns in the temperatures, from bird s eye point of view. however, more distinct patterns were observed within these large variations, when looked through a zoomed in perspective, suggesting hour-to-hour as well as daily variations in the temperatures. all these plots could be analyzed, to help us understand how we can separate the noise from the signals, identify the outliers, and use them to predict the values of the output variable in the future.",9,-9.149941,24.878586,7.888004,10.283539,"dataâ, analyse, analyses"
319,"the notes focus on estimating the population mean using a single sample and the principles of linear and multiple linear regression. to estimate the population mean, the first step is to calculate the sample mean and assume it is close to the true population mean. the sampling distribution of the mean is used, which follows a normal distribution for large samples. the standard error, which measures the variability of the sample mean, is calculated by dividing the sample standard deviation by the square root of the sample size. confidence intervals are then used to estimate the range within which the population mean is likely to lie. for smaller sample sizes, the t-distribution is used instead of the normal distribution to account for additional uncertainty.

the notes also delve into linear regression, which models the relationship between a dependent variable and an independent variable. the regression equation is represented as ( y = î²_0 + î²_1x ), where ( î²_0 ) is the intercept and ( î²_1) is the slope. to determine if the regression model is valid, the significance of ( î²_1) is checked. if the confidence interval for ( î²_1 ) includes zero, the regression is not statistically significant. a low p-value, typically less than 0.05, suggests that ( î²_1 ) is significantly different from zero, validating the regression model.

in multiple linear regression, the model expands to include multiple independent variables. the equation is ( y = î²_0 + î²_1x_1 + î²_2x_2 + ... + î²_kx_k ). anova, or analysis of variance, is used to assess the significance of the model by comparing the variability explained by the regression to the variability due to error. this is done using the f-statistic, which is the ratio of the mean square regression to the mean square error. a significant f-statistic indicates that the model is a good fit for the data.","the notes focus on estimating the population mean using a single sample and the principles of linear and multiple linear regression. to estimate the population mean, the first step is to calculate the sample mean and assume it is close to the true population mean. the sampling distribution of the mean is used, which follows a normal distribution for large samples. the standard error, which measures the variability of the sample mean, is calculated by dividing the sample standard deviation by the square root of the sample size. confidence intervals are then used to estimate the range within which the population mean is likely to lie. for smaller sample sizes, the t-distribution is used instead of the normal distribution to account for additional uncertainty. the notes also delve into linear regression, which models the relationship between a dependent variable and an independent variable. the regression equation is represented as ( y = _0 + _1x ), where ( _0 ) is the intercept and ( _1) is the slope. to determine if the regression model is valid, the significance of ( _1) is checked. if the confidence interval for ( _1 ) includes zero, the regression is not statistically significant. a low p-value, typically less than 0.05, suggests that ( _1 ) is significantly different from zero, validating the regression model. in multiple linear regression, the model expands to include multiple independent variables. the equation is ( y = _0 + _1x_1 + _2x_2 + ... + _kx_k ). anova, or analysis of variance, is used to assess the significance of the model by comparing the variability explained by the regression to the variability due to error. this is done using the f-statistic, which is the ratio of the mean square regression to the mean square error. a significant f-statistic indicates that the model is a good fit for the data.",7,32.783195,1.5632405,14.670941,3.1613162,"statistics, statistical, statisticsâ"
320,"the lecture covered logistic regression, a classification technique using a sigmoid function to predict probabilities. we explored clustering methods to group data points and discussed true positives and negatives in model evaluation. outlier detection techniques were examined for identifying anomalies in datasets. loss functions, which measure model errors, were explained in the context of optimization. finally, we reviewed methods for solving logistic regression, including gradient descent.","the lecture covered logistic regression, a classification technique using a sigmoid function to predict probabilities. we explored clustering methods to group data points and discussed true positives and negatives in model evaluation. outlier detection techniques were examined for identifying anomalies in datasets. loss functions, which measure model errors, were explained in the context of optimization. finally, we reviewed methods for solving logistic regression, including gradient descent.",13,-6.23481,-8.854705,8.439336,5.307045,"classification, classifying, classifications"
321,"simple linear regression in excel
	1.	objective: to find the relationship between two variables (dependent and independent) using a straight-line equation:

y = mx + b

where  m  is the slope and  b  is the y-intercept.
	2.	steps in excel:
	â€¢	input data in two columns: one for the independent variable ( x ) and one for the dependent variable ( y ).
	â€¢	use excelâ€™s data analysis toolpak:
	â€¢	go to data > data analysis > regression.
	â€¢	select the input ranges for  x  and  y .
	â€¢	excel generates outputs including the regression equation, r-squared value, and anova table.
	â€¢	alternatively, use the linest function for direct equation parameters.

error metrics

error metrics measure the accuracy of the regression modelâ€™s predictions.
	1.	sum of squares error (sse):

sse = \sum_{i=1}^n (y_i - \hat{y}_i)^2

	â€¢	measures total squared error between actual ( y_i ) and predicted ( \hat{y}_i ) values.
	â€¢	lower values indicate a better fit.
	2.	mean squared error (mse):

mse = \frac{sse}{n}

	â€¢	average squared error across all data points.
	â€¢	penalizes larger errors more due to squaring.
	3.	root mean squared error (rmse):

rmse = \sqrt{mse}

	â€¢	provides an error metric in the same units as the dependent variable ( y ).
	â€¢	useful for comparing models.
	4.	mean absolute error (mae):

mae = \frac{\sum_{i=1}^n |y_i - \hat{y}_i|}{n}

	â€¢	average of absolute errors.
	â€¢	less sensitive to outliers compared to mse and rmse.","simple linear regression in excel 1. objective: to find the relationship between two variables (dependent and independent) using a straight-line equation: y = mx + b where m is the slope and b is the y-intercept. 2. steps in excel: input data in two columns: one for the independent variable ( x ) and one for the dependent variable ( y ). use excel s data analysis toolpak: go to data > data analysis > regression. select the input ranges for x and y . excel generates outputs including the regression equation, r-squared value, and anova table. alternatively, use the linest function for direct equation parameters. error metrics error metrics measure the accuracy of the regression model s predictions. 1. sum of squares error (sse): sse = \sum_{i=1}^n (y_i - \hat{y}_i)^2 measures total squared error between actual ( y_i ) and predicted ( \hat{y}_i ) values. lower values indicate a better fit. 2. mean squared error (mse): mse = \frac{sse}{n} average squared error across all data points. penalizes larger errors more due to squaring. 3. root mean squared error (rmse): rmse = \sqrt{mse} provides an error metric in the same units as the dependent variable ( y ). useful for comparing models. 4. mean absolute error (mae): mae = \frac{\sum_{i=1}^n |y_i - \hat{y}_i|}{n} average of absolute errors. less sensitive to outliers compared to mse and rmse.",5,20.417353,-7.169427,14.285618,5.1973467,"regression, statistical, statistics"
322,"roc curves, which depict true positives against false positives at different threshold values, were the subject of our exploration today. we discovered that the classifier's performance decreased with a flatter curve. while the false positive rate continues to rise, a flat curve suggests that the classifier is making a large number of inaccurate predictions and that raising the threshold has no discernible effect on the true positive rate. the low auc (area under the curve) score in these situations further supports subpar performance. since the diagonal line (y = x) reflects random guessing, a successful classifier should ideally have an auc well above 0.5. the graph's area above this line shows improved categorization outcomes.","roc curves, which depict true positives against false positives at different threshold values, were the subject of our exploration today. we discovered that the classifier's performance decreased with a flatter curve. while the false positive rate continues to rise, a flat curve suggests that the classifier is making a large number of inaccurate predictions and that raising the threshold has no discernible effect on the true positive rate. the low auc (area under the curve) score in these situations further supports subpar performance. since the diagonal line (y = x) reflects random guessing, a successful classifier should ideally have an auc well above 0.5. the graph's area above this line shows improved categorization outcomes.",12,2.646664,-24.684786,7.4289517,0.71106297,"classifiers, logistic, roc"
323,"in todayâ€™s class we learnt about simple linear regression and how we estimate the coefficients,  m and  c, using sample data. these coefficients define the regression line. we learnt that these values are only point estimates of the population parameters, which means they wonâ€™t exactly match the true population values but should be close. the confidence for that exact point will be 0%. however we can obtain an interval with a given confidence. it is called estimation interval or confidence interval. if we increase the size of interval, the confidence will increase too. example- for confidence of 100% the interval needs to be infinity like entire domain.
for sample we get those coefficient and we call it statistic. for population it is called parameter. our main aim is to obtain parameter based on statistic. for the regression line we also define an error term which can be calculated in multiple ways. we can either take absolute difference of all points from the line and sum it up. other way we could take the square of distances of point from line and sum it up. in this way the error is calculated for each point and we aim to minimize it to get those linear regression coefficients.","in today s class we learnt about simple linear regression and how we estimate the coefficients, m and c, using sample data. these coefficients define the regression line. we learnt that these values are only point estimates of the population parameters, which means they won t exactly match the true population values but should be close. the confidence for that exact point will be 0%. however we can obtain an interval with a given confidence. it is called estimation interval or confidence interval. if we increase the size of interval, the confidence will increase too. example- for confidence of 100% the interval needs to be infinity like entire domain. for sample we get those coefficient and we call it statistic. for population it is called parameter. our main aim is to obtain parameter based on statistic. for the regression line we also define an error term which can be calculated in multiple ways. we can either take absolute difference of all points from the line and sum it up. other way we could take the square of distances of point from line and sum it up. in this way the error is calculated for each point and we aim to minimize it to get those linear regression coefficients.",1,34.04943,-9.743325,16.682623,4.022776,"population, models, estimating"
324,"today's class start with a discussion about  question on 1) how to calculate standard error of one sample 2) how to determine the width of bins of histograms.
starting with first question - we can calculate the standard error of one sample just by supposing that the standard deviation of that sample is equal to the standard deviation of population. in second question the width of bins depends on the problem what we are looking for or we can also done it by using some predefined rules like sturge's rule or freedman diaconis rule. in logistic regression we use sigmoid function in which we get two values either 0 or 1, it gives only probability. the boundary can be non-linear not necessarily to be linear. if we give more flexibility, then it works well on training data but not on test data.
there are many metrics that associated with classification just like  regression - 
1)precision -> of the events we have detected, how many we correctly detected.
2)accuracy-> overall how often is the classifier correct ? in case of data imbalance, it is not a accurate method.
3)recall -> of the specific class how many we have classified correctly 
4)f1 -> it is the harmonic mean of precision and recall.
we also learn about the confusion matrice in which there are 2 rows and 2 columns. 
1)true positive -> it is when a prediction is correct 
false positive -> it is when a prediction is 2)incorrect 
3)true negative -> it is an correct prediction that a sample is negative 
4)false negative -> it is an incorrect prediction that a sample is negative","today's class start with a discussion about question on 1) how to calculate standard error of one sample 2) how to determine the width of bins of histograms. starting with first question - we can calculate the standard error of one sample just by supposing that the standard deviation of that sample is equal to the standard deviation of population. in second question the width of bins depends on the problem what we are looking for or we can also done it by using some predefined rules like sturge's rule or freedman diaconis rule. in logistic regression we use sigmoid function in which we get two values either 0 or 1, it gives only probability. the boundary can be non-linear not necessarily to be linear. if we give more flexibility, then it works well on training data but not on test data. there are many metrics that associated with classification just like regression - 1)precision -> of the events we have detected, how many we correctly detected. 2)accuracy-> overall how often is the classifier correct ? in case of data imbalance, it is not a accurate method. 3)recall -> of the specific class how many we have classified correctly 4)f1 -> it is the harmonic mean of precision and recall. we also learn about the confusion matrice in which there are 2 rows and 2 columns. 1)true positive -> it is when a prediction is correct false positive -> it is when a prediction is 2)incorrect 3)true negative -> it is an correct prediction that a sample is negative 4)false negative -> it is an incorrect prediction that a sample is negative",10,13.625955,-18.637722,9.003661,-1.3763005,"classifications, histograms, histogram"
325,"today we learned about sample statistics and estimating the sample mean. estimating the sample mean and hypothesis testing are fundamental concepts in statistics, commonly used to draw conclusions about a population based on sample data.

the sample mean is a point estimate of the population mean, calculated by summing the values in a sample and dividing by the number of observations. it is an approximation for the population mean when it is impractical to collect data from every member of the population. the sample mean is unbiased, meaning its expected value equals the population mean. however, variability exists across different samples, and the sample meanâ€™s accuracy increases with larger sample sizes, as stated by the central limit theorem (clt).

hypothesis testing is a method to assess claims or hypotheses about a population parameter, such as the mean. the process starts with the formulation of a null hypothesis (hâ‚€) and an alternative hypothesis (hâ‚). using sample data, a test statistic is calculated, and its significance is determined by a p-value, which indicates the probability of observing the data if the null hypothesis is true. a common threshold is 0.05; if the p-value is smaller than this threshold, the null hypothesis is rejected. hypothesis tests, like t-tests, are widely used to determine if there is enough evidence to support or refute a claim about a population parameter, balancing false positive and false negative errors. 

we also discussed the normal and t-distributions, the latter having more weight in the tails and hence reflecting a higher probability of outliers. t-tests are used for n<30.","today we learned about sample statistics and estimating the sample mean. estimating the sample mean and hypothesis testing are fundamental concepts in statistics, commonly used to draw conclusions about a population based on sample data. the sample mean is a point estimate of the population mean, calculated by summing the values in a sample and dividing by the number of observations. it is an approximation for the population mean when it is impractical to collect data from every member of the population. the sample mean is unbiased, meaning its expected value equals the population mean. however, variability exists across different samples, and the sample mean s accuracy increases with larger sample sizes, as stated by the central limit theorem (clt). hypothesis testing is a method to assess claims or hypotheses about a population parameter, such as the mean. the process starts with the formulation of a null hypothesis (h ) and an alternative hypothesis (h ). using sample data, a test statistic is calculated, and its significance is determined by a p-value, which indicates the probability of observing the data if the null hypothesis is true. a common threshold is 0.05; if the p-value is smaller than this threshold, the null hypothesis is rejected. hypothesis tests, like t-tests, are widely used to determine if there is enough evidence to support or refute a claim about a population parameter, balancing false positive and false negative errors. we also discussed the normal and t-distributions, the latter having more weight in the tails and hence reflecting a higher probability of outliers. t-tests are used for n<30.",7,35.19686,5.912743,15.094447,2.5227072,"statistics, statistical, statisticsâ"
326,"today we started our lecture with brief discussion about some confusions related to tsne and looked how it is to be used for visualisation only. we then started learning about feature encoding. we looked at an example where we had to classify the entities into red, blue and green. we tried using one hot encoding where we vectorize the features. but it comes with it's own disadvantage, mainly the excess column generations. we then looked at frequency encoding and target encoding. there are 2 main issues with encoding. one is the multiclass problem and the other as mutlilabel problem where an image can contain both cat and dog and it should not be labeled for only one of them.","today we started our lecture with brief discussion about some confusions related to tsne and looked how it is to be used for visualisation only. we then started learning about feature encoding. we looked at an example where we had to classify the entities into red, blue and green. we tried using one hot encoding where we vectorize the features. but it comes with it's own disadvantage, mainly the excess column generations. we then looked at frequency encoding and target encoding. there are 2 main issues with encoding. one is the multiclass problem and the other as mutlilabel problem where an image can contain both cat and dog and it should not be labeled for only one of them.",3,-44.516884,6.8551,0.33695853,6.114912,"categorical, categorization, categorise"
327,"we revisited closed form solution of linear regression and how it was impractical.we also learned like if pvalue for that variable(feature) is more than 0.05 we drop that variable as it is not contributing more to the model.sir also discussed train and test split and how 80,20 split is done generally and if r_squared value is too close then overfitting might be present and sir also told that sst should have n-1 degree of freedom and also introduced adjusted r_squared.","we revisited closed form solution of linear regression and how it was impractical.we also learned like if pvalue for that variable(feature) is more than 0.05 we drop that variable as it is not contributing more to the model.sir also discussed train and test split and how 80,20 split is done generally and if r_squared value is too close then overfitting might be present and sir also told that sst should have n-1 degree of freedom and also introduced adjusted r_squared.",2,9.5702915,7.0527773,11.349698,5.0663548,"regression, regressions, features"
328,"it starts by calculating the sample mean and assuming itâ€™s close to the population mean. for large samples, the sampling distribution of the mean is roughly normal . to measure how much the sample mean might vary, we calculate the standard error , which is the sample standard deviation divided by the square root of the sample size . confidence intervals are then used to estimate the range where the population mean is likely to fall. for smaller samples, we switch to the t-distribution instead of the normal one. basically, confidence intervals help show how uncertain we are about the population mean, and they give us a range where itâ€™s likely to be. for example, if we repeatedly took samples, 95% of their means would fall within this interval.

linear regression is about modeling the relationship between two variables using the equation . here,  represents how much  changes when  changes. if  turns out to be zero (statistically), the model doesnâ€™t work. to check this, we look at the confidence interval of â€”if it includes zero, the regression isnâ€™t significant. a small p-value (like less than 0.05) tells us  is significantly different from zero, which means the model is valid.

for multiple linear regression (mlr), we extend this to include more predictors, so the equation becomes . to evaluate the modelâ€™s overall significance, we use anova (analysis of variance). the -statistic helps us compare the variability explained by the model (msr) to the variability left unexplained (mse). if the -statistic is significant, it means the model works as a whole.","it starts by calculating the sample mean and assuming it s close to the population mean. for large samples, the sampling distribution of the mean is roughly normal . to measure how much the sample mean might vary, we calculate the standard error , which is the sample standard deviation divided by the square root of the sample size . confidence intervals are then used to estimate the range where the population mean is likely to fall. for smaller samples, we switch to the t-distribution instead of the normal one. basically, confidence intervals help show how uncertain we are about the population mean, and they give us a range where it s likely to be. for example, if we repeatedly took samples, 95% of their means would fall within this interval. linear regression is about modeling the relationship between two variables using the equation . here, represents how much changes when changes. if turns out to be zero (statistically), the model doesn t work. to check this, we look at the confidence interval of if it includes zero, the regression isn t significant. a small p-value (like less than 0.05) tells us is significantly different from zero, which means the model is valid. for multiple linear regression (mlr), we extend this to include more predictors, so the equation becomes . to evaluate the model s overall significance, we use anova (analysis of variance). the -statistic helps us compare the variability explained by the model (msr) to the variability left unexplained (mse). if the -statistic is significant, it means the model works as a whole.",7,33.364086,1.5167577,14.720066,3.0316873,"statistics, statistical, statisticsâ"
329,"the closed-form solution in multiple lr was discussed with the formula for it(matrix one) .we do not select the features if the p-value is higher than about 0.05 of characteristics. data splitting is usually done by taking an 80-20 random partition between train and test. significant differences in r2 for test and train data indicate overfitting. in building a model to represent the population, we have also looked at modified r2, and talked about the idea of (n-1) degrees of freedom in total sum of squares .","the closed-form solution in multiple lr was discussed with the formula for it(matrix one) .we do not select the features if the p-value is higher than about 0.05 of characteristics. data splitting is usually done by taking an 80-20 random partition between train and test. significant differences in r2 for test and train data indicate overfitting. in building a model to represent the population, we have also looked at modified r2, and talked about the idea of (n-1) degrees of freedom in total sum of squares .",2,9.786845,8.899168,11.505769,5.018395,"regression, regressions, features"
330,"we calculated the sample mean (x bar) assuming it to be close to population mean.  we use the sampling distribution of the mean which is approximately equal to the normal distribution ( n(u, sigma)for large samples. then we discussed about the standard error (se) which is defined as sample deviation by root of sample size i.e. s/âˆšn . then we defined the confidence intervals to get the range in which the population mean will lie. and for small samples we use t-distribution instead of normal distribution.
we also continued the discussed about the linear regression: y= b0 +b1 x.  where if b1 is zero then the regression model is taken as invalid model. 
a low p-value indicates that b1 is significantly different from 0 hence making the model valid.
now for multiple linear regression (mlr) models the model extends to include multiple predictors i.e. b0, b1, b2,....bn i.e. y= b0+ b1x+ b2x2+ .....bkxk. to compare these variables we use anova (analysis of variance)","we calculated the sample mean (x bar) assuming it to be close to population mean. we use the sampling distribution of the mean which is approximately equal to the normal distribution ( n(u, sigma)for large samples. then we discussed about the standard error (se) which is defined as sample deviation by root of sample size i.e. s/ n . then we defined the confidence intervals to get the range in which the population mean will lie. and for small samples we use t-distribution instead of normal distribution. we also continued the discussed about the linear regression: y= b0 +b1 x. where if b1 is zero then the regression model is taken as invalid model. a low p-value indicates that b1 is significantly different from 0 hence making the model valid. now for multiple linear regression (mlr) models the model extends to include multiple predictors i.e. b0, b1, b2,....bn i.e. y= b0+ b1x+ b2x2+ .....bkxk. to compare these variables we use anova (analysis of variance)",7,32.911495,0.9336017,14.70839,3.1017604,"statistics, statistical, statisticsâ"
331,"calculated how to obtain the population mean from the sample mean, first single samples and then multiple samples. introduced confidence intervals as a method for estimation of the population mean and explained how one can calculate the chances of finding a range for the population mean given the level of certainty.

the standard error of the mean was covered. it uses the following formula to calculate it:
î¼ = ïƒ / âˆšn

we considered how the means of several samples together follow a normal distribution, where their average forms the estimate of the population's mean. the probabilities were expressed in terms of p-values.

we also calculated the values of the parameters î²â‚€ and î²â‚ in the linear regression model.

we end the class by discussing the anova table, which is used to test whether there are statistically significant differences between the means of three or more groups.

in the next lecture, we'll continue with multiple regression models.","calculated how to obtain the population mean from the sample mean, first single samples and then multiple samples. introduced confidence intervals as a method for estimation of the population mean and explained how one can calculate the chances of finding a range for the population mean given the level of certainty. the standard error of the mean was covered. it uses the following formula to calculate it: = / n we considered how the means of several samples together follow a normal distribution, where their average forms the estimate of the population's mean. the probabilities were expressed in terms of p-values. we also calculated the values of the parameters and in the linear regression model. we end the class by discussing the anova table, which is used to test whether there are statistically significant differences between the means of three or more groups. in the next lecture, we'll continue with multiple regression models.",7,35.306274,4.356635,15.022024,2.434604,"statistics, statistical, statisticsâ"
332,"we learnt how statistics is calculated from a sample, not the population and we estimate parameters based on statistics. few examples are count/ frequency, mean median mode, standard deviation variance, + - ã— ã· and the different arithmetic operations that can be applied on different levels of measurement. then we learnt about simple and multiple linear regression (multiple predictors x1, x2...) and about point estimation. then we learnt about bias which is the c intercept  which represents net sum of all unaccountable variables. beta0 and beta1 are estimators of the population parameters and then we learn about best fit line which minimizes sum of squared errors","we learnt how statistics is calculated from a sample, not the population and we estimate parameters based on statistics. few examples are count/ frequency, mean median mode, standard deviation variance, + - and the different arithmetic operations that can be applied on different levels of measurement. then we learnt about simple and multiple linear regression (multiple predictors x1, x2...) and about point estimation. then we learnt about bias which is the c intercept which represents net sum of all unaccountable variables. beta0 and beta1 are estimators of the population parameters and then we learn about best fit line which minimizes sum of squared errors",1,31.094023,-6.9045177,16.048086,4.0561795,"population, models, estimating"
333,"sampling distribution of the mean is a normal distribution. if we assume the sample standard deviation to be equal to the population standard deviation, we can approximate population distribution.
statistical significance is decided by the confidence level. 90% or 95% confidence level is used in practical scenarios. it directly links to the area under the curve of the normal distribution and can even be thought of as a probability.
regression inherently assumes normal distribution of errors. it is useless if not a single one of the coefficients is statistically same as zero.
statistically same means that the coefficient lies in the 90 or 95% confidence interval of zero. the are under the curve outside of the regression coefficient (or the probability that the absolute value of a random variable drawn from the normal distribution is more than the absolute value of the regression coefficient) is given by the p-value. hence, it should be less than 0.5 for 95& confidence considerations.
anova (analysis of variance) is used to perform the above mentioned analysis for the case of multiple linear regression (y = b0 + b1x1 + b2x2 + ... + bkxk). it returns an f-statistic which is the ratio of the variation explained by the regression and the total variation in the data and hence should be high.","sampling distribution of the mean is a normal distribution. if we assume the sample standard deviation to be equal to the population standard deviation, we can approximate population distribution. statistical significance is decided by the confidence level. 90% or 95% confidence level is used in practical scenarios. it directly links to the area under the curve of the normal distribution and can even be thought of as a probability. regression inherently assumes normal distribution of errors. it is useless if not a single one of the coefficients is statistically same as zero. statistically same means that the coefficient lies in the 90 or 95% confidence interval of zero. the are under the curve outside of the regression coefficient (or the probability that the absolute value of a random variable drawn from the normal distribution is more than the absolute value of the regression coefficient) is given by the p-value. hence, it should be less than 0.5 for 95& confidence considerations. anova (analysis of variance) is used to perform the above mentioned analysis for the case of multiple linear regression (y = b0 + b1x1 + b2x2 + ... + bkxk). it returns an f-statistic which is the ratio of the variation explained by the regression and the total variation in the data and hence should be high.",7,39.333763,3.0174227,15.156062,2.7803109,"statistics, statistical, statisticsâ"
334,"sir, my attendace was marked at 9:40:14 and the official start time of class is 9:30 and hence the timing should be 9:45, please consider this sir.
todays class topic was of calculating population means from the sample mean which was presented in the context of both single-sample and the multiple-sample methods. we also studied confidence intervals, which are used to predict the range within a population mean that may have a probability attached to it.the standard error of the mean (se) is represented by this formula: se= square root (pop. standard dev. squared/ sample size). once the mean of several different random samples is plotted in a graph, a distribution of the mean will be normally shaped and the mean of such sample means will act as an estimation of the population mean. we learned about p-value statistics for the calculations of probabilities.we also carried out the determination of linear regression model coefficients for beta zero as well as beta one.we also got to learn the anova table, which looks into differences between the means of three or more groups to ascertain if they are significant.","sir, my attendace was marked at 9:40:14 and the official start time of class is 9:30 and hence the timing should be 9:45, please consider this sir. todays class topic was of calculating population means from the sample mean which was presented in the context of both single-sample and the multiple-sample methods. we also studied confidence intervals, which are used to predict the range within a population mean that may have a probability attached to it.the standard error of the mean (se) is represented by this formula: se= square root (pop. standard dev. squared/ sample size). once the mean of several different random samples is plotted in a graph, a distribution of the mean will be normally shaped and the mean of such sample means will act as an estimation of the population mean. we learned about p-value statistics for the calculations of probabilities.we also carried out the determination of linear regression model coefficients for beta zero as well as beta one.we also got to learn the anova table, which looks into differences between the means of three or more groups to ascertain if they are significant.",7,33.98839,4.594207,14.933675,2.633566,"statistics, statistical, statisticsâ"
335,"we started with the equation y=f(x) (for fluid flowing in pipe) where we have the values of the inputs (i.e. flow values) and output (temperature). earlier people used to derive the relationship between the inputs and the outputs empirically using mathematical formulations.
but now we have many machine learning algorithms which can give us the relationship:
1. slr -> simple linear regression
2. mlr -> multiple linear regression
3. logistic regression
4. random forest
5. k-means clustering
6. hierarchical clustering

then we discussed about the 4 levels of measurement:

1. nominal: eg - gender, color
    the data in this level of measurement can only be classified into different groups. there is  
    no ordering and the data is discrete.
2. ordinal: eg - grades
    this is also discrete like nominal but has an inherent order associated with it.

for working with the data we encode it by assigning the labels some values (like male-1, female-2, etc.) which is fundamentally wrong as we are completely ignoring the values of the assigned numbers.
instead, we can use one-hot code for the labels which acts as a switch making only a single value 1 and rest of them to be zero. for example, if we are classifying images of animals then for an image of a monkey the values will be [dog-0, cat-0, monkey-1, horse-0].

3. interval: eg - temperature, ph, credit score
    the data in this level of measurement is continuous. zero here is arbitrarily defined and  
    has no real meaning.
4. ratio: eg - height, weight, salary
    this is also continuous but here zero has a meaning in this level of measurement.

both nominal and ordinal levels are discrete and they fall under the classification ml category whereas interval and ratio fall under the regression category.

in the equation y=f(x), y are the labels and x are the features.

1. supervised learning: when we have both labels and features it is called 'supervised 
     learning'.
2. unsupervised learning: when we only have features and no labels it is called 
    'unsupervised learning'. 
    in this we use the features to group data into clusters and then on the basis of their 
    similarities we can define their labels and then predict the correct group for any new x.

at last we looked into the population for our data is too large that either we don't have access to all the population's data or we can't normally process such a large data.
so we take samples from the population to get the data with the aim being getting as close as possible to the whole population.","we started with the equation y=f(x) (for fluid flowing in pipe) where we have the values of the inputs (i.e. flow values) and output (temperature). earlier people used to derive the relationship between the inputs and the outputs empirically using mathematical formulations. but now we have many machine learning algorithms which can give us the relationship: 1. slr -> simple linear regression 2. mlr -> multiple linear regression 3. logistic regression 4. random forest 5. k-means clustering 6. hierarchical clustering then we discussed about the 4 levels of measurement: 1. nominal: eg - gender, color the data in this level of measurement can only be classified into different groups. there is no ordering and the data is discrete. 2. ordinal: eg - grades this is also discrete like nominal but has an inherent order associated with it. for working with the data we encode it by assigning the labels some values (like male-1, female-2, etc.) which is fundamentally wrong as we are completely ignoring the values of the assigned numbers. instead, we can use one-hot code for the labels which acts as a switch making only a single value 1 and rest of them to be zero. for example, if we are classifying images of animals then for an image of a monkey the values will be [dog-0, cat-0, monkey-1, horse-0]. 3. interval: eg - temperature, ph, credit score the data in this level of measurement is continuous. zero here is arbitrarily defined and has no real meaning. 4. ratio: eg - height, weight, salary this is also continuous but here zero has a meaning in this level of measurement. both nominal and ordinal levels are discrete and they fall under the classification ml category whereas interval and ratio fall under the regression category. in the equation y=f(x), y are the labels and x are the features. 1. supervised learning: when we have both labels and features it is called 'supervised learning'. 2. unsupervised learning: when we only have features and no labels it is called 'unsupervised learning'. in this we use the features to group data into clusters and then on the basis of their similarities we can define their labels and then predict the correct group for any new x. at last we looked into the population for our data is too large that either we don't have access to all the population's data or we can't normally process such a large data. so we take samples from the population to get the data with the aim being getting as close as possible to the whole population.",4,-25.522404,-14.874148,1.4930005,0.3807177,"classification, classifying, classifications"
336,"first of all, we defined all the terms in excel like the x_bar, y_bar, xy_bar, xsq_bar and then used the formula of a and b which we learnt earlier. here, we manually calculated it. we also calculated yi_cap, ei_cap. then we used our scatter plot to show the y vs x. we also made a regression line of yi=ax+b. after doing it manually, we used the extension in which we took help of the data analysis tool kit. we used the regression feature to automatically calculate the the linear regression.we learnt about some terminologies in the tool kit and their importance. we also made a histogram for errors. it is desirable to get a normal distribution, but we didn't get it in our case.  then we learnt about some assumptions.","first of all, we defined all the terms in excel like the x_bar, y_bar, xy_bar, xsq_bar and then used the formula of a and b which we learnt earlier. here, we manually calculated it. we also calculated yi_cap, ei_cap. then we used our scatter plot to show the y vs x. we also made a regression line of yi=ax+b. after doing it manually, we used the extension in which we took help of the data analysis tool kit. we used the regression feature to automatically calculate the the linear regression.we learnt about some terminologies in the tool kit and their importance. we also made a histogram for errors. it is desirable to get a normal distribution, but we didn't get it in our case. then we learnt about some assumptions.",5,16.19256,-3.6847875,13.880076,5.257315,"regression, statistical, statistics"
337,"logistic regression is a binary classification method that predicts outcomes as either 0 or 1. the model's performance is assessed using metrics like accuracy, precision, recall, and the confusion matrix.
it is trained by optimizing weights through gradient descent, minimizing errors, and using the likelihood function to improve predictions. the model processes input features by multiplying them with weights, adding a bias, and applying a sigmoid function to obtain a probability score. if this probability is greater than 0.5, the outcome is classified as 1; otherwise, it is 0.","logistic regression is a binary classification method that predicts outcomes as either 0 or 1. the model's performance is assessed using metrics like accuracy, precision, recall, and the confusion matrix. it is trained by optimizing weights through gradient descent, minimizing errors, and using the likelihood function to improve predictions. the model processes input features by multiplying them with weights, adding a bias, and applying a sigmoid function to obtain a probability score. if this probability is greater than 0.5, the outcome is classified as 1; otherwise, it is 0.",12,8.350309,-21.419128,9.095895,-1.936625,"classifiers, logistic, roc"
338,"todayâ€™s lecture started off with the birdâ€™s eye view of the next one month, and we briefly discussed about the project. we then started off by discussing about feature encoding. both the independent or dependent variables might need to be encoded in order to convert them into numerical forms for training the ml models. basic feature encoding includes one-hot encoding, which involves vectorisation i.e. creating vectors of the size equal to the number of classes and then assigning 1 to one of the vector components based on the class. we then talked about different types of encoding like label encoding, which involves taking all possible values of the categorical variable and assigning integers to them sequentially. this kind of encoding works for the dependent variable, but not for the independent variable as the independent variable defines the model and hence any kind of ordering or hierarchy in its values will lead to errors. integer encoding has an inherent sense of order, where the assigned values may not be sequential and may have some meaning. one hot encoding has an issue that it increases the number of columns, and thus invites the curse of dimensionality. hence, one hot encoding should be used carefully and with data having lesser number of categories. we also discussed binary encoding, which uses bits for encoding. frequency encoding involves assigning the frequency of a class as its encoded value. this method is not very useful for the dependent variable as we need unique values for categorising y, whereas frequency encoding may lead to assigning same values if the frequency of two classes is the same.  we also have target encoding, where the average of the y values for a given class, are assigned to all the class values in the data columns. we then moved on to problems where we want to convert continuous data into discrete data such as height, weight, etc. in such cases, we might be moving on from regression to classification, which could lead to improvement in our metrics. we then talked about text processing, where we discussed large language models, which are examples of the manner in which statistical processing can generate a deterministic output. text processing basically involves converting text to numbers so that processing is efficient. usually the methodology includes dropping some very common words called stop words, which do not add to the meaning of the sentence. we then create a dictionary of words and then express the document using the dictionary.","today s lecture started off with the bird s eye view of the next one month, and we briefly discussed about the project. we then started off by discussing about feature encoding. both the independent or dependent variables might need to be encoded in order to convert them into numerical forms for training the ml models. basic feature encoding includes one-hot encoding, which involves vectorisation i.e. creating vectors of the size equal to the number of classes and then assigning 1 to one of the vector components based on the class. we then talked about different types of encoding like label encoding, which involves taking all possible values of the categorical variable and assigning integers to them sequentially. this kind of encoding works for the dependent variable, but not for the independent variable as the independent variable defines the model and hence any kind of ordering or hierarchy in its values will lead to errors. integer encoding has an inherent sense of order, where the assigned values may not be sequential and may have some meaning. one hot encoding has an issue that it increases the number of columns, and thus invites the curse of dimensionality. hence, one hot encoding should be used carefully and with data having lesser number of categories. we also discussed binary encoding, which uses bits for encoding. frequency encoding involves assigning the frequency of a class as its encoded value. this method is not very useful for the dependent variable as we need unique values for categorising y, whereas frequency encoding may lead to assigning same values if the frequency of two classes is the same. we also have target encoding, where the average of the y values for a given class, are assigned to all the class values in the data columns. we then moved on to problems where we want to convert continuous data into discrete data such as height, weight, etc. in such cases, we might be moving on from regression to classification, which could lead to improvement in our metrics. we then talked about text processing, where we discussed large language models, which are examples of the manner in which statistical processing can generate a deterministic output. text processing basically involves converting text to numbers so that processing is efficient. usually the methodology includes dropping some very common words called stop words, which do not add to the meaning of the sentence. we then create a dictionary of words and then express the document using the dictionary.",3,-43.97553,4.4326434,0.21517645,6.5570283,"categorical, categorization, categorise"
339,"we started with discussing how to perform linear regression analysis on a data set and also learnt how to make scatter plots for the same. we also used the data analysis toolpak on excel to perform the regression and got a variety of different statistics.
while performing regression on a certain sample of data, the model is best suited for the points within the sample limit. we can predict the values beyond the sample, but accuracy would not be satisfactory enough. for such cases, we need to use other tools like time series analysis and merely slr is not enough. we made scatter plots for the errors and analyzed the distribution of these errors. through the scatter plot it seemed that the errors were randomly oriented, but after plotting the histogram for the errors, we noticed that there was some kind of uniformity in the distribution of the errors. the histogram plot gives good insights about the distribution of data (errors). if the errors are perfectly random, then the histogram would have more or less a gaussian normal distribution. we also want the errors to be random, which means the trend in the data is well picked by our model. if the error distribution is not completely random, it means our model missed to capture some trend in the data, which is shown up in the errors. we also discussed upon the coefficient of determination, represented by r^2. it is represented so because it is exactly equal to the square of the correlation coefficient between x and y, r. r^2 is the ratio of sum of squares of variation that is captured by our model to that of the variation that is actually present in the data. so, we want our model to capture maximum variation in the data. hence, we want r^2 to be as close to 1 as possible. the total variance in the data is equal to the sum of variance captured by the model and the squares of errors.","we started with discussing how to perform linear regression analysis on a data set and also learnt how to make scatter plots for the same. we also used the data analysis toolpak on excel to perform the regression and got a variety of different statistics. while performing regression on a certain sample of data, the model is best suited for the points within the sample limit. we can predict the values beyond the sample, but accuracy would not be satisfactory enough. for such cases, we need to use other tools like time series analysis and merely slr is not enough. we made scatter plots for the errors and analyzed the distribution of these errors. through the scatter plot it seemed that the errors were randomly oriented, but after plotting the histogram for the errors, we noticed that there was some kind of uniformity in the distribution of the errors. the histogram plot gives good insights about the distribution of data (errors). if the errors are perfectly random, then the histogram would have more or less a gaussian normal distribution. we also want the errors to be random, which means the trend in the data is well picked by our model. if the error distribution is not completely random, it means our model missed to capture some trend in the data, which is shown up in the errors. we also discussed upon the coefficient of determination, represented by r^2. it is represented so because it is exactly equal to the square of the correlation coefficient between x and y, r. r^2 is the ratio of sum of squares of variation that is captured by our model to that of the variation that is actually present in the data. so, we want our model to capture maximum variation in the data. hence, we want r^2 to be as close to 1 as possible. the total variance in the data is equal to the sum of variance captured by the model and the squares of errors.",5,23.756525,-4.4127407,14.306883,4.67011,"regression, statistical, statistics"
340,"we started off with a bit of revision of previous class and talked more about statistics and then we entered the fun part. sir showed us his backend of this forms. we were shown the heat maps and a bit about how the observation how it is made and how to inferre stuff. and more more point, anything can be data. 
after that we were told about feature engineering, which is basically calculation of features based on existing features by performing certain operations. we also had a small discussion on solvers and gradient descent .","we started off with a bit of revision of previous class and talked more about statistics and then we entered the fun part. sir showed us his backend of this forms. we were shown the heat maps and a bit about how the observation how it is made and how to inferre stuff. and more more point, anything can be data. after that we were told about feature engineering, which is basically calculation of features based on existing features by performing certain operations. we also had a small discussion on solvers and gradient descent .",6,-1.2636266,6.619223,9.284035,5.6047974,"summarizing, summarize, summarization"
341,"in todayâ€™s class we mainly focused on statistics involved in ml. we discussed t-distribution and normal distribution . the process begins by calculating the sample mean (ð‘¥ì„) and assuming it's close to the population mean. the sampling distribution of the mean is used, which tends to be approximately normal for large sample sizes (ð‘(î¼,ïƒ)). the standard error (se) is calculated by dividing the sample standard deviation (ð‘†) by the square root of the sample size (ð‘›). confidence intervals are then created to show where the population mean is likely to fall. for smaller sample sizes, a t-distribution is used instead of a normal distribution.

we also discussed linear regression, where the relationship between two variables is modeled as ð‘œ = î²â‚€ + î²â‚ð‘‹.

in multiple linear regression (mlr), the model expands to include more predictors, like ð‘œ = î²â‚€ + î²â‚ð‘‹â‚ + î²â‚‚ð‘‹â‚‚ + ... + î²â‚–ð‘‹â‚–. to assess whether the model as a whole is significant, anova (analysis of variance) is used, which compares multiple averages or variables. the f-statistic is calculated as the ratio of the mean square regression (msr) to the mean square error (mse), helping to evaluate the overall significance of the model.","in today s class we mainly focused on statistics involved in ml. we discussed t-distribution and normal distribution . the process begins by calculating the sample mean ( ) and assuming it's close to the population mean. the sampling distribution of the mean is used, which tends to be approximately normal for large sample sizes ( ( , )). the standard error (se) is calculated by dividing the sample standard deviation ( ) by the square root of the sample size ( ). confidence intervals are then created to show where the population mean is likely to fall. for smaller sample sizes, a t-distribution is used instead of a normal distribution. we also discussed linear regression, where the relationship between two variables is modeled as = + . in multiple linear regression (mlr), the model expands to include more predictors, like = + + + ... + . to assess whether the model as a whole is significant, anova (analysis of variance) is used, which compares multiple averages or variables. the f-statistic is calculated as the ratio of the mean square regression (msr) to the mean square error (mse), helping to evaluate the overall significance of the model.",7,32.51866,1.220666,14.721513,3.0913038,"statistics, statistical, statisticsâ"
342,"today, first of all we saw the various population parameters like mean, variance and sample mean, variance. we learnt about the central limit theoram over some counter example of estimation of working hours of manager of a company. here we took the sample of 18 observations which was considered to represent the whole population. we then plotted the probability density function (pdf) for the sample mean and explored confidence intervals, along with the application of the t-distribution for sample sizes smaller than 30 observations. then we came to know about the practical importance of confidence intervals. thenafter, we saw statistical parameters and learnt to calculate them like t value, z and many more. in the end we had a discussion on p value and a brief discussion of multiple linear regression.","today, first of all we saw the various population parameters like mean, variance and sample mean, variance. we learnt about the central limit theoram over some counter example of estimation of working hours of manager of a company. here we took the sample of 18 observations which was considered to represent the whole population. we then plotted the probability density function (pdf) for the sample mean and explored confidence intervals, along with the application of the t-distribution for sample sizes smaller than 30 observations. then we came to know about the practical importance of confidence intervals. thenafter, we saw statistical parameters and learnt to calculate them like t value, z and many more. in the end we had a discussion on p value and a brief discussion of multiple linear regression.",7,32.377865,6.9964895,14.623569,2.445835,"statistics, statistical, statisticsâ"
343,"firstly we started discussing about feature engineering, which is when either the dependent variables categorcial  item they have to be properly encoded in oder to be used in ml models. and we discussed about the plan for upcoming lectures and also discussed about the group assigment which will be done in groups of 4 and the dataset will be given and test will be given later we also learnt about one hot encoding","firstly we started discussing about feature engineering, which is when either the dependent variables categorcial item they have to be properly encoded in oder to be used in ml models. and we discussed about the plan for upcoming lectures and also discussed about the group assigment which will be done in groups of 4 and the dataset will be given and test will be given later we also learnt about one hot encoding",3,-38.081013,3.3496418,0.42395517,5.990435,"categorical, categorization, categorise"
344,"gradient descent and newton raphson similarity, concept of multi collinearity causing the matrix  (x^t) x to become non invertible! (while performing mlr, y = (y1 ... ym)^t, x matrix = (1 x1... xk) where xi = (x1i ... xmi)^t and first column of x = (1... 1)^t, î² = (î²1 ... î²k)^t, îµ = (e1... em)^t, y = xî² + îµ. here if any of the xi's are dependent on some xj,... xn (j, j+1,... n âˆˆ {1,...k} then the matrix (x^t)x becomes non invertible since rank((x^t)x) <= min{rank(x), rank((x^t))} and since rank(x) = rank((x^t)) and their columns not being linearly independent, rank(x) < min{m, k+1} and thus the rank((x^t)x) < min{m, k+1}. the square matrix (x^t)x is a square matrix with m rows and columns so it could possibly be invertible but since it's not full rank, it's not invertible). the non invertibility of (x^t)x means the matrix p = x((x^t)x)â¯â¹x^t does not exist (the matrix is p is the projection matrix that orthogonally projects any vector b onto the column space of a if ax = b can't be solved and thus an orthogonal project is required for the best approximation of x, ax = b' where b' = pb) and thus the î²_hat can't be obtained for mlr. training of an ml model - while training ml model using a sample from a population, we shouldn't use entire sample but split it into 80%(training data) - 20%(test data) ratio where training data is used to train the model and the test data is used to test the model. two metrics are obtained ie. râ² for both training and test data where training râ² describes the goodness of fit of model however it being high doesn't indicate a good predictor model as it could be a case of over fitting and thus râ² of test data needs to be considered. adjusted râ² which keeps changing as more variables are added. the best for curve obtained through mlr might not be a straight line as linearity just refers to highest degree of labels in the equation. if the errors are normally distributed, they lie close to the a straight line ina q-q plot. introduced to ols algorithm, aic, bic, skewness and kurtosis, jarque bera test and durbin watson test.","gradient descent and newton raphson similarity, concept of multi collinearity causing the matrix (x^t) x to become non invertible! (while performing mlr, y = (y1 ... ym)^t, x matrix = (1 x1... xk) where xi = (x1i ... xmi)^t and first column of x = (1... 1)^t, = ( 1 ... k)^t, = (e1... em)^t, y = x + . here if any of the xi's are dependent on some xj,... xn (j, j+1,... n {1,...k} then the matrix (x^t)x becomes non invertible since rank((x^t)x) <= min{rank(x), rank((x^t))} and since rank(x) = rank((x^t)) and their columns not being linearly independent, rank(x) < min{m, k+1} and thus the rank((x^t)x) < min{m, k+1}. the square matrix (x^t)x is a square matrix with m rows and columns so it could possibly be invertible but since it's not full rank, it's not invertible). the non invertibility of (x^t)x means the matrix p = x((x^t)x) x^t does not exist (the matrix is p is the projection matrix that orthogonally projects any vector b onto the column space of a if ax = b can't be solved and thus an orthogonal project is required for the best approximation of x, ax = b' where b' = pb) and thus the _hat can't be obtained for mlr. training of an ml model - while training ml model using a sample from a population, we shouldn't use entire sample but split it into 80%(training data) - 20%(test data) ratio where training data is used to train the model and the test data is used to test the model. two metrics are obtained ie. r for both training and test data where training r describes the goodness of fit of model however it being high doesn't indicate a good predictor model as it could be a case of over fitting and thus r of test data needs to be considered. adjusted r which keeps changing as more variables are added. the best for curve obtained through mlr might not be a straight line as linearity just refers to highest degree of labels in the equation. if the errors are normally distributed, they lie close to the a straight line ina q-q plot. introduced to ols algorithm, aic, bic, skewness and kurtosis, jarque bera test and durbin watson test.",2,11.459702,0.35118866,11.181474,4.5636063,"regression, regressions, features"
345,"the session outlined core data science workflows, emphasizing systematic problem-solving through frameworks like crisp-dmâ€”a cyclical process spanning objective clarification, data exploration, preprocessing, model development, validation, and implementation. practical examples illustrated exploratory analysis (e.g., boxplots for outlier detection, addressing skewed class distributions) and challenges like heteroscedasticity (non-constant variance in models). strategies for incomplete data included deletion or basic imputation (mean/median) for isolated gaps, and advanced methods (knn, regression) for context-dependent gaps. anomalies, whether errors or valid extremes, were managed via statistical thresholds (iqr) or clustering algorithms. the discussion highlighted iterative refinement in workflows and prioritizing robust metrics (e.g., median over mean) to mitigate data irregularities.","the session outlined core data science workflows, emphasizing systematic problem-solving through frameworks like crisp-dm a cyclical process spanning objective clarification, data exploration, preprocessing, model development, validation, and implementation. practical examples illustrated exploratory analysis (e.g., boxplots for outlier detection, addressing skewed class distributions) and challenges like heteroscedasticity (non-constant variance in models). strategies for incomplete data included deletion or basic imputation (mean/median) for isolated gaps, and advanced methods (knn, regression) for context-dependent gaps. anomalies, whether errors or valid extremes, were managed via statistical thresholds (iqr) or clustering algorithms. the discussion highlighted iterative refinement in workflows and prioritizing robust metrics (e.g., median over mean) to mitigate data irregularities.",13,-19.454536,19.303019,8.279723,8.484363,"classification, classifying, classifications"
346,"before the advent of machine learning, research involving relationships between variables, such as predicting temperature difference from flow rate, typically involved manually fitting curves to data. this required researchers to hypothesize and test various equations (e.g., delta t = l^2, l^3, 1/l).

with the emergence of machine learning (ml), the approach shifted. instead of manually fitting curves, researchers employ a wide array of ml algorithms. these algorithms allow models to learn the underlying relationships within the data without the need to explicitly define the equation's form. essentially, ml algorithms search for the best fit within a predefined set of functions.

levels of measurement

the level of measurement of a variable significantly impacts the types of operations that can be performed on the data.

nominal: variables at this level have no inherent order (e.g., gender, color). they are discrete and categorical.
ordinal: these variables have an order but the intervals between them may not be equal (e.g., grades: a, b, c). they are also discrete.
interval: these variables have a consistent scale, but zero is arbitrary (e.g., temperature in celsius). they are continuous.
ratio: these variables have a true zero point and consistent intervals (e.g., height, temperature in kelvin, salary). they are continuous.
assigning values to nominal or ordinal variables can introduce bias. for example, arbitrarily assigning numerical values (e.g., a=1, b=2, c=3) may imply an incorrect quantitative relationship between categories. a more appropriate approach is to represent these variables using vectors (e.g., a=[1,0,0], b=[0,1,0], c=[0,0,1]).

while continuous variables like temperature are often measured discretely (e.g., by thermometers), they are typically assumed to be continuous for the purposes of data analysis.

supervised learning

in supervised learning, the goal is to predict a target variable (y) based on a set of input features (x).

if y is a nominal or ordinal variable, the task is classification.
if y is an interval or ratio variable, the task is regression.
unsupervised learning

in unsupervised learning, the data lacks labels (i.e., the value of y is unknown). the goal is to discover patterns and relationships within the data using only the features (x). common techniques include k-means clustering and hierarchical clustering.

data and sampling

the entire collection of data points constitutes the population. in ml, algorithms typically work with a sample of the data, which is a subset of the population. it's crucial to understand that no matter how large the sample, it will always represent a portion of the population.","before the advent of machine learning, research involving relationships between variables, such as predicting temperature difference from flow rate, typically involved manually fitting curves to data. this required researchers to hypothesize and test various equations (e.g., delta t = l^2, l^3, 1/l). with the emergence of machine learning (ml), the approach shifted. instead of manually fitting curves, researchers employ a wide array of ml algorithms. these algorithms allow models to learn the underlying relationships within the data without the need to explicitly define the equation's form. essentially, ml algorithms search for the best fit within a predefined set of functions. levels of measurement the level of measurement of a variable significantly impacts the types of operations that can be performed on the data. nominal: variables at this level have no inherent order (e.g., gender, color). they are discrete and categorical. ordinal: these variables have an order but the intervals between them may not be equal (e.g., grades: a, b, c). they are also discrete. interval: these variables have a consistent scale, but zero is arbitrary (e.g., temperature in celsius). they are continuous. ratio: these variables have a true zero point and consistent intervals (e.g., height, temperature in kelvin, salary). they are continuous. assigning values to nominal or ordinal variables can introduce bias. for example, arbitrarily assigning numerical values (e.g., a=1, b=2, c=3) may imply an incorrect quantitative relationship between categories. a more appropriate approach is to represent these variables using vectors (e.g., a=[1,0,0], b=[0,1,0], c=[0,0,1]). while continuous variables like temperature are often measured discretely (e.g., by thermometers), they are typically assumed to be continuous for the purposes of data analysis. supervised learning in supervised learning, the goal is to predict a target variable (y) based on a set of input features (x). if y is a nominal or ordinal variable, the task is classification. if y is an interval or ratio variable, the task is regression. unsupervised learning in unsupervised learning, the data lacks labels (i.e., the value of y is unknown). the goal is to discover patterns and relationships within the data using only the features (x). common techniques include k-means clustering and hierarchical clustering. data and sampling the entire collection of data points constitutes the population. in ml, algorithms typically work with a sample of the data, which is a subset of the population. it's crucial to understand that no matter how large the sample, it will always represent a portion of the population.",4,-27.656101,-15.098028,1.8695337,0.22115849,"classification, classifying, classifications"
347,"interpretation of confusion matrix and how to know what the rows and columns represent by keeping a track of number of elements in a particular class. crisp-dm (cross industry standard process for data mining) includes 6 steps that run cyclically - business understanding, data understanding, data preparation, modelling, evaluation and deployment. exploratory data analysis - summerizes main characteristics and discover patterns and trends in data sets, typically employs statistical graphics and data visualization. heteroscedasticity - assumption that variance doesn't change throughout the range of the dataset. problems encountered with data: too much, insufficient, incorrect, dealing with outliers etc. median not being affected by outliers but mean changing indicating betterness of median in this regard.","interpretation of confusion matrix and how to know what the rows and columns represent by keeping a track of number of elements in a particular class. crisp-dm (cross industry standard process for data mining) includes 6 steps that run cyclically - business understanding, data understanding, data preparation, modelling, evaluation and deployment. exploratory data analysis - summerizes main characteristics and discover patterns and trends in data sets, typically employs statistical graphics and data visualization. heteroscedasticity - assumption that variance doesn't change throughout the range of the dataset. problems encountered with data: too much, insufficient, incorrect, dealing with outliers etc. median not being affected by outliers but mean changing indicating betterness of median in this regard.",9,-16.799215,20.219666,8.435622,8.703238,"dataâ, analyse, analyses"
348,"we started by talking about population v/s sample. we consider a sample and determine the various attributes associated with it, like- mean mode, median and based on the values of these attributes, we predict those for our population. so, it is important to consider a sample that is good and represents the population well. the attributes estimated for population are known as parameters, while those calculated using the sample data are called as estimates/ statistics. we talked about the different kinds of attributes that can be associated with the population data. some of these include- mean, median, mode, standard deviation. our goal is to estimate these parameters of the population using the calculated statistics. all levels of measurement can be assigned different attributes of the data, depending on the type of data they account for. for example, nominal level can have the attribute of mode or count(frequency) but assigning the attribute of mean would be completely senseless, as nominal data is qualitative.
then we started discussing simple linear regression. in this model, we fit the data using a line (a simple linear equation with just one independent variable).
we can always use a point to represent any data, but this choice is very naive. so we instead use a simple equation like: y=ax+b where a and b are the estimates of population parameters and x is the independent variable. the y intercept, b here represents the sum total of the effects of all â€˜unknownâ€™ independent variables, which are not considered in the equation. so, as we include more and more independent variables, the value of b would start decreasing. also, depending upon the chosen sample, we can have various values of a and b. so, every sample will have a unique value for a and b. these values- a and b are known as the point estimates of the actual parameters. we can say with 100% confidence that the values of these point estimates do not coincide with the actual parameter values. so, in order to establish a certain level of certainty about the parameter values, we convert the point estimates to interval estimates, thereby giving a confidence interval, within which the population parameters would lie, with a given level of certainty. as the length of confidence interval increases, we become more and more confident that the estimated values resemble the parameter values. now, since different samples would give us different values of the estimates, we need to determine the values, which minimize the error. error for an observation is defined as the difference between the actual value and that obtained by estimating the data set with a line. so, to find the values of a and b, we need to minimize the sum of squares of individual errors. we do this by partially differentiating the expression with respect to a and b separately and by solving the two equations for two unknowns, we get the values of a and b in terms of the means and the mean of squares of the data set values. after getting the values of a and b, we put them in the equation to obtain the best model for the given data set. we consider the squares while minimizing errors instead of simply minimizing their sum as- it magnifies the errors, also it prevents cancellation of the errors, which can result when one point is above the line and other is below. since, we are getting the exact values of a and b, these are known as â€˜closed-form solutionsâ€™.
we do observe that the means of x and y (all observed values) satisfy the equation of the regression line. so, the predicted/ approximated line for the data values, pass through their mean.","we started by talking about population v/s sample. we consider a sample and determine the various attributes associated with it, like- mean mode, median and based on the values of these attributes, we predict those for our population. so, it is important to consider a sample that is good and represents the population well. the attributes estimated for population are known as parameters, while those calculated using the sample data are called as estimates/ statistics. we talked about the different kinds of attributes that can be associated with the population data. some of these include- mean, median, mode, standard deviation. our goal is to estimate these parameters of the population using the calculated statistics. all levels of measurement can be assigned different attributes of the data, depending on the type of data they account for. for example, nominal level can have the attribute of mode or count(frequency) but assigning the attribute of mean would be completely senseless, as nominal data is qualitative. then we started discussing simple linear regression. in this model, we fit the data using a line (a simple linear equation with just one independent variable). we can always use a point to represent any data, but this choice is very naive. so we instead use a simple equation like: y=ax+b where a and b are the estimates of population parameters and x is the independent variable. the y intercept, b here represents the sum total of the effects of all unknown independent variables, which are not considered in the equation. so, as we include more and more independent variables, the value of b would start decreasing. also, depending upon the chosen sample, we can have various values of a and b. so, every sample will have a unique value for a and b. these values- a and b are known as the point estimates of the actual parameters. we can say with 100% confidence that the values of these point estimates do not coincide with the actual parameter values. so, in order to establish a certain level of certainty about the parameter values, we convert the point estimates to interval estimates, thereby giving a confidence interval, within which the population parameters would lie, with a given level of certainty. as the length of confidence interval increases, we become more and more confident that the estimated values resemble the parameter values. now, since different samples would give us different values of the estimates, we need to determine the values, which minimize the error. error for an observation is defined as the difference between the actual value and that obtained by estimating the data set with a line. so, to find the values of a and b, we need to minimize the sum of squares of individual errors. we do this by partially differentiating the expression with respect to a and b separately and by solving the two equations for two unknowns, we get the values of a and b in terms of the means and the mean of squares of the data set values. after getting the values of a and b, we put them in the equation to obtain the best model for the given data set. we consider the squares while minimizing errors instead of simply minimizing their sum as- it magnifies the errors, also it prevents cancellation of the errors, which can result when one point is above the line and other is below. since, we are getting the exact values of a and b, these are known as closed-form solutions . we do observe that the means of x and y (all observed values) satisfy the equation of the regression line. so, the predicted/ approximated line for the data values, pass through their mean.",1,34.66657,-8.626863,16.715479,3.8851943,"population, models, estimating"
349,"today's lecture covered the quality of classifiers, emphasizing that accuracy alone is not a reliable metric. instead, we should analyze precision, recall, and the roc curve to assess performance. precision (tp / (tp + fp)) and recall (tp / (tp + fn)) help measure how well a model predicts positive cases while minimizing false negatives.

we also discussed clustering, an unsupervised learning method where data is grouped without predefined labels. k-means clustering requires specifying the number of clusters, while hierarchical clustering determines clusters automatically based on data structure.

the confusion matrix was introduced to evaluate models using true positives, true negatives, false positives, and false negatives. accuracy is calculated as (tp + tn) / total samples. we also explored logistic regression and neural networks, highlighting their role in classification.

finally, the roc curve and auc were discussed. a higher auc (closer to 1) indicates a better classifier, while 0.5 suggests random performance. a poor classifier flattens the roc curve, reducing its ability to distinguish between classes. overall, the lecture focused on evaluating classifiers and clustering techniques effectively.","today's lecture covered the quality of classifiers, emphasizing that accuracy alone is not a reliable metric. instead, we should analyze precision, recall, and the roc curve to assess performance. precision (tp / (tp + fp)) and recall (tp / (tp + fn)) help measure how well a model predicts positive cases while minimizing false negatives. we also discussed clustering, an unsupervised learning method where data is grouped without predefined labels. k-means clustering requires specifying the number of clusters, while hierarchical clustering determines clusters automatically based on data structure. the confusion matrix was introduced to evaluate models using true positives, true negatives, false positives, and false negatives. accuracy is calculated as (tp + tn) / total samples. we also explored logistic regression and neural networks, highlighting their role in classification. finally, the roc curve and auc were discussed. a higher auc (closer to 1) indicates a better classifier, while 0.5 suggests random performance. a poor classifier flattens the roc curve, reducing its ability to distinguish between classes. overall, the lecture focused on evaluating classifiers and clustering techniques effectively.",8,-1.849064,-18.144318,6.824114,0.55660015,"classification, clusterings, classifying"
350,"discussion upon exploratory data analysis was taken further.  example of average words per summary was taken , upon which we did statistical analysis. pivot tables can be very helpful in such cases. another example of a chemical factory plant was taken up . we analysed a column of the data and made out various references from it .","discussion upon exploratory data analysis was taken further. example of average words per summary was taken , upon which we did statistical analysis. pivot tables can be very helpful in such cases. another example of a chemical factory plant was taken up . we analysed a column of the data and made out various references from it .",6,-8.75913,29.000034,7.7117443,9.72193,"summarizing, summarize, summarization"
351,"in todayâ€™s class, we discussed how to predict population parameters using a single sample. we calculated the sample mean and standard deviation, assuming they approximate the population values, and derived the standard deviation of the sampling distribution to plot the sampling curve. a 95% confidence interval was explained as the range within which the mean of 95 out of 100 samples would lie, providing a probabilistic understanding. the difference between t-statistics (used for sample distributions) and z-statistics (used for population distributions) was highlighted, with the formula differing due to the standard deviation source. we explored the concept of p-value, emphasizing that a p-value below 0.05 indicates statistical significance, and discussed its role in determining whether coefficients are statistically equivalent to zero. this aids in feature selection for multiple linear regression, where anova is used to assess the statistical equivalence of multiple averages simultaneously.","in today s class, we discussed how to predict population parameters using a single sample. we calculated the sample mean and standard deviation, assuming they approximate the population values, and derived the standard deviation of the sampling distribution to plot the sampling curve. a 95% confidence interval was explained as the range within which the mean of 95 out of 100 samples would lie, providing a probabilistic understanding. the difference between t-statistics (used for sample distributions) and z-statistics (used for population distributions) was highlighted, with the formula differing due to the standard deviation source. we explored the concept of p-value, emphasizing that a p-value below 0.05 indicates statistical significance, and discussed its role in determining whether coefficients are statistically equivalent to zero. this aids in feature selection for multiple linear regression, where anova is used to assess the statistical equivalence of multiple averages simultaneously.",7,33.545925,3.4713311,14.923276,2.7414865,"statistics, statistical, statisticsâ"
352,"we started with a small recap of previous class on confusion matrix for multiple classes. we then started exploratory data analysis where we looked at it's six cyclic steps. 1) business understanding 2) data understanding 3) data preparation 4) modelling 5) evaluation 6) deployment out of which the 2nd and 3rd one were the ones which we dug deeper into. we had a brief discussion on them. then a ta joined us to show an example of this. the problem was to predict if the patient had diabetes. we saw how some parameters lime glucose and bmi had normal distribution and paramaters like insulin and pregnancies had exponential distribution.  we then looked at inter feature relation plots where we also looked at box plots where insulin showed some outliers. then we had discussion on what to do when some of the data is missing. we can either drop it or fill it with parameters like mean or median. when data is clustered, we can fill the missing data with mean/median but when the data follows say a parabolic distribution, filling it with a mean will result in creation of outlier. we can use the closest value instead of using mean which is known as multivariate data imputation. we then had a brief discussion on t-sne plot which helps in visualing higher dimensional data. we then looked at a recent example of nvidia stock fall which might look like an outlier but it's not. we ended our lecture on a note on how median is better than mean when we want to replace missing data because it is not affected by outliers.","we started with a small recap of previous class on confusion matrix for multiple classes. we then started exploratory data analysis where we looked at it's six cyclic steps. 1) business understanding 2) data understanding 3) data preparation 4) modelling 5) evaluation 6) deployment out of which the 2nd and 3rd one were the ones which we dug deeper into. we had a brief discussion on them. then a ta joined us to show an example of this. the problem was to predict if the patient had diabetes. we saw how some parameters lime glucose and bmi had normal distribution and paramaters like insulin and pregnancies had exponential distribution. we then looked at inter feature relation plots where we also looked at box plots where insulin showed some outliers. then we had discussion on what to do when some of the data is missing. we can either drop it or fill it with parameters like mean or median. when data is clustered, we can fill the missing data with mean/median but when the data follows say a parabolic distribution, filling it with a mean will result in creation of outlier. we can use the closest value instead of using mean which is known as multivariate data imputation. we then had a brief discussion on t-sne plot which helps in visualing higher dimensional data. we then looked at a recent example of nvidia stock fall which might look like an outlier but it's not. we ended our lecture on a note on how median is better than mean when we want to replace missing data because it is not affected by outliers.",9,-10.516858,17.3512,9.009116,9.126898,"dataâ, analyse, analyses"
353,"the best way to make use of a single sample (with multiple observations)

introducing the concept of sampling distribution of the mean to predict the range in which popoulation mean will lie. introduce confidence interval. introduce sample standard deviation or standard error with formula. less than 30 samples, t-distribution is used to model the distribtution. difference in t-score and z-score. values in a range being statistically the same. regression, the condition of beta1 not be statistically same as zero. use of p-value for statistically similarity. 

multiple linear regression.

anova. statistical equivalence of multiple aberages suimultaneously.
f-static, msr over mse.","the best way to make use of a single sample (with multiple observations) introducing the concept of sampling distribution of the mean to predict the range in which popoulation mean will lie. introduce confidence interval. introduce sample standard deviation or standard error with formula. less than 30 samples, t-distribution is used to model the distribtution. difference in t-score and z-score. values in a range being statistically the same. regression, the condition of beta1 not be statistically same as zero. use of p-value for statistically similarity. multiple linear regression. anova. statistical equivalence of multiple aberages suimultaneously. f-static, msr over mse.",7,32.643555,3.3847349,14.773489,3.0769696,"statistics, statistical, statisticsâ"
354,"in today's class, we discuss midsem problem and focused on exploratory data analysis (eda) and feature selection techniques. we began by identifying and handling missing values in the dataset. next, we discussed whether to standardize or normalize the data before proceeding with outlier detection using box plots. for categorical data, we visualized distributions using pie charts and observed that in a heart disease dataset, the disease was underrepresented, making it difficult for a model to predict it accurately. moving forward, we explored correlation analysis and its limitations, emphasizing that correlation only shows pairwise relationships and can be misleading. to address multicollinearity, we introduced the variance inflation factor (vif), which compares one column with all others and helps in selecting the most relevant features. we briefly discussed the random forest algorithm and reviewed the e3 assignment. later, we delved into the challenges posed by high-dimensional data, where too many features per observation can lead to inefficiencies. possible solutions include dimensionality reduction techniques like pca, feature selection, or collecting more dataâ€”though the latter is often expensive and difficult. lastly, we introduced vif as a tool for handling multicollinearity, we will systematically eliminate features with high vif until we obtain a set of variables with an acceptable threshold.","in today's class, we discuss midsem problem and focused on exploratory data analysis (eda) and feature selection techniques. we began by identifying and handling missing values in the dataset. next, we discussed whether to standardize or normalize the data before proceeding with outlier detection using box plots. for categorical data, we visualized distributions using pie charts and observed that in a heart disease dataset, the disease was underrepresented, making it difficult for a model to predict it accurately. moving forward, we explored correlation analysis and its limitations, emphasizing that correlation only shows pairwise relationships and can be misleading. to address multicollinearity, we introduced the variance inflation factor (vif), which compares one column with all others and helps in selecting the most relevant features. we briefly discussed the random forest algorithm and reviewed the e3 assignment. later, we delved into the challenges posed by high-dimensional data, where too many features per observation can lead to inefficiencies. possible solutions include dimensionality reduction techniques like pca, feature selection, or collecting more data though the latter is often expensive and difficult. lastly, we introduced vif as a tool for handling multicollinearity, we will systematically eliminate features with high vif until we obtain a set of variables with an acceptable threshold.",9,-12.479804,12.507846,8.888736,8.101684,"dataâ, analyse, analyses"
355,"in today's class we discussed the midsem paper and e3 assignment that was given before. apart from that we discussed about the curse of dimensionality and variance inflation factor (vif) in machine learning.

curse of dimensionality: as the number of features in a dataset increases, data points become more spread out, making pattern recognition harder. this also leads to more complex models, higher computational costs, and difficulty in measuring distances between points.

variance inflation factor (vif): we discussed how vif helps detect multicollinearity, which occurs when features in a dataset are highly correlated. high multicollinearity can make a model unstable and reduce its reliability.","in today's class we discussed the midsem paper and e3 assignment that was given before. apart from that we discussed about the curse of dimensionality and variance inflation factor (vif) in machine learning. curse of dimensionality: as the number of features in a dataset increases, data points become more spread out, making pattern recognition harder. this also leads to more complex models, higher computational costs, and difficulty in measuring distances between points. variance inflation factor (vif): we discussed how vif helps detect multicollinearity, which occurs when features in a dataset are highly correlated. high multicollinearity can make a model unstable and reduce its reliability.",13,-8.2162285,6.9196086,9.264797,7.314453,"classification, classifying, classifications"
356,"in today's session, the professor began by reviewing the mid-semester exam and explaining the approach to solving the given problem. he first covered the primary steps of exploratory data analysis (eda), including how to visualize data, handle missing values, determine appropriate imputation values, and deal with outliers. the professor then discussed the importance of determining whether to normalize or standardize the data.

by reviewing the bar plot of the target variable, we observed that the target for ""heart diseases"" is under-sampled. in this case, if obtaining more data is not possible, we must acknowledge that accurately predicting the target is not feasible. the professor also demonstrated how to extract insights from histograms and heat maps of the features, and how to decide whether we need to drop certain columns, apply feature scaling, or use dimensionality reduction.

he then explained when to use techniques like pca (principal component analysis) or t-sne. the selection of the appropriate method, such as a tree-based model, svm, or logistic regression, depends on the type of data and the problem we are aiming to solve.

curse of dimensionality arises in high-dimensional spaces, causing sparsity, overfitting, and computational inefficiency. vif detects multicollinearity; high vif (>10) indicates predictors are near-linear combinations, causing unstable estimates. while vif addresses redundancy, the curse focuses on feature volume. mitigate vif by removing features or regularization; combat the curse via dimensionality reduction (e.g., pca) or feature selection. both ensure robust models.

finally, the teaching assistant presented the assessment for exercise 3.","in today's session, the professor began by reviewing the mid-semester exam and explaining the approach to solving the given problem. he first covered the primary steps of exploratory data analysis (eda), including how to visualize data, handle missing values, determine appropriate imputation values, and deal with outliers. the professor then discussed the importance of determining whether to normalize or standardize the data. by reviewing the bar plot of the target variable, we observed that the target for ""heart diseases"" is under-sampled. in this case, if obtaining more data is not possible, we must acknowledge that accurately predicting the target is not feasible. the professor also demonstrated how to extract insights from histograms and heat maps of the features, and how to decide whether we need to drop certain columns, apply feature scaling, or use dimensionality reduction. he then explained when to use techniques like pca (principal component analysis) or t-sne. the selection of the appropriate method, such as a tree-based model, svm, or logistic regression, depends on the type of data and the problem we are aiming to solve. curse of dimensionality arises in high-dimensional spaces, causing sparsity, overfitting, and computational inefficiency. vif detects multicollinearity; high vif (>10) indicates predictors are near-linear combinations, causing unstable estimates. while vif addresses redundancy, the curse focuses on feature volume. mitigate vif by removing features or regularization; combat the curse via dimensionality reduction (e.g., pca) or feature selection. both ensure robust models. finally, the teaching assistant presented the assessment for exercise 3.",9,-13.412581,12.245562,8.788852,8.10989,"dataâ, analyse, analyses"
357,"there are 4 different types of measurements:-
1. nominal:- no absolute ordering between data types. ex. gender
2. ordinal:- discrete data with ordering. ex. education level.
3. interval:- has no absolute zero and is continuous. ex. temperature.
4. ratio:- an absolute zero is defined and is continuous. ex. length
 for ordinal and nominal we use classification.
interval and ratio we use regression.
general equation of a ml problem is f(x)=y where x is the input y is the output and f is our algorithm.
supervised learning is used in data with labels and then we try to predict accurate labels for unknown data, whereas in unsupervised learning unlabeled datasets are classified into clusters.","there are 4 different types of measurements:- 1. nominal:- no absolute ordering between data types. ex. gender 2. ordinal:- discrete data with ordering. ex. education level. 3. interval:- has no absolute zero and is continuous. ex. temperature. 4. ratio:- an absolute zero is defined and is continuous. ex. length for ordinal and nominal we use classification. interval and ratio we use regression. general equation of a ml problem is f(x)=y where x is the input y is the output and f is our algorithm. supervised learning is used in data with labels and then we try to predict accurate labels for unknown data, whereas in unsupervised learning unlabeled datasets are classified into clusters.",4,-26.129232,-17.605196,1.4230968,0.2313399,"classification, classifying, classifications"
358,"exploring data basically eda 
 data is tabulated in multiple tables and sometimes merge multiple files into one table. 
data mining 
when to say model is success : acceptance criteria also when to stop exit criteria.
problems with dependent variables and independent variables. 
also one ta ( a student) explained about eda and a project:
univariate and multivariate data discussion.",exploring data basically eda data is tabulated in multiple tables and sometimes merge multiple files into one table. data mining when to say model is success : acceptance criteria also when to stop exit criteria. problems with dependent variables and independent variables. also one ta ( a student) explained about eda and a project: univariate and multivariate data discussion.,6,-21.774223,17.734684,8.322425,8.941168,"summarizing, summarize, summarization"
359,"basics of linear regression - outcome might not be linear, it is simply a linear combination of independent variables y = î²0 + î²1x1 + î²2x2... (powers of independent variables are 1). a taylor series and polynomial regression. independence of (x1)^0,x1, (x1)â²,(x1)â³... ie. powers of (x1). this independence can be proven by computing the wronskian of all the powers of x1 - w(1, x1, (x1) â²...) which is a determinant with its rows consisting of derivatives of functions w.r.t. some independent variable. if this determinant is non zero then the functions are independent and this determinant comes out to be non zero. if the trend of the data is close to a particular xi then the coefficient of that xi would have the most weight in the linear combination of (xj)''s. different models that can be fit - parametric and non parametric. parametric models are flexible and allow for delta analysis directly ie. observing how the label changes upon changing a particular feature. non parametric models examples - random forest, xg boost, k nearest neighbors etc. neural networks (parametric model) introduction. logistic regression (outcome is a classifier). finding eqn for boundaries in logistic regression. sigmoid function of variable (a) = 1/(1+exp(-a)) which can be thought of as an on or off functions (cut off and step up functions).","basics of linear regression - outcome might not be linear, it is simply a linear combination of independent variables y = 0 + 1x1 + 2x2... (powers of independent variables are 1). a taylor series and polynomial regression. independence of (x1)^0,x1, (x1) ,(x1) ... ie. powers of (x1). this independence can be proven by computing the wronskian of all the powers of x1 - w(1, x1, (x1) ...) which is a determinant with its rows consisting of derivatives of functions w.r.t. some independent variable. if this determinant is non zero then the functions are independent and this determinant comes out to be non zero. if the trend of the data is close to a particular xi then the coefficient of that xi would have the most weight in the linear combination of (xj)''s. different models that can be fit - parametric and non parametric. parametric models are flexible and allow for delta analysis directly ie. observing how the label changes upon changing a particular feature. non parametric models examples - random forest, xg boost, k nearest neighbors etc. neural networks (parametric model) introduction. logistic regression (outcome is a classifier). finding eqn for boundaries in logistic regression. sigmoid function of variable (a) = 1/(1+exp(-a)) which can be thought of as an on or off functions (cut off and step up functions).",2,7.1373043,-2.8235006,10.249617,4.093364,"regression, regressions, features"
360,"as sir discussed, exploratory data analysis (eda) is a crucial step in machine learning, helping to check hypotheses and find patterns. handling missing values depends on column distribution and can be done using interpolation, forward/backward filling, or ml models like regression. sir demonstrated box and matrix plots, which help detect outliers and feature relations. dbscan, as sir explained, clusters data based on neighborhood density. outliers affect the mean but not the median. sir introduced the crisp-dm framework, which guides data mining in six cyclic steps. setting deadlines in ml projects, as sir advised, is essential to avoid endless work.","as sir discussed, exploratory data analysis (eda) is a crucial step in machine learning, helping to check hypotheses and find patterns. handling missing values depends on column distribution and can be done using interpolation, forward/backward filling, or ml models like regression. sir demonstrated box and matrix plots, which help detect outliers and feature relations. dbscan, as sir explained, clusters data based on neighborhood density. outliers affect the mean but not the median. sir introduced the crisp-dm framework, which guides data mining in six cyclic steps. setting deadlines in ml projects, as sir advised, is essential to avoid endless work.",9,-18.140469,16.610186,8.616261,8.409705,"dataâ, analyse, analyses"
361,"so in today's session we learnt about eda and how can we gain insights about our data along with the techniques used to solve the problems that exist in our raw data. till now, we've been working on only one excel file containing all the data but in reality we have tons of files with data to work on when we're deailing with real-life problems. then we learnt about how crisp(cross industry standard process for data mining) dm outlines the steps when we're working with real-life projects which includes business and data understanding, preparation, modeling, evaluation and deployment. then we delved into how we actually do eda on our datasets. we use boxplots to check the variability in our data, feature correlation matrix can be used to understand the data, inter-feature correlation can be checked via matrix plots two at a time. class imbalance is also an important aspect of a data as it can severely affect the performance of our model. then, we jumped on how to deal with missing values in our data. trends can be useful in filling up missing values. some ml algorithms can on their own, identify the missing values and deal with it whereas simple models such as linear regression cannot. mar and mnar are two types of missing values in which mnar is tough to deal with. we can either drop the row but doing this for several rows could impact the quantity of data we are working on so we fix it by filling up the missing values by various methods which include replacing the missing values with the columns with data statistics(mean, median or mode) , using knn or mice to solve for missing values or we could use value of last neighbours when we're dealing with a time series data. when dealing with outliers, we use median value (q2) , q1 and q3 in boxplots and define a tolerance value outside of which a data value will be considered an outlier, we could also use column std. deviation and anything outside of  mean +-3 x std.dev will be considered an outlier. dbscan can also be used. while dealing with outliers, median is a good method and mean is not because mean is affected by the outliers and median is not.","so in today's session we learnt about eda and how can we gain insights about our data along with the techniques used to solve the problems that exist in our raw data. till now, we've been working on only one excel file containing all the data but in reality we have tons of files with data to work on when we're deailing with real-life problems. then we learnt about how crisp(cross industry standard process for data mining) dm outlines the steps when we're working with real-life projects which includes business and data understanding, preparation, modeling, evaluation and deployment. then we delved into how we actually do eda on our datasets. we use boxplots to check the variability in our data, feature correlation matrix can be used to understand the data, inter-feature correlation can be checked via matrix plots two at a time. class imbalance is also an important aspect of a data as it can severely affect the performance of our model. then, we jumped on how to deal with missing values in our data. trends can be useful in filling up missing values. some ml algorithms can on their own, identify the missing values and deal with it whereas simple models such as linear regression cannot. mar and mnar are two types of missing values in which mnar is tough to deal with. we can either drop the row but doing this for several rows could impact the quantity of data we are working on so we fix it by filling up the missing values by various methods which include replacing the missing values with the columns with data statistics(mean, median or mode) , using knn or mice to solve for missing values or we could use value of last neighbours when we're dealing with a time series data. when dealing with outliers, we use median value (q2) , q1 and q3 in boxplots and define a tolerance value outside of which a data value will be considered an outlier, we could also use column std. deviation and anything outside of mean +-3 x std.dev will be considered an outlier. dbscan can also be used. while dealing with outliers, median is a good method and mean is not because mean is affected by the outliers and median is not.",9,-13.4544935,17.697441,9.189171,9.062099,"dataâ, analyse, analyses"
362,"we first explored some algorithms used for checking assignments, which work by measuring the similarity between answers. this gave us insight into how automated grading systems function. next, we dived into multiple linear regression, where a dependent variable is influenced by multiple independent variables, also known as features. these features are represented as a vector (x1 x2. xn), and the model follows the equation: y=b0+b1x1+b2x2+.+bnxn. alongside the ubiquitous r-values and p-values, we also discussed the f-value, which provides an estimate for assessing the goodness of fit of the model. by definition, an f-value is the ratio of explained to unexplained variation, so the greater the f-value, the better the model.

to test this, we worked with a dataset, calculated error metrics, and analyzed different parameters. one key takeaway was that the f-value, unlike r-values, does not have a fixed reference point as to what counts as ""good."" instead, it is better to compare the f-values of two models and say that whatever model has a higher f-value is generally a better model.","we first explored some algorithms used for checking assignments, which work by measuring the similarity between answers. this gave us insight into how automated grading systems function. next, we dived into multiple linear regression, where a dependent variable is influenced by multiple independent variables, also known as features. these features are represented as a vector (x1 x2. xn), and the model follows the equation: y=b0+b1x1+b2x2+.+bnxn. alongside the ubiquitous r-values and p-values, we also discussed the f-value, which provides an estimate for assessing the goodness of fit of the model. by definition, an f-value is the ratio of explained to unexplained variation, so the greater the f-value, the better the model. to test this, we worked with a dataset, calculated error metrics, and analyzed different parameters. one key takeaway was that the f-value, unlike r-values, does not have a fixed reference point as to what counts as ""good."" instead, it is better to compare the f-values of two models and say that whatever model has a higher f-value is generally a better model.",0,10.492668,-4.1996274,9.787129,4.924036,"models, feature, features"
363,"this session was mainly focused on simple linear regression.
in the starting of the session we learnt how to implement a simple linear regression model in ms excel using two ways which were as follows:
1) manually entering the formulas for calculating the regression coefficients:- here we first calculated all the required entities to calculate the regression coefficients that are xbar, ybar, xy, xybar, xsqbar etc. after this using the derived formula we calculate the regression coefficients beta0 and beta1. after that we created a scatter plot of the data as well as the predicted values using the regression coefficients.

2) using data analysis tool - just select the data and you will get a complete analysis of the data along with the regression coefficients there confidence intervals, r2 score etc.

after this we learnt how to decide whether a model is a ""good model"" or not. a good model is a model that is able to explain the variation in the data. for this we calculate metrics like r2 (c.o.d), co-relation and co-relation coefficient. we have other metrics also like mae, mse etc. we train the model on a sample of the population so we want to create a general model which can generalize things well and do prediction for population. we create confidence intervals. and the class ended with the discussion on the central limit theorem.","this session was mainly focused on simple linear regression. in the starting of the session we learnt how to implement a simple linear regression model in ms excel using two ways which were as follows: 1) manually entering the formulas for calculating the regression coefficients:- here we first calculated all the required entities to calculate the regression coefficients that are xbar, ybar, xy, xybar, xsqbar etc. after this using the derived formula we calculate the regression coefficients beta0 and beta1. after that we created a scatter plot of the data as well as the predicted values using the regression coefficients. 2) using data analysis tool - just select the data and you will get a complete analysis of the data along with the regression coefficients there confidence intervals, r2 score etc. after this we learnt how to decide whether a model is a ""good model"" or not. a good model is a model that is able to explain the variation in the data. for this we calculate metrics like r2 (c.o.d), co-relation and co-relation coefficient. we have other metrics also like mae, mse etc. we train the model on a sample of the population so we want to create a general model which can generalize things well and do prediction for population. we create confidence intervals. and the class ended with the discussion on the central limit theorem.",5,19.51192,-1.6171525,13.684335,4.8445773,"regression, statistical, statistics"
364,"today's class covered the concepts of simple and multiple linear regression, logistic regression and k-means clustering theoretically. we also covered the 4 different levels of measurement namely nominal, ordinal, interval and ratio. nominal and ordinal deal with categorical and discrete variables while interval and ratio are for continuous variables. nominal is primarily used for qualitative and quantitative data presentation and ordinal like in case of gender or colour and ordinal is primarily for ranking data in a particular order. interval deals with a particular order too but between two specific points. in this case zero value is considered arbitrary. an example is temperature- relation between 100â°c and 50â°c is not the same as 50â°c and 25â°c but same as 150â°c and 100â°c. they do not have a linear relationship as zero is not the true zero in this level. ratio considers a true zero and linear relations may exist.
another concept we learnt was the basic equation in machine learning, y=f(x). here y is called label and x constitutes the features. models having labels are called supervised learning techniques and those without labels are called unsupervised learning techniques. k-means clustering and hierarchical clustering are used to make labels.","today's class covered the concepts of simple and multiple linear regression, logistic regression and k-means clustering theoretically. we also covered the 4 different levels of measurement namely nominal, ordinal, interval and ratio. nominal and ordinal deal with categorical and discrete variables while interval and ratio are for continuous variables. nominal is primarily used for qualitative and quantitative data presentation and ordinal like in case of gender or colour and ordinal is primarily for ranking data in a particular order. interval deals with a particular order too but between two specific points. in this case zero value is considered arbitrary. an example is temperature- relation between 100 c and 50 c is not the same as 50 c and 25 c but same as 150 c and 100 c. they do not have a linear relationship as zero is not the true zero in this level. ratio considers a true zero and linear relations may exist. another concept we learnt was the basic equation in machine learning, y=f(x). here y is called label and x constitutes the features. models having labels are called supervised learning techniques and those without labels are called unsupervised learning techniques. k-means clustering and hierarchical clustering are used to make labels.",4,-24.073204,-17.135778,1.5089443,0.08963678,"classification, classifying, classifications"
365,"we first revised a few concepts from the previous class. to assess the quality of any classification model, we use the confusion matrix. the confusion matrix gives us the number of points that are correctly identified and those that are incorrectly identified, in a tabulated manner. sir showed us some code for fitting a given data set with a logistic regression model and obtaining the various metrics associated with it. the â€˜scoreâ€™ represents the accuracy in a classification model, just like it represents r in regression. we also found out the true positive rate and the false positive rate. these are key metrics used to evaluate the performance of a classification model. true positive rate measures the fraction of the actual positives that are correctly determined by the model. similarly, false positives represent the proportion of actual negatives that are incorrectly classified as positive.
the receiver operating curve (roc) plots tpr vs fpr. if the curve is nearly vertical, then it means before classifying any points as â€˜false positivesâ€™, all the â€˜true positivesâ€™ have already been identified. on the other hand, if the line is exactly 45 degrees, it means the classification is completely random, or we can say that we are not using any classifier at all. so ideally, we want our plot to be as steep as possible. also, the area under the curve for a good classifier should be as close to 1 as possible. various points along the curve represent different threshold values. as you increase the threshold, the model becomes more stricter and hence, there are lesser chances of obtaining true positives, thereby increasing the false negatives. 
a good classifier is fine until there arenâ€™t any significant overlaps in the data sets. however, if the two sets have significantly high overlap, then no classifier will work well for it.
so, we can improve the quality of our classifier by either transforming the data or creating new and relevant features using feature engineering.
we also looked at yet another metric- â€˜supportâ€™. this represents the total number of observations that are in â€˜supportâ€™ of the class, i.e. the number of observations, that have been classified under that class by the model.
next, we started with â€˜clusteringâ€™. this is another unsupervised learning algorithm. in this, the points in a given data set are clustered and different groups are made among these. this is usually a part of eda and is used to assign labels to unlabeled data. there are two types of models in it- k means clustering and hierarchical clustering. 
in k- means clustering, we have to specify the number of â€˜meansâ€™ or in other words the number of clusters that we want. this is not the case for hierarchical clustering. 
in k- means clustering, all the points are randomly classified first and based on these, a mean value is obtained. using this mean value, the distance between a point and the means of various clusters is calculated and this is done for every point. the point is reassigned to a cluster, whose mean value is closest to the point. the new mean for a cluster is re- calculated based on the new points. then again, the points are reassigned into different clusters. this is repeatedly done, until we reach a step wherein there is no change in the arrangement of the points. this is our final clustering. 
if the initial assignment of the points to various clusters differs, we can get different clusters every time for the same data set. so, we run multiple such algorithms and the final cluster for a point is chosen to be that cluster in which it falls for the maximum number of times. 
hierarchical clustering algorithm considers every point as a cluster in itself, and starts clubbing all the clusters one by one, until it reaches a final single cluster with all the points. this is represented as a dendrogram. the height of the leg of the diagram represents the distance between the two points/clusters. so, hierarchical clustering gives many different clusters, which are combined and recombined to form larger and larger clusters. we can decide upto which point we want the clusters. thatâ€™s it for this class.","we first revised a few concepts from the previous class. to assess the quality of any classification model, we use the confusion matrix. the confusion matrix gives us the number of points that are correctly identified and those that are incorrectly identified, in a tabulated manner. sir showed us some code for fitting a given data set with a logistic regression model and obtaining the various metrics associated with it. the score represents the accuracy in a classification model, just like it represents r in regression. we also found out the true positive rate and the false positive rate. these are key metrics used to evaluate the performance of a classification model. true positive rate measures the fraction of the actual positives that are correctly determined by the model. similarly, false positives represent the proportion of actual negatives that are incorrectly classified as positive. the receiver operating curve (roc) plots tpr vs fpr. if the curve is nearly vertical, then it means before classifying any points as false positives , all the true positives have already been identified. on the other hand, if the line is exactly 45 degrees, it means the classification is completely random, or we can say that we are not using any classifier at all. so ideally, we want our plot to be as steep as possible. also, the area under the curve for a good classifier should be as close to 1 as possible. various points along the curve represent different threshold values. as you increase the threshold, the model becomes more stricter and hence, there are lesser chances of obtaining true positives, thereby increasing the false negatives. a good classifier is fine until there aren t any significant overlaps in the data sets. however, if the two sets have significantly high overlap, then no classifier will work well for it. so, we can improve the quality of our classifier by either transforming the data or creating new and relevant features using feature engineering. we also looked at yet another metric- support . this represents the total number of observations that are in support of the class, i.e. the number of observations, that have been classified under that class by the model. next, we started with clustering . this is another unsupervised learning algorithm. in this, the points in a given data set are clustered and different groups are made among these. this is usually a part of eda and is used to assign labels to unlabeled data. there are two types of models in it- k means clustering and hierarchical clustering. in k- means clustering, we have to specify the number of means or in other words the number of clusters that we want. this is not the case for hierarchical clustering. in k- means clustering, all the points are randomly classified first and based on these, a mean value is obtained. using this mean value, the distance between a point and the means of various clusters is calculated and this is done for every point. the point is reassigned to a cluster, whose mean value is closest to the point. the new mean for a cluster is re- calculated based on the new points. then again, the points are reassigned into different clusters. this is repeatedly done, until we reach a step wherein there is no change in the arrangement of the points. this is our final clustering. if the initial assignment of the points to various clusters differs, we can get different clusters every time for the same data set. so, we run multiple such algorithms and the final cluster for a point is chosen to be that cluster in which it falls for the maximum number of times. hierarchical clustering algorithm considers every point as a cluster in itself, and starts clubbing all the clusters one by one, until it reaches a final single cluster with all the points. this is represented as a dendrogram. the height of the leg of the diagram represents the distance between the two points/clusters. so, hierarchical clustering gives many different clusters, which are combined and recombined to form larger and larger clusters. we can decide upto which point we want the clusters. that s it for this class.",8,-4.4179482,-21.47417,6.3050957,0.4053739,"classification, clusterings, classifying"
366,"in this session, the instructor discussed several key statistical concepts, including error distributions, standard error, sample means, and the central limit theorem (clt). a significant takeaway was that for regression models to accurately reflect the true trend in the data, the errors must be normally distributed. if the errors can be predicted, it suggests that the model has not fully captured the underlying trend of the sample, which may lead to biased results.

the session outlined how to estimate the population mean from a sample, which involves three main steps:

1.) first, calculate the sample mean, assuming it is close to 0. the standard error (sxbar) is found by dividing the population's standard deviation (sigma) by the square root of the sample size (n).
2.) next, compute the sampleâ€™s standard deviation, assuming it is a good estimate of the population's standard deviation.
3.) finally, calculate the standard deviation of the sample distribution, which is essential for determining the confidence interval (ci) that indicates where the population mean is likely to be. for example, a 95% ci means that 95 out of 100 sample means will fall within that range.

the session also delved into the concept of standard error, which reflects how much the sample mean might differ from the actual population mean. for a normal distribution, the standard error is calculated as
ð‘ /âˆšð‘›
, where ð‘  represents the sample standard deviation and ð‘› is the sample size.

following this, the instructor explained the differences between normal and t distributions. when the population standard deviation is known, the errors follow a normal distribution. conversely, if the population standard deviation is unknown and the sample size is less than 30, the data adheres to a t distribution. the z-statistic and t-statistic serve as test statistics for normal and t distributions, respectively.

the concept of the p-value was introduced, with a focus on its connection to the confidence interval. a low p-value indicates strong evidence against the null hypothesis, while a high p-value suggests weak evidence. the session also pointed out that if ð›½1 in the regression equation ð‘¦=ð›½0+ð›½1(ð‘¥) is statistically equivalent to 0, then the regression model lacks significance.

additionally, the session delved into multiple linear regression and anova (analysis of variance) as methods for comparing statistical equivalence among multiple averages. the f-statistic, calculated as ð‘€ð‘†ð‘…/ð‘€ð‘†ð¸, is utilized to evaluate the overall fit of a model in anova, determining whether the group means differ significantly.

in summary, the session offered a thorough understanding of statistical inferences, emphasizing essential concepts like standard error, p-values, confidence intervals, and the importance of regression models in data analysis. these concepts are crucial for effective data analysis and informed decision-making in statistics.","in this session, the instructor discussed several key statistical concepts, including error distributions, standard error, sample means, and the central limit theorem (clt). a significant takeaway was that for regression models to accurately reflect the true trend in the data, the errors must be normally distributed. if the errors can be predicted, it suggests that the model has not fully captured the underlying trend of the sample, which may lead to biased results. the session outlined how to estimate the population mean from a sample, which involves three main steps: 1.) first, calculate the sample mean, assuming it is close to 0. the standard error (sxbar) is found by dividing the population's standard deviation (sigma) by the square root of the sample size (n). 2.) next, compute the sample s standard deviation, assuming it is a good estimate of the population's standard deviation. 3.) finally, calculate the standard deviation of the sample distribution, which is essential for determining the confidence interval (ci) that indicates where the population mean is likely to be. for example, a 95% ci means that 95 out of 100 sample means will fall within that range. the session also delved into the concept of standard error, which reflects how much the sample mean might differ from the actual population mean. for a normal distribution, the standard error is calculated as / , where represents the sample standard deviation and is the sample size. following this, the instructor explained the differences between normal and t distributions. when the population standard deviation is known, the errors follow a normal distribution. conversely, if the population standard deviation is unknown and the sample size is less than 30, the data adheres to a t distribution. the z-statistic and t-statistic serve as test statistics for normal and t distributions, respectively. the concept of the p-value was introduced, with a focus on its connection to the confidence interval. a low p-value indicates strong evidence against the null hypothesis, while a high p-value suggests weak evidence. the session also pointed out that if 1 in the regression equation = 0+ 1( ) is statistically equivalent to 0, then the regression model lacks significance. additionally, the session delved into multiple linear regression and anova (analysis of variance) as methods for comparing statistical equivalence among multiple averages. the f-statistic, calculated as / , is utilized to evaluate the overall fit of a model in anova, determining whether the group means differ significantly. in summary, the session offered a thorough understanding of statistical inferences, emphasizing essential concepts like standard error, p-values, confidence intervals, and the importance of regression models in data analysis. these concepts are crucial for effective data analysis and informed decision-making in statistics.",7,33.4065,2.5416522,14.839415,2.988492,"statistics, statistical, statisticsâ"
367,"the lecture began with an recap of multiple linear regression did in last lecture , where we have multiple independent variables (x_1, x_2, x_3, ... x_n). although multicollinearity is an important consideration, it was noted that this would be discussed later.  

next, we covered the concept of training and test data, emphasizing that the entire sample should not be used to train the machine learning model. a typical 80-20% split (80% training, 20% testing) was recommended, as a 50-50% split might lead to an unrepresentative training sample.  

we then discussed two outcome sets: training matrix and test matrix. for the training matrix, we introduced the concept of the confidence interval. this led to a discussion on overfitting, which occurs when training accuracy is much higher than test accuracy. a graphical representation was used to illustrate this issue.  

after this, we performed regression statistics in excel. several key terms were defined:  
- multiple r: the square root of r^2 (coefficient of determination), which provides a measure of the nonlinear correlation between y and the independent variables.  
- adjusted r^2 the formula for adjusted r^2 was explained.  
- the denominator in variance formulas: we examined why it is n-1 instead of (n)due to degrees of freedom being reduced when using the mean of x.  

we clarified that linear regression does not necessarily imply a straight line, but rather a linear relationship between yand x 

towards the end, we transitioned to python programming for linear regression. several statistical tests were covered, including:  
- omnibus test and its p-value criteria
- skewness and kurtosis statistics
- durbin-watson test for autocorrelation  

additionally, we discussed quantile-quantile (q-q) plots, which help assess whether a dataset follows a particular distribution by dividing the sample distribution curve into sections.","the lecture began with an recap of multiple linear regression did in last lecture , where we have multiple independent variables (x_1, x_2, x_3, ... x_n). although multicollinearity is an important consideration, it was noted that this would be discussed later. next, we covered the concept of training and test data, emphasizing that the entire sample should not be used to train the machine learning model. a typical 80-20% split (80% training, 20% testing) was recommended, as a 50-50% split might lead to an unrepresentative training sample. we then discussed two outcome sets: training matrix and test matrix. for the training matrix, we introduced the concept of the confidence interval. this led to a discussion on overfitting, which occurs when training accuracy is much higher than test accuracy. a graphical representation was used to illustrate this issue. after this, we performed regression statistics in excel. several key terms were defined: - multiple r: the square root of r^2 (coefficient of determination), which provides a measure of the nonlinear correlation between y and the independent variables. - adjusted r^2 the formula for adjusted r^2 was explained. - the denominator in variance formulas: we examined why it is n-1 instead of (n)due to degrees of freedom being reduced when using the mean of x. we clarified that linear regression does not necessarily imply a straight line, but rather a linear relationship between yand x towards the end, we transitioned to python programming for linear regression. several statistical tests were covered, including: - omnibus test and its p-value criteria - skewness and kurtosis statistics - durbin-watson test for autocorrelation additionally, we discussed quantile-quantile (q-q) plots, which help assess whether a dataset follows a particular distribution by dividing the sample distribution curve into sections.",2,16.80437,0.98370683,12.18699,4.8112764,"regression, regressions, features"
368,"linear regression is expressed as a sum of linearly independent variables. experimented with sin(x) is a variable that results in the error being distributed evenly about zero, although not normally. an effort to add more variable could result in overfitting. a comparision of different models performance on a certain nonlinear data was demonstrated. xgboost, random forest and ann performed better than slr and mlr judging from the fit and the error distribution. slr and mlr are parametric models while the former three are non-parametric models. introduction to logistic regression as a classifier. the distance from the separatin plane is passed through the sigmoid function to obtain value 'a' between 0 and 1.","linear regression is expressed as a sum of linearly independent variables. experimented with sin(x) is a variable that results in the error being distributed evenly about zero, although not normally. an effort to add more variable could result in overfitting. a comparision of different models performance on a certain nonlinear data was demonstrated. xgboost, random forest and ann performed better than slr and mlr judging from the fit and the error distribution. slr and mlr are parametric models while the former three are non-parametric models. introduction to logistic regression as a classifier. the distance from the separatin plane is passed through the sigmoid function to obtain value 'a' between 0 and 1.",2,6.641489,-1.7240952,10.349578,4.2547393,"regression, regressions, features"
369,"we started today's lecture with a recap about what happened in the last class and a bit of discussion about the data analysis done by the professor regarding the responses he was getting as the session summaries. then we started our discussion about multiple linear regression whose formulation consists of more than one independent variable. these independent variables are also called as features. we came to know that for multiple linear regression, there does not exist a closed form solution. hence we need to derive it's solution numerically. for this we use something called as gradient descent. first we write all the mlr equations in the matrix form as the solution for a n featured mlr exists in the n th dimension. writing in matrix form helps us in calculation. then after this we derived its cost function and minimising this cost function provides us with a condition which we use in a numerical method to obtain the solutions. then we hoped onto excel to try mlr ourselves and calculate the regression coefficients and tried to gain some insights by manipulating some of the statistics.","we started today's lecture with a recap about what happened in the last class and a bit of discussion about the data analysis done by the professor regarding the responses he was getting as the session summaries. then we started our discussion about multiple linear regression whose formulation consists of more than one independent variable. these independent variables are also called as features. we came to know that for multiple linear regression, there does not exist a closed form solution. hence we need to derive it's solution numerically. for this we use something called as gradient descent. first we write all the mlr equations in the matrix form as the solution for a n featured mlr exists in the n th dimension. writing in matrix form helps us in calculation. then after this we derived its cost function and minimising this cost function provides us with a condition which we use in a numerical method to obtain the solutions. then we hoped onto excel to try mlr ourselves and calculate the regression coefficients and tried to gain some insights by manipulating some of the statistics.",2,18.369635,9.032325,12.485927,4.3494725,"regression, regressions, features"
370,prof. started with introducing excel and finding errors and predicted output. then scatter plot the x and y. then also plotted errors and histograms.moving on we studied distribution is random or not then get to know outcome dependent on large number unknown cases the distribution observed is gaussian distribution. then understood coefficient of determination and square of the correlation coefficient.then also understood population mean and sample mean,prof. started with introducing excel and finding errors and predicted output. then scatter plot the x and y. then also plotted errors and histograms.moving on we studied distribution is random or not then get to know outcome dependent on large number unknown cases the distribution observed is gaussian distribution. then understood coefficient of determination and square of the correlation coefficient.then also understood population mean and sample mean,5,26.315283,-4.154914,14.191052,4.5783215,"regression, statistical, statistics"
371,"we started our lecture by revisiting the goals in exploratory data analysis which were getting insights, observing anomalies, testing hypothesis and checking assumptions. we then moved moved to a dataset having 500 summaries of our own course obtained over the past lectures. we analysed the data by observing things like number of words/characters per submission. each submission had nearly 1200 words on average but there were some outliers reaching upto 6000 words. we had a breif look on how to use pitot tables and its uses. we then looked on a real word problem which was based on optimisation in chemical plant. there were nearly 250 parameters to analyse. we used multiple plots to get an overview of the data. we also saw a histogram which was following normal distribution but there some points at 0 (outliers). it may be attributed to sensor failure but we cannot directly discard this outliers keeping in mind the nvidia stock example. thus domain knowledge is very important before discarding this values and requires cross questioning with the stakeholders. we then saw second example based on tranformer failures. later, tas joined the lecture and gave feedback on assignment e2 and concluded the lecture.","we started our lecture by revisiting the goals in exploratory data analysis which were getting insights, observing anomalies, testing hypothesis and checking assumptions. we then moved moved to a dataset having 500 summaries of our own course obtained over the past lectures. we analysed the data by observing things like number of words/characters per submission. each submission had nearly 1200 words on average but there were some outliers reaching upto 6000 words. we had a breif look on how to use pitot tables and its uses. we then looked on a real word problem which was based on optimisation in chemical plant. there were nearly 250 parameters to analyse. we used multiple plots to get an overview of the data. we also saw a histogram which was following normal distribution but there some points at 0 (outliers). it may be attributed to sensor failure but we cannot directly discard this outliers keeping in mind the nvidia stock example. thus domain knowledge is very important before discarding this values and requires cross questioning with the stakeholders. we then saw second example based on tranformer failures. later, tas joined the lecture and gave feedback on assignment e2 and concluded the lecture.",9,-6.662406,25.518604,8.174148,9.724904,"dataâ, analyse, analyses"
372,we recapped in the starting about the closed form solution in multiple linear regression which is impractical as we have to deal with matrix inversion and also one more issue is presence of multi collinearity. also during feature selection if p value is greater than 0.05 we remove that feature as it is not contributing much. then we learnt about train and test data today . typically use 80-20 %. randomly split after doing exploratory data analysis. also we saw if r2 values of training and testing matrixes are close enough otherwise issue of overfit can happen. we want model to learn and represent population. we also in detail saw what is adjusted r2 multiple r and why there is only n-1 degree of freedom in tss. we saw quartile plots and importance of errors being normally distributed or not. we saw python lib called as pandas and see training testing and fitting data. also we saw the case where sometikes test stats is inside ci and sometimes not basically hypothesis testing.,we recapped in the starting about the closed form solution in multiple linear regression which is impractical as we have to deal with matrix inversion and also one more issue is presence of multi collinearity. also during feature selection if p value is greater than 0.05 we remove that feature as it is not contributing much. then we learnt about train and test data today . typically use 80-20 %. randomly split after doing exploratory data analysis. also we saw if r2 values of training and testing matrixes are close enough otherwise issue of overfit can happen. we want model to learn and represent population. we also in detail saw what is adjusted r2 multiple r and why there is only n-1 degree of freedom in tss. we saw quartile plots and importance of errors being normally distributed or not. we saw python lib called as pandas and see training testing and fitting data. also we saw the case where sometikes test stats is inside ci and sometimes not basically hypothesis testing.,2,7.9648438,7.388111,11.20075,5.0662036,"regression, regressions, features"
373,"firstly we learnt about the basic ml techniques which are used in data science such as simple linear regression, multiple linear regression, logistic regression etc.

and how using ml techniques curve or graph are plotted for scatter plot for given data to know the relation between input and output which forms empirical equation. along with that we learnt the four levels of measurement which are :
1. nominal - in this no particular order or sequence is followed for data values. it takes discrete values. for example, gender or colours in which one value is not at higher or lower value from the other this are not in particular order but at same level.
2. ordinal - in this data  follows a certain specific order or sequence. basically it has a inherited order but it also has discrete data values. for example, grades which have discrete values but grades have a high or low value.
3. interval - in this data is divided into certain intervals in a particular order, but zero has a arbitrary meaning. it has continuous data values. for example, temperature ( 5â°c is not twice as cold as 10â°c in actual. )
4. ratio - data follows a particular order with equal intervals but zero do have a meaning. it also has a continuous data. for example, height and weight.

nominal and ordinal falls in classification category of ml while internal and ratio falls in regression category of ml.

learnt about y=f(x), where u is label and x known as features. in supervised learning labels are known but in unsupervised they aren't. in unsupervised k means clustering and hierarchical clustering are used to make label for problems.

also a question was discussed 
question: if y is nominal, is it possible x1 is also nominal? 
ans: yes","firstly we learnt about the basic ml techniques which are used in data science such as simple linear regression, multiple linear regression, logistic regression etc. and how using ml techniques curve or graph are plotted for scatter plot for given data to know the relation between input and output which forms empirical equation. along with that we learnt the four levels of measurement which are : 1. nominal - in this no particular order or sequence is followed for data values. it takes discrete values. for example, gender or colours in which one value is not at higher or lower value from the other this are not in particular order but at same level. 2. ordinal - in this data follows a certain specific order or sequence. basically it has a inherited order but it also has discrete data values. for example, grades which have discrete values but grades have a high or low value. 3. interval - in this data is divided into certain intervals in a particular order, but zero has a arbitrary meaning. it has continuous data values. for example, temperature ( 5 c is not twice as cold as 10 c in actual. ) 4. ratio - data follows a particular order with equal intervals but zero do have a meaning. it also has a continuous data. for example, height and weight. nominal and ordinal falls in classification category of ml while internal and ratio falls in regression category of ml. learnt about y=f(x), where u is label and x known as features. in supervised learning labels are known but in unsupervised they aren't. in unsupervised k means clustering and hierarchical clustering are used to make label for problems. also a question was discussed question: if y is nominal, is it possible x1 is also nominal? ans: yes",4,-25.130827,-16.81905,1.4710377,0.20980509,"classification, classifying, classifications"
374,"today's lesson began with an introduction to excel pivot tables and their applications in summarizing and analyzing huge datasets. in order to understand the data, we practiced making pivot tables, dragging and dropping fields, and computing important metrics (totals, averages, counts, and percentages). after that, we did exploratory data analysis (eda), to observe data distribution using summary statistics such as mean, median, variance, and standard deviation. to find trends, patterns, and outliers, we also observed graphs, scatter plots, box plots, and histograms. then, we saw class imbalance, where results are highly influenced by a single category. then we saw. feature engineering and data transformation done before further analysis to clean the data.","today's lesson began with an introduction to excel pivot tables and their applications in summarizing and analyzing huge datasets. in order to understand the data, we practiced making pivot tables, dragging and dropping fields, and computing important metrics (totals, averages, counts, and percentages). after that, we did exploratory data analysis (eda), to observe data distribution using summary statistics such as mean, median, variance, and standard deviation. to find trends, patterns, and outliers, we also observed graphs, scatter plots, box plots, and histograms. then, we saw class imbalance, where results are highly influenced by a single category. then we saw. feature engineering and data transformation done before further analysis to clean the data.",6,-13.716919,26.162598,7.20648,9.985091,"summarizing, summarize, summarization"
375,in today's class we mainly discussed vif and pca. vif stands for variance inflation factor which is used to see if there is any relation between the features or not. higher value of vif suggests a high co-relation. then we also discussed a plot between r squared and vif where we saw elbows at 5 and 10. then we discussed pca  which stands for principle component analysis where we choose an axis where the variance is maximum of the features. then the other axis has some less variance and this goes on. single value decomposition is used while performing principle component analysis.,in today's class we mainly discussed vif and pca. vif stands for variance inflation factor which is used to see if there is any relation between the features or not. higher value of vif suggests a high co-relation. then we also discussed a plot between r squared and vif where we saw elbows at 5 and 10. then we discussed pca which stands for principle component analysis where we choose an axis where the variance is maximum of the features. then the other axis has some less variance and this goes on. single value decomposition is used while performing principle component analysis.,11,-16.577301,-0.07694044,10.147197,12.89473,"pca, heatmap, heatmaps"
376,"the class started with the recap of the previous class. we looked at an example of linear regression today , used a csv file for data and excel as tool to analyse the data. we used the formulas obtained for ""a"" and ""b"" for one dimensional simple linear regression which were derived in the previous class. used the formula methods of the excel to find error term of the  slr and plotted a scatter plot of the data with the predicted line. then we plotted a scatter plot of the error. learnt that the errors as to be random that is there should be no trend in the error plot. if there is a trend left in the error plot it suggests that the model is best there can be improvements. we used the in built regression tool of the excel to get various analysis scores of the linear model. we discussed about the rsqr term it measures how much of the actual variance is captured by the regression line. higher r^2 value means model is good.  we started the discussion on how to find the lower and upper bounds of confidence level","the class started with the recap of the previous class. we looked at an example of linear regression today , used a csv file for data and excel as tool to analyse the data. we used the formulas obtained for ""a"" and ""b"" for one dimensional simple linear regression which were derived in the previous class. used the formula methods of the excel to find error term of the slr and plotted a scatter plot of the data with the predicted line. then we plotted a scatter plot of the error. learnt that the errors as to be random that is there should be no trend in the error plot. if there is a trend left in the error plot it suggests that the model is best there can be improvements. we used the in built regression tool of the excel to get various analysis scores of the linear model. we discussed about the rsqr term it measures how much of the actual variance is captured by the regression line. higher r^2 value means model is good. we started the discussion on how to find the lower and upper bounds of confidence level",5,19.335794,-5.9952097,14.454001,5.0257573,"regression, statistical, statistics"
377,"to keep us interested in ds and making models in genera, till now we have been given good/tame data- data without inherent problems. but now we deal with data that has problems associated with it. and fixing them is a big part of data science
midsem is going to test you on understanding the problem

crisp-dm
1. business understanding
2. data understanding
3. data preparation
4. .
..
..

in today's session we focused on- data understanding and preparation
we tried to understand what quality of data is?

heteroscedasticity- when variance of errors is not constant

to begin with, one common eda technique is to plot histograms
one more technique is boxplot- helps us visualise outliers, basically what are the variations of the data
feature correlation- by a heat map
matrix plot- to show relations between the features 2 at a time
we should be wary that if a feature is dependent on more than one feature it may not be captured by the matrix plot or through boxplots

while looking at data, we also have to handle class imbalance

by observing past trends i can predict missing values; past data would tell us that the missing value would lie in a narrow band of possibility
there are 3 types of missing data- mcar, mar, mnar
so the question is- how to fix missig data??
-do nothing-  
-drop missing fields-
-inference from mean-
some things to give a thought about?
>> why focus on median rather than mean for detecting outliers: mean is influenced by outliers but the median is not
>> how is median calculated?
>> do not apply techniques blindly, just because they have been mentioned in class","to keep us interested in ds and making models in genera, till now we have been given good/tame data- data without inherent problems. but now we deal with data that has problems associated with it. and fixing them is a big part of data science midsem is going to test you on understanding the problem crisp-dm 1. business understanding 2. data understanding 3. data preparation 4. . .. .. in today's session we focused on- data understanding and preparation we tried to understand what quality of data is? heteroscedasticity- when variance of errors is not constant to begin with, one common eda technique is to plot histograms one more technique is boxplot- helps us visualise outliers, basically what are the variations of the data feature correlation- by a heat map matrix plot- to show relations between the features 2 at a time we should be wary that if a feature is dependent on more than one feature it may not be captured by the matrix plot or through boxplots while looking at data, we also have to handle class imbalance by observing past trends i can predict missing values; past data would tell us that the missing value would lie in a narrow band of possibility there are 3 types of missing data- mcar, mar, mnar so the question is- how to fix missig data?? -do nothing- -drop missing fields- -inference from mean- some things to give a thought about? >> why focus on median rather than mean for detecting outliers: mean is influenced by outliers but the median is not >> how is median calculated? >> do not apply techniques blindly, just because they have been mentioned in class",9,-14.074505,17.528942,9.218719,9.012863,"dataâ, analyse, analyses"
378,"today's class started with discussing about confusion matrix (which was also discussed in the last class) for multiple classes. after this we started to discuss about the six cyclic steps in exploratory data analysis which includes business understanding, data understanding, data preparation, modelling, evaluation and deployment where data understanding was discussed briefly. an example was also discussed. the example constituted of how to check whether the person has diabetes or not. for this, we took some metrics about the patients health like glucose level, body mass index, insulin levels and tried to find out their distributions. we tried find some relation between these parameters. we also saw outliers in the case of box plots of insulin. if some data is missing, we can either drop it or replace it with the mean or median, but doing this will not always result in something good. incase of parabolic distribution, replacing may cause an outlier. to avoid this we can use something called as multivariate data imputation where the closest value is chosed. we also looked at an example of an nvidia stock to discuss more about outliers. then we concluded our lecture by saying that median is better than mean when it's comes to replacing that missing data as median is not affected by outliers.","today's class started with discussing about confusion matrix (which was also discussed in the last class) for multiple classes. after this we started to discuss about the six cyclic steps in exploratory data analysis which includes business understanding, data understanding, data preparation, modelling, evaluation and deployment where data understanding was discussed briefly. an example was also discussed. the example constituted of how to check whether the person has diabetes or not. for this, we took some metrics about the patients health like glucose level, body mass index, insulin levels and tried to find out their distributions. we tried find some relation between these parameters. we also saw outliers in the case of box plots of insulin. if some data is missing, we can either drop it or replace it with the mean or median, but doing this will not always result in something good. incase of parabolic distribution, replacing may cause an outlier. to avoid this we can use something called as multivariate data imputation where the closest value is chosed. we also looked at an example of an nvidia stock to discuss more about outliers. then we concluded our lecture by saying that median is better than mean when it's comes to replacing that missing data as median is not affected by outliers.",9,-10.64033,17.285692,8.952839,9.142514,"dataâ, analyse, analyses"
379,"we started our lecture with a recap of previous lecture seeing the examples of force fitted data. the lecture then covered the statistical foundations of linear regression. we looked at a case where sample has very few elements, like around 30. we studied how to deal with such samples by using the t distribution focusing on the interpretation of p-values and their role in validating regression models. it began by explaining that regression coefficients estimated from a sample may vary if a different sample were used. this variability necessitates an assessment of whether the estimated coefficients are statistically significant or just a result of chance. if the 95% confidence interval does not contain zero, then the coefficient is significant. the lecture also introduced the p-value, which measures the probability that the observed coefficient value could happen by chance. a p-value below 0.05 indicates strong chances of the coefficient being correct. additionally, the effect of sample size on the reliability of regression was underscored. more extensive samples generate smaller standard errors, and consequently, narrower confidence intervals with greater chances of picking up true parameters. conversely, a smaller sample is associated with larger uncertainty.","we started our lecture with a recap of previous lecture seeing the examples of force fitted data. the lecture then covered the statistical foundations of linear regression. we looked at a case where sample has very few elements, like around 30. we studied how to deal with such samples by using the t distribution focusing on the interpretation of p-values and their role in validating regression models. it began by explaining that regression coefficients estimated from a sample may vary if a different sample were used. this variability necessitates an assessment of whether the estimated coefficients are statistically significant or just a result of chance. if the 95% confidence interval does not contain zero, then the coefficient is significant. the lecture also introduced the p-value, which measures the probability that the observed coefficient value could happen by chance. a p-value below 0.05 indicates strong chances of the coefficient being correct. additionally, the effect of sample size on the reliability of regression was underscored. more extensive samples generate smaller standard errors, and consequently, narrower confidence intervals with greater chances of picking up true parameters. conversely, a smaller sample is associated with larger uncertainty.",7,27.128078,5.3626666,14.231245,3.342308,"statistics, statistical, statisticsâ"
380,"in today's lecture first we started by answering the question how to improve the quality of results? there are three ways to do so:
1-  to increase the quality  and size of sample|
2- improve the method like by using multiple methods and selecting the best one
3- fine tuning or properly using the methods
then we learnt that in linear regression outcome is expressed as linear combination of independent variables and there is nothing else linear in that.
when we have four features which are like x , x^2,x^3,x^4 and sin x , then
eventually your model will try to minimise coefficients of all the x1 to x4 and the coefficient of sin x will resemble the most that is will be the maximum so many of the features have to be eliminated otherwise the model will become unfavourable.
then we learn the basic difference between forward feature engineering and backward feature engineering. forward feature engineering is that we start with one feature and then keep adding until the performance becomes better and backward feature engineering is just the reverse of it that is we start with all the features and eliminate 1 by 1 based on the performance.
so naturally what we do is that we get the data we perform exploratory data analysis then we preprocesses and then we see multiple methods and then we compare them using the matrix and then select the best one.
after that we learnt about parametric like neural networks like which has the weights involved and if there is more than one layer in a neural network it is called as deep learning network.
in artificial neural network the data should be large otherwise overfitting will happen.
if we have to compare just the model within itself then we compare mean squared error with the value of y bar to get percentage and check whether the model is good within itself or not.
can we started with classification which is supervised learning in which why denotes the class and we have x and y both are available it is used for distinguishing between the discrete values.
then we learnt about regression and in which we learnt about the basic of logistic regression in which outcome is a classifier and our main purpose is to draw the boundaries between different types of classes.","in today's lecture first we started by answering the question how to improve the quality of results? there are three ways to do so: 1- to increase the quality and size of sample| 2- improve the method like by using multiple methods and selecting the best one 3- fine tuning or properly using the methods then we learnt that in linear regression outcome is expressed as linear combination of independent variables and there is nothing else linear in that. when we have four features which are like x , x^2,x^3,x^4 and sin x , then eventually your model will try to minimise coefficients of all the x1 to x4 and the coefficient of sin x will resemble the most that is will be the maximum so many of the features have to be eliminated otherwise the model will become unfavourable. then we learn the basic difference between forward feature engineering and backward feature engineering. forward feature engineering is that we start with one feature and then keep adding until the performance becomes better and backward feature engineering is just the reverse of it that is we start with all the features and eliminate 1 by 1 based on the performance. so naturally what we do is that we get the data we perform exploratory data analysis then we preprocesses and then we see multiple methods and then we compare them using the matrix and then select the best one. after that we learnt about parametric like neural networks like which has the weights involved and if there is more than one layer in a neural network it is called as deep learning network. in artificial neural network the data should be large otherwise overfitting will happen. if we have to compare just the model within itself then we compare mean squared error with the value of y bar to get percentage and check whether the model is good within itself or not. can we started with classification which is supervised learning in which why denotes the class and we have x and y both are available it is used for distinguishing between the discrete values. then we learnt about regression and in which we learnt about the basic of logistic regression in which outcome is a classifier and our main purpose is to draw the boundaries between different types of classes.",0,2.3502734,-4.828102,9.445024,3.906697,"models, feature, features"
381,"we discussed the midsem paper. we started with eda using scatter plots and histograms, which showed our data needed normalization. boxplots revealed no outliers. bar and pie charts showed very few heart disease cases, so we dropped that data from our study. we also used a heatmap to check correlations and random forest model, which performed well except for heart disease. then we switched to kde plots and discussed ways to reduce dimensions.","we discussed the midsem paper. we started with eda using scatter plots and histograms, which showed our data needed normalization. boxplots revealed no outliers. bar and pie charts showed very few heart disease cases, so we dropped that data from our study. we also used a heatmap to check correlations and random forest model, which performed well except for heart disease. then we switched to kde plots and discussed ways to reduce dimensions.",9,-5.1377096,15.217167,9.455352,8.18334,"dataâ, analyse, analyses"
382,"we started by analysing the data of session summary, and created a pivot table of different formats and plotted the values to get a better understanding. data should technically be a normal distribution, but our mean was closer to the lower value as compared to higher, so our graph was skewed- which means there are outliers. to go one step further we can try to understand if the outliers are consistent or if they keep changing every session. we could also see how session summaries changes through the days. we went through a few more things, like length variation of summaries, frequency of submission per person, min and max trends over the weeks, all this can be done by simple tool pivot table. then we went through a pressure and temperature dataset, and did similar analysis. wherever temperature is autofilled as 0, we have to take care of that and ensure that it's corrected. outliers can be identified by box plot, we can split the data into 4 quartiles. take care that it's not always right to reject outliers, because they may actually mean something, not just be there by accident... the we learnt about exploratory data analysis, and then another library in python called plotly. in the last 20-30 minutes we went over assignment e3, common mistakes everyone made like file naming, plotting the wrong kind of graph (to show normal distribution a bell curve should've been used most people used scatter plot), not using the right method (using python instead of excel to create plots), not labelling plots, etc. overall need to improve documentation and analysis...","we started by analysing the data of session summary, and created a pivot table of different formats and plotted the values to get a better understanding. data should technically be a normal distribution, but our mean was closer to the lower value as compared to higher, so our graph was skewed- which means there are outliers. to go one step further we can try to understand if the outliers are consistent or if they keep changing every session. we could also see how session summaries changes through the days. we went through a few more things, like length variation of summaries, frequency of submission per person, min and max trends over the weeks, all this can be done by simple tool pivot table. then we went through a pressure and temperature dataset, and did similar analysis. wherever temperature is autofilled as 0, we have to take care of that and ensure that it's corrected. outliers can be identified by box plot, we can split the data into 4 quartiles. take care that it's not always right to reject outliers, because they may actually mean something, not just be there by accident... the we learnt about exploratory data analysis, and then another library in python called plotly. in the last 20-30 minutes we went over assignment e3, common mistakes everyone made like file naming, plotting the wrong kind of graph (to show normal distribution a bell curve should've been used most people used scatter plot), not using the right method (using python instead of excel to create plots), not labelling plots, etc. overall need to improve documentation and analysis...",9,-12.4678545,24.41688,7.708048,10.242345,"dataâ, analyse, analyses"
383,"variance inflation factor (vif)
purpose: detects and addresses multicollinearity among independent features.
process: involves an iterative approach to select independent features by evaluating their correlation.
key consideration: itâ€™s crucial to justify the chosen threshold for vif, as it determines the level of multicollinearity deemed acceptable.
principal component analysis (pca)
core idea: reduces dimensionality by transforming data into new axes (principal components) that capture the maximum variance.
principal components: components like pc1, pc2, etc., are ranked by the amount of variance they explain.
loadings: represent the contribution of original features to each principal component.
preprocessing: data normalization is essential before applying pca to ensure accurate results.
trade-off: while pca reduces dimensions, it often sacrifices interpretability of the original features.
visualization: pc1 vs. pc2 plots are commonly used to visualize high-dimensional data in a 2d space.
role in eda: pca is a valuable tool in exploratory data analysis for uncovering patterns and simplifying data.
comparison of vif and pca
order of application: vif should be used first to remove multicollinearity among features.
pcaâ€™s role: while pca effectively reduces dimensions, it can obscure the interpretability of the original variables.
t-distributed stochastic neighbor embedding (t-sne)
purpose: a visualization technique for projecting high-dimensional data into two or three dimensions.
methodology: utilizes t-distribution and gradient descent to map data points while preserving local relationships.
limitations: computationally intensive, non-deterministic (results may vary), and less consistent compared to other methods.
mechanism: constructs a probability distribution to represent the proximity of points in high-dimensional space, then reduces dimensions while maintaining these relationships.
applications: particularly useful for classification and clustering tasks, as it emphasizes differences between data points.","variance inflation factor (vif) purpose: detects and addresses multicollinearity among independent features. process: involves an iterative approach to select independent features by evaluating their correlation. key consideration: it s crucial to justify the chosen threshold for vif, as it determines the level of multicollinearity deemed acceptable. principal component analysis (pca) core idea: reduces dimensionality by transforming data into new axes (principal components) that capture the maximum variance. principal components: components like pc1, pc2, etc., are ranked by the amount of variance they explain. loadings: represent the contribution of original features to each principal component. preprocessing: data normalization is essential before applying pca to ensure accurate results. trade-off: while pca reduces dimensions, it often sacrifices interpretability of the original features. visualization: pc1 vs. pc2 plots are commonly used to visualize high-dimensional data in a 2d space. role in eda: pca is a valuable tool in exploratory data analysis for uncovering patterns and simplifying data. comparison of vif and pca order of application: vif should be used first to remove multicollinearity among features. pca s role: while pca effectively reduces dimensions, it can obscure the interpretability of the original variables. t-distributed stochastic neighbor embedding (t-sne) purpose: a visualization technique for projecting high-dimensional data into two or three dimensions. methodology: utilizes t-distribution and gradient descent to map data points while preserving local relationships. limitations: computationally intensive, non-deterministic (results may vary), and less consistent compared to other methods. mechanism: constructs a probability distribution to represent the proximity of points in high-dimensional space, then reduces dimensions while maintaining these relationships. applications: particularly useful for classification and clustering tasks, as it emphasizes differences between data points.",11,-15.58055,3.6673355,10.265583,13.036871,"pca, heatmap, heatmaps"
384,"first of all sir reviewed what we learned before about statistics. the we learned about tools to analyze data, what they do, and how they are connected. then sir talked about using graphs to show data and where mistakes can happen. then we learned about beta and beta 0 and how they are used in models. then we learned what the p-value is and why it helps in choosing the right features. then we started learning about anova (analysis of variance) and the f-statistic. then we talked about why a bigger f-statistic means a better model.  
then sir told us how to use these ideas in real life.","first of all sir reviewed what we learned before about statistics. the we learned about tools to analyze data, what they do, and how they are connected. then sir talked about using graphs to show data and where mistakes can happen. then we learned about beta and beta 0 and how they are used in models. then we learned what the p-value is and why it helps in choosing the right features. then we started learning about anova (analysis of variance) and the f-statistic. then we talked about why a bigger f-statistic means a better model. then sir told us how to use these ideas in real life.",13,3.1572187,14.81148,12.418968,5.822141,"classification, classifying, classifications"
385,"so today's discussion start with two types of machine learning model which is supervised and unsupervised. for example supervised ml includes - simple linear regression, multiple linear regression, random forest and unsupervised ml includes k-means clustering and hierarchal clustering. the most fascinating things is that whatever type of ml we will use always get a generic equation - y(x)=b0 + b1x + b2x2 .....
further ahead we have learnt about 4 levels of measurement -
1) nominal - it have discrete values and we can only categorize them. also there is no ordering between them. example - gender, color
2) ordinal - it also have discrete values and work same as nominal level of measurement. example - grades
3) interval - it have continuous value. the concept of 0 is arbitrary in this case. example  temperature
4) ratio - it also have continuous value. in this measurement 0 has a meaning. example - height, weight, salary
 in case of nominal and ordinal we use one-hot encoding to change words into numbers by making vector.
we learned that in y =f(x) where y is the label and x is called features.
when we have both labels and features then we use supervised learning and when we have only features then we use unsupervised learning method. nominal and ordinal are use for classification purpose whereas interval and ratio are use for regression.
second thing what we learnt today about data. in ml we use sample instead of population.
sample is a small chunk of population.
that's what we have learnt today.","so today's discussion start with two types of machine learning model which is supervised and unsupervised. for example supervised ml includes - simple linear regression, multiple linear regression, random forest and unsupervised ml includes k-means clustering and hierarchal clustering. the most fascinating things is that whatever type of ml we will use always get a generic equation - y(x)=b0 + b1x + b2x2 ..... further ahead we have learnt about 4 levels of measurement - 1) nominal - it have discrete values and we can only categorize them. also there is no ordering between them. example - gender, color 2) ordinal - it also have discrete values and work same as nominal level of measurement. example - grades 3) interval - it have continuous value. the concept of 0 is arbitrary in this case. example temperature 4) ratio - it also have continuous value. in this measurement 0 has a meaning. example - height, weight, salary in case of nominal and ordinal we use one-hot encoding to change words into numbers by making vector. we learned that in y =f(x) where y is the label and x is called features. when we have both labels and features then we use supervised learning and when we have only features then we use unsupervised learning method. nominal and ordinal are use for classification purpose whereas interval and ratio are use for regression. second thing what we learnt today about data. in ml we use sample instead of population. sample is a small chunk of population. that's what we have learnt today.",4,-24.74817,-13.987045,1.6322078,0.14610466,"classification, classifying, classifications"
386,"elaborated upon different levels of measurement like ordinary , nominal , interval and ratios .listed different models of machine learning for regression and classification.gave a brief about the difference in supervised and unsupervised learning like when labels are known , it is supervised when they are unknown it is unsupervised.i","elaborated upon different levels of measurement like ordinary , nominal , interval and ratios .listed different models of machine learning for regression and classification.gave a brief about the difference in supervised and unsupervised learning like when labels are known , it is supervised when they are unknown it is unsupervised.i",4,-17.76542,-13.151846,2.3060014,0.91905665,"classification, classifying, classifications"
387,"today's class, midsem paper was discussed. the following are the key take aways that i got:
1. in the case of multivariable dataset, checking for correlation between only two variables alone is not sufficient. one must look for any correlation between a variable and the linear combination of the rest as well. thus one fits a linear regression between them, gets the r^2 value and finds a metric called variance inflation factor. we can set a threshold for this metric(say around 8). one can conclude that the variable has a strong correlation with the linear combination of other variables. thus one may consider excluding that variable from the dataset. as one keeps doing this one by one, until only those variables with vif less than a threshold are present, one can reduce the number of independent variables in our dataset.
2. every step of data-analysis must be supported then and there by clear thinking and reasoning. one must be able to justify all the assumptions and steps in their workflow.
3. learnt that in cases where a class is under-represented in the training dataset, if the difference between the support of that class and an another is very huge, it is ok to drop the class for model creation. undersampling majority class means, one is giving too few samples to the model=> weak training, oversampling minority ones also won't make up meaningful data.
4. the concept of dimensionality reduction: some datasets have large number of variables. this has certain disadvantages. more variables means, the complex will be the pattern between them. if as many data is not available to capture the complexity, the output of the training will not be appreciable. some of the solutions to handle this issue are dimensionality reduction, feature selection, regularization and trying to increase the data in hand.","today's class, midsem paper was discussed. the following are the key take aways that i got: 1. in the case of multivariable dataset, checking for correlation between only two variables alone is not sufficient. one must look for any correlation between a variable and the linear combination of the rest as well. thus one fits a linear regression between them, gets the r^2 value and finds a metric called variance inflation factor. we can set a threshold for this metric(say around 8). one can conclude that the variable has a strong correlation with the linear combination of other variables. thus one may consider excluding that variable from the dataset. as one keeps doing this one by one, until only those variables with vif less than a threshold are present, one can reduce the number of independent variables in our dataset. 2. every step of data-analysis must be supported then and there by clear thinking and reasoning. one must be able to justify all the assumptions and steps in their workflow. 3. learnt that in cases where a class is under-represented in the training dataset, if the difference between the support of that class and an another is very huge, it is ok to drop the class for model creation. undersampling majority class means, one is giving too few samples to the model=> weak training, oversampling minority ones also won't make up meaningful data. 4. the concept of dimensionality reduction: some datasets have large number of variables. this has certain disadvantages. more variables means, the complex will be the pattern between them. if as many data is not available to capture the complexity, the output of the training will not be appreciable. some of the solutions to handle this issue are dimensionality reduction, feature selection, regularization and trying to increase the data in hand.",2,2.367669,8.5682335,9.767155,6.9862823,"regression, regressions, features"
388,"4 levels of measurement
(nominal, ordinal, interval, ratio) 
different learning methods
slr, mlr, random forest, logistic reg.
label and features
supervised and unsupervised learning","4 levels of measurement (nominal, ordinal, interval, ratio) different learning methods slr, mlr, random forest, logistic reg. label and features supervised and unsupervised learning",4,-17.07461,-14.382041,2.3687341,0.9191289,"classification, classifying, classifications"
389,"in the beginning of the lecture we started with learning pivot tables in excel, seeing how they make summarizing big sets of data easy through the organization and grouping of values. we got to experiment with creating pivot tables, reorganizing fields to fit our perspective, and doing important calculations like totals, averages, counts, and percentages to improve data analysis. moving on in the lecture, we learned some exploratory data analysis techniques. we practiced summary statistics such as mean, median, variance, and standard deviation which were used to observe the distribution of the data and we also used visualization tools like histograms, box plots, and scatter plots to learn about the patterns, outliers, and trends in the data. later on, we briefly touched on some universal problems, such as class imbalance, where there is a prevailing category that throws off the analysis. we also introduced feature engineering and data transformation briefly which are the central steps in pre-processing data prior to delving into analysis in more depth.","in the beginning of the lecture we started with learning pivot tables in excel, seeing how they make summarizing big sets of data easy through the organization and grouping of values. we got to experiment with creating pivot tables, reorganizing fields to fit our perspective, and doing important calculations like totals, averages, counts, and percentages to improve data analysis. moving on in the lecture, we learned some exploratory data analysis techniques. we practiced summary statistics such as mean, median, variance, and standard deviation which were used to observe the distribution of the data and we also used visualization tools like histograms, box plots, and scatter plots to learn about the patterns, outliers, and trends in the data. later on, we briefly touched on some universal problems, such as class imbalance, where there is a prevailing category that throws off the analysis. we also introduced feature engineering and data transformation briefly which are the central steps in pre-processing data prior to delving into analysis in more depth.",6,-14.533658,26.5902,7.2576184,9.859743,"summarizing, summarize, summarization"
390,"we learnt terms from the data analysis toolpak, the concepts, inferences, graphical interpretations, and the confidence intervals associated with them. specific topics included cases of beta and beta 0 under different conditions, the p-value and its basic use in feature selection, and terms relevant to multiple linear regression. we also learnt the concept of anova, discussing the f- statistic and its relevance.","we learnt terms from the data analysis toolpak, the concepts, inferences, graphical interpretations, and the confidence intervals associated with them. specific topics included cases of beta and beta 0 under different conditions, the p-value and its basic use in feature selection, and terms relevant to multiple linear regression. we also learnt the concept of anova, discussing the f- statistic and its relevance.",13,5.1837416,16.291273,12.636348,5.761374,"classification, classifying, classifications"
391,"sir initially explained about exploratory analysis. then ta explained the steps of eda with examples of pima india etc. then we discussed the box plot,matrix plot etc helps analyze how class imbalances affect our multivariate approach involving options like knn, fatal regression model etc. then we learnt to handle outliers which arise due to data corruption, faulty measurements etc. there are techniques through which one can determine which value is outliers. handling outliers also involves univariate and multivariate approaches.","sir initially explained about exploratory analysis. then ta explained the steps of eda with examples of pima india etc. then we discussed the box plot,matrix plot etc helps analyze how class imbalances affect our multivariate approach involving options like knn, fatal regression model etc. then we learnt to handle outliers which arise due to data corruption, faulty measurements etc. there are techniques through which one can determine which value is outliers. handling outliers also involves univariate and multivariate approaches.",13,-15.656262,14.70162,8.723686,8.819163,"classification, classifying, classifications"
392,"we started the lecture by having a quick recap of the concepts from the previous class. we discussed what we exactly mean by saying â€˜a given number is statistically significant/ a number is not statistically different from 0â€™. we then started with a new topic- multiple linear regression. before starting, we discussed that before performing mlr, we need to convert the file/ data available to us in a vector form - [x1,x2,x3â€¦]. this vector contains various features of the data. this process is called as â€˜embedding vectorâ€™. sometimes, we also need to derive some new features based on the given ones, which matter more/ are more relevant in the context. so, this process of transforming the already existing features or performing operations on the existing features, so as to get new features, is known as â€˜feature engineeringâ€™.
we saw that we donâ€™t get a closed form solution for the coefficient values in mlr, like that in slr. however, the procedure needed to perform to obtain their values remains the same. like slr, in mlr also, we try to minimize the sum of squares of errors. so, if we have â€˜kâ€™ such independent variables/ features, we get â€˜kâ€™ such equations, on which we perform numerical methods to get the solutions. we also learnt about a statistic called the â€˜f-statisticâ€™. it is the ratio of average variance explained by our regression model to the variance explained by errors. so, we want most of our variations to be explained by the regression model. hence, we want msr to be greater than mse, which means f-statistic should be as large as possible. we learnt that the error metrics that we use to assess the validity of a model, are better interpreted when used to compare different models, rather than using it within the same model. also, since rmse and mae are in the same dimensions as the data, they are easier to interpret or relate as compared to other metrics, like sse, mse. 
in any ml model we first start by assuming that the errors in the predicted and actual data values are random. also, for any ml model, if more independent variables are available then the value of r2 increases, since more variables are available to explain the variability in the data. at last, we saw how we can use the p values for each independent variable to assess whether it has any effect on the data. if we get the p-value >0.025 (for 95% interval) then we can say that this particular coefficient is not statistically different from â€˜0â€™. hence, we can ignore it and reduce the number of independent variables in the regression model. we can continue this until we get only those variables whose p -values are <0.025. this implies that only these coefficients are significant and rest can be neglected. this gives us the true/ actual model.","we started the lecture by having a quick recap of the concepts from the previous class. we discussed what we exactly mean by saying a given number is statistically significant/ a number is not statistically different from 0 . we then started with a new topic- multiple linear regression. before starting, we discussed that before performing mlr, we need to convert the file/ data available to us in a vector form - [x1,x2,x3 ]. this vector contains various features of the data. this process is called as embedding vector . sometimes, we also need to derive some new features based on the given ones, which matter more/ are more relevant in the context. so, this process of transforming the already existing features or performing operations on the existing features, so as to get new features, is known as feature engineering . we saw that we don t get a closed form solution for the coefficient values in mlr, like that in slr. however, the procedure needed to perform to obtain their values remains the same. like slr, in mlr also, we try to minimize the sum of squares of errors. so, if we have k such independent variables/ features, we get k such equations, on which we perform numerical methods to get the solutions. we also learnt about a statistic called the f-statistic . it is the ratio of average variance explained by our regression model to the variance explained by errors. so, we want most of our variations to be explained by the regression model. hence, we want msr to be greater than mse, which means f-statistic should be as large as possible. we learnt that the error metrics that we use to assess the validity of a model, are better interpreted when used to compare different models, rather than using it within the same model. also, since rmse and mae are in the same dimensions as the data, they are easier to interpret or relate as compared to other metrics, like sse, mse. in any ml model we first start by assuming that the errors in the predicted and actual data values are random. also, for any ml model, if more independent variables are available then the value of r2 increases, since more variables are available to explain the variability in the data. at last, we saw how we can use the p values for each independent variable to assess whether it has any effect on the data. if we get the p-value >0.025 (for 95% interval) then we can say that this particular coefficient is not statistically different from 0 . hence, we can ignore it and reduce the number of independent variables in the regression model. we can continue this until we get only those variables whose p -values are <0.025. this implies that only these coefficients are significant and rest can be neglected. this gives us the true/ actual model.",2,13.135721,5.9743233,12.143179,4.1101823,"regression, regressions, features"
393,"learned to use a tool in excel that organizes big data into groups and sums up values.
practiced making this tool, moving items around, and calculating sums, averages, counts, and percentages.
studied methods to understand data better by looking at key numbers like middle value, spread, and difference from the average.
used charts like bar graphs, box charts, and dot plots to spot trends, shapes, and unusual points in data.
discussed problems when one type of data appears much more than others, which can affect results.
briefly looked at ways to improve data by modifying and preparing it for deeper study.","learned to use a tool in excel that organizes big data into groups and sums up values. practiced making this tool, moving items around, and calculating sums, averages, counts, and percentages. studied methods to understand data better by looking at key numbers like middle value, spread, and difference from the average. used charts like bar graphs, box charts, and dot plots to spot trends, shapes, and unusual points in data. discussed problems when one type of data appears much more than others, which can affect results. briefly looked at ways to improve data by modifying and preparing it for deeper study.",6,-15.517435,27.333193,7.1253867,9.848569,"summarizing, summarize, summarization"
394,"pivot tables were introduced as a powerful tool for data summarization and analysis. they allow us to dynamically organize, filter, and aggregate data by different categories, making them especially useful for large datasets. we discussed their applications in quickly generating insights, such as identifying trends and patterns. we delved into principal component analysis (pca), focusing on how to derive principal components from original variables while ensuring they remain uncorrelated. finally, we emphasized the role of clear, concise, and insightful reporting in data analysis. a well-structured report should present findings effectively, highlighting key takeaways using visuals and narratives. this ensures that stakeholders can make data-driven decisions efficiently.","pivot tables were introduced as a powerful tool for data summarization and analysis. they allow us to dynamically organize, filter, and aggregate data by different categories, making them especially useful for large datasets. we discussed their applications in quickly generating insights, such as identifying trends and patterns. we delved into principal component analysis (pca), focusing on how to derive principal components from original variables while ensuring they remain uncorrelated. finally, we emphasized the role of clear, concise, and insightful reporting in data analysis. a well-structured report should present findings effectively, highlighting key takeaways using visuals and narratives. this ensures that stakeholders can make data-driven decisions efficiently.",6,-17.170057,25.827852,7.2715874,9.660882,"summarizing, summarize, summarization"
395,"we learnt as per the problem handed to us we should use different models, we extended our knowledge to polynomial regression this time. then we studied feature selection in two ways: forward and backward . the latter we did in last class by eliminating the variables according to p-values as they are not important. non parametric models like random forest, k-nn. got introduced to the world of neural networks and deep learning. after spending some time on regression for few weeks, we started classification problem. we dealt with function of sigmoid in the end.","we learnt as per the problem handed to us we should use different models, we extended our knowledge to polynomial regression this time. then we studied feature selection in two ways: forward and backward . the latter we did in last class by eliminating the variables according to p-values as they are not important. non parametric models like random forest, k-nn. got introduced to the world of neural networks and deep learning. after spending some time on regression for few weeks, we started classification problem. we dealt with function of sigmoid in the end.",0,-2.4579537,-4.2574167,9.001818,4.580668,"models, feature, features"
396,"we learnt about the confidence interval of beta 1. we also learnt that beta1 can also be 0 in the confidence interval range. but we generally don't consider it as a good value because if beta1=0, then it doesn't really make a regression. we discussed about the method which you created to find duplicate submissions in class summary submission. there was a correlation matrix which you used and 1-x represented the similarity between 2 submissions. then it was a discussion that from now onwards we will  shift from excel to python. then we learnt about feature engineering. finally we started multiple linear regression. we saw about the k dimensional hyper surface in the multiple gradient descent method. then we saw its prove and discussed about some solvers. later on we opened excel and then discussed that wether we should use linear regression or not. we saw about feature selection and dropping. we dropped x5 because it had largest p-value. we also saw that larger the f value, better is the  model.","we learnt about the confidence interval of beta 1. we also learnt that beta1 can also be 0 in the confidence interval range. but we generally don't consider it as a good value because if beta1=0, then it doesn't really make a regression. we discussed about the method which you created to find duplicate submissions in class summary submission. there was a correlation matrix which you used and 1-x represented the similarity between 2 submissions. then it was a discussion that from now onwards we will shift from excel to python. then we learnt about feature engineering. finally we started multiple linear regression. we saw about the k dimensional hyper surface in the multiple gradient descent method. then we saw its prove and discussed about some solvers. later on we opened excel and then discussed that wether we should use linear regression or not. we saw about feature selection and dropping. we dropped x5 because it had largest p-value. we also saw that larger the f value, better is the model.",2,16.694843,9.737737,12.358855,4.231754,"regression, regressions, features"
397,"some of the points we covered in today's class were :
for heatmaps we studied that dark lines in heatmap signifies that the given submission has no resemblance with other submissions.

steps in ds are 
understanding the problem 
exploratory data analysis which includes sources of data and format of data such as text,files, database etc.
visualization that includes visual and mathematical visualizations.

six steps that we run cyclically for data analysis are business understanding, data understanding, data preparation, modelling, evaluation, deployment 
and when we should stop this repetitive cycle is the acceptance criteria of model

eda is an approach used in data science and statistics to analyse and investigate the data sets.
we study whether the data is continuous or discrete, dependant or independent, columns of data etc.

before data processing it is assumed that dataset would be cleaned and all the kinks have been removed.
if the data is skewed you need to apply transformations to make it normally distributed.

we studied a dataset of diabetes and pm10 also in which we studied about feature distribution, types of missing data, inter feature relation, multivariate approaches, situation of heteroskedasticity and homoskedasticity etc.","some of the points we covered in today's class were : for heatmaps we studied that dark lines in heatmap signifies that the given submission has no resemblance with other submissions. steps in ds are understanding the problem exploratory data analysis which includes sources of data and format of data such as text,files, database etc. visualization that includes visual and mathematical visualizations. six steps that we run cyclically for data analysis are business understanding, data understanding, data preparation, modelling, evaluation, deployment and when we should stop this repetitive cycle is the acceptance criteria of model eda is an approach used in data science and statistics to analyse and investigate the data sets. we study whether the data is continuous or discrete, dependant or independent, columns of data etc. before data processing it is assumed that dataset would be cleaned and all the kinks have been removed. if the data is skewed you need to apply transformations to make it normally distributed. we studied a dataset of diabetes and pm10 also in which we studied about feature distribution, types of missing data, inter feature relation, multivariate approaches, situation of heteroskedasticity and homoskedasticity etc.",9,-15.726947,21.230968,8.424795,8.96505,"dataâ, analyse, analyses"
398,"in todayâ€™s class, we learnt about how we categorize coefficients of regression and what it means by statistically similar and statistically significant. then we learnt about confidence intervals in determining what values can be statistically significant. then we learnt about multiple linear regression. it simply means now we have more than one independent variable which would be impacting the independent variable. we then learnt that we could use standard libraries in python for this purpose. also, not all independent variables could be useful to us; some of them might not be useful for analysis. those independent variables will have a small dependency with the independent variable. hence, our model will remain the same even if we could remove that variable. this can be inferred from the p-values.","in today s class, we learnt about how we categorize coefficients of regression and what it means by statistically similar and statistically significant. then we learnt about confidence intervals in determining what values can be statistically significant. then we learnt about multiple linear regression. it simply means now we have more than one independent variable which would be impacting the independent variable. we then learnt that we could use standard libraries in python for this purpose. also, not all independent variables could be useful to us; some of them might not be useful for analysis. those independent variables will have a small dependency with the independent variable. hence, our model will remain the same even if we could remove that variable. this can be inferred from the p-values.",2,19.429365,3.877432,12.776188,4.8352103,"regression, regressions, features"
399,"in today's class we learnt some more things about variance inflation factor or vif. we latch on to the highest value and remove it and recalculate the vif. if we change vif threshold to like 5 then it will just be all those features with r square corresponding to five or less. if there is some important feature then we have to keep it irrespective of its vif. at last we can create classification model with left out variables.
then we learnt about pca or principal component analysis.principal components are necessarily perpendicular to each other. if we have 10pcs then we can decide like upto 2 or 3 are important .
eg - pc1 captures the best variance , pc2 : second best and so on....
now instead of y=f(x), we will have y= f(pc) ,  this is pc regression instead of normal regression , we can use pc for prediction but not for what if or explain the results.
purposes of pca analysis are dimension reduction, data reduction, prediction models and visualisation or understanding the structure of data using eda
pca is sensitive to data scale so we need to normalise the data beforehand.
then we learnt about t-sne or t-distributed stochastic neighbour encoding which helps to map multidimension data to 2 or 3 dimension","in today's class we learnt some more things about variance inflation factor or vif. we latch on to the highest value and remove it and recalculate the vif. if we change vif threshold to like 5 then it will just be all those features with r square corresponding to five or less. if there is some important feature then we have to keep it irrespective of its vif. at last we can create classification model with left out variables. then we learnt about pca or principal component analysis.principal components are necessarily perpendicular to each other. if we have 10pcs then we can decide like upto 2 or 3 are important . eg - pc1 captures the best variance , pc2 : second best and so on.... now instead of y=f(x), we will have y= f(pc) , this is pc regression instead of normal regression , we can use pc for prediction but not for what if or explain the results. purposes of pca analysis are dimension reduction, data reduction, prediction models and visualisation or understanding the structure of data using eda pca is sensitive to data scale so we need to normalise the data beforehand. then we learnt about t-sne or t-distributed stochastic neighbour encoding which helps to map multidimension data to 2 or 3 dimension",11,-20.281715,3.8173733,10.53553,13.506784,"pca, heatmap, heatmaps"
400,"learnt about population and sample.if not impossible then it will be very expensive to work with the data of entire population so we always study some parts of the population i.e. sample.our main objective is to estimate different parameters of the population based on the statistics that we obtained from our sample. there are various attributes associated to a data.for example if we have the data of marbles in the box then we can calculate frequency of particular colour balls,mean,median,mode,standard deviation,variance and also can perform certain operations like addition, subtraction,etc.we use different models like line,point depending on the data we have.point is considered a very naive model.
then took an example of simple linear regression(assuming only one predictor i.e. advertisement expenditure) to predict sales.for that the equation is y=b_0+b_1x where b_0 and b_1 of sample are point estimates of population parameters i e. we can claim with 0% confidence that b_0 and b_1 for population will be these values only.our aim is to find a confidence interval so that with good enough confidence we can claim that the values of b_0 and b_1 for the population will lie in that interval.the ""b_0"" term is called the 'bias' and it accounts for other factors that might affect sales so our natural intuition is that as we increase the number of predictor variables bias will decrease and if at all we can include all the predictor variables then bias will be zero.
lastly,we found out how to actually calculate this b_0 and b_1 of a given sample.we try to minimize the sum of the square of the errors where error is the distance between actual value and the value that we get from the line.one interesting thing that we got after the derivation was that mean of the predictor values and response values lie on the best fit line.","learnt about population and sample.if not impossible then it will be very expensive to work with the data of entire population so we always study some parts of the population i.e. sample.our main objective is to estimate different parameters of the population based on the statistics that we obtained from our sample. there are various attributes associated to a data.for example if we have the data of marbles in the box then we can calculate frequency of particular colour balls,mean,median,mode,standard deviation,variance and also can perform certain operations like addition, subtraction,etc.we use different models like line,point depending on the data we have.point is considered a very naive model. then took an example of simple linear regression(assuming only one predictor i.e. advertisement expenditure) to predict sales.for that the equation is y=b_0+b_1x where b_0 and b_1 of sample are point estimates of population parameters i e. we can claim with 0% confidence that b_0 and b_1 for population will be these values only.our aim is to find a confidence interval so that with good enough confidence we can claim that the values of b_0 and b_1 for the population will lie in that interval.the ""b_0"" term is called the 'bias' and it accounts for other factors that might affect sales so our natural intuition is that as we increase the number of predictor variables bias will decrease and if at all we can include all the predictor variables then bias will be zero. lastly,we found out how to actually calculate this b_0 and b_1 of a given sample.we try to minimize the sum of the square of the errors where error is the distance between actual value and the value that we get from the line.one interesting thing that we got after the derivation was that mean of the predictor values and response values lie on the best fit line.",1,35.175594,-6.6060877,16.432985,3.6796112,"population, models, estimating"
401,"in this session, we explored the variance inflation factor (vif) and principal component analysis (pca), focusing on their applications, differences, and use cases.

variance inflation factor (vif):

used to detect multicollinearity among features.
computed as 
vif=1/1-r^2, where r^2 is obtained by regressing a feature against all others.
a vif threshold of 10 (corresponding to r^2 = 0.9) is commonly used for feature elimination.
preserves real-world features, making it suitable for sensitivity analysis and interpretability in models.

principal component analysis (pca):

transforms features into new uncorrelated components (pcs) ranked by variance.
pc1 > pc2 > pc3 in terms of variance captured, with a decreasing trend in the explained variance graph.
common applications include dimensionality reduction, data visualization (e.g., pc1 vs. pc2 plots), and prediction modeling.
requires normalization to ensure fair variance contribution from all features.
unlike vif, pca does not retain original features, making sensitivity analysis difficult.

comparison:

vif works in feature space, removing redundant variables while keeping real-world interpretability.
pca creates mathematical features, making it more suitable for dimensionality reduction rather than feature selection.
vif is often preferred over pca when feature interpretability is essential.

additionally, the emnist dataset was mentioned as an example for handwritten digit classification using pca.

t-sne:  t-sne is a non-linear dimensionality reduction technique primarily used for visualizing high-dimensional data in a lower-dimensional space (typically 2d or 3d). 

how t-sne works:
measures similarity in high dimensions:
computes the probability that two points are neighbors in the original high-dimensional space using a gaussian distribution.
maps data to lower dimensions:
assigns similar points in a lower-dimensional space using a studentâ€™s t-distribution, ensuring that points that were close in high-dimensional space remain close.
optimizes for local structure:
unlike pca (which captures global variance), t-sne preserves local neighborhoods, making it great for cluster visualization.","in this session, we explored the variance inflation factor (vif) and principal component analysis (pca), focusing on their applications, differences, and use cases. variance inflation factor (vif): used to detect multicollinearity among features. computed as vif=1/1-r^2, where r^2 is obtained by regressing a feature against all others. a vif threshold of 10 (corresponding to r^2 = 0.9) is commonly used for feature elimination. preserves real-world features, making it suitable for sensitivity analysis and interpretability in models. principal component analysis (pca): transforms features into new uncorrelated components (pcs) ranked by variance. pc1 > pc2 > pc3 in terms of variance captured, with a decreasing trend in the explained variance graph. common applications include dimensionality reduction, data visualization (e.g., pc1 vs. pc2 plots), and prediction modeling. requires normalization to ensure fair variance contribution from all features. unlike vif, pca does not retain original features, making sensitivity analysis difficult. comparison: vif works in feature space, removing redundant variables while keeping real-world interpretability. pca creates mathematical features, making it more suitable for dimensionality reduction rather than feature selection. vif is often preferred over pca when feature interpretability is essential. additionally, the emnist dataset was mentioned as an example for handwritten digit classification using pca. t-sne: t-sne is a non-linear dimensionality reduction technique primarily used for visualizing high-dimensional data in a lower-dimensional space (typically 2d or 3d). how t-sne works: measures similarity in high dimensions: computes the probability that two points are neighbors in the original high-dimensional space using a gaussian distribution. maps data to lower dimensions: assigns similar points in a lower-dimensional space using a student s t-distribution, ensuring that points that were close in high-dimensional space remain close. optimizes for local structure: unlike pca (which captures global variance), t-sne preserves local neighborhoods, making it great for cluster visualization.",11,-15.778107,3.638666,10.281718,13.0869665,"pca, heatmap, heatmaps"
402,"after exploratory data analysis on our dataset, we split the data into two classes: 80% for training and 20% for testing. training data was used to create a linear regression model, and hence, we had two key sets of output metricsâ€”training results and testing results.

overfitting was discussed as a potential issue in model performance. this arises when the model performs well with the training data but poorly on the testing data, which gives an indication that the model learns specific patterns specific to the training data rather than generalizable trends.

we also discussed the idea of adjusted râ². while regular râ² increases with each additional predictor, adjusted râ² adjusts for the number of predictors in a model and provides a better measure of the fit of the model. this value gives us the amount of variance explained per degree of freedom, giving a more realistic view of model fit.

linear regression models: simple linear regression (slr) and multiple linear regression (mlr) were covered. these are parametric models that depend upon the use of p-values, coefficients like î²â‚ and other statistical measures in order to understand the relationships between the variables.

we then looked at how to determine if errors are normally distributed. a q-q plot, or quantile-quantile plot, is used for this purpose. if the error values fall along a straight line in the q-q plot, it suggests that the errors are normally distributed, which is an assumption of linear regression.

the session explained the basic setup of scikit-learn, or simply sklearn, the python library for building and evaluating ml models. in this session, we learned how to use this tool to successfully implement linear regression.

hypothesis testing: the session concluded with an introduction to hypothesis testing, including an overview of p-values. we touched on some common statistical tests, including the omnibus test, jarque-bera test, and durbin-watson test. these tests help assess the underlying assumptions and validity of the model, such as checking for autocorrelation, normality of residuals, and other model fit aspects.","after exploratory data analysis on our dataset, we split the data into two classes: 80% for training and 20% for testing. training data was used to create a linear regression model, and hence, we had two key sets of output metrics training results and testing results. overfitting was discussed as a potential issue in model performance. this arises when the model performs well with the training data but poorly on the testing data, which gives an indication that the model learns specific patterns specific to the training data rather than generalizable trends. we also discussed the idea of adjusted r . while regular r increases with each additional predictor, adjusted r adjusts for the number of predictors in a model and provides a better measure of the fit of the model. this value gives us the amount of variance explained per degree of freedom, giving a more realistic view of model fit. linear regression models: simple linear regression (slr) and multiple linear regression (mlr) were covered. these are parametric models that depend upon the use of p-values, coefficients like and other statistical measures in order to understand the relationships between the variables. we then looked at how to determine if errors are normally distributed. a q-q plot, or quantile-quantile plot, is used for this purpose. if the error values fall along a straight line in the q-q plot, it suggests that the errors are normally distributed, which is an assumption of linear regression. the session explained the basic setup of scikit-learn, or simply sklearn, the python library for building and evaluating ml models. in this session, we learned how to use this tool to successfully implement linear regression. hypothesis testing: the session concluded with an introduction to hypothesis testing, including an overview of p-values. we touched on some common statistical tests, including the omnibus test, jarque-bera test, and durbin-watson test. these tests help assess the underlying assumptions and validity of the model, such as checking for autocorrelation, normality of residuals, and other model fit aspects.",13,7.095363,0.75807285,10.515421,4.701695,"classification, classifying, classifications"
403,"in the class of 15th january, we studied about features and labels. we represent in the form of y = f(x): where y represent labels and x represent features. earlier these equations were found manually but now we have ml algorithms to do this work. how we classify learning based on the presence of labels: supervised learned (labels present) and unsupervised learning (no labels present). we also learned about the level of measurements with their examples. there are four level of measurements: nominal, ordinal, interval and ratio. the first two belong to the ml category of ""classification"" while the other two belong to the ml category of ""regression"". then we also understood the meaning of the word ""sample"". how it differs from population.","in the class of 15th january, we studied about features and labels. we represent in the form of y = f(x): where y represent labels and x represent features. earlier these equations were found manually but now we have ml algorithms to do this work. how we classify learning based on the presence of labels: supervised learned (labels present) and unsupervised learning (no labels present). we also learned about the level of measurements with their examples. there are four level of measurements: nominal, ordinal, interval and ratio. the first two belong to the ml category of ""classification"" while the other two belong to the ml category of ""regression"". then we also understood the meaning of the word ""sample"". how it differs from population.",4,-22.838732,-18.60714,1.6149691,0.24394327,"classification, classifying, classifications"
404,"got to know about unsupervised, supervised and semi-superwised learning algorithms. there are 4 types of measurements, they are: nominal(discrete), ordinal(discrete but has order associated with it), interval(continuous & have an arbitary zero) and ratio(continuous & have an absolute zero). difference between these measurements was also discussed. also, we came know about different types of machine learning used to tackle the data within these types of measurements. e.g., for nominal and ordinal, classification is used. for interval and ratio type of measurement data, regression is used. unsupervised learning algorithm is used when label is not defined priorly, it uses clustering methods like k-means and hierarchical clustering to analyse the data and then make meaningful outcome from it. sometime it is also possible that we don't know the label, from y=f(x) and thus we don't know the relation between y and x which essentially the function f which is unknown.  machine learning algorithm uses the large amount of data available (sample of the population) and try to draw relations between y and x (i.e, it tries to find function f). once the f is known, the algorithm or model is capable enough to predict a y(label) for a new given input x.","got to know about unsupervised, supervised and semi-superwised learning algorithms. there are 4 types of measurements, they are: nominal(discrete), ordinal(discrete but has order associated with it), interval(continuous & have an arbitary zero) and ratio(continuous & have an absolute zero). difference between these measurements was also discussed. also, we came know about different types of machine learning used to tackle the data within these types of measurements. e.g., for nominal and ordinal, classification is used. for interval and ratio type of measurement data, regression is used. unsupervised learning algorithm is used when label is not defined priorly, it uses clustering methods like k-means and hierarchical clustering to analyse the data and then make meaningful outcome from it. sometime it is also possible that we don't know the label, from y=f(x) and thus we don't know the relation between y and x which essentially the function f which is unknown. machine learning algorithm uses the large amount of data available (sample of the population) and try to draw relations between y and x (i.e, it tries to find function f). once the f is known, the algorithm or model is capable enough to predict a y(label) for a new given input x.",4,-24.103893,-14.385343,1.7452192,0.4659483,"classification, classifying, classifications"
405,"we started the session by differentiating between sample and our population (or the universal data), and defined our goal which is predicting the population and trends existing within from the sample set. we looked at the different results and operations which can be performed on the data, attributes and when they are called parameters (if based on population) and when statistics (based on sample). we also looked at the simple linear regression (slr) and biases which exist within the system. biases are unexplained terms which influence the predictions and account for the missing parameters, not accounted for due to some error. if we account for all parameters then the prediction curve (or straight line) should pass from the origin. we then looked at point predictions, the confidence intervals and how their size affects the confidence percentage. following this and from the scatter plot, we saw the different types of error terms which can be formed, like the use of sum of modulus or sum of squares, spheres of influence and which one is more suitable. further we tried to calculate the formulae for the different parameters, basically their closed forms, and how it is not always possible to calculate them, in multivariate case.","we started the session by differentiating between sample and our population (or the universal data), and defined our goal which is predicting the population and trends existing within from the sample set. we looked at the different results and operations which can be performed on the data, attributes and when they are called parameters (if based on population) and when statistics (based on sample). we also looked at the simple linear regression (slr) and biases which exist within the system. biases are unexplained terms which influence the predictions and account for the missing parameters, not accounted for due to some error. if we account for all parameters then the prediction curve (or straight line) should pass from the origin. we then looked at point predictions, the confidence intervals and how their size affects the confidence percentage. following this and from the scatter plot, we saw the different types of error terms which can be formed, like the use of sum of modulus or sum of squares, spheres of influence and which one is more suitable. further we tried to calculate the formulae for the different parameters, basically their closed forms, and how it is not always possible to calculate them, in multivariate case.",1,31.412066,-12.455695,16.21802,3.8803225,"population, models, estimating"
406,"today we studied logtistic regression. we have to maximise the likelihood function which is similiar to the error function in linear regression. since products are hard to work with we use the natural logarithm of the products instead. we studied about the condusion matrix.
accuracy is tp+tn/total. precession is number of detected correctly/total.",today we studied logtistic regression. we have to maximise the likelihood function which is similiar to the error function in linear regression. since products are hard to work with we use the natural logarithm of the products instead. we studied about the condusion matrix. accuracy is tp+tn/total. precession is number of detected correctly/total.,2,17.240946,10.721795,12.44105,4.1854367,"regression, regressions, features"
407,"today we saw how a neural network shows decision boundaries for datasets with the aid of playground.tensorflow.org. and i was able to take key performance metrics such as precision, accuracy, recall, and f1-score under consideration. true positive and false positive rates were elaborated on. where the true positive rate (sense) is derived from the formula tp/(tp + fn) and that of the false positive trend is fp/(fp + tn). in turn, accuracy basically evaluates the proportion between true-positive and true-negative cases against the total number of predictions made. on top of that, i computed a classifier using positive and negative distributions tagged differently that accounted for the likelihood of misclassification within the regions where those two distributions overlap. the evaluation of classifier effectiveness using the roc curve: the steeper the curve, the better the classifier; a 45-degree line indicates random guessing. the area under the curve-auc-was also mentioned: that the closer to 1 the auc is, the better the classifier, while an auc of 0.5 indicates no meaningful discrimination against the class. the precision rate was defined as the ratio between correctly classified instances of a class and the total instances predicted as that class, while the recall gives the ratio between correctly classified instances of a class and the total actual instances of that class. besides, going to be opened concerning unsupervised learning, where labeled data are not required; hence, the function y = f(x)- where y need not be presented. in this area, there are clustering methods such as k-means clustering, hierarchical clustering, cluster items without any label, and any without any target labels.","today we saw how a neural network shows decision boundaries for datasets with the aid of playground.tensorflow.org. and i was able to take key performance metrics such as precision, accuracy, recall, and f1-score under consideration. true positive and false positive rates were elaborated on. where the true positive rate (sense) is derived from the formula tp/(tp + fn) and that of the false positive trend is fp/(fp + tn). in turn, accuracy basically evaluates the proportion between true-positive and true-negative cases against the total number of predictions made. on top of that, i computed a classifier using positive and negative distributions tagged differently that accounted for the likelihood of misclassification within the regions where those two distributions overlap. the evaluation of classifier effectiveness using the roc curve: the steeper the curve, the better the classifier; a 45-degree line indicates random guessing. the area under the curve-auc-was also mentioned: that the closer to 1 the auc is, the better the classifier, while an auc of 0.5 indicates no meaningful discrimination against the class. the precision rate was defined as the ratio between correctly classified instances of a class and the total instances predicted as that class, while the recall gives the ratio between correctly classified instances of a class and the total actual instances of that class. besides, going to be opened concerning unsupervised learning, where labeled data are not required; hence, the function y = f(x)- where y need not be presented. in this area, there are clustering methods such as k-means clustering, hierarchical clustering, cluster items without any label, and any without any target labels.",8,0.5265428,-21.021,6.8937054,0.5094369,"classification, clusterings, classifying"
408,"sir started with exploratory data analysis. sir explained them with the example of the session summary we have been uploading. in there data there were length of character in response submitted by each, average length of summary submitted. pivot table- a tool which helps us analyse and summarise data of a column. we get different values computed for a given column of data such as mean max min std dev and other such statistics. this initially gives us some inferences about data which we can use to further process the data.  linear regression is not valid beyond training data. doing exploratory data analysis gives us ideas on what to focus on. statistical summaries, varies kinds of plots can be created using plots. sir then showed how to analyse data of a chemical plant. we have 250 columns and we have to understand nature of these columns. the data has certain parameters at which the plant operates. edge computing- analysis data when we acquire data. sensors getting more advanced that the they acquire and analyse data and give us kind of summary. the pivot table automatically identifies columns like date and time. when we see min and max of data we will find blank or outliers or erroneous data, anomalies. by using this we can remove such data. we can use box plot or scatter plot to identify outliers. in the report of this data analysis, we will give some basic definitions of some metrics, plots and inferences. iqr=q3-q1(75 percentile-25 percentile). outliers will be =q1-1.5*iqr and =q3+1.5*iqr. the factor of 1.5 can be changed as per our requirement. some times we can identify a boundary between outliers and other data based on plots. in some cases the mathematical boundary may not be suitable. so we have to choose accordingly. while plotting we selectively pick some parameters and them analyse them from plots. based on the plots or data we need to ask questions about possible how the data is generated or what process behind that parameter. we can identify relationship between variables, and we can combine them together. we have to ask questions about data and add those into our data. when we get data we will get incorrect incomplete data. we have to attribute why such an error has arises. then tas gave us feedback on e2 assignment. sir then said the importance of documentation.","sir started with exploratory data analysis. sir explained them with the example of the session summary we have been uploading. in there data there were length of character in response submitted by each, average length of summary submitted. pivot table- a tool which helps us analyse and summarise data of a column. we get different values computed for a given column of data such as mean max min std dev and other such statistics. this initially gives us some inferences about data which we can use to further process the data. linear regression is not valid beyond training data. doing exploratory data analysis gives us ideas on what to focus on. statistical summaries, varies kinds of plots can be created using plots. sir then showed how to analyse data of a chemical plant. we have 250 columns and we have to understand nature of these columns. the data has certain parameters at which the plant operates. edge computing- analysis data when we acquire data. sensors getting more advanced that the they acquire and analyse data and give us kind of summary. the pivot table automatically identifies columns like date and time. when we see min and max of data we will find blank or outliers or erroneous data, anomalies. by using this we can remove such data. we can use box plot or scatter plot to identify outliers. in the report of this data analysis, we will give some basic definitions of some metrics, plots and inferences. iqr=q3-q1(75 percentile-25 percentile). outliers will be =q1-1.5*iqr and =q3+1.5*iqr. the factor of 1.5 can be changed as per our requirement. some times we can identify a boundary between outliers and other data based on plots. in some cases the mathematical boundary may not be suitable. so we have to choose accordingly. while plotting we selectively pick some parameters and them analyse them from plots. based on the plots or data we need to ask questions about possible how the data is generated or what process behind that parameter. we can identify relationship between variables, and we can combine them together. we have to ask questions about data and add those into our data. when we get data we will get incorrect incomplete data. we have to attribute why such an error has arises. then tas gave us feedback on e2 assignment. sir then said the importance of documentation.",9,-10.787537,23.608389,8.030954,10.150037,"dataâ, analyse, analyses"
409,we discussed mid sem exam and assignment 3 and later we discussed some problems in data.,we discussed mid sem exam and assignment 3 and later we discussed some problems in data.,6,-4.58692,20.580606,8.333061,9.493719,"summarizing, summarize, summarization"
410,"summary of today's lecture: histograms help visualize data distribution; mismatched error histograms indicate poor model fit. in excel, linear regression was performed using a csv file, with scatterplots showing random errors and a uniform error distribution in the histogram. statistical concepts covered include sst (total variation), ssr (explained variation), and sse (unexplained variation). râ² measures model fit, while the correlation coefficient (r) indicates relationship strength. confidence intervals were derived from regression coefficients, with sample means forming a normal distribution. the session also covered râ², 95% confidence intervals, and positive/negative correlations, emphasizing practical application in excel.","summary of today's lecture: histograms help visualize data distribution; mismatched error histograms indicate poor model fit. in excel, linear regression was performed using a csv file, with scatterplots showing random errors and a uniform error distribution in the histogram. statistical concepts covered include sst (total variation), ssr (explained variation), and sse (unexplained variation). r measures model fit, while the correlation coefficient (r) indicates relationship strength. confidence intervals were derived from regression coefficients, with sample means forming a normal distribution. the session also covered r , 95% confidence intervals, and positive/negative correlations, emphasizing practical application in excel.",5,22.452213,-7.04528,14.439644,4.6487594,"regression, statistical, statistics"
411,"feature encoding converts categorical and textual data into numerical form for machine learning. one-hot encoding is common for multiclass and multilabel problems but can cause the curse of dimensionality. label encoding assigns unique integers, while integer encoding is used for ordered categories.

binary encoding reduces dimensionality by converting categories into binary formâ€”three columns can represent eight categories. frequency encoding replaces categories with their occurrence rates, while target encoding assigns the mean target value to each category.

lastly, vectorization techniques like tf-idf, bag-of-words, and word embeddings convert text into numbers for nlp tasks.","feature encoding converts categorical and textual data into numerical form for machine learning. one-hot encoding is common for multiclass and multilabel problems but can cause the curse of dimensionality. label encoding assigns unique integers, while integer encoding is used for ordered categories. binary encoding reduces dimensionality by converting categories into binary form three columns can represent eight categories. frequency encoding replaces categories with their occurrence rates, while target encoding assigns the mean target value to each category. lastly, vectorization techniques like tf-idf, bag-of-words, and word embeddings convert text into numbers for nlp tasks.",3,-41.859657,3.9418237,0.017876664,6.265567,"categorical, categorization, categorise"
412,"introduction to crisp-dm framework
the lecture provided a comprehensive guide to data analysis and problem-solving using the crisp-dm (cross-industry standard process for data mining) framework, which consists of six iterative steps: domain knowledge, data understanding, data preparation, modeling, evaluation, and deployment. it begins by emphasizing the importance of defining constraints, success criteria, and a project plan before diving into the data. the focus is on understanding the data, including identifying dependent variables (targets) and independent variables (features), and determining what problems can be solved with the available data.
classification of data problems
data problems are classified into two main categories: issues with dependent variables, such as missing labels (requiring clustering), incorrect or noisy labels (requiring noise removal), insufficient data (needing more data collection or synthetic data generation), or imbalanced data (requiring techniques like oversampling or undersampling); and issues with independent variables, such as within-column problems like missing values, outliers, incorrect representations, duplicates, uneven distributions, or too much data, and cross-column problems like insufficient features, too many features (requiring feature elimination using methods like p-values or heatmaps), or feature scaling issues.
exploratory data analysis (eda)
exploratory data analysis (eda) is a critical step, involving checking the distribution of variables, using box plots to understand variability, analyzing feature correlations with heatmaps, creating scatter plots and matrix plots to visualize relationships, identifying class imbalances and trends, and handling missing data by determining if itâ€™s mcar (missing completely at random), mar (missing at random), or mnar (missing not at random).
handling missing data
missing data can be addressed through methods like replacing with statistics (mean, median, etc.), using k-nearest neighbors or linear regression for imputation, or for time-series data, using interpolation or nearby values.
outlier detection and management
outlier detection involves techniques like sorting data and using the interquartile range (iqr) or standard deviation to identify outliers, applying dbscan clustering for multivariate outliers, or using t-sne to visualize high-dimensional data and spot anomalies. outliers can be true anomalies or errors, and true outliers may need to be processed separately. the lecture also highlighted the difference between univariate and multivariate analysis, where outliers may not be visible in one dimension but become apparent in multiple dimensions.
finally, it stresses the importance of iterative validation and thorough data preparation to ensure the data is clean and ready for modeling, leading to reliable and actionable insights.","introduction to crisp-dm framework the lecture provided a comprehensive guide to data analysis and problem-solving using the crisp-dm (cross-industry standard process for data mining) framework, which consists of six iterative steps: domain knowledge, data understanding, data preparation, modeling, evaluation, and deployment. it begins by emphasizing the importance of defining constraints, success criteria, and a project plan before diving into the data. the focus is on understanding the data, including identifying dependent variables (targets) and independent variables (features), and determining what problems can be solved with the available data. classification of data problems data problems are classified into two main categories: issues with dependent variables, such as missing labels (requiring clustering), incorrect or noisy labels (requiring noise removal), insufficient data (needing more data collection or synthetic data generation), or imbalanced data (requiring techniques like oversampling or undersampling); and issues with independent variables, such as within-column problems like missing values, outliers, incorrect representations, duplicates, uneven distributions, or too much data, and cross-column problems like insufficient features, too many features (requiring feature elimination using methods like p-values or heatmaps), or feature scaling issues. exploratory data analysis (eda) exploratory data analysis (eda) is a critical step, involving checking the distribution of variables, using box plots to understand variability, analyzing feature correlations with heatmaps, creating scatter plots and matrix plots to visualize relationships, identifying class imbalances and trends, and handling missing data by determining if it s mcar (missing completely at random), mar (missing at random), or mnar (missing not at random). handling missing data missing data can be addressed through methods like replacing with statistics (mean, median, etc.), using k-nearest neighbors or linear regression for imputation, or for time-series data, using interpolation or nearby values. outlier detection and management outlier detection involves techniques like sorting data and using the interquartile range (iqr) or standard deviation to identify outliers, applying dbscan clustering for multivariate outliers, or using t-sne to visualize high-dimensional data and spot anomalies. outliers can be true anomalies or errors, and true outliers may need to be processed separately. the lecture also highlighted the difference between univariate and multivariate analysis, where outliers may not be visible in one dimension but become apparent in multiple dimensions. finally, it stresses the importance of iterative validation and thorough data preparation to ensure the data is clean and ready for modeling, leading to reliable and actionable insights.",9,-16.813606,18.88821,8.6187935,8.639878,"dataâ, analyse, analyses"
413,"first, we looked at all the summaries. we had a pattern where the number of people submitting summaries decreased as sessions went on while the average words per summary were increasing.

the second case that we looked into was when the function was looking more like a sine curve. in this case, we only used one feature, x1 but used more polynomial features like x1^2,x1^3, x1^4, using polynomial regression to get a good fit. this model gave us a p-value. to further improve the model, we added a new feature, sin x1. in this second case, the resulting p-value was lower than in the first case. this means that the sine feature was statistically significant and improved the model's performance. however, adding too many features beyond a certain point does not necessarily improve the estimation and can lead to a decline in adjusted râ² values. in general, if we can fit the data using a single model, it should be preferred over the fit using multiple models. we then discussed parametric methods, including neural networks.

in these models, an input layer takes in features, which are passed through computational layers. when computations happen across more than one layer, it is called deep learning. the input layer is connected to computational layers in a way representing the degrees of freedom. while increasing these degrees of freedom may improve prediction it also means that there is an increased chance of overfitting, which is the model overly fits the training data and, therefore underfits new data.","first, we looked at all the summaries. we had a pattern where the number of people submitting summaries decreased as sessions went on while the average words per summary were increasing. the second case that we looked into was when the function was looking more like a sine curve. in this case, we only used one feature, x1 but used more polynomial features like x1^2,x1^3, x1^4, using polynomial regression to get a good fit. this model gave us a p-value. to further improve the model, we added a new feature, sin x1. in this second case, the resulting p-value was lower than in the first case. this means that the sine feature was statistically significant and improved the model's performance. however, adding too many features beyond a certain point does not necessarily improve the estimation and can lead to a decline in adjusted r values. in general, if we can fit the data using a single model, it should be preferred over the fit using multiple models. we then discussed parametric methods, including neural networks. in these models, an input layer takes in features, which are passed through computational layers. when computations happen across more than one layer, it is called deep learning. the input layer is connected to computational layers in a way representing the degrees of freedom. while increasing these degrees of freedom may improve prediction it also means that there is an increased chance of overfitting, which is the model overly fits the training data and, therefore underfits new data.",0,-2.7176323,1.5166829,8.950558,3.9765127,"models, feature, features"
414,"we explored playground tensorflow and worked on classifying two spirals. then, we studied logistic regression, focusing on accuracy, precision, recall, true positives, and true negatives. we learned about the roc curve, which helps analyze when a classifier starts detecting false positives. a good classifier has an inverted l-shaped roc curve with an auc close to 1, while a poor classifier has a flatter curve. we also realized that if data is too overlapping, it becomes difficult to develop a reliable classifier. when working with imbalanced data, we observed that the underrepresented class had a very low f1 score, highlighting the challenges in fraud detection where data imbalance is common. additionally, we explored unsupervised learning techniques like k-means clustering and hierarchical clustering, where dendrograms provide insights into how observations are related.","we explored playground tensorflow and worked on classifying two spirals. then, we studied logistic regression, focusing on accuracy, precision, recall, true positives, and true negatives. we learned about the roc curve, which helps analyze when a classifier starts detecting false positives. a good classifier has an inverted l-shaped roc curve with an auc close to 1, while a poor classifier has a flatter curve. we also realized that if data is too overlapping, it becomes difficult to develop a reliable classifier. when working with imbalanced data, we observed that the underrepresented class had a very low f1 score, highlighting the challenges in fraud detection where data imbalance is common. additionally, we explored unsupervised learning techniques like k-means clustering and hierarchical clustering, where dendrograms provide insights into how observations are related.",8,-1.1956742,-18.78434,6.9448185,0.46667153,"classification, clusterings, classifying"
415,"sir discussed handling outliers and missing data, emphasizing their impact on analysis. he explained that outliers can be dropped, replaced using data sampling, or analyzed separately since the mean is influenced by them, but the median is not.
for missing values, filling methods depend on the column's distribution, and techniques like mnar, mar, mcar, regression, and interpolation help. sir also highlighted box plots, matrix plots, and dbscan for outlier detection.
he introduced crisp-dm, a cyclical six-step process for data mining: business understanding, data understanding, data preparation, modelling, evaluation, and deployment. lastly, he stressed setting deadlines in ml work to avoid endless optimizations (he joked about ""asymptoting forever""). preprocessing is key before applying ml models!","sir discussed handling outliers and missing data, emphasizing their impact on analysis. he explained that outliers can be dropped, replaced using data sampling, or analyzed separately since the mean is influenced by them, but the median is not. for missing values, filling methods depend on the column's distribution, and techniques like mnar, mar, mcar, regression, and interpolation help. sir also highlighted box plots, matrix plots, and dbscan for outlier detection. he introduced crisp-dm, a cyclical six-step process for data mining: business understanding, data understanding, data preparation, modelling, evaluation, and deployment. lastly, he stressed setting deadlines in ml work to avoid endless optimizations (he joked about ""asymptoting forever""). preprocessing is key before applying ml models!",9,-18.220715,16.683868,8.511242,8.543375,"dataâ, analyse, analyses"
416,today's lecture started with discussing about some doubts asked in the lecture summaries. here we discussed about how to change the the bin size of the histogram according to some rules which helps use to use the metrics more efficiently. how two models can be used to fit a single dataset but the problem with this is that the continuity is not maintained. some discussions on expectation algebra were also done. then we shifted our discussion on logistic regression where we discussed how it is used for classification by implementing probability and mapping this probability to classes. then we discussed about some metrics which can be used to gauge the precision of our classification. confusion matrix which can be used to depict the classified and misclassified data. precision is the correctly classified events out of all the events. recall is the correctly detected events of a particular class. f1 score is the harmonic mean of precision and recall.harmonic mean is taken so as to predict whether we have a balanced model or not. thank you.,today's lecture started with discussing about some doubts asked in the lecture summaries. here we discussed about how to change the the bin size of the histogram according to some rules which helps use to use the metrics more efficiently. how two models can be used to fit a single dataset but the problem with this is that the continuity is not maintained. some discussions on expectation algebra were also done. then we shifted our discussion on logistic regression where we discussed how it is used for classification by implementing probability and mapping this probability to classes. then we discussed about some metrics which can be used to gauge the precision of our classification. confusion matrix which can be used to depict the classified and misclassified data. precision is the correctly classified events out of all the events. recall is the correctly detected events of a particular class. f1 score is the harmonic mean of precision and recall.harmonic mean is taken so as to predict whether we have a balanced model or not. thank you.,10,2.1446307,-16.462698,7.3307824,-0.059822273,"classifications, histograms, histogram"
417,"multiple linear regression: 
linear regression = outcome is expressed as linear combination of independent variables. 
taylor series ==> apply to mlr to capture the nonlinearity in the datset
this is just polynomial regression. the original was nonlinear so we engineer more features and then try to fit the model, the feature that fits best (like the sinusoid the in class example) will have more weight. so p value decreases and adjacent r2 value will decreases if you keep adding lot of feature more than required. 

forward feature engineering: put all possibl features and eliminate them one by one
backward feature engineering: start minimal set of feature and keep adding 1 by 1 based on your domain knowledge

usual methodology:
get data ==> preprocess & eda ==> good data ==> multiple ml methods ==> compare metrics ==> select best (or a combination of multiple methods with a weighted mean)

for a given data (the one in class): is it better to fit 1 model and 2? i think 1 because it would take into account for the variation in unknown data to be tested or applied later on. but 2 models might overfit.
then we got an overview of non parameter models like knn and random forest.

moving on to neural nets: input features ==> multiple layers ==> weighted average to get output 
we get a lot of tunable params which help us fit the model better to the data","multiple linear regression: linear regression = outcome is expressed as linear combination of independent variables. taylor series ==> apply to mlr to capture the nonlinearity in the datset this is just polynomial regression. the original was nonlinear so we engineer more features and then try to fit the model, the feature that fits best (like the sinusoid the in class example) will have more weight. so p value decreases and adjacent r2 value will decreases if you keep adding lot of feature more than required. forward feature engineering: put all possibl features and eliminate them one by one backward feature engineering: start minimal set of feature and keep adding 1 by 1 based on your domain knowledge usual methodology: get data ==> preprocess & eda ==> good data ==> multiple ml methods ==> compare metrics ==> select best (or a combination of multiple methods with a weighted mean) for a given data (the one in class): is it better to fit 1 model and 2? i think 1 because it would take into account for the variation in unknown data to be tested or applied later on. but 2 models might overfit. then we got an overview of non parameter models like knn and random forest. moving on to neural nets: input features ==> multiple layers ==> weighted average to get output we get a lot of tunable params which help us fit the model better to the data",0,3.0786464,-3.5627944,9.616072,4.054386,"models, feature, features"
418,"1. understanding the generated outputs  
   - when using built-in linear regression (lr) tools such as excel, several statistical values are generated, which help in interpreting the model's effectiveness and reliability.  

2. sample vs. population  
   - the given dataset (x, y) represents a sample (99 observations), and the calculated regression coefficients (intercept 'a' and slope 'b') are only estimates of the true population values.  
   - the goal is to develop a general model that can predict values beyond the given sample and reflect the actual population behavior.  

3. key statistical values explained  
   - regression statistics  
     - multiple r: represents the correlation between predicted and actual values.  
     - r square (râ²): measures how well the independent variable explains the variation in the dependent variable.  
     - adjusted r square: adjusts râ² for the number of predictors, providing a more accurate measure when multiple variables are involved.  
     - standard error: indicates the average error in predictions.  
     - observations: number of data points used in the model.  

   - anova (analysis of variance)  
     - ss (sum of squares): measures total variation in data, divided into regression (explained) and residual (unexplained) parts.  
     - ms (mean squares): average squared deviation, derived from ss.  
     - f-statistic: tests the overall significance of the regression model.  
     - significance f (p-value): determines if the independent variable has a statistically significant effect on the dependent variable.  

   - regression coefficients table  
     - coefficients (intercept and slope): estimated values for the regression equation.  
     - standard error: measures variability in the coefficient estimates.  
     - t-stat: tests whether each coefficient is significantly different from zero.  
     - p-value: indicates the statistical significance of the coefficients.  
     - lower/upper 95%: confidence interval for the coefficients, indicating the range within which the true population parameter likely falls.  

4. importance of statistical confidence  
   - emphasized the need to evaluate how reliable the coefficient estimates (a, b) are.  
   - concepts of sampling distributions and confidence intervals were introduced to understand the variability and precision of these estimates.","1. understanding the generated outputs - when using built-in linear regression (lr) tools such as excel, several statistical values are generated, which help in interpreting the model's effectiveness and reliability. 2. sample vs. population - the given dataset (x, y) represents a sample (99 observations), and the calculated regression coefficients (intercept 'a' and slope 'b') are only estimates of the true population values. - the goal is to develop a general model that can predict values beyond the given sample and reflect the actual population behavior. 3. key statistical values explained - regression statistics - multiple r: represents the correlation between predicted and actual values. - r square (r ): measures how well the independent variable explains the variation in the dependent variable. - adjusted r square: adjusts r for the number of predictors, providing a more accurate measure when multiple variables are involved. - standard error: indicates the average error in predictions. - observations: number of data points used in the model. - anova (analysis of variance) - ss (sum of squares): measures total variation in data, divided into regression (explained) and residual (unexplained) parts. - ms (mean squares): average squared deviation, derived from ss. - f-statistic: tests the overall significance of the regression model. - significance f (p-value): determines if the independent variable has a statistically significant effect on the dependent variable. - regression coefficients table - coefficients (intercept and slope): estimated values for the regression equation. - standard error: measures variability in the coefficient estimates. - t-stat: tests whether each coefficient is significantly different from zero. - p-value: indicates the statistical significance of the coefficients. - lower/upper 95%: confidence interval for the coefficients, indicating the range within which the true population parameter likely falls. 4. importance of statistical confidence - emphasized the need to evaluate how reliable the coefficient estimates (a, b) are. - concepts of sampling distributions and confidence intervals were introduced to understand the variability and precision of these estimates.",5,22.5168,1.4265125,14.678943,4.6155276,"regression, statistical, statistics"
419,"today we continued discussing the implications of the different statistical measures of the population parameters. we briefly discussed confidence intervals and then we went into a discussion of multiple regression. in many real-world scenarios, a single explanatory variable is insufficient to model an outcome accurately. simple linear regression, which models a dependent variable y as a function of a single independent variable x, assumes that y is only influenced by one factor. however, most processes are multifactorial, meaning multiple variables interact to determine the outcome. consider the standard simple regression model: y=beta0+beta1x+epsilon, where epsilon represents random error. this approach assumes that all variation in y can be explained by x, which is often too simplistic.

now, if multiple factors influence y, we need a multiple regression model:
y=beta0+beta1x1+beta2x2+â‹¯+betanxn+îµ, where x1,x2,â€¦,xnx1â€‹,x2â€‹,â€¦,xnâ€‹ are different independent variables. this allows us to capture more complex relationships between data. excel's toolpak enables multiple regression as well. we get f-statistics for multiple regression but we have not discussed that in detail yet.","today we continued discussing the implications of the different statistical measures of the population parameters. we briefly discussed confidence intervals and then we went into a discussion of multiple regression. in many real-world scenarios, a single explanatory variable is insufficient to model an outcome accurately. simple linear regression, which models a dependent variable y as a function of a single independent variable x, assumes that y is only influenced by one factor. however, most processes are multifactorial, meaning multiple variables interact to determine the outcome. consider the standard simple regression model: y=beta0+beta1x+epsilon, where epsilon represents random error. this approach assumes that all variation in y can be explained by x, which is often too simplistic. now, if multiple factors influence y, we need a multiple regression model: y=beta0+beta1x1+beta2x2+ +betanxn+ , where x1,x2, ,xnx1 ,x2 , ,xn are different independent variables. this allows us to capture more complex relationships between data. excel's toolpak enables multiple regression as well. we get f-statistics for multiple regression but we have not discussed that in detail yet.",2,20.06692,3.597917,12.712105,4.7584467,"regression, regressions, features"
420,"four categories of measurements exist: 1. nominal: there is no precise hierarchy among data kinds. for example, gender 2. ordinal: ordering of discrete data. for example, educational attainment. 3. the interval is continuous and lacks an absolute zero. for example, temperature. 4. ratio: a continuous, absolute zero is defined. for example, lengthclassification is used for nominal and ordinal data. regression is used for intervals and ratios. when x is the input, y is the output, and f is our algorithm, the general equation for an ml problem is f(x)=y. while unsupervised learning groups unlabeled datasets into clusters, supervised learning uses labeled data and attempts to predict appropriate labels for unknown data.","four categories of measurements exist: 1. nominal: there is no precise hierarchy among data kinds. for example, gender 2. ordinal: ordering of discrete data. for example, educational attainment. 3. the interval is continuous and lacks an absolute zero. for example, temperature. 4. ratio: a continuous, absolute zero is defined. for example, lengthclassification is used for nominal and ordinal data. regression is used for intervals and ratios. when x is the input, y is the output, and f is our algorithm, the general equation for an ml problem is f(x)=y. while unsupervised learning groups unlabeled datasets into clusters, supervised learning uses labeled data and attempts to predict appropriate labels for unknown data.",4,-26.051422,-17.381392,1.3870986,0.17321357,"classification, classifying, classifications"
421,"today we discussed a bit about vif and how a threshold should be selected with a reason. and we moved to pca, a technique that helps us reduce dimensionality. so in pca the pc axes are all orthogonal to each other, and they are linear combination of the initial features. pc1 has the highest variance, followed by pc2 and so on.
between vif and pca, vif ids thee one that should be done first as it gives an idea of the multicollinearity of the features that is more pressing to know and then pca can be used for visualization of the data.
there was another technique discussed for visualization, which was tsne, which gives a depiction in 2 dimensions. but this transformation is a lossy one. its comparatively slower as it is a stochastic process involving the t distribution.","today we discussed a bit about vif and how a threshold should be selected with a reason. and we moved to pca, a technique that helps us reduce dimensionality. so in pca the pc axes are all orthogonal to each other, and they are linear combination of the initial features. pc1 has the highest variance, followed by pc2 and so on. between vif and pca, vif ids thee one that should be done first as it gives an idea of the multicollinearity of the features that is more pressing to know and then pca can be used for visualization of the data. there was another technique discussed for visualization, which was tsne, which gives a depiction in 2 dimensions. but this transformation is a lossy one. its comparatively slower as it is a stochastic process involving the t distribution.",11,-17.475594,1.303922,10.240517,13.071827,"pca, heatmap, heatmaps"
422,"- we learned how to measure key things like the average (mean) and how spread out the data is (variance) within a population.
- the prof used a real-world example â€“ figuring out how much extra time managers work on average. we used a small group of managers (only 18!) to represent the whole company. then, we visualized how the average overtime hours might vary across different samples using a special graph called a probability density function (pdf).
- making smart decisions: we learned about confidence intervals, which are like a range where the true average likely falls. since we had a small sample size, we used something called the t-distribution to calculate these intervals. our teacher emphasized how useful confidence intervals are for making informed decisions based on data.
- we explored important terms like t-values and z-values, which help us understand how likely our results are. we also discussed what it means when we say two things are ""statistically different"" â€“ basically, that the difference between them is real and not just due to chance.
- we got a quick introduction to p-values, which are another way to assess the significance of our findings.","- we learned how to measure key things like the average (mean) and how spread out the data is (variance) within a population. - the prof used a real-world example figuring out how much extra time managers work on average. we used a small group of managers (only 18!) to represent the whole company. then, we visualized how the average overtime hours might vary across different samples using a special graph called a probability density function (pdf). - making smart decisions: we learned about confidence intervals, which are like a range where the true average likely falls. since we had a small sample size, we used something called the t-distribution to calculate these intervals. our teacher emphasized how useful confidence intervals are for making informed decisions based on data. - we explored important terms like t-values and z-values, which help us understand how likely our results are. we also discussed what it means when we say two things are ""statistically different"" basically, that the difference between them is real and not just due to chance. - we got a quick introduction to p-values, which are another way to assess the significance of our findings.",7,32.002117,8.571524,14.613914,2.3649504,"statistics, statistical, statisticsâ"
423,"first of all we recap on the previous part like confidence interval and saw that if we have a sample which has an element called b from lower probability region then that sample is of importance, but in the case of broader 95% region 2 values are not really distinct values and if come distinct then it is by chance. then we saw that how summary is checked if it's similar to other session summary or not. we may get the data in future. we saw around 9 lines which shows that about 9 are pretty similar summaries. we convert text into features of x (vector). we talked about feature engineering and why rate is important and significant. we also saw about f which is equal to msr/mse and it shows behaviour of model itself and in comparision to other. we also dropped different features for our model for multiple linear regression and learnt about how to remove feature which are not important depending on p values. f will be large everytime and r2 remains around constant. multiple linear regression became clear in the class and also feature selection. we talked about a little about topic name like embedding for text to vector conversion also , will discuss in detail later. we saw there are n dimensional dataset in multiple linear regression. also to reduce error we saw we can use matrix to solve and get our matrices. we also saw error matrices like sse, mse , rmse and mae.","first of all we recap on the previous part like confidence interval and saw that if we have a sample which has an element called b from lower probability region then that sample is of importance, but in the case of broader 95% region 2 values are not really distinct values and if come distinct then it is by chance. then we saw that how summary is checked if it's similar to other session summary or not. we may get the data in future. we saw around 9 lines which shows that about 9 are pretty similar summaries. we convert text into features of x (vector). we talked about feature engineering and why rate is important and significant. we also saw about f which is equal to msr/mse and it shows behaviour of model itself and in comparision to other. we also dropped different features for our model for multiple linear regression and learnt about how to remove feature which are not important depending on p values. f will be large everytime and r2 remains around constant. multiple linear regression became clear in the class and also feature selection. we talked about a little about topic name like embedding for text to vector conversion also , will discuss in detail later. we saw there are n dimensional dataset in multiple linear regression. also to reduce error we saw we can use matrix to solve and get our matrices. we also saw error matrices like sse, mse , rmse and mae.",2,5.288161,8.606012,11.910225,4.1086936,"regression, regressions, features"
424,"in todays class (14/2/25)
we started with the discussion about the evaluation of confusion matric via dataframe function and scikit function where we found they both are transpose of each other and while calculating the recall and precision values, we need to understand which function we are using to find accurate values for which we can analyse the number of classes we have and estimate which of the matrix results in perfect number.
next up we stated with exploratory data analysis which helps us to understand the nature of data. the brief steps in data science includes 
1. problem understanding
2. eds
3. visualization
the 2 and 3 helps us to visualize and understand mathematics behind the data.
next up we answered the question what to do when you get the data using the crisp-dm 6 membered cycle
1. business/ domain understanding [determine the objectives/ hypothesis/ constraints, etc]
2. data understanding [describe the data in format/ columns/ rows/ missing data, etc]
3. dara preparation [perform the required transformation to use the data]
4. modelling
5. evaluation [using the metrics]
6. deployment
the main 4 uses of eda involves
1. get insights
2. spot anomalies
3. test hypothesis
4. check assumptions
it completes the following task using 2 techniques: statistical graphics and data visualization
than we gradually understood the importance of eda 
after which we entered to learning about missing values where we found 3 types
1. mcar (missing completely at random)
2. mar (missing at random): there is some relation between value and other feature due to which we can neglect them
3. mnar (missing not at random): it includes data which is not relevant based on the domain for example the co sensor above 110 should give a null value, we have no use for it and thus the sensor is set in the particular way
what can we do? --> drop/ let it be/ give a good number like mean or median these are univariate solutions while knns(average o nearest neighbors) / mice (train a function and predict the value for it) are the multi-variate solutions. for time-series data we can use forward/ backward fill/ linear interpolation (calculating the value by line connecting the alternate avalaible points)/ simple moving averages
than we moved onto handling outliers which may arrive  due to data corruption/ faulty measurements or true outliers where median values can be used the best since they are not influenced by the outliers whereas means are so need to keep that in mind","in todays class (14/2/25) we started with the discussion about the evaluation of confusion matric via dataframe function and scikit function where we found they both are transpose of each other and while calculating the recall and precision values, we need to understand which function we are using to find accurate values for which we can analyse the number of classes we have and estimate which of the matrix results in perfect number. next up we stated with exploratory data analysis which helps us to understand the nature of data. the brief steps in data science includes 1. problem understanding 2. eds 3. visualization the 2 and 3 helps us to visualize and understand mathematics behind the data. next up we answered the question what to do when you get the data using the crisp-dm 6 membered cycle 1. business/ domain understanding [determine the objectives/ hypothesis/ constraints, etc] 2. data understanding [describe the data in format/ columns/ rows/ missing data, etc] 3. dara preparation [perform the required transformation to use the data] 4. modelling 5. evaluation [using the metrics] 6. deployment the main 4 uses of eda involves 1. get insights 2. spot anomalies 3. test hypothesis 4. check assumptions it completes the following task using 2 techniques: statistical graphics and data visualization than we gradually understood the importance of eda after which we entered to learning about missing values where we found 3 types 1. mcar (missing completely at random) 2. mar (missing at random): there is some relation between value and other feature due to which we can neglect them 3. mnar (missing not at random): it includes data which is not relevant based on the domain for example the co sensor above 110 should give a null value, we have no use for it and thus the sensor is set in the particular way what can we do? --> drop/ let it be/ give a good number like mean or median these are univariate solutions while knns(average o nearest neighbors) / mice (train a function and predict the value for it) are the multi-variate solutions. for time-series data we can use forward/ backward fill/ linear interpolation (calculating the value by line connecting the alternate avalaible points)/ simple moving averages than we moved onto handling outliers which may arrive due to data corruption/ faulty measurements or true outliers where median values can be used the best since they are not influenced by the outliers whereas means are so need to keep that in mind",9,-14.242258,18.313686,9.168799,9.073401,"dataâ, analyse, analyses"
425,"eda's core purpose is to explore data to understand its structure, identify patterns and anomalies, and formulate hypotheses, ultimately preparing it for more rigorous analysis and modeling. we now analysed the summary dataset. qty of words or average length of each entry was close to 1200 words, although some submissions were as long as 6000 .after that, we examined a real-world situation that involved chemical plant optimization.we looked at several graphs/charts to get an overview of data.we observed a histogram with some points at 0 but mostly following a normal distribution. given the example of the nvidia stock, we cannot simply rule out this outlier, even though it might be the result of sensor failure.pitot tables were analysed. prior to rejecting these ideals, domain understanding is crucial and necessitates cross-questioning with stakeholders.in order to prepare for more precise analysis, we tackled data challenges such as skewed categories and underlined the significance of data refinement using methods like feature engineering.e2 feedback was given at the end.","eda's core purpose is to explore data to understand its structure, identify patterns and anomalies, and formulate hypotheses, ultimately preparing it for more rigorous analysis and modeling. we now analysed the summary dataset. qty of words or average length of each entry was close to 1200 words, although some submissions were as long as 6000 .after that, we examined a real-world situation that involved chemical plant optimization.we looked at several graphs/charts to get an overview of data.we observed a histogram with some points at 0 but mostly following a normal distribution. given the example of the nvidia stock, we cannot simply rule out this outlier, even though it might be the result of sensor failure.pitot tables were analysed. prior to rejecting these ideals, domain understanding is crucial and necessitates cross-questioning with stakeholders.in order to prepare for more precise analysis, we tackled data challenges such as skewed categories and underlined the significance of data refinement using methods like feature engineering.e2 feedback was given at the end.",9,-6.0575395,25.164072,8.319503,9.381973,"dataâ, analyse, analyses"
426,"this lecture addresses practical data analysis challenges, particularly those encountered when applying a trained model to new data, and introduces the ""curse of dimensionality.""

data distribution mismatch: the lecture begins by discussing exam performance with new data.  the model, trained on original data, performs poorly on the new data.  kernel density estimation (kde) plots reveal that the feature distributions differ significantly between the original and new datasets. this indicates the data were sampled from different populations, explaining the model's poor performance.

exploratory data analysis (eda): the lecture then moves to a solution walkthrough, starting with eda.  this involves:

missing value handling: identifying and addressing missing values using techniques like imputation (mean, median, etc.) or more advanced algorithms.
descriptive statistics: calculating and analyzing descriptive statistics (min, max, mean, median, etc.) for each feature.
outlier detection: using box plots to check for outliers. in this case, no outliers were found.
imbalanced data: analyzing the ""ailment"" (target variable) and finding it heavily imbalanced, specifically with very low counts for heart disease. this imbalance makes accurate prediction for heart disease extremely difficult. techniques like oversampling and undersampling are mentioned, but obtaining more data is suggested as the most effective solution.
kde plots: generating kde plots for all features to visualize and compare their distributions.
curse of dimensionality: the lecture then introduces the ""curse of dimensionality,"" which arises when the number of features in a dataset increases significantly. this leads to several problems:

increased sparsity: data points become more spread out, making it harder to find meaningful patterns.
increased complexity: model complexity increases, leading to potential overfitting.
increased computational cost: training and processing become more resource-intensive.
distance distortion: distances between data points become less meaningful, as they tend to become more uniform.
consequences and solutions: the consequences of high dimensionality include overfitting, increased computational resources, and data sparsity.

solutions to the curse of dimensionality include:

dimensionality reduction: techniques like principal component analysis (pca) to reduce the number of features.
feature selection: selecting the most relevant features.
regularization: techniques that penalize model complexity.
increasing the amount of data: more data can mitigate the effects of sparsity.","this lecture addresses practical data analysis challenges, particularly those encountered when applying a trained model to new data, and introduces the ""curse of dimensionality."" data distribution mismatch: the lecture begins by discussing exam performance with new data. the model, trained on original data, performs poorly on the new data. kernel density estimation (kde) plots reveal that the feature distributions differ significantly between the original and new datasets. this indicates the data were sampled from different populations, explaining the model's poor performance. exploratory data analysis (eda): the lecture then moves to a solution walkthrough, starting with eda. this involves: missing value handling: identifying and addressing missing values using techniques like imputation (mean, median, etc.) or more advanced algorithms. descriptive statistics: calculating and analyzing descriptive statistics (min, max, mean, median, etc.) for each feature. outlier detection: using box plots to check for outliers. in this case, no outliers were found. imbalanced data: analyzing the ""ailment"" (target variable) and finding it heavily imbalanced, specifically with very low counts for heart disease. this imbalance makes accurate prediction for heart disease extremely difficult. techniques like oversampling and undersampling are mentioned, but obtaining more data is suggested as the most effective solution. kde plots: generating kde plots for all features to visualize and compare their distributions. curse of dimensionality: the lecture then introduces the ""curse of dimensionality,"" which arises when the number of features in a dataset increases significantly. this leads to several problems: increased sparsity: data points become more spread out, making it harder to find meaningful patterns. increased complexity: model complexity increases, leading to potential overfitting. increased computational cost: training and processing become more resource-intensive. distance distortion: distances between data points become less meaningful, as they tend to become more uniform. consequences and solutions: the consequences of high dimensionality include overfitting, increased computational resources, and data sparsity. solutions to the curse of dimensionality include: dimensionality reduction: techniques like principal component analysis (pca) to reduce the number of features. feature selection: selecting the most relevant features. regularization: techniques that penalize model complexity. increasing the amount of data: more data can mitigate the effects of sparsity.",9,-13.277381,12.506387,8.865551,8.044532,"dataâ, analyse, analyses"
427,"we discussed steps of solution for the midsem test. 1) eda- firstly intra column issues and representation, then inter stats using heat maps.  2) predictive model: random forest are good. confusion matrix, metrics should be done. do for validation data as well. 3) analysis: distribution very different among the datasets


some points discussed: 1)as heart disease data not much and difference is too high among the columns so under-over sampling should not be done, it will reduce data spread so better to say heart disease will not be predicted that good. 2)got to know an interesting fact, even if heat map shows no correlation, we can still have multi collinearity.
we started curse of dimensionality. sparse data,  distance in n dimensional space how to segregate.","we discussed steps of solution for the midsem test. 1) eda- firstly intra column issues and representation, then inter stats using heat maps. 2) predictive model: random forest are good. confusion matrix, metrics should be done. do for validation data as well. 3) analysis: distribution very different among the datasets some points discussed: 1)as heart disease data not much and difference is too high among the columns so under-over sampling should not be done, it will reduce data spread so better to say heart disease will not be predicted that good. 2)got to know an interesting fact, even if heat map shows no correlation, we can still have multi collinearity. we started curse of dimensionality. sparse data, distance in n dimensional space how to segregate.",9,-10.10614,13.852311,9.613817,8.480652,"dataâ, analyse, analyses"
428,"it was discussed that how to improve the results: 
by increasing the quality of the sample 
or by increasing the size .
to improve the method we use multiple methods and then select the best one .
fine tune and properly use that method.
linear regression doesn't mean to fit a line but rather that the output is represented in a form of linear combination of independent variables.
in real life we get the data then preprocess it , from which we get good data then we apply different ml techniques(i.e form matrices ) , compare them and then select the best one.
lr and similar techniques are parametric methods.
then neural networks were discussed briefly , how different paths are given different weights.
after that we moved to nominal and ordinal types of data in the function y=f(x). where we classify the data into different categories. 
logistic regression was talked about , how our function focuses on giving us the boundaries between different classes.","it was discussed that how to improve the results: by increasing the quality of the sample or by increasing the size . to improve the method we use multiple methods and then select the best one . fine tune and properly use that method. linear regression doesn't mean to fit a line but rather that the output is represented in a form of linear combination of independent variables. in real life we get the data then preprocess it , from which we get good data then we apply different ml techniques(i.e form matrices ) , compare them and then select the best one. lr and similar techniques are parametric methods. then neural networks were discussed briefly , how different paths are given different weights. after that we moved to nominal and ordinal types of data in the function y=f(x). where we classify the data into different categories. logistic regression was talked about , how our function focuses on giving us the boundaries between different classes.",0,3.132198,-5.5034876,9.49295,4.145028,"models, feature, features"
429,"one small insight i learned in todayâ€™s class is that one sample can have multiple observations because a sample is like a smaller set of data taken from the entire population.

in todayâ€™s class, we focused on confidence intervals and why theyâ€™re important.

we spent almost half of the class understanding how we can use a sample to figure out the populationâ€™s parameters, like the mean, by making some assumptions:

1. the sample represents the population well.
2. the standard deviation of the sample is roughly the same as the standard deviation of the population.

to estimate the population mean, we used the **central limit theorem**. it says that if you have a sample size n>30, you can use the z-distribution to calculate the confidence interval. if n<30, you use the t-distribution instead.

toward the end of the class, we looked at the **anova table** in excel and tried to understand some of the values it gives, like:

- **p-stats**: this is calculated using the formula:
    - p-stats= (x - x_) / (standard_deviation / sqrt(n)) ; where n is the number of samples
- **p-value**: this tells us how good a regression model is. it checks if the parameter î²1 (slope of the regression line) is statistically different from 0. if the p-value is very small, it means î²1 is significant, so the regression line is meaningful. if the p-value is large, it means î²1 is not statistically different from 0, which implies thereâ€™s no slope, and therefore no valid regression line.

we also discussed why confidence intervals are helpful. they donâ€™t just give us a single estimate (like the mean) but a range where the actual value is likely to fall. this makes our predictions more reliable.

lastly, the class touched on how these concepts connect to regression analysis. for example, when using the regression output in excel, understanding the p-value, confidence intervals, and other stats helps us decide if the model is good enough or needs improvement.","one small insight i learned in today s class is that one sample can have multiple observations because a sample is like a smaller set of data taken from the entire population. in today s class, we focused on confidence intervals and why they re important. we spent almost half of the class understanding how we can use a sample to figure out the population s parameters, like the mean, by making some assumptions: 1. the sample represents the population well. 2. the standard deviation of the sample is roughly the same as the standard deviation of the population. to estimate the population mean, we used the **central limit theorem**. it says that if you have a sample size n>30, you can use the z-distribution to calculate the confidence interval. if n<30, you use the t-distribution instead. toward the end of the class, we looked at the **anova table** in excel and tried to understand some of the values it gives, like: - **p-stats**: this is calculated using the formula: - p-stats= (x - x_) / (standard_deviation / sqrt(n)) ; where n is the number of samples - **p-value**: this tells us how good a regression model is. it checks if the parameter 1 (slope of the regression line) is statistically different from 0. if the p-value is very small, it means 1 is significant, so the regression line is meaningful. if the p-value is large, it means 1 is not statistically different from 0, which implies there s no slope, and therefore no valid regression line. we also discussed why confidence intervals are helpful. they don t just give us a single estimate (like the mean) but a range where the actual value is likely to fall. this makes our predictions more reliable. lastly, the class touched on how these concepts connect to regression analysis. for example, when using the regression output in excel, understanding the p-value, confidence intervals, and other stats helps us decide if the model is good enough or needs improvement.",7,33.864586,1.5991555,14.76091,3.0847163,"statistics, statistical, statisticsâ"
430,"today we discussed simple linear regression (slr). simple linear regression is a statistical method used to model the relationship between a dependent variable y and an independent variable x by fitting a straight line to the data. the objective is to find the best-fitting line that minimizes the squared difference between the observed data points and the predicted values. the simple linear regression model is represented by the equation: y=a+bx+ïµ. the goal is to estimate the parameters a, bâ€‹ in summary, simple linear regression finds the best-fitting straight line by minimizing the sum of squared differences between the observed and predicted values, leading to the estimation of the slope and intercept.by minimizing the sum of squared errors, i.e., error - the difference between the observed values and actual values. to minimize the sse, take partial derivatives with respect to a, bâ€‹, set them to zero, and solve. after calculating a-hatâ€‹ and b-hatâ€‹, the fitted regression line is obtained.this is how we estimate the population parameters based on the data sample. we can also calculate confidence intervals on the parameter estimates using their known mean and variance.","today we discussed simple linear regression (slr). simple linear regression is a statistical method used to model the relationship between a dependent variable y and an independent variable x by fitting a straight line to the data. the objective is to find the best-fitting line that minimizes the squared difference between the observed data points and the predicted values. the simple linear regression model is represented by the equation: y=a+bx+ . the goal is to estimate the parameters a, b in summary, simple linear regression finds the best-fitting straight line by minimizing the sum of squared differences between the observed and predicted values, leading to the estimation of the slope and intercept.by minimizing the sum of squared errors, i.e., error - the difference between the observed values and actual values. to minimize the sse, take partial derivatives with respect to a, b , set them to zero, and solve. after calculating a-hat and b-hat , the fitted regression line is obtained.this is how we estimate the population parameters based on the data sample. we can also calculate confidence intervals on the parameter estimates using their known mean and variance.",1,28.561468,-7.517883,15.490796,4.53214,"population, models, estimating"
431,"today's class began with the discussion of midsem problem which was followed by e3 discussion. problems which arise due to high dimensional data were discussed. significance of distance between data points decreases in higher dimensions. it becomes difficult to distinguish between nearest and farthest points. complexity of the model increases . data becomes more sparse i.e more spread out . v.i.f was discussed, it is called as the variance inflation factor. it tells us how much the value of râ² increases due to correlation between predictor variables. we can drop the values with high v.i.f .","today's class began with the discussion of midsem problem which was followed by e3 discussion. problems which arise due to high dimensional data were discussed. significance of distance between data points decreases in higher dimensions. it becomes difficult to distinguish between nearest and farthest points. complexity of the model increases . data becomes more sparse i.e more spread out . v.i.f was discussed, it is called as the variance inflation factor. it tells us how much the value of r increases due to correlation between predictor variables. we can drop the values with high v.i.f .",11,-7.612312,7.367774,9.277276,7.256018,"pca, heatmap, heatmaps"
432,"in today's class, we move on to data smoothening using moving averages. in the practical datasets, we see many fluctuations due to detrimental noise, which can lead to difficulties in obtaining trends and patterns in the dataset. to handle the missing values in the dataset, for example in stock analysis, we don't have data of the upcoming days but we can fill the data using moving averages. in the simple moving average(sma) method, we need to consider a window around the missing data point and average the values for this. you can select the points from both sides of the point in the window or points from the left side of the point. for points which are appearing after endpoints, we use the final endpoints more than once. after performing sma we can create the plot again, the plot will be refined based on the window size chosen by us. if we select a very large window size, the plot nearly becomes a straight line, so we need to choose the width of the window based on the data for better analysis. we also have better methods like exponential moving averages that weigh nearby samples than moving averages. if in a dataset one particular feature column's features have a larger magnitude compared to other features, then we apply normalization to this column, but due to this change only the coefficient of this column in the mlr model changes. clustering methods based on distance can be highly influenced if one column has larger magnitudes or more variance than others. then we saw the difference between normalization and standardization. in normalization, the data is transformed as xn=(x-xmin)/(xmax-xmin) while in standardization, the data is transformed to a mean of 0 and standard deviation of 1 by the transformation 
zn=(x-mean)/(std. dev), in which the mean and std. dev. are of the sample. then we learned about the box-cox transformation, in which we transform the original x to remove the skewness from the data and remove the situation of heteroskedasticity, meaning the variance is changing in the data. it is based on the maximum likelihood estimation technique. then we saw the confusion matrix of a cluster plot in which a particular class was creating data imbalance. data imbalance means certain class shows more frequency than others, we need to handle this else we will get misleading results as an algorithm trained on imbalanced data may show biases toward a class. techniques handling data imbalance are either to undersample the majority class or oversample the minority class. a better approach is to generate synthetic data for the minority class(oversampling). we do this by performing a technique called smote in which we perform linear interpolation between existing samples. smote is used to oversample the minority class while the technique of tomek links on the other hand undersample the data. majority class samples with the nearest neighbours as minority class samples are removed from the data.","in today's class, we move on to data smoothening using moving averages. in the practical datasets, we see many fluctuations due to detrimental noise, which can lead to difficulties in obtaining trends and patterns in the dataset. to handle the missing values in the dataset, for example in stock analysis, we don't have data of the upcoming days but we can fill the data using moving averages. in the simple moving average(sma) method, we need to consider a window around the missing data point and average the values for this. you can select the points from both sides of the point in the window or points from the left side of the point. for points which are appearing after endpoints, we use the final endpoints more than once. after performing sma we can create the plot again, the plot will be refined based on the window size chosen by us. if we select a very large window size, the plot nearly becomes a straight line, so we need to choose the width of the window based on the data for better analysis. we also have better methods like exponential moving averages that weigh nearby samples than moving averages. if in a dataset one particular feature column's features have a larger magnitude compared to other features, then we apply normalization to this column, but due to this change only the coefficient of this column in the mlr model changes. clustering methods based on distance can be highly influenced if one column has larger magnitudes or more variance than others. then we saw the difference between normalization and standardization. in normalization, the data is transformed as xn=(x-xmin)/(xmax-xmin) while in standardization, the data is transformed to a mean of 0 and standard deviation of 1 by the transformation zn=(x-mean)/(std. dev), in which the mean and std. dev. are of the sample. then we learned about the box-cox transformation, in which we transform the original x to remove the skewness from the data and remove the situation of heteroskedasticity, meaning the variance is changing in the data. it is based on the maximum likelihood estimation technique. then we saw the confusion matrix of a cluster plot in which a particular class was creating data imbalance. data imbalance means certain class shows more frequency than others, we need to handle this else we will get misleading results as an algorithm trained on imbalanced data may show biases toward a class. techniques handling data imbalance are either to undersample the majority class or oversample the minority class. a better approach is to generate synthetic data for the minority class(oversampling). we do this by performing a technique called smote in which we perform linear interpolation between existing samples. smote is used to oversample the minority class while the technique of tomek links on the other hand undersample the data. majority class samples with the nearest neighbours as minority class samples are removed from the data.",9,-19.246498,10.996578,10.135446,10.052767,"dataâ, analyse, analyses"
433,"in today's class we discussed about data analysis. we learned about the time series data analysis, which we perform for limited data i.e. when the data is insufficient beyond a certain time for prediction.
we also discussed other types of distributions like small scale distributions- in which the sample may follow a uniform distribution than gaussian one. 
outcome distribution - when we have a large number of unknown variables then the outcome distribution follows gaussian (normal) distribution.
then we also discussed about regression analysis:- 
error decomposition: sst = sse + ssr , where sst is total sum of squares, sse is sum of squares of errors, ssr is regression of this sum of squares. we also talked about coefficient of determination(r2) = 1- (sse/sst) 
now, correlation coefficient (r): 
this indicates the strength of linear relationship, like higher the mod(r) , stronger the relation and vice versa.
we also discussed about some examples of this +ve and -ve correlation. we also used the software to make","in today's class we discussed about data analysis. we learned about the time series data analysis, which we perform for limited data i.e. when the data is insufficient beyond a certain time for prediction. we also discussed other types of distributions like small scale distributions- in which the sample may follow a uniform distribution than gaussian one. outcome distribution - when we have a large number of unknown variables then the outcome distribution follows gaussian (normal) distribution. then we also discussed about regression analysis:- error decomposition: sst = sse + ssr , where sst is total sum of squares, sse is sum of squares of errors, ssr is regression of this sum of squares. we also talked about coefficient of determination(r2) = 1- (sse/sst) now, correlation coefficient (r): this indicates the strength of linear relationship, like higher the mod(r) , stronger the relation and vice versa. we also discussed about some examples of this +ve and -ve correlation. we also used the software to make",5,24.40356,-5.7129536,14.049733,4.562102,"regression, statistical, statistics"
434,"we learned about crisp-dm (cross industry standard process for data mining), a six-step, cyclical process. it starts with business understanding, where we define the problem and assess relevant statistics. then comes data understanding, where we collect and explore the dataset. the modeling phase involves building and evaluating different models, followed by evaluation, where we assess the results to ensure they align with business needs. finally, in the deployment stage, the model is finalized, and reports are generated. after that, we explored exploratory data analysis (eda), a crucial approach in statistics and data science for investigating datasets. we also looked at outliers and quartiles, understanding how boxplots help visualize variability and detect outliers. additionally, we studied inter-feature relationships using matrix plots to identify correlations between different features. we then learned about three types of missing data: missing completely at random (mcar), where the missing values have no pattern; missing at random (mar), where missing data is related to some observed variables; and missing not at random (mnar), where the missing values are dependent on unobserved factors. we discussed true outliers, which are extreme values in a dataset that are not errors but actual observations.","we learned about crisp-dm (cross industry standard process for data mining), a six-step, cyclical process. it starts with business understanding, where we define the problem and assess relevant statistics. then comes data understanding, where we collect and explore the dataset. the modeling phase involves building and evaluating different models, followed by evaluation, where we assess the results to ensure they align with business needs. finally, in the deployment stage, the model is finalized, and reports are generated. after that, we explored exploratory data analysis (eda), a crucial approach in statistics and data science for investigating datasets. we also looked at outliers and quartiles, understanding how boxplots help visualize variability and detect outliers. additionally, we studied inter-feature relationships using matrix plots to identify correlations between different features. we then learned about three types of missing data: missing completely at random (mcar), where the missing values have no pattern; missing at random (mar), where missing data is related to some observed variables; and missing not at random (mnar), where the missing values are dependent on unobserved factors. we discussed true outliers, which are extreme values in a dataset that are not errors but actual observations.",9,-12.910924,20.712582,8.870055,8.782712,"dataâ, analyse, analyses"
435,"population vs sample, sample is a subset of population. we also learned that which statistics measure can be used in which of the level of measurement.
if we estimate, for population, than it is called a parameter and in case of sample, when we calculate, it is called statistic. before applying any ml techniques, we should first gain a basic knowledge about the data which is usually done by a scatter plot. we learnt about the simple linear regression, where there is only one predictor. generally, in slr, we take y as the dependent variable and x as the independent variable. we also say that the y intercept on slr is called bias. here we find that b0 and b1 are estimates of parameters of the population. to take account of error, we define a confidence interval. if we have a large confidence, then our interval will be large and vice versa.
to provide uniformity in all the directions of errors, we take sigma ei^2 instead of sigma |ei|. we also discussed about the derivation of the parameters for linear regression where we minimixe the sigma (ei)^2","population vs sample, sample is a subset of population. we also learned that which statistics measure can be used in which of the level of measurement. if we estimate, for population, than it is called a parameter and in case of sample, when we calculate, it is called statistic. before applying any ml techniques, we should first gain a basic knowledge about the data which is usually done by a scatter plot. we learnt about the simple linear regression, where there is only one predictor. generally, in slr, we take y as the dependent variable and x as the independent variable. we also say that the y intercept on slr is called bias. here we find that b0 and b1 are estimates of parameters of the population. to take account of error, we define a confidence interval. if we have a large confidence, then our interval will be large and vice versa. to provide uniformity in all the directions of errors, we take sigma ei^2 instead of sigma |ei|. we also discussed about the derivation of the parameters for linear regression where we minimixe the sigma (ei)^2",1,34.785686,-4.4203434,16.183859,3.4973347,"population, models, estimating"
436,"population and sample differences, sample being a subset of population and any attributes calculated for sample are called statistics whereas for population are called parameters. parameters can be estimated based on the statistics(sample mean, variance, standard deviation, etc). simple linear regression and obtaining equation of best fit line for trying to make predictions based on available data. regression line equation - y = î²o + î²1x, where î²o and î²1 are estimates of population parameters. finding confidence interval for parameter estimates and estimates by minimizing the sum of squares of errors (square of distances(euclidean dist.) since taking the absolute value(manhattan dist.) differentiates between directions. minimizing î£(eiâ²) by partial differentiating wrt. î²o and î²1 to obtain estimates for the population parameters based on available data, ei = (yi - î²o + î²1xi). expressions obtained -
î²o = mean(y) - î²1*mean(x)
î²1 = (mean(xy) - mean(x) * mean(y) )/(mean(xâ²) - (mean(x))â² )","population and sample differences, sample being a subset of population and any attributes calculated for sample are called statistics whereas for population are called parameters. parameters can be estimated based on the statistics(sample mean, variance, standard deviation, etc). simple linear regression and obtaining equation of best fit line for trying to make predictions based on available data. regression line equation - y = o + 1x, where o and 1 are estimates of population parameters. finding confidence interval for parameter estimates and estimates by minimizing the sum of squares of errors (square of distances(euclidean dist.) since taking the absolute value(manhattan dist.) differentiates between directions. minimizing (ei ) by partial differentiating wrt. o and 1 to obtain estimates for the population parameters based on available data, ei = (yi - o + 1xi). expressions obtained - o = mean(y) - 1*mean(x) 1 = (mean(xy) - mean(x) * mean(y) )/(mean(x ) - (mean(x)) )",1,31.471495,-9.129668,16.621908,4.2497187,"population, models, estimating"
437,"class 8,

explained how multicollinearity can be a problem and why we cant solve mlr by closed form solution. the exact form (xtx)^-1... type form.-> impractical, inverse matrix is very hard to compute for so many features and higher number of data. hence we need to shift to gradient descent for reduction of computation.

splitting of data: we split a sample. do not consider a whole sample for training purpose. around 80%-20% split? why? because we want to test the model whether it is really eefective om the unseen data or not.

why not 50-50? why 80-20? we want the data on which the model is going to be trained to be representative enough of the data for variance or complexity capturing.

two sets of outcomes: -training metric and test metrics. training metrics include sse,rmse, f-statistics, r^2. test metrics also include various operation like accuracy r^2.

overfitting: if the r^2 test is significantly lower than r^2 training. model is not generalized.

then we tried mlr on excel. showed result. did the analysis. various things we found out.  multiple r -> sqrt(r^2), correlation of y and all the (x1,x2..) taken all together.(non regression statistical result resemble)
adjusted r^2: gives you an idea. how effective the addition of new independent variable is.
1-[(sse/its respective dof)/(sst/its dof)]  we divide by degree of freedom because we want to get the adjusted equivalent r^2 if there was one independent feature. dof is n-1 for sst because considering the mean is known. similarly for sse its n-k-1.

moved to python -> did same data analysis.  residual plots-> how do we know this residual plot or e^2 plot is okay? take histogram check whether is normal distribution or not.

q-q plots if the errors lying on the histogram perfectly aligns with the normal curve or not.
these are hardcore statistical analysis not present in sklearn -> hence we use stats model.

stats model give a lot of other analysis: aic,bic omnibus statistic, omnibus p-value, jarque bera test, durbin watson test.","class 8, explained how multicollinearity can be a problem and why we cant solve mlr by closed form solution. the exact form (xtx)^-1... type form.-> impractical, inverse matrix is very hard to compute for so many features and higher number of data. hence we need to shift to gradient descent for reduction of computation. splitting of data: we split a sample. do not consider a whole sample for training purpose. around 80%-20% split? why? because we want to test the model whether it is really eefective om the unseen data or not. why not 50-50? why 80-20? we want the data on which the model is going to be trained to be representative enough of the data for variance or complexity capturing. two sets of outcomes: -training metric and test metrics. training metrics include sse,rmse, f-statistics, r^2. test metrics also include various operation like accuracy r^2. overfitting: if the r^2 test is significantly lower than r^2 training. model is not generalized. then we tried mlr on excel. showed result. did the analysis. various things we found out. multiple r -> sqrt(r^2), correlation of y and all the (x1,x2..) taken all together.(non regression statistical result resemble) adjusted r^2: gives you an idea. how effective the addition of new independent variable is. 1-[(sse/its respective dof)/(sst/its dof)] we divide by degree of freedom because we want to get the adjusted equivalent r^2 if there was one independent feature. dof is n-1 for sst because considering the mean is known. similarly for sse its n-k-1. moved to python -> did same data analysis. residual plots-> how do we know this residual plot or e^2 plot is okay? take histogram check whether is normal distribution or not. q-q plots if the errors lying on the histogram perfectly aligns with the normal curve or not. these are hardcore statistical analysis not present in sklearn -> hence we use stats model. stats model give a lot of other analysis: aic,bic omnibus statistic, omnibus p-value, jarque bera test, durbin watson test.",2,8.315503,2.5730405,11.025599,4.8905706,"regression, regressions, features"
438,"in our last session, we had continued to follow up on our statistical concepts based on the background we had developed earlier. we explored terms from our data analysis toolpack, discussed their meanings, interconnections, how to represent them graphically, and the potential sources of uncertainty or errors in applying them.
the significant portion of the class was assigned to understanding cases involving beta and beta 0 under specific conditions, with emphasis on how they feature in different statistical models. we also went ahead and introduced discussion on the concept of the p-value, bringing into light its role on critical decisions especially regarding feature selection.
at the end of the class, we started to touch on anova, or analysis of variance, and focused on the f-statistic. we discussed why it was important for this value to be large, also relating to how it relates to evaluating model performance.","in our last session, we had continued to follow up on our statistical concepts based on the background we had developed earlier. we explored terms from our data analysis toolpack, discussed their meanings, interconnections, how to represent them graphically, and the potential sources of uncertainty or errors in applying them. the significant portion of the class was assigned to understanding cases involving beta and beta 0 under specific conditions, with emphasis on how they feature in different statistical models. we also went ahead and introduced discussion on the concept of the p-value, bringing into light its role on critical decisions especially regarding feature selection. at the end of the class, we started to touch on anova, or analysis of variance, and focused on the f-statistic. we discussed why it was important for this value to be large, also relating to how it relates to evaluating model performance.",13,2.8935049,16.231901,12.53668,5.8433485,"classification, classifying, classifications"
439,"as the sessions went forward the number of submissions reduced but the qualtity increased.
we analysed a function using a sin wave and it reduced the p value hence it was deemed usefull. including too many features increased the p value. 
we then studied neural networks where the features pass through multiple layers",as the sessions went forward the number of submissions reduced but the qualtity increased. we analysed a function using a sin wave and it reduced the p value hence it was deemed usefull. including too many features increased the p value. we then studied neural networks where the features pass through multiple layers,0,-4.530559,2.1328802,8.843484,3.8781133,"models, feature, features"
440,"we started of by looking at the website tensorflow playground and then changed various parameters of the neural network and had a look at the prediction, and errors which the neural network model makes in determining a given dataset with different kinds of distribution. we then looked at the metrics involved in the classifier mechanism (like the auc metric: area under the receiver operating characteristic curve which is plotted vs false positive rate) and how classifier becomes more complex than regression mechanisms. we can seperate the data by transformations and feature engineering","we started of by looking at the website tensorflow playground and then changed various parameters of the neural network and had a look at the prediction, and errors which the neural network model makes in determining a given dataset with different kinds of distribution. we then looked at the metrics involved in the classifier mechanism (like the auc metric: area under the receiver operating characteristic curve which is plotted vs false positive rate) and how classifier becomes more complex than regression mechanisms. we can seperate the data by transformations and feature engineering",8,-3.5281515,-15.581649,6.5680084,0.8823232,"classification, clusterings, classifying"
441,"exploratory data analysis (eda) is essential for understanding datasets, identifying patterns, and handling issues related to data quality. problems with the dependent variable (y) may include missing, incorrect, insufficient, or excessive data, while issues with independent variables (x) can arise within or across columns. outlier detection and management are crucial to maintaining data integrity, using statistical techniques and transformations where necessary. model evaluation often involves interpreting a confusion matrix, where tracking class distributions helps in understanding its structure. eda fits within the crisp-dm framework, a cyclic process encompassing business understanding, data exploration, preparation, modeling, evaluation, and deployment. visualization techniques aid in summarizing key characteristics and trends, ensuring meaningful insights. additionally, concepts like heteroscedasticity, where variance changes across data ranges, must be considered to refine models and interpretations.","exploratory data analysis (eda) is essential for understanding datasets, identifying patterns, and handling issues related to data quality. problems with the dependent variable (y) may include missing, incorrect, insufficient, or excessive data, while issues with independent variables (x) can arise within or across columns. outlier detection and management are crucial to maintaining data integrity, using statistical techniques and transformations where necessary. model evaluation often involves interpreting a confusion matrix, where tracking class distributions helps in understanding its structure. eda fits within the crisp-dm framework, a cyclic process encompassing business understanding, data exploration, preparation, modeling, evaluation, and deployment. visualization techniques aid in summarizing key characteristics and trends, ensuring meaningful insights. additionally, concepts like heteroscedasticity, where variance changes across data ranges, must be considered to refine models and interpretations.",13,-18.148462,20.472395,8.169454,8.619024,"classification, classifying, classifications"
442,"we studied different types of roc curves today, which depict the relationship between true positives and false positives at various thresholds. the classifierâ€™s performance deteriorated the more the curve is flat. a flat roc curve indicates that the model is consistently mispredicting at higher rate because there is no effect on the true positive when the threshold is increased but the false positive keeps increasing. in these settings, the low auc value offers additional justification for the classifier failure. any part of the curve that customarily exceeds the diagonal x=y line represents a better model because it had a value of auc of 0.5.","we studied different types of roc curves today, which depict the relationship between true positives and false positives at various thresholds. the classifier s performance deteriorated the more the curve is flat. a flat roc curve indicates that the model is consistently mispredicting at higher rate because there is no effect on the true positive when the threshold is increased but the false positive keeps increasing. in these settings, the low auc value offers additional justification for the classifier failure. any part of the curve that customarily exceeds the diagonal x=y line represents a better model because it had a value of auc of 0.5.",12,2.4610336,-25.11364,7.415174,0.70455116,"classifiers, logistic, roc"
443,"we started the lecture plotting the regression line. the data had x and y values and we determined the values of a and b of the line y=ax+b using the formulae given on excel. we plot the scatter plot showing relationship between and x and y variables and we plot the regression line calculated which gives the estimated function. we calculated the metrics, sse, mse, rmse, mae, used for assessing accuracy of the models. we plotted the histogram for assessing randomness in errors. then we used data analysis tool pak to conduct regression using anova and uploaded the t-statistics and p-values at particular confidence intervals. the standard deviation represents the standard error, hence more the sample size, smaller the error. then we studied the central limit theorem which helps in deriving inferences about population from sample means.","we started the lecture plotting the regression line. the data had x and y values and we determined the values of a and b of the line y=ax+b using the formulae given on excel. we plot the scatter plot showing relationship between and x and y variables and we plot the regression line calculated which gives the estimated function. we calculated the metrics, sse, mse, rmse, mae, used for assessing accuracy of the models. we plotted the histogram for assessing randomness in errors. then we used data analysis tool pak to conduct regression using anova and uploaded the t-statistics and p-values at particular confidence intervals. the standard deviation represents the standard error, hence more the sample size, smaller the error. then we studied the central limit theorem which helps in deriving inferences about population from sample means.",5,20.8281,-4.312889,14.290376,4.9400034,"regression, statistical, statistics"
444,"we explored more terms from the data analysis toolpack, their meanings, interconnections, graphical interpretations, and the uncertainties or errors associated with them. specific topics included cases of beta and beta 0 under different conditions, the p-value and its basic use in feature selection, and terms relevant to multiple linear regression. additionally, we introduced the concept of anova, discussing the f-statistic and its importance, which will be further explored in the next class.","we explored more terms from the data analysis toolpack, their meanings, interconnections, graphical interpretations, and the uncertainties or errors associated with them. specific topics included cases of beta and beta 0 under different conditions, the p-value and its basic use in feature selection, and terms relevant to multiple linear regression. additionally, we introduced the concept of anova, discussing the f-statistic and its importance, which will be further explored in the next class.",13,4.8653255,16.356995,12.50747,5.8526607,"classification, classifying, classifications"
445,"we started with feature engineering where we focused on feature encoding. since machine learning models work with numbers, categorical data needs to be converted.

we discussed one-hot encoding with an example but also highlighted its drawbackâ€”adding too many columns, which can lead to sparsity and the curse of dimensionality. while it works for nominal data with few categories, ordinal data requires a different approach, like assigning numerical ranks.

next, we explored binary encoding, which converts categories into numbers and then into binary form, along with frequency encoding and target encoding as other techniques.

before wrapping up, we briefly touched on llms (large language models) and how textual data is transformed into numerical vectors for machine learning.","we started with feature engineering where we focused on feature encoding. since machine learning models work with numbers, categorical data needs to be converted. we discussed one-hot encoding with an example but also highlighted its drawback adding too many columns, which can lead to sparsity and the curse of dimensionality. while it works for nominal data with few categories, ordinal data requires a different approach, like assigning numerical ranks. next, we explored binary encoding, which converts categories into numbers and then into binary form, along with frequency encoding and target encoding as other techniques. before wrapping up, we briefly touched on llms (large language models) and how textual data is transformed into numerical vectors for machine learning.",3,-41.241386,2.5001152,0.09874203,6.3029785,"categorical, categorization, categorise"
446,"in today's lecture we learned about various techniques to make predictions for certain parameters. these techniques include simple linear regression ,multiple linear regression, logistic regression, random forest, etc. then we discussed the 4 levels of measurement named nominal, ordinal, interval and ratio. nominal and ordinal are discrete while interval and ratio are continuous. nominal has no real meaning of order like team a, team b being represented as 0 and 1, but there's a problem as in representation of these 2 teams the numbers used create an order(0<1) and hence a solution to this was by vectors. some examples of interval scale include temperature, ratio include age which can be compared etc. in ml ,nominal and ordinal are used for classification and interval and ratio are used for regression. then we discussed the differnence between unsupervised and supervised learning which is supervised learning uses labeled input and output data, while an unsupervised learning algorithm does not. at the end we discussed difference between population and sample and talked about how both lack and abundance of data can affect the algorithm.","in today's lecture we learned about various techniques to make predictions for certain parameters. these techniques include simple linear regression ,multiple linear regression, logistic regression, random forest, etc. then we discussed the 4 levels of measurement named nominal, ordinal, interval and ratio. nominal and ordinal are discrete while interval and ratio are continuous. nominal has no real meaning of order like team a, team b being represented as 0 and 1, but there's a problem as in representation of these 2 teams the numbers used create an order(0<1) and hence a solution to this was by vectors. some examples of interval scale include temperature, ratio include age which can be compared etc. in ml ,nominal and ordinal are used for classification and interval and ratio are used for regression. then we discussed the differnence between unsupervised and supervised learning which is supervised learning uses labeled input and output data, while an unsupervised learning algorithm does not. at the end we discussed difference between population and sample and talked about how both lack and abundance of data can affect the algorithm.",4,-22.303034,-14.208127,1.9828322,0.49638233,"classification, classifying, classifications"
447,"we first started by exploring playground.tensorflow.org then we continued our discussion on confusion matrix (false positive, true negative etc). when does classifier start detecting incorrect false positives? all true positive detected before detecting false positive. distribution of observations or classifier is sharp to distinguish between classes (ability is seen in receiver operating characteristic curve ie roc curve - if probability >0.5 then 1, else 0). area under the curve (auc) = 1 is best classifier, worst is 0.5 (y=x line). then we discussed data imbalance (fraudulent transaction). do not judge classifier by accuracy; judge by confusion matrix ie precision, recall, etc. clustering is unsupervised learning. we discussed k means clustering (means is a representation of points) - algorithm assigns data points randomly to clusters, derives centroids, finds distances of points from centroids, re-assigns. we saw dendrogram (hierarchial clustering/ bottom up - tree like) - we will use complete linkage.","we first started by exploring playground.tensorflow.org then we continued our discussion on confusion matrix (false positive, true negative etc). when does classifier start detecting incorrect false positives? all true positive detected before detecting false positive. distribution of observations or classifier is sharp to distinguish between classes (ability is seen in receiver operating characteristic curve ie roc curve - if probability >0.5 then 1, else 0). area under the curve (auc) = 1 is best classifier, worst is 0.5 (y=x line). then we discussed data imbalance (fraudulent transaction). do not judge classifier by accuracy; judge by confusion matrix ie precision, recall, etc. clustering is unsupervised learning. we discussed k means clustering (means is a representation of points) - algorithm assigns data points randomly to clusters, derives centroids, finds distances of points from centroids, re-assigns. we saw dendrogram (hierarchial clustering/ bottom up - tree like) - we will use complete linkage.",8,-0.41639856,-19.99172,7.0197105,0.45326853,"classification, clusterings, classifying"
448,"improvement in results can be done in two ways, either model improvement or sample improvement i.e. either we collect high quality/more data or try many different models and select best model. one technique for model selection is grid search.
in regression methods, a more general method is polynomial regression where we use exponents of feature like x^2,x^3. feature selection has two methods i.e. forward selection: start with empty set of features and add them from knowledge , backward selection : start with complete set and remove features based on metrics like p-value.
in real life, interpretability and maintenance aspect of model is also important.
neural networks have hidden layers between input and output with weight parameters. more layers add more degree of freedom but requires huge amount of data to train like chatgpt.
in logistic regression, model needs to make boundaries to classify different classes, reduce misclassification.","improvement in results can be done in two ways, either model improvement or sample improvement i.e. either we collect high quality/more data or try many different models and select best model. one technique for model selection is grid search. in regression methods, a more general method is polynomial regression where we use exponents of feature like x^2,x^3. feature selection has two methods i.e. forward selection: start with empty set of features and add them from knowledge , backward selection : start with complete set and remove features based on metrics like p-value. in real life, interpretability and maintenance aspect of model is also important. neural networks have hidden layers between input and output with weight parameters. more layers add more degree of freedom but requires huge amount of data to train like chatgpt. in logistic regression, model needs to make boundaries to classify different classes, reduce misclassification.",0,1.1964056,-5.6974826,9.45687,3.9715374,"models, feature, features"
449,"we want to minimize the number of features to counteract the curse of dimensionality. variance inflation factor (vif) assists in identifying multicollinearity between independent variables and needs to be run prior to principal component analysis (pca), as pca does not eliminate redundancy but projects the feature space. r-square quantifies the amount of variance explained by the model but does not reveal multicollinearity. there is a compromise between the number of features and r-squareâ€”increasing the number of features can increase r-square but at the cost of instability. pca, which is an application of singular value decomposition (svd), is a dimensionality reduction technique that creates principal components (pcs) that are orthogonal and ordered by variance explained. the first pc explains the most variance, followed by the rest of the pcs. this conversion enables dimensionality reduction of a multi-dimensional data set to fewer dimensions (e.g., reducing a 2d problem to 1d). pca, however, diminishes interpretability, as ""what-if"" analysis is not possible because pcs are mathematical abstractions and not actual features.it is mostly used for dimensionality reduction, predictive modeling, and visualization in exploratory data analysis (eda) to decide between ordinary regression and pca regression. while pca is good at reducing high-dimensional data, it is at the expense of interpretability as it complicates the linking of predictions to original variables. another dimensionality reduction method, t-distributed stochastic neighbor embedding (t-sne), differs from pca in that it uses a probabilistic approach rather than a linear transformation. t-sne constructs a probability distribution for each point, estimating the proximity to others with gaussian normal distribution at high dimensions and t-distribution at low dimensions in an attempt to preserve similarities of nearby points. t-sne is particularly useful in projecting raw high-dimensional data into labeled clusters for better visual and comprehendible purposes.","we want to minimize the number of features to counteract the curse of dimensionality. variance inflation factor (vif) assists in identifying multicollinearity between independent variables and needs to be run prior to principal component analysis (pca), as pca does not eliminate redundancy but projects the feature space. r-square quantifies the amount of variance explained by the model but does not reveal multicollinearity. there is a compromise between the number of features and r-square increasing the number of features can increase r-square but at the cost of instability. pca, which is an application of singular value decomposition (svd), is a dimensionality reduction technique that creates principal components (pcs) that are orthogonal and ordered by variance explained. the first pc explains the most variance, followed by the rest of the pcs. this conversion enables dimensionality reduction of a multi-dimensional data set to fewer dimensions (e.g., reducing a 2d problem to 1d). pca, however, diminishes interpretability, as ""what-if"" analysis is not possible because pcs are mathematical abstractions and not actual features.it is mostly used for dimensionality reduction, predictive modeling, and visualization in exploratory data analysis (eda) to decide between ordinary regression and pca regression. while pca is good at reducing high-dimensional data, it is at the expense of interpretability as it complicates the linking of predictions to original variables. another dimensionality reduction method, t-distributed stochastic neighbor embedding (t-sne), differs from pca in that it uses a probabilistic approach rather than a linear transformation. t-sne constructs a probability distribution for each point, estimating the proximity to others with gaussian normal distribution at high dimensions and t-distribution at low dimensions in an attempt to preserve similarities of nearby points. t-sne is particularly useful in projecting raw high-dimensional data into labeled clusters for better visual and comprehendible purposes.",11,-17.172922,3.1781192,10.317242,13.164253,"pca, heatmap, heatmaps"
450,"the session started with a review of gradient descent before introducing logistic regression. the professor explained the confusion matrix and showed a website for experimenting with neural network clustering.
we then explored logistic regression code and learned about key performance metrics like precision, recall, and the f1 score. the roc curve was introduced, with the false positive rate on the x-axis and the true positive rate on the y-axis, showing different curves for each class.
later, the topic shifted to clustering. it began with hierarchical clustering, followed by k-means clustering, where the number of clusters is set in advance. this helped students understand different ways to group data.","the session started with a review of gradient descent before introducing logistic regression. the professor explained the confusion matrix and showed a website for experimenting with neural network clustering. we then explored logistic regression code and learned about key performance metrics like precision, recall, and the f1 score. the roc curve was introduced, with the false positive rate on the x-axis and the true positive rate on the y-axis, showing different curves for each class. later, the topic shifted to clustering. it began with hierarchical clustering, followed by k-means clustering, where the number of clusters is set in advance. this helped students understand different ways to group data.",8,-5.7538395,-17.426306,6.281188,0.7182578,"classification, clusterings, classifying"
451,"we continued our discussions on eda. eda is the first step before performing any complex algorithms on the available data. it involves discovering problems associated with the data as well as few possibilities and insights from the data. in todayâ€™s class we looked at some specific problems associated with the raw data and the methods/ algorithms used to tackle these.
the first problem which we discussed was that of presence of lots of noise in the data, which causes hindrance in detecting the true signal. we discussed few methods to solve this problem. we specifically looked at /moving averages for data smootheningâ€™, in which we first learnt about the simple moving average. in this method, we take nearly about 50 data points surrounding a particular data point and evaluate their average inorder to get an average value associated with that particular point. we do the same with all the other points and finally get a smoother curve with the values as averages of some 50-60 points in the neighborhood. the smoothness of the curve depends on the number of data points we are choosing to evaluate the average. as we increase this number, the curve becomes smoother and smoother and eventually becomes a flat horizontal line, when all the points in the set are included in the window. this creates an â€˜artificial signalâ€™ and the original variations in the data vanish completely.  there can also be different ways in which we consider these points. for example, we may consider only those points which lie behind the selected point. this causes a problem at the end point, particularly the one on the left. similarly, we can also consider that window which includes some points behind the selected point and some ahead of it. this also causes problem at the end points. we can choose any of this, however our choice should be justifiable, according to the particular data set. 
next, we can also use exponential/ weighted averages, in which we assign higher weights to the points in the proximity. we must select an optimal size of window or the moving average method, such that we are able to extract the signal from the highly fluctuating values. 
before creating moving averages, we should address the problems of missing values and outliers, else they would cause trouble while creating the ma. 
the next problem is that some values in a column have significantly higher values than some others. this makes the gradient decent algorithm, more biased towards the larger values, thereby causing data imbalance and giving false results. hence, it is important to normalize the values in the columns so that every value lies between 0 and 1. standardization is another process wherein we convert the existing data into standard normal distributions, with mean 0 and standard deviation 1. both of these do not change the shape of the data. if we transform or scale the data, then any algorithm based on calculating euclidean distances would be largely affected. it is important to transform the data first because some algorithms assume that the data is already normally distributed. 
there is another kind of transformation- box cox transformation in which we evaluate the transformed value of each x by using a parameter lambda, which is optimized such that we get the closest approximation to the normally distributed data set. 
apart from transforming the features, we also have to reverse transform the transform the response variables, to get the values in the original form back. 
the third problem which we discussed was regarding the data imbalance, which occurs whenever we have a class whose number of samples are very small compared to the other classes. hence, the class is eventually suppressed by the others. we need to fix this problem, as we may get incorrect results/ predictions on using the imbalanced data. also, the algorithm may not learn well using the imbalanced data. 
to fix this data imbalance, we can do the following:
1)	under sample the majority class.
2)	over sample the minority class. the most naive method is to duplicate the values of the existing points.
3)	ideally, we should sample more points in the surrounding of these points by linearly interpolating between any two points. this is known as smote.
4)	next, instead of just randomly dropping any sample value from the majority class, we can drop those values which have the nearest sample belonging from the minor class. this has two advantages- first is that it creates a distinct boundary between the classes and also in this way we can get rid of some possibly misclassified points. this is known as tomek links.
5)	so, first we can apply smote then use tomek links to improve the quality of the classification model.","we continued our discussions on eda. eda is the first step before performing any complex algorithms on the available data. it involves discovering problems associated with the data as well as few possibilities and insights from the data. in today s class we looked at some specific problems associated with the raw data and the methods/ algorithms used to tackle these. the first problem which we discussed was that of presence of lots of noise in the data, which causes hindrance in detecting the true signal. we discussed few methods to solve this problem. we specifically looked at /moving averages for data smoothening , in which we first learnt about the simple moving average. in this method, we take nearly about 50 data points surrounding a particular data point and evaluate their average inorder to get an average value associated with that particular point. we do the same with all the other points and finally get a smoother curve with the values as averages of some 50-60 points in the neighborhood. the smoothness of the curve depends on the number of data points we are choosing to evaluate the average. as we increase this number, the curve becomes smoother and smoother and eventually becomes a flat horizontal line, when all the points in the set are included in the window. this creates an artificial signal and the original variations in the data vanish completely. there can also be different ways in which we consider these points. for example, we may consider only those points which lie behind the selected point. this causes a problem at the end point, particularly the one on the left. similarly, we can also consider that window which includes some points behind the selected point and some ahead of it. this also causes problem at the end points. we can choose any of this, however our choice should be justifiable, according to the particular data set. next, we can also use exponential/ weighted averages, in which we assign higher weights to the points in the proximity. we must select an optimal size of window or the moving average method, such that we are able to extract the signal from the highly fluctuating values. before creating moving averages, we should address the problems of missing values and outliers, else they would cause trouble while creating the ma. the next problem is that some values in a column have significantly higher values than some others. this makes the gradient decent algorithm, more biased towards the larger values, thereby causing data imbalance and giving false results. hence, it is important to normalize the values in the columns so that every value lies between 0 and 1. standardization is another process wherein we convert the existing data into standard normal distributions, with mean 0 and standard deviation 1. both of these do not change the shape of the data. if we transform or scale the data, then any algorithm based on calculating euclidean distances would be largely affected. it is important to transform the data first because some algorithms assume that the data is already normally distributed. there is another kind of transformation- box cox transformation in which we evaluate the transformed value of each x by using a parameter lambda, which is optimized such that we get the closest approximation to the normally distributed data set. apart from transforming the features, we also have to reverse transform the transform the response variables, to get the values in the original form back. the third problem which we discussed was regarding the data imbalance, which occurs whenever we have a class whose number of samples are very small compared to the other classes. hence, the class is eventually suppressed by the others. we need to fix this problem, as we may get incorrect results/ predictions on using the imbalanced data. also, the algorithm may not learn well using the imbalanced data. to fix this data imbalance, we can do the following: 1) under sample the majority class. 2) over sample the minority class. the most naive method is to duplicate the values of the existing points. 3) ideally, we should sample more points in the surrounding of these points by linearly interpolating between any two points. this is known as smote. 4) next, instead of just randomly dropping any sample value from the majority class, we can drop those values which have the nearest sample belonging from the minor class. this has two advantages- first is that it creates a distinct boundary between the classes and also in this way we can get rid of some possibly misclassified points. this is known as tomek links. 5) so, first we can apply smote then use tomek links to improve the quality of the classification model.",9,-18.036608,10.920635,10.097404,9.974742,"dataâ, analyse, analyses"
452,"class 3 summary: 
first of all we talked about how to fit a curve. with some example he made us clear what were the intentions behind y=f(x) in statistics and ml. he told there are various methods to approximate f() and listed some of the methods like simple linear regression, mlr, logistic, random forest.
then we moved on to levels of measurement: there are four types: nominal, ordinal, interval, ratio. 

1. nominal includes discrete data which we can not arrange in a specific order( no one is bigger or smaller) like colours. 
2. whereas ordinals are discrete and can be comapred. 
3. intervals are the units where only difference holds not ratio. suppose, there are 5c and 10 deg c. we can not say 10 is twice as 5 deg c because we compare wrt kelvin where this statement will not hold true.
4. ratio. as the name suggest we can also take ratio. for eg height.

y=f(x1,x2,x3,x4..)
where y is the label and x1,x2,x3... are generally features in a problem of classification.

supervised learning :
        labels and features are known we predict and approximate the function f.
unsupervised learning:
        only features are present. we can just group the data on the basis of some parameters.

data availability: 
               the whole data set which includes each and everything is called population.
                  sample data is a subset of population.
we always take a sample because we can never have the population. no matter what the data can be be never called as population practically because it is always possible to leave some data behind.","class 3 summary: first of all we talked about how to fit a curve. with some example he made us clear what were the intentions behind y=f(x) in statistics and ml. he told there are various methods to approximate f() and listed some of the methods like simple linear regression, mlr, logistic, random forest. then we moved on to levels of measurement: there are four types: nominal, ordinal, interval, ratio. 1. nominal includes discrete data which we can not arrange in a specific order( no one is bigger or smaller) like colours. 2. whereas ordinals are discrete and can be comapred. 3. intervals are the units where only difference holds not ratio. suppose, there are 5c and 10 deg c. we can not say 10 is twice as 5 deg c because we compare wrt kelvin where this statement will not hold true. 4. ratio. as the name suggest we can also take ratio. for eg height. y=f(x1,x2,x3,x4..) where y is the label and x1,x2,x3... are generally features in a problem of classification. supervised learning : labels and features are known we predict and approximate the function f. unsupervised learning: only features are present. we can just group the data on the basis of some parameters. data availability: the whole data set which includes each and everything is called population. sample data is a subset of population. we always take a sample because we can never have the population. no matter what the data can be be never called as population practically because it is always possible to leave some data behind.",4,-27.51935,-13.062742,1.4042879,0.39999965,"classification, classifying, classifications"
453,"in today's lecture, we about analyzing the distribution of the data. on performing linear regression the error plot of the data appears very random, so we need to choose an appropriate error metric to properly make sense of our observations. thus we need to choose an suitable model so that we can explain the behavior being depicted by a data set. metrics can be sse, mse, rmse, mae, etc. we validated this on the dataset in excel. we used excel's lr tool to approximate a and b value of regression. then we learnt about central limit theorem, ""the distribution of sample means will be approximately normal if the sample size is large enough"" using the population example. this means that as n approaches infinity, the distribution of sample means (and standard deviations) will closely resemble the mean(and standard deviations)  of the population","in today's lecture, we about analyzing the distribution of the data. on performing linear regression the error plot of the data appears very random, so we need to choose an appropriate error metric to properly make sense of our observations. thus we need to choose an suitable model so that we can explain the behavior being depicted by a data set. metrics can be sse, mse, rmse, mae, etc. we validated this on the dataset in excel. we used excel's lr tool to approximate a and b value of regression. then we learnt about central limit theorem, ""the distribution of sample means will be approximately normal if the sample size is large enough"" using the population example. this means that as n approaches infinity, the distribution of sample means (and standard deviations) will closely resemble the mean(and standard deviations) of the population",5,27.372898,-0.44479635,14.530791,4.124911,"regression, statistical, statistics"
454,"we explored the 4 different types of measurements nominal, ordinal, interval and ratio scales, and discussed how each applies to data analysis. the class also focused on the difference between supervised and unsupervised learning, emphasizing that supervised learning uses labeled data for training, while unsupervised learning is designed to identify patterns in unlabeled data.","we explored the 4 different types of measurements nominal, ordinal, interval and ratio scales, and discussed how each applies to data analysis. the class also focused on the difference between supervised and unsupervised learning, emphasizing that supervised learning uses labeled data for training, while unsupervised learning is designed to identify patterns in unlabeled data.",4,-18.733707,-12.720817,2.3154764,0.925164,"classification, classifying, classifications"
455,"first sir showed us the website play around which allows us to visualise neural networks changing the number of layers, epochs, etc. further sir showed us a piece of code in a jupyter notebook and demonstrated models fitting, confusion matrix, true positive rate and false positive rate. then we studied receiver operating characteristic curve were an area under the curve closer to 1 indicates a good classification model and vise versa. then we saw a model which classified 4 classes where one of the classes was under represented. here class imbalance affected the model and it was not interpreted in the accuracy but was found out by confusion matrix and precision, recall parameters. then it was discussed how a regression problem could be converted into a classification problem but not the vise versa due to information loss. then sir taught clustering which is an unsupervised learning method. here the model has to identify the clusters and assign labels. to identify the clusters dendrogram is plotted to find the clusters. further k means clustering was discussed where desired number of clusters are to be specified beforehand via domain knowledge and every point in data is assigned to one of these clusters. the model here initially assigns random labels and later converges to clusters on the basis of distance from the means. later cluster means are recalculated and reassignment of labels is done until the model converges. other algorithm for clustering is hierarchical clustering where every point is a unique cluster clusters are formed by defining a 'linkage' upon which further clustering is done and clusters are merged and a dendrogram is formed to show the steps and appropriate number of clusters could be formed from the dendrogram.","first sir showed us the website play around which allows us to visualise neural networks changing the number of layers, epochs, etc. further sir showed us a piece of code in a jupyter notebook and demonstrated models fitting, confusion matrix, true positive rate and false positive rate. then we studied receiver operating characteristic curve were an area under the curve closer to 1 indicates a good classification model and vise versa. then we saw a model which classified 4 classes where one of the classes was under represented. here class imbalance affected the model and it was not interpreted in the accuracy but was found out by confusion matrix and precision, recall parameters. then it was discussed how a regression problem could be converted into a classification problem but not the vise versa due to information loss. then sir taught clustering which is an unsupervised learning method. here the model has to identify the clusters and assign labels. to identify the clusters dendrogram is plotted to find the clusters. further k means clustering was discussed where desired number of clusters are to be specified beforehand via domain knowledge and every point in data is assigned to one of these clusters. the model here initially assigns random labels and later converges to clusters on the basis of distance from the means. later cluster means are recalculated and reassignment of labels is done until the model converges. other algorithm for clustering is hierarchical clustering where every point is a unique cluster clusters are formed by defining a 'linkage' upon which further clustering is done and clusters are merged and a dendrogram is formed to show the steps and appropriate number of clusters could be formed from the dendrogram.",8,-5.192567,-20.884903,6.3380103,0.3584021,"classification, clusterings, classifying"
456,"population v/s sample: the quality of a sample is measured by how well it represents the population. once we have a sample, we need to predict the population from the sample.

there are various attributes like mean, median, mode and various operations like addition, subtraction, multiplication etc.  

depending on the level of measurement ,we can or cannot calculate certain attributes or perform certain operations.

any variable measured for the sample is deemed as a statistic whereas any variable measured for the population is deemed as a parameter.

simple linear regression: fitting a best fit line through the set of data to try to model it accurately.

the type of model to fit is highly dependent on how the data is spread. for a circular distribution, a point may also seem like a valid model but it is a little naive and inaccurate to do so.

a simple linear regression has one dependent and one independent variable where y=b0+b1*(x).the coefficients b0 and b1 are calculated so that the resulting line is best fitted to the given data. we try to find a confidence interval around b0 and b1 so that we can confidently say that a large fraction(most of the data) lies within that interval.the error is the difference between the predicted y value and the ctual y value and the sum of squares of errors is minimized in what is known as the least squares fitting method.","population v/s sample: the quality of a sample is measured by how well it represents the population. once we have a sample, we need to predict the population from the sample. there are various attributes like mean, median, mode and various operations like addition, subtraction, multiplication etc. depending on the level of measurement ,we can or cannot calculate certain attributes or perform certain operations. any variable measured for the sample is deemed as a statistic whereas any variable measured for the population is deemed as a parameter. simple linear regression: fitting a best fit line through the set of data to try to model it accurately. the type of model to fit is highly dependent on how the data is spread. for a circular distribution, a point may also seem like a valid model but it is a little naive and inaccurate to do so. a simple linear regression has one dependent and one independent variable where y=b0+b1*(x).the coefficients b0 and b1 are calculated so that the resulting line is best fitted to the given data. we try to find a confidence interval around b0 and b1 so that we can confidently say that a large fraction(most of the data) lies within that interval.the error is the difference between the predicted y value and the ctual y value and the sum of squares of errors is minimized in what is known as the least squares fitting method.",1,34.00268,-5.4272513,16.13947,3.6639273,"population, models, estimating"
457,"in today's session, we explored the process of performing linear regression using excel. we began by analyzing a scatter plot of the data, which exhibited a linear trend, prompting us to apply linear regression. using the closed form expressions derived earlier, we calculated the regression coefficients and plotted the regression line over the scatter plot of the original data. this provided a visual representation of the relationship between the variables.
we learned that predictions made using a regression model are valid only within the range of data on which the model was trained. extrapolating beyond this range can lead to inaccurate predictions. to assess the model's performance, we examined the residuals by creating a scatter plot and a histogram. the scatter plot of residuals appeared random, but upon further inspection, the histogram indicated a somewhat uniform distribution rather than the expected normal distribution. an example was provided where a regression line was force-fitted onto non-linear data, resulting in a pattern in the residuals. this demonstrated the model's failure to capture the underlying trend, emphasizing the importance of a good model that effectively explains the variation in the data. additionally, we derived the relationship sst=ssr+sse, where sst represents the total variance in the data, ssr is the component explained by the regression model, and sse represents the unexplained variance. the coefficient of determination r^2 was explained as the ratio ssr/sst, indicating the proportion of variation in the data that the model accounts for. r^2=r^2 ,here r is coefficient of correlation which indicated how y changes with respect to x.","in today's session, we explored the process of performing linear regression using excel. we began by analyzing a scatter plot of the data, which exhibited a linear trend, prompting us to apply linear regression. using the closed form expressions derived earlier, we calculated the regression coefficients and plotted the regression line over the scatter plot of the original data. this provided a visual representation of the relationship between the variables. we learned that predictions made using a regression model are valid only within the range of data on which the model was trained. extrapolating beyond this range can lead to inaccurate predictions. to assess the model's performance, we examined the residuals by creating a scatter plot and a histogram. the scatter plot of residuals appeared random, but upon further inspection, the histogram indicated a somewhat uniform distribution rather than the expected normal distribution. an example was provided where a regression line was force-fitted onto non-linear data, resulting in a pattern in the residuals. this demonstrated the model's failure to capture the underlying trend, emphasizing the importance of a good model that effectively explains the variation in the data. additionally, we derived the relationship sst=ssr+sse, where sst represents the total variance in the data, ssr is the component explained by the regression model, and sse represents the unexplained variance. the coefficient of determination r^2 was explained as the ratio ssr/sst, indicating the proportion of variation in the data that the model accounts for. r^2=r^2 ,here r is coefficient of correlation which indicated how y changes with respect to x.",5,20.51969,-0.27926537,13.854967,4.8307633,"regression, statistical, statistics"
458,"in today's class, we first see that the regression coefficient beta1 should not be zero for a particular sample out of the solution, so the model is not good. the confidence interval of beta1 should not contain zero. suppose we have taken two samples, and we get the value of beta1 to be a and b if they both lie inside the confidence interval, then they are statistically the same, and if anyone is outside it, then they are statistically different. if we have o in the interval and the p-value of the coefficient is higher than 0.05, then we don't consider that coefficient, but if the p-value is less, we can consider the coefficient. then, we move on to analyzing the summary we wrote about the class, noticing some of the summaries have similarities. still, they will be different, especially based on the vocabulary. then, we move on to multiple linear regression, where the dependent variable y is based on more than one independent variable. we can get the model from the data by the gradient descent method. then we perform the data analysis on data on which mlr can be done and we get the value of r^2 of nearly 0.83, which is very good stating the model was a good fit, but the y variable was not exactly dependent on all x. there was 0 in the confidence interval so we removed the coefficient with a p-value greater than 0.05 and the highest among the coefficients, doing these we reduced the regressions coefficient to 3 including the intercept and now the p-value for all them was less than 0.05. these tell that whether the data contains various x which changes y but all do not significantly changes the value of y, and those x who do not impact y significantly can not be taken into consideration.","in today's class, we first see that the regression coefficient beta1 should not be zero for a particular sample out of the solution, so the model is not good. the confidence interval of beta1 should not contain zero. suppose we have taken two samples, and we get the value of beta1 to be a and b if they both lie inside the confidence interval, then they are statistically the same, and if anyone is outside it, then they are statistically different. if we have o in the interval and the p-value of the coefficient is higher than 0.05, then we don't consider that coefficient, but if the p-value is less, we can consider the coefficient. then, we move on to analyzing the summary we wrote about the class, noticing some of the summaries have similarities. still, they will be different, especially based on the vocabulary. then, we move on to multiple linear regression, where the dependent variable y is based on more than one independent variable. we can get the model from the data by the gradient descent method. then we perform the data analysis on data on which mlr can be done and we get the value of r^2 of nearly 0.83, which is very good stating the model was a good fit, but the y variable was not exactly dependent on all x. there was 0 in the confidence interval so we removed the coefficient with a p-value greater than 0.05 and the highest among the coefficients, doing these we reduced the regressions coefficient to 3 including the intercept and now the p-value for all them was less than 0.05. these tell that whether the data contains various x which changes y but all do not significantly changes the value of y, and those x who do not impact y significantly can not be taken into consideration.",2,16.660568,6.460369,12.7089815,3.9245057,"regression, regressions, features"
459,"there were 3 main topic of discussion in todays lecture:

1. types of ml algorithms like simple linear regression, multiple linear regression, logistic regression, random forest, k-means clustering, etc.

2. level of measurement:
    1. nominal scale: datapoints are discrete, mutually exclusive and no arithmetic operation can be performed on this. eg: gender, color

    2. ordinal scale: datapoints are discrete, numerical but same unit of distance doesnâ€™t have same significance. for eg: grading, you have values from letâ€™s say 10 (for aa) to 4 (for dd). now why i was saying that the numerical values doesnâ€™t have significance because the difference between (cd and dd) is 1 grade points and the difference between the (aa and ab) is also 1 grade points. so numerically this means the jump from ddâ†’cd and abâ†’aa is same which is definitely not true.
        
        here we shortly discussed about how to encode the datapoints gathered using nominal scale. and we delved into the problem that directly using numbers to represent the datapoints is incorrect so we used the concept of vectors and how we use vectors to represent these data. and this vector way of representing things is called onehotencoding in the ml landscape.
        
    3. interval scale: datapoints are discrete, numerical and same unit of distance have same significance. for eg: temperature scale, you have values from letâ€™s say 100(degrees c) to 0(degrees c). if you go from 40â†’50 or 80â†’90 the change in temperature is 10 and this will feel same in both the cases.

    4. ratio scale: datapoints are discrete, numerical, same unit of distance have same significance and has a zero. for eg: in temperature scale if you have 0 it doesnâ€™t means you have absence of temperature which means 0 doesnâ€™t have significance in the scale. now take the example of height, weight, etc. where letâ€™s say you have 0 as the datapoint so it means that the object has no height according to the measuring scale (by the way this depends on the least count measuring scale).

3. types of problems in machine learning:
    1. supervised learning: we have a set of input features and an output feature. and our job is to determine the relation between input and output. this has 2 types of sub-problems: regression and classification. here regression refers to those problem which have continuous set of values in the output feature and in classification you have some finite number of different labels and our job is to determine the label of a datapoint based on input features.

    2. unsupervised learning: when we only have input features. here we discussed about clustering algorithms which is used to cluster the datapoints based on some sort of similarity finding algorithm (using euclidean distance, cosine similarity, etc).

4. we also shortly delved into the difference between population and sample. got to know about how all the machine learning problems are trying to learn from samples and based on the finding of relation of input and output (y = f(x)) trying to predict the behavior of population.","there were 3 main topic of discussion in todays lecture: 1. types of ml algorithms like simple linear regression, multiple linear regression, logistic regression, random forest, k-means clustering, etc. 2. level of measurement: 1. nominal scale: datapoints are discrete, mutually exclusive and no arithmetic operation can be performed on this. eg: gender, color 2. ordinal scale: datapoints are discrete, numerical but same unit of distance doesn t have same significance. for eg: grading, you have values from let s say 10 (for aa) to 4 (for dd). now why i was saying that the numerical values doesn t have significance because the difference between (cd and dd) is 1 grade points and the difference between the (aa and ab) is also 1 grade points. so numerically this means the jump from dd cd and ab aa is same which is definitely not true. here we shortly discussed about how to encode the datapoints gathered using nominal scale. and we delved into the problem that directly using numbers to represent the datapoints is incorrect so we used the concept of vectors and how we use vectors to represent these data. and this vector way of representing things is called onehotencoding in the ml landscape. 3. interval scale: datapoints are discrete, numerical and same unit of distance have same significance. for eg: temperature scale, you have values from let s say 100(degrees c) to 0(degrees c). if you go from 40 50 or 80 90 the change in temperature is 10 and this will feel same in both the cases. 4. ratio scale: datapoints are discrete, numerical, same unit of distance have same significance and has a zero. for eg: in temperature scale if you have 0 it doesn t means you have absence of temperature which means 0 doesn t have significance in the scale. now take the example of height, weight, etc. where let s say you have 0 as the datapoint so it means that the object has no height according to the measuring scale (by the way this depends on the least count measuring scale). 3. types of problems in machine learning: 1. supervised learning: we have a set of input features and an output feature. and our job is to determine the relation between input and output. this has 2 types of sub-problems: regression and classification. here regression refers to those problem which have continuous set of values in the output feature and in classification you have some finite number of different labels and our job is to determine the label of a datapoint based on input features. 2. unsupervised learning: when we only have input features. here we discussed about clustering algorithms which is used to cluster the datapoints based on some sort of similarity finding algorithm (using euclidean distance, cosine similarity, etc). 4. we also shortly delved into the difference between population and sample. got to know about how all the machine learning problems are trying to learn from samples and based on the finding of relation of input and output (y = f(x)) trying to predict the behavior of population.",4,-24.847136,-12.968753,1.679617,0.15631633,"classification, classifying, classifications"
460,"we started with discussion of empirical relationship with example of flow rates v/s temperatures and gradually listing ml algorithms used widely. the whole problem breakdowns to 2 components: machine learning and statistics where stats forms the basis for ml. henceforth, we discussed the 4 levels of measurements:
nominal: discrete and unordered data; eg. gender/ color
ordinal: discrete but an ordered data; eg. grades/ age
internal: continuous but undefined ratio; eg. temperature
ratio: continuous along with well defined ratio; eg. height/ weight
later, we discussed that nominal and ordinal levels come into account for classification problems whereas interval and ration boils down for regression problems.
next, we discussed about supervised learning (labels and features both present) and unsupervised learning (only features present)","we started with discussion of empirical relationship with example of flow rates v/s temperatures and gradually listing ml algorithms used widely. the whole problem breakdowns to 2 components: machine learning and statistics where stats forms the basis for ml. henceforth, we discussed the 4 levels of measurements: nominal: discrete and unordered data; eg. gender/ color ordinal: discrete but an ordered data; eg. grades/ age internal: continuous but undefined ratio; eg. temperature ratio: continuous along with well defined ratio; eg. height/ weight later, we discussed that nominal and ordinal levels come into account for classification problems whereas interval and ration boils down for regression problems. next, we discussed about supervised learning (labels and features both present) and unsupervised learning (only features present)",4,-22.93529,-15.152684,1.6969848,0.46282244,"classification, classifying, classifications"
461,today we looked at different types of roc curves.roc curve plots true positive vs false positive at various threshold values.we deduced that flatter the curve is worse is the classifier because a flat curve tells that increasing the threshold does not improve the true positive much but the false positive continues to increase which suggests that the classifier is making a lot of incorrect predictions.in this case the auc value is low which also shows poor performance of classifiers. normally y=x line has aoc value of 0.5 so the portion of the graph that is above the line y=x gives better results.,today we looked at different types of roc curves.roc curve plots true positive vs false positive at various threshold values.we deduced that flatter the curve is worse is the classifier because a flat curve tells that increasing the threshold does not improve the true positive much but the false positive continues to increase which suggests that the classifier is making a lot of incorrect predictions.in this case the auc value is low which also shows poor performance of classifiers. normally y=x line has aoc value of 0.5 so the portion of the graph that is above the line y=x gives better results.,12,2.7071843,-24.95607,7.4401116,0.7242246,"classifiers, logistic, roc"
462,"starting the session with the importance of the course of finding y=f(x) using various techniques like regression, random forest , clustering etc. moved on to levels of measurement as nominal, ordinal , interval and ratio and their various examples, characteristics and handling in machine learning. analyzed 2 techniques supervised and unsupervised learning. made a table of varying x and y as the 4 level of measurement and knowing whether classification or regression is suitable for their analysis.","starting the session with the importance of the course of finding y=f(x) using various techniques like regression, random forest , clustering etc. moved on to levels of measurement as nominal, ordinal , interval and ratio and their various examples, characteristics and handling in machine learning. analyzed 2 techniques supervised and unsupervised learning. made a table of varying x and y as the 4 level of measurement and knowing whether classification or regression is suitable for their analysis.",4,-19.166513,-14.546891,2.2717686,0.7221175,"classification, classifying, classifications"
463,"the class started with a hands on demo. sir uploaded a file on moodle. we then downloaded the file and applied simple linear regression on it. sir then explained about some features in excel and how to use them. sir then focused on errors in linear regression. then sir explained about histogram. outcome dependent on a large number of unknown causes(random), the distribution is gaussian normal distribution.
we need to understand what each number we get from analysis tells us. lower and upper 95% means that the real value of population lies with 95% confidence in the interval [lower 95,upper 95].
what is a good model- one that explains most of the variations in the data. 
sst=sum of (yi-y_bar)â² (measure of total variation in given dataset)
sst=sse+ssr
ssr- sum of square of regression line (total variation explained by the regression line)
sse- variation not explained by the model, attributed to random errors. 
sst=ssr+sse
1=(ssr/sst)+sse/sst
ssr/sst portion of total variation described by the regression model. for a nice model we need this value to be as high as possible. this value is called r squared value. lies in between 0 and 1.  this is known as coefficient of determination. for the case of simple linear regression, the coefficient of determination is same as square of the correlation coefficient r between x and y. so this is called 'r square'. correlation coefficient of x and y=cov(xy)/root(var(x)*var(y))
r square should be ideally be close to 1. 
then sir explained theory behind interval estimates and point estimates.","the class started with a hands on demo. sir uploaded a file on moodle. we then downloaded the file and applied simple linear regression on it. sir then explained about some features in excel and how to use them. sir then focused on errors in linear regression. then sir explained about histogram. outcome dependent on a large number of unknown causes(random), the distribution is gaussian normal distribution. we need to understand what each number we get from analysis tells us. lower and upper 95% means that the real value of population lies with 95% confidence in the interval [lower 95,upper 95]. what is a good model- one that explains most of the variations in the data. sst=sum of (yi-y_bar) (measure of total variation in given dataset) sst=sse+ssr ssr- sum of square of regression line (total variation explained by the regression line) sse- variation not explained by the model, attributed to random errors. sst=ssr+sse 1=(ssr/sst)+sse/sst ssr/sst portion of total variation described by the regression model. for a nice model we need this value to be as high as possible. this value is called r squared value. lies in between 0 and 1. this is known as coefficient of determination. for the case of simple linear regression, the coefficient of determination is same as square of the correlation coefficient r between x and y. so this is called 'r square'. correlation coefficient of x and y=cov(xy)/root(var(x)*var(y)) r square should be ideally be close to 1. then sir explained theory behind interval estimates and point estimates.",5,22.408117,-0.83882046,13.794065,4.4397388,"regression, statistical, statistics"
464,"we recapped some earlier topics like expectation algebra and clustering, then jumped into logistic regression, seeing how we can use the softmax function to classify things and how we find the best settings for our model. we talked about how to make our model better by tweaking it bit by bit (gradient descent) and how we measure how well it's doing (accuracy, precision, recall, and the f1-score). we also went over some common mistakes on the last assignment and how to make our reports clearer.","we recapped some earlier topics like expectation algebra and clustering, then jumped into logistic regression, seeing how we can use the softmax function to classify things and how we find the best settings for our model. we talked about how to make our model better by tweaking it bit by bit (gradient descent) and how we measure how well it's doing (accuracy, precision, recall, and the f1-score). we also went over some common mistakes on the last assignment and how to make our reports clearer.",13,2.7360296,-13.0270605,9.302111,4.888875,"classification, classifying, classifications"
465,"the session started with an introduction to the confusion matrix, a tool used to assess classification models by a two-way representation of actual and predicted values. this was then followed by an overview of exploratory data analysis, organized into six steps: business understanding, data understanding, data preparation, modeling, evaluation, and deployment. a ta illustrated a diabetes prediction model, utilizing different plots to examine data distributions. glucose and bmi were normally distributed, whereas insulin and pregnancies were exponentially distributed.
the session proceeded to touch on handling missing data, such as dropping values, imputation with the mean or median, regression, or estimation of values nearest to it ,this is referred to as multivariate data imputation.
a sample of nvidia stock price variations showed how drastic falls, while not being real outliers, at times may be falsely identified. t-sne plots were proposed for visualizing high-dimensional data in two dimensions and facilitating the observation of patterns and clusters that are otherwise not easy to discern.
the utility of employing the median over the mean in missing data imputation was emphasized, given that the median is less impacted by outliers","the session started with an introduction to the confusion matrix, a tool used to assess classification models by a two-way representation of actual and predicted values. this was then followed by an overview of exploratory data analysis, organized into six steps: business understanding, data understanding, data preparation, modeling, evaluation, and deployment. a ta illustrated a diabetes prediction model, utilizing different plots to examine data distributions. glucose and bmi were normally distributed, whereas insulin and pregnancies were exponentially distributed. the session proceeded to touch on handling missing data, such as dropping values, imputation with the mean or median, regression, or estimation of values nearest to it ,this is referred to as multivariate data imputation. a sample of nvidia stock price variations showed how drastic falls, while not being real outliers, at times may be falsely identified. t-sne plots were proposed for visualizing high-dimensional data in two dimensions and facilitating the observation of patterns and clusters that are otherwise not easy to discern. the utility of employing the median over the mean in missing data imputation was emphasized, given that the median is less impacted by outliers",9,-11.689935,16.235703,8.712413,8.982592,"dataâ, analyse, analyses"
466,"we explored a dataset containing 500 course summaries compiled from previous lectures. 
introduction to pivot tables and their practical applications. 
we examined a real-world optimization challenge in a chemical plant
we observed a histogram that largely followed a normal distribution
case study related to transformer failures",we explored a dataset containing 500 course summaries compiled from previous lectures. introduction to pivot tables and their practical applications. we examined a real-world optimization challenge in a chemical plant we observed a histogram that largely followed a normal distribution case study related to transformer failures,6,-6.1100235,27.09662,7.7530184,9.544717,"summarizing, summarize, summarization"
467,"roc curves plot tpvsfp, flatter roc is wose as chaning threshold value doesnt affect much and auc is also low when roc flat portion above x=y better","roc curves plot tpvsfp, flatter roc is wose as chaning threshold value doesnt affect much and auc is also low when roc flat portion above x=y better",12,1.4942734,-25.705315,7.427518,0.7397673,"classifiers, logistic, roc"
468,"from the graph, we saw that for each sequence of sessions, less people produced summaries but more words.

we analyzed a model like a sine wave. by applying a feature y1, we produced multiple polynomial features such as y1^2, y1^3, and y1^4. a p value was produced. applying another feature sin(yâ‚) decreases the p value; thus, this feature is relevant. but when too many features are included, adjusted r^2 may decline. it is desirable to have just one model fitting well.

finally, we discussed neural networks, where input features pass through computational layers. stacking these layers forms deep learning, but excessive complexity that is increasing deg if freedom can cause overfitting.","from the graph, we saw that for each sequence of sessions, less people produced summaries but more words. we analyzed a model like a sine wave. by applying a feature y1, we produced multiple polynomial features such as y1^2, y1^3, and y1^4. a p value was produced. applying another feature sin(y ) decreases the p value; thus, this feature is relevant. but when too many features are included, adjusted r^2 may decline. it is desirable to have just one model fitting well. finally, we discussed neural networks, where input features pass through computational layers. stacking these layers forms deep learning, but excessive complexity that is increasing deg if freedom can cause overfitting.",0,-3.6965854,1.2313029,8.846677,3.906464,"models, feature, features"
469,"the class started off by analysing the summary submission data, and we discussed the use of pivot tables, which is a very strong tool for data analysis, and provides us with a lot of insights about the data like the skewness, the minimum and maximum value. we performed a very detailed exploratory data analysis on our submission data, where we went into the depths uptill which student has submitted how many character summary. we then moved on to a different dataset, and tried to extract preliminary insights from it. we discussed a term called as edge computing, which means computing at the end of a particular process. we talked about quartiles again, where we said that they were based on the number of observations and hence they are calculated after the data is sorted. we then created line plots for each and every column of the data, before and after removing outliers. this gives us much information about the trend without outliers and how the outliers are influencing our dataset. we also plot histograms for each column of our dataset which gives us an idea about the distribution of each column data. also, sir mentioned binning, which is the process of converting a continuous variable into discrete variable. correlation heatmaps are a very good way to find relations between different parameters. we can also plot trend line plots which also give us some interesting observations, which can be analysed. we then talked about our assignment 2 submissions and explored some data analysis.","the class started off by analysing the summary submission data, and we discussed the use of pivot tables, which is a very strong tool for data analysis, and provides us with a lot of insights about the data like the skewness, the minimum and maximum value. we performed a very detailed exploratory data analysis on our submission data, where we went into the depths uptill which student has submitted how many character summary. we then moved on to a different dataset, and tried to extract preliminary insights from it. we discussed a term called as edge computing, which means computing at the end of a particular process. we talked about quartiles again, where we said that they were based on the number of observations and hence they are calculated after the data is sorted. we then created line plots for each and every column of the data, before and after removing outliers. this gives us much information about the trend without outliers and how the outliers are influencing our dataset. we also plot histograms for each column of our dataset which gives us an idea about the distribution of each column data. also, sir mentioned binning, which is the process of converting a continuous variable into discrete variable. correlation heatmaps are a very good way to find relations between different parameters. we can also plot trend line plots which also give us some interesting observations, which can be analysed. we then talked about our assignment 2 submissions and explored some data analysis.",9,-10.752921,23.98607,7.922468,10.033499,"dataâ, analyse, analyses"
470,"in multiple linear regression, output is calculated based on more than one features. selection of useful features based upon domain knowledge and some other techniques is called as feature engineering. mlr does not have closed form solution. we use gradient descent to find optimal solution. this method is similar to newtown-rhapson method. this method involves start from a random point and then move towards optimal solution iteratively. 
error matrics such as mae and rmse have physical meaning since they have same dimensions as features. sse and mse are used for model selections. we saw an example with all coefficients were zero, but after feature selection we got better outputs.","in multiple linear regression, output is calculated based on more than one features. selection of useful features based upon domain knowledge and some other techniques is called as feature engineering. mlr does not have closed form solution. we use gradient descent to find optimal solution. this method is similar to newtown-rhapson method. this method involves start from a random point and then move towards optimal solution iteratively. error matrics such as mae and rmse have physical meaning since they have same dimensions as features. sse and mse are used for model selections. we saw an example with all coefficients were zero, but after feature selection we got better outputs.",0,3.1797397,-7.5792437,9.651481,4.081979,"models, feature, features"
471,"in real life we should never use the entire sample for training the model, instead randomly take some (practically 20%) of the data and treat it as test data unseen by the model. this is used to compare the performance of different we may create to solve the problem.
multiple r is nothing but the square root of the r^2 metric for the case of mlr, and it represents some form of correlation between y and all of the x.
linear regression only means that we will solve for a linear combination of the features, it does not restrain what those features may be, and whether the trend of the predictions will be linear (line or hyperplane) or not.
the matrix form doesn't restrict y to be just one row / column, it can also be a matrix, in which case we would just be predicting multiple outputs from the features.
if we know the mean, then one of the sample loses its value, and hence we say that the degrees of freedom are reduced when we are calculating the values of ssr and sst because both formulas require the mean of something. when we divide them by their respective degrees of freedom and then use them to calculate r^2, it is called adjusted r^2.
quantile-quantile plot or qq plot is a plot that can tell how close a distribution is to the normal distribution. ideally it should be the y = x line, more the deviation from this line, more the distribution is different than a normal distribution.","in real life we should never use the entire sample for training the model, instead randomly take some (practically 20%) of the data and treat it as test data unseen by the model. this is used to compare the performance of different we may create to solve the problem. multiple r is nothing but the square root of the r^2 metric for the case of mlr, and it represents some form of correlation between y and all of the x. linear regression only means that we will solve for a linear combination of the features, it does not restrain what those features may be, and whether the trend of the predictions will be linear (line or hyperplane) or not. the matrix form doesn't restrict y to be just one row / column, it can also be a matrix, in which case we would just be predicting multiple outputs from the features. if we know the mean, then one of the sample loses its value, and hence we say that the degrees of freedom are reduced when we are calculating the values of ssr and sst because both formulas require the mean of something. when we divide them by their respective degrees of freedom and then use them to calculate r^2, it is called adjusted r^2. quantile-quantile plot or qq plot is a plot that can tell how close a distribution is to the normal distribution. ideally it should be the y = x line, more the deviation from this line, more the distribution is different than a normal distribution.",2,13.583168,0.63191664,11.824481,4.6361623,"regression, regressions, features"
472,"in today's class our discussion on feature engineering was continued upon .feature engineering transforms raw data into meaningful inputs for machine learning, with feature encoding playing a key role in handling categorical data. common encoding techniques include label encoding (assigns unique integers to categories), one-hot encoding (creates binary columns for each category), binary encoding (converts categories into binary format), integer encoding (assigns integers, suitable for ordinal data), frequency encoding (replaces categories with their occurrence count), and target encoding (uses the mean target value for each category, prone to overfitting). in classification, a multiclass problem assigns each instance to one of several categories, whereas a multilabel problem allows multiple labels per instance. feature binning helps manage continuous data by grouping it into discrete bins, reducing complexity and overfitting. for text data, key techniques include tokenization, stopword removal, stemming/lemmatization, and vectorization methods like tf-idf and word embeddings to convert text into numerical features.","in today's class our discussion on feature engineering was continued upon .feature engineering transforms raw data into meaningful inputs for machine learning, with feature encoding playing a key role in handling categorical data. common encoding techniques include label encoding (assigns unique integers to categories), one-hot encoding (creates binary columns for each category), binary encoding (converts categories into binary format), integer encoding (assigns integers, suitable for ordinal data), frequency encoding (replaces categories with their occurrence count), and target encoding (uses the mean target value for each category, prone to overfitting). in classification, a multiclass problem assigns each instance to one of several categories, whereas a multilabel problem allows multiple labels per instance. feature binning helps manage continuous data by grouping it into discrete bins, reducing complexity and overfitting. for text data, key techniques include tokenization, stopword removal, stemming/lemmatization, and vectorization methods like tf-idf and word embeddings to convert text into numerical features.",3,-42.129395,3.4925008,-0.034860235,6.2466693,"categorical, categorization, categorise"
473,"sir discussed the mid sem paper and i realised that i have done many mistakes. the main thing was i didnt understand the question properly 2nd i wasted too much time on writing code manually i could have used chat gpt for it. sir had written functions for cleaning the data and imputing it with the mean value of the columns and the missing data rows were only 4 so it would have not mattered if had dropped them. then we looked into the visualisation part of it we plotted box plots to check for outliers then made histograms to know the distribution of the different columns , and made kde plots for both the intial and validation data sets to know the differences. trained random forest classifier on the train and test data and then compared the metrics by running the model on the validation data set to find out that it performs poorly indicating that the sample is not from pune and this could also been concluded from the kde plots. plotted the bar plots of ailments and got to know that heart disease is the least in number and this will no have good results in the classifier and thus we tell that our model is not going to predict the heart disease and reliably and move forward. we saw the heat map of corelation coefficient and saw no corelation among the columns but this was linear corelation but there might be non-linear corelation and thus we were introduced to multi variate analysis where we fix a model for xi with all other xj where j not equal to 1 find there r^2 value amd find 1/(1-r^2) to find out the corelation if its high then we drop the column","sir discussed the mid sem paper and i realised that i have done many mistakes. the main thing was i didnt understand the question properly 2nd i wasted too much time on writing code manually i could have used chat gpt for it. sir had written functions for cleaning the data and imputing it with the mean value of the columns and the missing data rows were only 4 so it would have not mattered if had dropped them. then we looked into the visualisation part of it we plotted box plots to check for outliers then made histograms to know the distribution of the different columns , and made kde plots for both the intial and validation data sets to know the differences. trained random forest classifier on the train and test data and then compared the metrics by running the model on the validation data set to find out that it performs poorly indicating that the sample is not from pune and this could also been concluded from the kde plots. plotted the bar plots of ailments and got to know that heart disease is the least in number and this will no have good results in the classifier and thus we tell that our model is not going to predict the heart disease and reliably and move forward. we saw the heat map of corelation coefficient and saw no corelation among the columns but this was linear corelation but there might be non-linear corelation and thus we were introduced to multi variate analysis where we fix a model for xi with all other xj where j not equal to 1 find there r^2 value amd find 1/(1-r^2) to find out the corelation if its high then we drop the column",9,-7.158438,14.89943,9.682719,8.2615185,"dataâ, analyse, analyses"
474,"in this class, sir told that he will discuss about data problems. if the correlation between data is not proper, we use variance inflation factor. vif=1-/(1-r^2). vif= 10 occurs for r^2 around 0.8. we progressively keep eliminating the variables with higher values of vif.  if we go on removing variables with high vif values, we will have some variables with low vif values which have high r^2 square values. we need to remove some variables to eliminate the curse of dimensionality. another method to handle data problems is principal component analysis. principle components are orthogonal to each other. we can find principal components equal to the dimension of data. we transform axis so that transformed axes capture variance of data best. we plot graph of two variables x1 and x2 say. if the points are spread around a straight line passing through origin, this straight line will be a principle component. this line will capture most of the variance. if we create plot % variance explained by each principle component, this is known as elbow curve. based on this curve we eliminate variables. but the problem with this is pc1=c11.x1+c12.x2+.....-linear combination of all variables, where c11,c12,c13,... are called loadings. we have to perform vif first and then use pca. interpretability- after doing pca, y=f(pc) using this we can do prediction or do 'what if analysis'. but it is difficult to explain the results of prediction or what if analysis with pca. because pca are a mathematical representation of variables. we will lose explain ability if we perform pca. vif reduces the features, so we do vif first and see if many features are getting eliminated. then we can do pca if required. pc analysis- dimension reduction, prediction models and visualization. lets consider image data- dimensionality of this data is number of pixels. before pca we might need to normalise the data. mnist data set is a collection of hand written numbers. these have been converted into 28x28 pixels. we want to train a classifier which can recognise numbers. we perform pca on this data and realise that there are 784 variables but only 70-80 are useful. this also results in data reduction. then we can do 2d and 3d plots to visualise this data. pca is mathematically exact. if our idea is to visualise the data effectively we need to tsne. t distribution- derived from gaussian distribution and it is more spread out as compared to gaussian distribution. tsne is a lossy transform, we loose exactness of data and gain relative closeness of data. tsne is lossy transformation, so use it carefully. the process of tsne itself give the structure of data. t-sne is a machine learning algorithm that is used for dimensionality reduction and data visualisation. it works by finding the similarity measures between pairs of instances in higher and lower dimensional spaces and tries to maintain the probability distribution for data samples in lower dimensions same as the probability distribution of data samples in higher dimensions. t-sne- t-distributed stochastic neighbour encoding.","in this class, sir told that he will discuss about data problems. if the correlation between data is not proper, we use variance inflation factor. vif=1-/(1-r^2). vif= 10 occurs for r^2 around 0.8. we progressively keep eliminating the variables with higher values of vif. if we go on removing variables with high vif values, we will have some variables with low vif values which have high r^2 square values. we need to remove some variables to eliminate the curse of dimensionality. another method to handle data problems is principal component analysis. principle components are orthogonal to each other. we can find principal components equal to the dimension of data. we transform axis so that transformed axes capture variance of data best. we plot graph of two variables x1 and x2 say. if the points are spread around a straight line passing through origin, this straight line will be a principle component. this line will capture most of the variance. if we create plot % variance explained by each principle component, this is known as elbow curve. based on this curve we eliminate variables. but the problem with this is pc1=c11.x1+c12.x2+.....-linear combination of all variables, where c11,c12,c13,... are called loadings. we have to perform vif first and then use pca. interpretability- after doing pca, y=f(pc) using this we can do prediction or do 'what if analysis'. but it is difficult to explain the results of prediction or what if analysis with pca. because pca are a mathematical representation of variables. we will lose explain ability if we perform pca. vif reduces the features, so we do vif first and see if many features are getting eliminated. then we can do pca if required. pc analysis- dimension reduction, prediction models and visualization. lets consider image data- dimensionality of this data is number of pixels. before pca we might need to normalise the data. mnist data set is a collection of hand written numbers. these have been converted into 28x28 pixels. we want to train a classifier which can recognise numbers. we perform pca on this data and realise that there are 784 variables but only 70-80 are useful. this also results in data reduction. then we can do 2d and 3d plots to visualise this data. pca is mathematically exact. if our idea is to visualise the data effectively we need to tsne. t distribution- derived from gaussian distribution and it is more spread out as compared to gaussian distribution. tsne is a lossy transform, we loose exactness of data and gain relative closeness of data. tsne is lossy transformation, so use it carefully. the process of tsne itself give the structure of data. t-sne is a machine learning algorithm that is used for dimensionality reduction and data visualisation. it works by finding the similarity measures between pairs of instances in higher and lower dimensional spaces and tries to maintain the probability distribution for data samples in lower dimensions same as the probability distribution of data samples in higher dimensions. t-sne- t-distributed stochastic neighbour encoding.",11,-19.218966,4.561899,10.391935,13.362207,"pca, heatmap, heatmaps"
475,"we looked at several types of receiver operating characteristic curves. a roc curve visually depicts the performance of a binary classification model across all possible classification thresholds, plotting the true positive rate (sensitivity) on the y-axis against the false positive rate (1 - specificity) on the x-axis, essentially showing how well a model can distinguish between positive and negative classes at different cut-off points; a higher area under the curve (auc) indicates better overall classification performance. this occurs as a result of false positives continuing to rise, suggesting a high frequency of inaccurate forecasts, but genuine positives are not considerably increased by raising the threshold. in these situations, a low auc value validates poor classifier performance. better performance is shown by any portion of the graph above the x=y line, which typically correlates to an auc of 1/2.","we looked at several types of receiver operating characteristic curves. a roc curve visually depicts the performance of a binary classification model across all possible classification thresholds, plotting the true positive rate (sensitivity) on the y-axis against the false positive rate (1 - specificity) on the x-axis, essentially showing how well a model can distinguish between positive and negative classes at different cut-off points; a higher area under the curve (auc) indicates better overall classification performance. this occurs as a result of false positives continuing to rise, suggesting a high frequency of inaccurate forecasts, but genuine positives are not considerably increased by raising the threshold. in these situations, a low auc value validates poor classifier performance. better performance is shown by any portion of the graph above the x=y line, which typically correlates to an auc of 1/2.",12,2.600813,-24.160566,7.4218454,0.7091288,"classifiers, logistic, roc"
476,"logistic regression, classification, and clustering took center stage in todayâ€™s session. the discussion kicked off with error metrics and the confusion matrix, highlighting their significance in evaluating model performance. tensorflow playground provided an interactive way to explore classification problems visually, reinforcing key concepts.

diving into the coding aspect, we implemented logistic regression while emphasizing why metrics like the confusion matrix and roc curves offer deeper insights than simple accuracy scores. the roc curve emerged as a crucial tool for assessing classifiers, with an auc value close to 1 indicating strong performance. we also explored how adjusting classification thresholds impacts the roc curve and tackled the complexities of multi-class classification, particularly when dealing with imbalanced datasets.

shifting gears to unsupervised learning, the focus moved to clustering techniques. k-means, with its requirement to predefine the number of clusters, was explored alongside its iterative approach to grouping data points. in contrast, hierarchical clustering provided a more flexible alternative, generating a dendrogram to reveal cluster relationships and guide decisions on the optimal number of clusters.","logistic regression, classification, and clustering took center stage in today s session. the discussion kicked off with error metrics and the confusion matrix, highlighting their significance in evaluating model performance. tensorflow playground provided an interactive way to explore classification problems visually, reinforcing key concepts. diving into the coding aspect, we implemented logistic regression while emphasizing why metrics like the confusion matrix and roc curves offer deeper insights than simple accuracy scores. the roc curve emerged as a crucial tool for assessing classifiers, with an auc value close to 1 indicating strong performance. we also explored how adjusting classification thresholds impacts the roc curve and tackled the complexities of multi-class classification, particularly when dealing with imbalanced datasets. shifting gears to unsupervised learning, the focus moved to clustering techniques. k-means, with its requirement to predefine the number of clusters, was explored alongside its iterative approach to grouping data points. in contrast, hierarchical clustering provided a more flexible alternative, generating a dendrogram to reveal cluster relationships and guide decisions on the optimal number of clusters.",8,-4.0205665,-17.83282,6.491335,0.83162755,"classification, clusterings, classifying"
477,"we started the session by learning how to calculate things like the average (mean), variability (variance), and other important statistics for a population. the instructor explained the central limit theorem with an example of estimating the extra work hours of managers in a company. we worked with a small sample of 18 people, assuming it represented the entire population.

we plotted a graph (pdf) showing the likelihood of different average work hours for the sample and learned about confidence intervals, including how to use the t-distribution when the sample size is small (less than 30). the instructor also explained why confidence intervals are important in real-life applications.

later, we covered other statistical concepts like t-values, z-values, and how to calculate them. the idea of being ""statistically different"" was introduced, and the session ended with a discussion on p-values and a quick introduction to multiple linear regression.","we started the session by learning how to calculate things like the average (mean), variability (variance), and other important statistics for a population. the instructor explained the central limit theorem with an example of estimating the extra work hours of managers in a company. we worked with a small sample of 18 people, assuming it represented the entire population. we plotted a graph (pdf) showing the likelihood of different average work hours for the sample and learned about confidence intervals, including how to use the t-distribution when the sample size is small (less than 30). the instructor also explained why confidence intervals are important in real-life applications. later, we covered other statistical concepts like t-values, z-values, and how to calculate them. the idea of being ""statistically different"" was introduced, and the session ended with a discussion on p-values and a quick introduction to multiple linear regression.",7,31.778309,7.7429013,14.564844,2.4473796,"statistics, statistical, statisticsâ"
478,"we again started our discussion of clustering and logistic regression ( it doesn't directly predict the value of class but probability that value will be in this class or another ). also we were discussing expectation algebra in the starting as a doubt question. if probability > 0.5 (kind of set level) then we classify it to one class otherwise to another class . features can be many in the class we started with 3 features and output to 0 or 1 . predicted is p. then we want to calculate weights such that likelihood of getting desired output is maximised (minimise differences). now the output we wanted can be either predict the class or the probability ..  (it is kind of binomial distirbution) .. discussion on p and 1-p for all observations. we also discussed on log likelihood and error function was negative log likelihood for classification. we also saw false positive , true positive , false negative and true negative. accuracy and precision (of the events you detected how many were correct) and recall(of the specific class how many were correct) . combination of precision and recall makes f1 value (harmonic mean) and confusion matrix.","we again started our discussion of clustering and logistic regression ( it doesn't directly predict the value of class but probability that value will be in this class or another ). also we were discussing expectation algebra in the starting as a doubt question. if probability > 0.5 (kind of set level) then we classify it to one class otherwise to another class . features can be many in the class we started with 3 features and output to 0 or 1 . predicted is p. then we want to calculate weights such that likelihood of getting desired output is maximised (minimise differences). now the output we wanted can be either predict the class or the probability .. (it is kind of binomial distirbution) .. discussion on p and 1-p for all observations. we also discussed on log likelihood and error function was negative log likelihood for classification. we also saw false positive , true positive , false negative and true negative. accuracy and precision (of the events you detected how many were correct) and recall(of the specific class how many were correct) . combination of precision and recall makes f1 value (harmonic mean) and confusion matrix.",10,11.722351,-19.86379,8.890704,-1.4123063,"classifications, histograms, histogram"
479,"today's lecture covered the levels of measurement: nominal (categorical), ordinal (ordered), interval, and ratio. explained supervised learning (labeled data for tasks like classification) and unsupervised learning (unlabeled data for clustering). key algorithms discussed included linear regression (continuous prediction), logistic regression (binary classification), and k-means clustering .","today's lecture covered the levels of measurement: nominal (categorical), ordinal (ordered), interval, and ratio. explained supervised learning (labeled data for tasks like classification) and unsupervised learning (unlabeled data for clustering). key algorithms discussed included linear regression (continuous prediction), logistic regression (binary classification), and k-means clustering .",4,-19.248207,-16.393826,2.2426858,0.652633,"classification, classifying, classifications"
480,"in today's class, we discussed exploratory data analysis. crisp-dm is a widely used framework for data analysis. it has six steps that run cyclically: business understanding, which means knowing the domain, understanding the data, preparing data that is either transforming it, removing outliers, or adding missing data. then, we make the model using the data. then, we evaluate the model by applying it to the test data and checking its effectiveness. afterward, we think about what to do with the model and how to deploy it. eda uses statistical graphics and data visualization methods to represent the data. then we discussed doing the task during a particular duration is important, and having deadlines helps people to work accordingly. then we saw a mind map stating the methods when we know different situations about the dependent variable. then we applied eda to pima indians diabetes data, in which the outcome is either the person is diabetic(1) or not(0), we saw the histogram of each factor considered to get the outcome in which we see some were nearly normal, then we see the boxplot of these factors stating the variability in the data and outlier instances present in them, afterward we see their correlation coefficient with the outcome and we see a high correlation with glucose and age as more aged and people with more glucose level are prone to diabetes. we also see the scatter plots and matrix plots which shows more relation between the features, clusters are seen more visibly, but it can show the correlation between two factors only at a time. then we see the problem of class imbalance in which one class occurs more frequently than the other. we also see the india temperature data analysis, which majorly states that the temperatures are increasing as years pass for a particular time in a year, and also in a year, it first increases and then decreases. then we see about handling missing data values which are classified as: mcar- completely random data points missing, mar- some relationship between the missing data point and values in different columns, and mnar- the unobserved values themselves are responsible for the data being missed. we analyze the data and decide whether we need to add them or not, then we see that we have univariable data if we have just one column as independent variables and multivariable data if we have the combinations of such columns. then we handle the outliers by the median method and the standard deviation method, also we need to consider outliers sometimes as they have taken place and have an impact. among the quantile for outliers which are median and mean, the median is good as it is not influenced by outliers but the mean is impacted due to the outliers. at last, we saw that we just don't need to apply every technique to the data, we should get the domain knowledge, understand and analyze the data, and then perform the process of eda through the required and significant techniques.","in today's class, we discussed exploratory data analysis. crisp-dm is a widely used framework for data analysis. it has six steps that run cyclically: business understanding, which means knowing the domain, understanding the data, preparing data that is either transforming it, removing outliers, or adding missing data. then, we make the model using the data. then, we evaluate the model by applying it to the test data and checking its effectiveness. afterward, we think about what to do with the model and how to deploy it. eda uses statistical graphics and data visualization methods to represent the data. then we discussed doing the task during a particular duration is important, and having deadlines helps people to work accordingly. then we saw a mind map stating the methods when we know different situations about the dependent variable. then we applied eda to pima indians diabetes data, in which the outcome is either the person is diabetic(1) or not(0), we saw the histogram of each factor considered to get the outcome in which we see some were nearly normal, then we see the boxplot of these factors stating the variability in the data and outlier instances present in them, afterward we see their correlation coefficient with the outcome and we see a high correlation with glucose and age as more aged and people with more glucose level are prone to diabetes. we also see the scatter plots and matrix plots which shows more relation between the features, clusters are seen more visibly, but it can show the correlation between two factors only at a time. then we see the problem of class imbalance in which one class occurs more frequently than the other. we also see the india temperature data analysis, which majorly states that the temperatures are increasing as years pass for a particular time in a year, and also in a year, it first increases and then decreases. then we see about handling missing data values which are classified as: mcar- completely random data points missing, mar- some relationship between the missing data point and values in different columns, and mnar- the unobserved values themselves are responsible for the data being missed. we analyze the data and decide whether we need to add them or not, then we see that we have univariable data if we have just one column as independent variables and multivariable data if we have the combinations of such columns. then we handle the outliers by the median method and the standard deviation method, also we need to consider outliers sometimes as they have taken place and have an impact. among the quantile for outliers which are median and mean, the median is good as it is not influenced by outliers but the mean is impacted due to the outliers. at last, we saw that we just don't need to apply every technique to the data, we should get the domain knowledge, understand and analyze the data, and then perform the process of eda through the required and significant techniques.",9,-11.855387,18.362236,9.045778,9.096358,"dataâ, analyse, analyses"
481,"learnt about 4 different levels of measurement:-
(i)nominal:data is categorized into groups with no ordering.used for discrete data.example:gender,color,etc.
(ii)ordinal:data is categorised into groups which have ordering.used for discrete data.example:grades
(iii)interval:data is categorised into groups which have ordering and intervals between consecutive points are measurable but zero has arbitrary meaning.used for continuous data.example:temperature(5â°c is not twice as cold as 10â°c)
(iv)ratio:data is categorised into groups which have ordering,intervals between consecutive points are measurable and zero has a definite meaning.used for continuous data.example:height,weight
the machine learning category for both nominal and ordinal is ""classification"" whereas for interval and ratio it is ""regression"".

also learnt about y=f(x) where y is label and x contains features.the problems which contain label are called supervised learning problems and the ones which do not contain are unsupervised learning problems in which we use k-means clustering and hierarchial clustering to make labels.

there was one key point made that no matter how large the size of the data is,it is always considered a sample of the population.","learnt about 4 different levels of measurement:- (i)nominal:data is categorized into groups with no ordering.used for discrete data.example:gender,color,etc. (ii)ordinal:data is categorised into groups which have ordering.used for discrete data.example:grades (iii)interval:data is categorised into groups which have ordering and intervals between consecutive points are measurable but zero has arbitrary meaning.used for continuous data.example:temperature(5 c is not twice as cold as 10 c) (iv)ratio:data is categorised into groups which have ordering,intervals between consecutive points are measurable and zero has a definite meaning.used for continuous data.example:height,weight the machine learning category for both nominal and ordinal is ""classification"" whereas for interval and ratio it is ""regression"". also learnt about y=f(x) where y is label and x contains features.the problems which contain label are called supervised learning problems and the ones which do not contain are unsupervised learning problems in which we use k-means clustering and hierarchial clustering to make labels. there was one key point made that no matter how large the size of the data is,it is always considered a sample of the population.",4,-24.5113,-18.466831,1.5202205,0.12459806,"classification, classifying, classifications"
482,"in today's class, we revisited the formula for a and b in the simple linear regression equation:
y = ax + b.

using a csv file in excel, we created columns for y_predicted and error. before that, we calculated the averages (xì„ and yì„), as well as xx_bar, xy_bar, and xì„â². from these values, we calculated a and b, which were used to compute y_predicted. the error was calculated as the difference between the actual y and the predicted y.

we plotted the best-fit line for our data and then visualized the errors. on a 2-d scatter plot, the errors looked fine, but when plotted as a histogram, they did not follow a gaussian (normal) distribution. this indicated that the model couldn't fully capture some trends in the data.

next, we used the regression feature in excelâ€™s data analysis toolpack to get statistical details about the regression model. we learned that the standard error measures how much the sample mean varies from the population mean. additionally, we discussed that for many samples, the distribution of sample means tends to follow a normal distribution.

lastly, we introduced three key terms:

sst (total sum of squares): measures the total variation in the data.
ssr (regression sum of squares): the variation explained by the model.
sse (error sum of squares): the variation not explained by the model.
these terms are related by the formula:
sst = ssr + sse

the râ² value, calculated as râ² = ssr / sst, represents the proportion of the total variation explained by the model.","in today's class, we revisited the formula for a and b in the simple linear regression equation: y = ax + b. using a csv file in excel, we created columns for y_predicted and error. before that, we calculated the averages (x and y ), as well as xx_bar, xy_bar, and x . from these values, we calculated a and b, which were used to compute y_predicted. the error was calculated as the difference between the actual y and the predicted y. we plotted the best-fit line for our data and then visualized the errors. on a 2-d scatter plot, the errors looked fine, but when plotted as a histogram, they did not follow a gaussian (normal) distribution. this indicated that the model couldn't fully capture some trends in the data. next, we used the regression feature in excel s data analysis toolpack to get statistical details about the regression model. we learned that the standard error measures how much the sample mean varies from the population mean. additionally, we discussed that for many samples, the distribution of sample means tends to follow a normal distribution. lastly, we introduced three key terms: sst (total sum of squares): measures the total variation in the data. ssr (regression sum of squares): the variation explained by the model. sse (error sum of squares): the variation not explained by the model. these terms are related by the formula: sst = ssr + sse the r value, calculated as r = ssr / sst, represents the proportion of the total variation explained by the model.",5,22.067287,-3.7453864,14.299479,4.7321506,"regression, statistical, statistics"
483,"in todays class we learned about logistics regression and talked about weights . logistic regression is a way to predict outcomes that are either 0 or 1. it works by taking input values, multiplying them by weights, adding a bias, and passing the result through a sigmoid function to get a probability. the model is trained by adjusting weights using the gradient descent method to minimize errors. the likelihood function helps in optimizing weights by maximizing correct predictions. the model's performance is evaluated using a confusion matrix, accuracy, precision, and recall. if the predicted probability is greater than 0.5, the output is 1; otherwise, itâ€™s 0.","in todays class we learned about logistics regression and talked about weights . logistic regression is a way to predict outcomes that are either 0 or 1. it works by taking input values, multiplying them by weights, adding a bias, and passing the result through a sigmoid function to get a probability. the model is trained by adjusting weights using the gradient descent method to minimize errors. the likelihood function helps in optimizing weights by maximizing correct predictions. the model's performance is evaluated using a confusion matrix, accuracy, precision, and recall. if the predicted probability is greater than 0.5, the output is 1; otherwise, it s 0.",12,8.57678,-21.159668,9.106644,-1.8214202,"classifiers, logistic, roc"
484,"vif (variance inflation factor)
purpose: vif is used to detect multicollinearity in regression models.
process: we progressively removed values of ei (likely referring to certain features or variables).
outcome: the test was conducted in code 2, and it helped in removing extra features.

bca (principal component analysis)
principal components (pc):
  principal components are orthogonal, meaning they are uncorrelated.
  the maximum number of components is equal to the number of dimensions in the original dataset.

variance explained by components:
  the first principal component (pc1) explains most of the variance in the data.
  the second principal component (pc2) explains the next largest portion of the variance, and so on.

loadings:
  if pc1 is a linear combination of original features, say c1x1 + c2x2 + c3x3, the parameters c1, c2, c3 are called loadings.
  the loading values indicate the strength of influence that each variable has on the component. for example, if c3 is large for pc1, then x3 (the original variable) greatly influences the variance explained by pc1.

multicollinearity:
  vif is typically calculated first to detect and address multicollinearity, which can be an issue in regression models.

loss of interpretability with pca:
  one downside of pca is that it loses interpretability. for instance, if x1 is a known parameter, we cannot easily quantify how changes in x1 will affect the result after applying pca.

uses of pca:
  dimension reduction: pca helps reduce the number of variables, making the data easier to handle.
  prediction models: pca can be used to create more efficient predictive models.
  visualization: pca is often used for visualizing high-dimensional data by reducing it to two or three dimensions.

choosing between models:
  there is a choice between using y = fx (where f is a matrix of features) or using y = pcs (principal components).
  it is generally better to normalize the data before performing pca to ensure that all features are treated equally in terms of their influence on the components.

mnist dataset and normalization
normalization importance:
  pca on the mnist dataset highlighted the importance of normalizing the data. normalization ensures that all features contribute equally to the principal components.

t-sne (t-distributed stochastic neighbor embedding)
overview: t-sne is a lossy transformation technique that is commonly used for dimensionality reduction and visualization, particularly for high-dimensional data.

methodology:
  t-sne uses gradient descent and stochastic methods.
  step 1: create a probability distribution for each observation in the dataset.
  more accurately, we calculate a distribution that represents the distance between each point and every other point in the dataset.
  hence, there are n x n distributions, where n is the number of data points.

why t-distribution:
  a t-distribution is used because it is flatter compared to a normal distribution, making it more sensitive to long-distance points. this helps in maintaining the relationships between distant points.

goal: the goal of t-sne is to minimize the kullback-leibler (kl) divergence, which measures the difference between the original high-dimensional probability distribution and the low-dimensional projection.","vif (variance inflation factor) purpose: vif is used to detect multicollinearity in regression models. process: we progressively removed values of ei (likely referring to certain features or variables). outcome: the test was conducted in code 2, and it helped in removing extra features. bca (principal component analysis) principal components (pc): principal components are orthogonal, meaning they are uncorrelated. the maximum number of components is equal to the number of dimensions in the original dataset. variance explained by components: the first principal component (pc1) explains most of the variance in the data. the second principal component (pc2) explains the next largest portion of the variance, and so on. loadings: if pc1 is a linear combination of original features, say c1x1 + c2x2 + c3x3, the parameters c1, c2, c3 are called loadings. the loading values indicate the strength of influence that each variable has on the component. for example, if c3 is large for pc1, then x3 (the original variable) greatly influences the variance explained by pc1. multicollinearity: vif is typically calculated first to detect and address multicollinearity, which can be an issue in regression models. loss of interpretability with pca: one downside of pca is that it loses interpretability. for instance, if x1 is a known parameter, we cannot easily quantify how changes in x1 will affect the result after applying pca. uses of pca: dimension reduction: pca helps reduce the number of variables, making the data easier to handle. prediction models: pca can be used to create more efficient predictive models. visualization: pca is often used for visualizing high-dimensional data by reducing it to two or three dimensions. choosing between models: there is a choice between using y = fx (where f is a matrix of features) or using y = pcs (principal components). it is generally better to normalize the data before performing pca to ensure that all features are treated equally in terms of their influence on the components. mnist dataset and normalization normalization importance: pca on the mnist dataset highlighted the importance of normalizing the data. normalization ensures that all features contribute equally to the principal components. t-sne (t-distributed stochastic neighbor embedding) overview: t-sne is a lossy transformation technique that is commonly used for dimensionality reduction and visualization, particularly for high-dimensional data. methodology: t-sne uses gradient descent and stochastic methods. step 1: create a probability distribution for each observation in the dataset. more accurately, we calculate a distribution that represents the distance between each point and every other point in the dataset. hence, there are n x n distributions, where n is the number of data points. why t-distribution: a t-distribution is used because it is flatter compared to a normal distribution, making it more sensitive to long-distance points. this helps in maintaining the relationships between distant points. goal: the goal of t-sne is to minimize the kullback-leibler (kl) divergence, which measures the difference between the original high-dimensional probability distribution and the low-dimensional projection.",11,-18.044289,3.4005527,10.433199,13.341631,"pca, heatmap, heatmaps"
485,"in todayâ€™s class, we explored how statistics serves as the foundation for machine learning, with ml being an application of statistical principles. we discussed data typesâ€”nominal, ordinal, and intervalâ€”using examples like temperature to illustrate their differences. the session also introduced key concepts like linear regression for predictive analysis, logistic regression for classification tasks, and random forests as powerful ensemble methods. additionally, we touched on k-means clustering, a popular unsupervised learning technique for grouping data. overall, the session provided a solid understanding of the statistical and algorithmic tools that drive data science.","in today s class, we explored how statistics serves as the foundation for machine learning, with ml being an application of statistical principles. we discussed data types nominal, ordinal, and interval using examples like temperature to illustrate their differences. the session also introduced key concepts like linear regression for predictive analysis, logistic regression for classification tasks, and random forests as powerful ensemble methods. additionally, we touched on k-means clustering, a popular unsupervised learning technique for grouping data. overall, the session provided a solid understanding of the statistical and algorithmic tools that drive data science.",13,-6.1110444,-6.1531835,8.872654,5.24521,"classification, classifying, classifications"
487,"today, we covered different ways to encode categorical data for machine learning. we started with one-hot encoding and vectorization, then looked at label and integer encodingâ€”useful when the target variable has an order. we discussed the downside of one-hot encoding (curse of dimensionality) and alternatives like binary encoding, which uses fewer columns, and frequency encoding, which assigns values based on class occurrence. target encoding, which factors in relationships with the target variable, was also introduced. finally, we briefly touched on turning text into numbers using vectorization.","today, we covered different ways to encode categorical data for machine learning. we started with one-hot encoding and vectorization, then looked at label and integer encoding useful when the target variable has an order. we discussed the downside of one-hot encoding (curse of dimensionality) and alternatives like binary encoding, which uses fewer columns, and frequency encoding, which assigns values based on class occurrence. target encoding, which factors in relationships with the target variable, was also introduced. finally, we briefly touched on turning text into numbers using vectorization.",3,-42.724113,5.3449907,0.3171784,6.2432575,"categorical, categorization, categorise"
488,"first of all we learnt about names of different types of techniques like slr which is simple linear regression, multiple linear regression ,logistic regression, random forest, k means clustering and hierarchal clustering we learnt about their names only first. whatever type of ml technique we use we will always get generic equations of type y=b0+b1x1+b2x2.....
there are four different levels of measurement: nominal, ordinal ,interval and ratio. we learnt about each of them in brief: nominal like gender and colour, we can only categorise them there is no ordering, we can only create a count from them .we can't calculate mean median mode .measurements are always discrete in this category. then comes the ordinal level of measurement for example grades aa , ab like that.this is also for discrete values .assigning numbers like 1,2 is wrong as 'b' will have double value then 'a' so we use encoding like 1and 0 -giving values. then we have the interval level of measurement which is used for measuring continuous values and it is used where zero has any arbitrary function like temperature. the last is ratio which is again used for measuring continuous but in this zero has a meaning like height weight or salary. then we learned that in y =f(x): y is the label and x is called features. supervised learning is when we have both labels and features available. phenomenal and ordinal levels of we use classification and for interval and ratio level of measurement we use regression. when why is not known then we use unsupervised learning under which comes k means clustering and hierarchical clustering like we have only x and which is spreaded into clusters and from that we try to predict y using some ml techniques. the last thing we learned was the difference between sample and population sample is the data we have access to and population is the total amount of data and however the large amount of data we have access to it is always the sample for us and never the population.","first of all we learnt about names of different types of techniques like slr which is simple linear regression, multiple linear regression ,logistic regression, random forest, k means clustering and hierarchal clustering we learnt about their names only first. whatever type of ml technique we use we will always get generic equations of type y=b0+b1x1+b2x2..... there are four different levels of measurement: nominal, ordinal ,interval and ratio. we learnt about each of them in brief: nominal like gender and colour, we can only categorise them there is no ordering, we can only create a count from them .we can't calculate mean median mode .measurements are always discrete in this category. then comes the ordinal level of measurement for example grades aa , ab like that.this is also for discrete values .assigning numbers like 1,2 is wrong as 'b' will have double value then 'a' so we use encoding like 1and 0 -giving values. then we have the interval level of measurement which is used for measuring continuous values and it is used where zero has any arbitrary function like temperature. the last is ratio which is again used for measuring continuous but in this zero has a meaning like height weight or salary. then we learned that in y =f(x): y is the label and x is called features. supervised learning is when we have both labels and features available. phenomenal and ordinal levels of we use classification and for interval and ratio level of measurement we use regression. when why is not known then we use unsupervised learning under which comes k means clustering and hierarchical clustering like we have only x and which is spreaded into clusters and from that we try to predict y using some ml techniques. the last thing we learned was the difference between sample and population sample is the data we have access to and population is the total amount of data and however the large amount of data we have access to it is always the sample for us and never the population.",4,-24.9293,-13.846892,1.4968146,0.22646931,"classification, classifying, classifications"
489,"for multiple linear regression, the closed form solution for b exists but it might be impractical to calculate these as matrix inversions might not exist and etc. if we have a sample of data, we should not use the entire data for creating the ml model. we need to split the data into two parts in the ratio 80%-training data and 20%-testing data. this splitting has to be done randomly. two sets of outcomes that we have to derive and measure: training metrics and test metrics. some of metrics will only be relevant to the training data but may not have any meaning for test data. as we are building model using the training data there are some metrics which are suitable for this only. overfit  situation: when r square value of training data is much greater(generally a difference grater than 0.2) than r squared value of test data. the training errors maybe less but test errors will be too much in case of overfitting. this is a practical tradeoff how much error we are allowing for test data and training data. both these errors are important. as we add more variables, r square value increases. this doesnot mean that the data is better fit. so we introduce a adjusted r square value which is the part of variance captured by each independent variable. n-1 comes in the denominator of calculations involving variance and standard deviation. the calculation of variance involves mean of x. this is already calculated using all n variables. this reduces one degree of freedom and it becomes n-1. the outcome of a linear regression need not always be straight line. slr and mlr are called parametric methods of model creation. there are non parametric methods of model creation. data drift- if we create a model now, the data coming after one month maybe far away from this model. so we have to change our models also frequently according to the data. then we shifted on to python. sir explained how to do multiple linear regression in python. q-q plot tells us how much our data is similar to normal distribution. regression model can be imported from scipy or stats model libraries. sm.ols: sm-statsmodel, ols-optimised least square. stats model gives more statistical measures and parameter than scipy. low values of aic and bic are better. aic and bic are to be discussed later. omnibus statistics, omnibus p value, skewness. we cant use the old p value everywhere. we need to use different type of p-value. omnibus- normality of distribuition. jarque - bera test- check the normality of residuals. durbin watson test tries to asses based on value of error can we predict next error or kind of autocorrelation in the residuals of the models.","for multiple linear regression, the closed form solution for b exists but it might be impractical to calculate these as matrix inversions might not exist and etc. if we have a sample of data, we should not use the entire data for creating the ml model. we need to split the data into two parts in the ratio 80%-training data and 20%-testing data. this splitting has to be done randomly. two sets of outcomes that we have to derive and measure: training metrics and test metrics. some of metrics will only be relevant to the training data but may not have any meaning for test data. as we are building model using the training data there are some metrics which are suitable for this only. overfit situation: when r square value of training data is much greater(generally a difference grater than 0.2) than r squared value of test data. the training errors maybe less but test errors will be too much in case of overfitting. this is a practical tradeoff how much error we are allowing for test data and training data. both these errors are important. as we add more variables, r square value increases. this doesnot mean that the data is better fit. so we introduce a adjusted r square value which is the part of variance captured by each independent variable. n-1 comes in the denominator of calculations involving variance and standard deviation. the calculation of variance involves mean of x. this is already calculated using all n variables. this reduces one degree of freedom and it becomes n-1. the outcome of a linear regression need not always be straight line. slr and mlr are called parametric methods of model creation. there are non parametric methods of model creation. data drift- if we create a model now, the data coming after one month maybe far away from this model. so we have to change our models also frequently according to the data. then we shifted on to python. sir explained how to do multiple linear regression in python. q-q plot tells us how much our data is similar to normal distribution. regression model can be imported from scipy or stats model libraries. sm.ols: sm-statsmodel, ols-optimised least square. stats model gives more statistical measures and parameter than scipy. low values of aic and bic are better. aic and bic are to be discussed later. omnibus statistics, omnibus p value, skewness. we cant use the old p value everywhere. we need to use different type of p-value. omnibus- normality of distribuition. jarque - bera test- check the normality of residuals. durbin watson test tries to asses based on value of error can we predict next error or kind of autocorrelation in the residuals of the models.",2,7.639434,2.4902909,10.76303,4.7169223,"regression, regressions, features"
490,"sir started by revising eda,identifying outliers,testing hypothesis , sir then showed eda of the summary data which we fill after every class in which he told avg word count was approx 1.2k. sir then introduced pivot tables and their practical applications . he showed the application on a real life problem of chemical plant which had approx 250 parameters , and various plots  were made like histogram of different parameters and mostly it followed normal distribution except for a few outliers  h sir related this issue with nvidia stock spikes at the end ta came to discuss exercise 2 of moodle","sir started by revising eda,identifying outliers,testing hypothesis , sir then showed eda of the summary data which we fill after every class in which he told avg word count was approx 1.2k. sir then introduced pivot tables and their practical applications . he showed the application on a real life problem of chemical plant which had approx 250 parameters , and various plots were made like histogram of different parameters and mostly it followed normal distribution except for a few outliers h sir related this issue with nvidia stock spikes at the end ta came to discuss exercise 2 of moodle",6,-7.626255,26.38857,7.9274273,9.990452,"summarizing, summarize, summarization"
491,"in this session, we covered methods of analyzing and condensing high-dimensional data without sacrificing interpretability. we started by talking about heatmaps, which, while perhaps not showing all the minute variations, are well worth it to compare several parameters at once. heatmaps facilitate rapid pairwise comparisons that would be difficult to view individually, thus serving as an effective tool when performing exploratory data analysis (eda).

the variance inflation factor (vif) was subsequently developed as a multicollinearity diagnostic. vif measures the extent to which a particular parameter is accounted for by the other parameters in the data. for any feature, a large vif means that the feature is strongly redundant with others. if a feature's vif is greater than some chosen threshold, it should be eliminated to prevent multicollinearityâ€”a problem that is frequently more important than the requirement for dimensionality reduction through pca.

principal component analysis (pca) was explained as a technique that employs singular value decomposition to project data onto new, mutually orthogonal components. the principal components cover the original dataset's complete dimensionality, yet you can opt to keep only the most important ones in order to bring down the dimensionality. the kept components are weighted sums of the original features, whose weights are referred to as loadings. although pca is ideal for prediction, visualization in the context of eda, and data compression capturing the largest amount of variance, it is less ideal for ""what if"" or sensitivity analysis because the transformed components do not map directly onto the original parameters. in addition, pca is sensitive to scaling of the data, so normalization is necessary.

by contrast, t-sne employs a t-distribution and stochastic gradient descent to create a low-dimensional lossy representation of high-dimensional data. while giving up on precise distances, t-sne performs very well on preserving the relative proximity of the data points, and thus is very effective for visualizing clusters and intricate patterns.","in this session, we covered methods of analyzing and condensing high-dimensional data without sacrificing interpretability. we started by talking about heatmaps, which, while perhaps not showing all the minute variations, are well worth it to compare several parameters at once. heatmaps facilitate rapid pairwise comparisons that would be difficult to view individually, thus serving as an effective tool when performing exploratory data analysis (eda). the variance inflation factor (vif) was subsequently developed as a multicollinearity diagnostic. vif measures the extent to which a particular parameter is accounted for by the other parameters in the data. for any feature, a large vif means that the feature is strongly redundant with others. if a feature's vif is greater than some chosen threshold, it should be eliminated to prevent multicollinearity a problem that is frequently more important than the requirement for dimensionality reduction through pca. principal component analysis (pca) was explained as a technique that employs singular value decomposition to project data onto new, mutually orthogonal components. the principal components cover the original dataset's complete dimensionality, yet you can opt to keep only the most important ones in order to bring down the dimensionality. the kept components are weighted sums of the original features, whose weights are referred to as loadings. although pca is ideal for prediction, visualization in the context of eda, and data compression capturing the largest amount of variance, it is less ideal for ""what if"" or sensitivity analysis because the transformed components do not map directly onto the original parameters. in addition, pca is sensitive to scaling of the data, so normalization is necessary. by contrast, t-sne employs a t-distribution and stochastic gradient descent to create a low-dimensional lossy representation of high-dimensional data. while giving up on precise distances, t-sne performs very well on preserving the relative proximity of the data points, and thus is very effective for visualizing clusters and intricate patterns.",11,-16.403347,3.7761822,10.344656,12.9875145,"pca, heatmap, heatmaps"
492,"today in class, we covered multiple linear regression, correlation coefficients, the role of standard deviation, and confidence intervals. i learned how multiple linear regression extends simple linear regression by using multiple independent variables to predict a dependent variable. we discussed how correlation coefficients measure the strength and direction of relationships between variables, helping to determine which variables are most relevant.
additionally, i gained understanding of standard deviation and its role in measuring data dispersion. this led to a discussion on confidence intervals, which provide a range of values within which we expect the true parameter to lie with a certain probability.","today in class, we covered multiple linear regression, correlation coefficients, the role of standard deviation, and confidence intervals. i learned how multiple linear regression extends simple linear regression by using multiple independent variables to predict a dependent variable. we discussed how correlation coefficients measure the strength and direction of relationships between variables, helping to determine which variables are most relevant. additionally, i gained understanding of standard deviation and its role in measuring data dispersion. this led to a discussion on confidence intervals, which provide a range of values within which we expect the true parameter to lie with a certain probability.",5,20.890762,3.7234452,13.010035,4.973325,"regression, statistical, statistics"
493,"1. we learnt solutions in multiple linear regression.
2. if the p - value is greater than 0.05 we reject it 
3. 80/20 data split is idea in test train dataset 
4. overfit if r2 value of test train not close enough
5. we learned about adjusted r2",1. we learnt solutions in multiple linear regression. 2. if the p - value is greater than 0.05 we reject it 3. 80/20 data split is idea in test train dataset 4. overfit if r2 value of test train not close enough 5. we learned about adjusted r2,2,10.06217,10.966158,11.378584,5.0703826,"regression, regressions, features"
494,"the session focused on key concepts in multiple linear regression, model evaluation, and data partitioning. it began with a discussion on the importance of not using the entire sample population for model training. instead, datasets are typically divided into training and test sets in an 80-20% ratio to ensure unbiased model performance evaluation. the significance of multiple r and r-square was explored, where multiple r represents the correlation between dependent and independent variables, and r-square measures the explained variance. the calculation of variance in samples was also addressed, emphasizing the adjustment for degrees of freedom by using (xi - xì„)â² / (n - 1) instead of (xi - xì„)â² / n.

feature selection was discussed in the context of p-values, where features with a p-value greater than 0.05 were considered statistically insignificant and removed to optimize the model. additionally, model selection and evaluation metrics such as the akaike information criterion (aic) were introduced, highlighting that a lower aic value indicates a better-fitting model. the omnibus statistic and omnibus p-value were also covered as measures for assessing the overall goodness of fit and the normality of residuals.

the session concluded with an explanation of the closed-form solution in multiple linear regression, which relies on matrix inversion techniques to obtain optimal parameter estimates. overall, the discussion provided a structured understanding of statistical foundations, feature selection, and model evaluation techniques essential for effective machine learningâ applications.","the session focused on key concepts in multiple linear regression, model evaluation, and data partitioning. it began with a discussion on the importance of not using the entire sample population for model training. instead, datasets are typically divided into training and test sets in an 80-20% ratio to ensure unbiased model performance evaluation. the significance of multiple r and r-square was explored, where multiple r represents the correlation between dependent and independent variables, and r-square measures the explained variance. the calculation of variance in samples was also addressed, emphasizing the adjustment for degrees of freedom by using (xi - x ) / (n - 1) instead of (xi - x ) / n. feature selection was discussed in the context of p-values, where features with a p-value greater than 0.05 were considered statistically insignificant and removed to optimize the model. additionally, model selection and evaluation metrics such as the akaike information criterion (aic) were introduced, highlighting that a lower aic value indicates a better-fitting model. the omnibus statistic and omnibus p-value were also covered as measures for assessing the overall goodness of fit and the normality of residuals. the session concluded with an explanation of the closed-form solution in multiple linear regression, which relies on matrix inversion techniques to obtain optimal parameter estimates. overall, the discussion provided a structured understanding of statistical foundations, feature selection, and model evaluation techniques essential for effective machine learning applications.",13,20.250067,5.5409174,12.395322,5.1624603,"classification, classifying, classifications"
495,"we saw there are two methods of improving quality of results : improve sample or improve method.. there is third one fine tune the methods.  then we start with understanding mlr for non linear cases. we again used feature engineering and have more terms involving x but in form of polynomials like xâ², and trigonometric function sin(x) : polynomial regression. we saw as we incorporate more terms adjusted râ² will gradually decrease. and at the end only significant term will remain due to p value . we saw backward and forward feature engineering and the issues like overfit. also we saw examples of non linear involving two things : trignometry and straight line.. if possible we want to have a single model to solve this instead of two models which can be ade by combination of two models. randomforest is a method which does the same thing. we saw parametric and non parametric method and delta analysis. we saw little bit definition of neural network and deep learning model which have more than one hidden layer. at the end we saw classification.  definition of regress and logistic regression . we started calling weights. we also saw the graphs and explanation for sigmoid function.","we saw there are two methods of improving quality of results : improve sample or improve method.. there is third one fine tune the methods. then we start with understanding mlr for non linear cases. we again used feature engineering and have more terms involving x but in form of polynomials like x , and trigonometric function sin(x) : polynomial regression. we saw as we incorporate more terms adjusted r will gradually decrease. and at the end only significant term will remain due to p value . we saw backward and forward feature engineering and the issues like overfit. also we saw examples of non linear involving two things : trignometry and straight line.. if possible we want to have a single model to solve this instead of two models which can be ade by combination of two models. randomforest is a method which does the same thing. we saw parametric and non parametric method and delta analysis. we saw little bit definition of neural network and deep learning model which have more than one hidden layer. at the end we saw classification. definition of regress and logistic regression . we started calling weights. we also saw the graphs and explanation for sigmoid function.",0,-0.88176376,-1.2810783,9.270838,4.348469,"models, feature, features"
496,in today's class first we discuss the solution for the midsem examination. after that one of the ta gave us some comments about our e3 assignment. and after that we learnt about the codes of dimensionality which says that two less data chasing too many features. it is basically a challenge for machine learning which arises when we deal with data in high dimensional spaces it describes the problems that occur when the number of features or dimensions in datasets increase significantly. as the number of dimension increases the available data points becomes increasingly spread out or sparse in the high dimensional space which makes it harder to find meaningful patterns and relationships. can we learnt about how to address this curse like you can use dimensionality reduction for feature selection or regularisation or we can increase the number of data. and then we learnt that if y is a function of different kinds of x then whether xi is dependent on some other x i not = j . we can find this with the help of creating regression models between xi and all other xs. we can also use this variance inflation factor and once we found the value we can start eliminating the ones with larger variance inflation factors one by one.,in today's class first we discuss the solution for the midsem examination. after that one of the ta gave us some comments about our e3 assignment. and after that we learnt about the codes of dimensionality which says that two less data chasing too many features. it is basically a challenge for machine learning which arises when we deal with data in high dimensional spaces it describes the problems that occur when the number of features or dimensions in datasets increase significantly. as the number of dimension increases the available data points becomes increasingly spread out or sparse in the high dimensional space which makes it harder to find meaningful patterns and relationships. can we learnt about how to address this curse like you can use dimensionality reduction for feature selection or regularisation or we can increase the number of data. and then we learnt that if y is a function of different kinds of x then whether xi is dependent on some other x i not = j . we can find this with the help of creating regression models between xi and all other xs. we can also use this variance inflation factor and once we found the value we can start eliminating the ones with larger variance inflation factors one by one.,0,-5.8036947,6.6987443,9.441019,7.098093,"models, feature, features"
497,"this lecture focuses on multiple linear regression (mlr) and its implementation in python.  while mlr has a closed-form solution, it's computationally expensive for large datasets. therefore, gradient descent is the preferred optimization method.

a crucial practice in machine learning is splitting the available data into training and testing sets.  a common split is 80% for training and 20% for testing, performed randomly.  this allows for evaluating the model's performance on unseen data.  two sets of evaluation metrics are generated: one for the training data and one for the test data. a good model exhibits strong performance on both sets.  if the model performs well on the training data but poorly on the test data, it's a sign of overfitting. conversely, poor performance on both sets indicates underfitting.

the ""multiple r"" metric is simply the square root of r-squared.  it represents the correlation between the multiple independent variables (x) and the dependent variable (y).  adjusted r-squared is a modified version of r-squared, calculated using the residual sum of squares (rss) and the total sum of squares (tss).  the formula incorporates  n-1 (degrees of freedom) because we lose a degree of freedom for each estimated parameter. adjusted r-squared penalizes the model if the rss doesn't decrease sufficiently relative to the increase in the number of predictors.  it helps to prevent overfitting by considering model complexity.  it's important to remember that mlr doesn't always result in a straight line; it can model more complex relationships.

the lecture then transitions to implementing mlr in python using the sklearn library.  sklearn is excellent for model building, predictions, and handling large datasets.  however, it can lack fine-grained control for researchers needing detailed model analysis.  to assess the model's fit, the distribution of residuals (errors) is examined using a histogram and a q-q plot.  ideally, the residuals should be normally distributed.

for more in-depth analysis, the statsmodels library is introduced.  statsmodels provides access to a wider range of statistical metrics.  one such metric is the ""omnibus"" test, which combines skewness and kurtosis to assess how closely the residuals approximate a normal distribution.  the jarque-bera test is another normality test for residuals.  a desirable outcome is a non-significant p-value (typically greater than 0.05) and a test statistic below a certain threshold (e.g., 2), indicating that the residuals are likely normally distributed.","this lecture focuses on multiple linear regression (mlr) and its implementation in python. while mlr has a closed-form solution, it's computationally expensive for large datasets. therefore, gradient descent is the preferred optimization method. a crucial practice in machine learning is splitting the available data into training and testing sets. a common split is 80% for training and 20% for testing, performed randomly. this allows for evaluating the model's performance on unseen data. two sets of evaluation metrics are generated: one for the training data and one for the test data. a good model exhibits strong performance on both sets. if the model performs well on the training data but poorly on the test data, it's a sign of overfitting. conversely, poor performance on both sets indicates underfitting. the ""multiple r"" metric is simply the square root of r-squared. it represents the correlation between the multiple independent variables (x) and the dependent variable (y). adjusted r-squared is a modified version of r-squared, calculated using the residual sum of squares (rss) and the total sum of squares (tss). the formula incorporates n-1 (degrees of freedom) because we lose a degree of freedom for each estimated parameter. adjusted r-squared penalizes the model if the rss doesn't decrease sufficiently relative to the increase in the number of predictors. it helps to prevent overfitting by considering model complexity. it's important to remember that mlr doesn't always result in a straight line; it can model more complex relationships. the lecture then transitions to implementing mlr in python using the sklearn library. sklearn is excellent for model building, predictions, and handling large datasets. however, it can lack fine-grained control for researchers needing detailed model analysis. to assess the model's fit, the distribution of residuals (errors) is examined using a histogram and a q-q plot. ideally, the residuals should be normally distributed. for more in-depth analysis, the statsmodels library is introduced. statsmodels provides access to a wider range of statistical metrics. one such metric is the ""omnibus"" test, which combines skewness and kurtosis to assess how closely the residuals approximate a normal distribution. the jarque-bera test is another normality test for residuals. a desirable outcome is a non-significant p-value (typically greater than 0.05) and a test statistic below a certain threshold (e.g., 2), indicating that the residuals are likely normally distributed.",2,7.8549056,1.4396812,10.820528,4.8543544,"regression, regressions, features"
498,"polynomial regression extends linear regression by modeling the relationship between variables as a polynomial function, often using taylor expansion to express functions in powers of x for better approximation. some key points in feature engineering - simply adding more features doesnâ€™t always improve a model; adjusted râ² can start decreasing if the new features donâ€™t add real value. a lower p-value indicates a better model fit. feature engineering plays a key roleâ€”forward selection involves adding relevant features one by one using domain knowledge, while backward selection starts with multiple features and removes the irrelevant ones. exploratory data analysis (eda) helps determine which type of model may fit the data best. choosing the right model requires balancing complexity and performance, avoiding overfitting, using visualizations and key metrics. a good model should have error values within an acceptable range compared to the original variable range. logistic regression, often used for classification, finds boundaries between labeled observations, which may sometimes be non-linear.","polynomial regression extends linear regression by modeling the relationship between variables as a polynomial function, often using taylor expansion to express functions in powers of x for better approximation. some key points in feature engineering - simply adding more features doesn t always improve a model; adjusted r can start decreasing if the new features don t add real value. a lower p-value indicates a better model fit. feature engineering plays a key role forward selection involves adding relevant features one by one using domain knowledge, while backward selection starts with multiple features and removes the irrelevant ones. exploratory data analysis (eda) helps determine which type of model may fit the data best. choosing the right model requires balancing complexity and performance, avoiding overfitting, using visualizations and key metrics. a good model should have error values within an acceptable range compared to the original variable range. logistic regression, often used for classification, finds boundaries between labeled observations, which may sometimes be non-linear.",0,2.0532582,-2.7216249,9.798363,4.1952243,"models, feature, features"
499,"the class started with a brief discussion about the previous lecture, and then we moved on to logistic regression and regression with multiple variables. we looked at the methodology of assigning weights to different parameters to make the model more accurate. after this, we looked at the confusion matrix, which was used to check the quality of the model and different calculations for the four types of entries, namely true positive and negative and false positive and negative. the class concluded by discussing assignment one and how it can be improved.","the class started with a brief discussion about the previous lecture, and then we moved on to logistic regression and regression with multiple variables. we looked at the methodology of assigning weights to different parameters to make the model more accurate. after this, we looked at the confusion matrix, which was used to check the quality of the model and different calculations for the four types of entries, namely true positive and negative and false positive and negative. the class concluded by discussing assignment one and how it can be improved.",13,4.598062,-11.847667,9.47774,5.004145,"classification, classifying, classifications"
500,"in today's class, the professor discussed the 95% confidence interval for the beta1 value, explaining that if any beta1 value lies outside this interval, it is considered significant. he also covered session summary analysis and how it can be used to detect instances of copying. additionally, he introduced multiple linear regression, explaining that it involves more than one independent variable and emphasizing the importance of feature selection and feature engineering. he also highlighted that the goal is to minimize the sum of squared errors (sse).","in today's class, the professor discussed the 95% confidence interval for the beta1 value, explaining that if any beta1 value lies outside this interval, it is considered significant. he also covered session summary analysis and how it can be used to detect instances of copying. additionally, he introduced multiple linear regression, explaining that it involves more than one independent variable and emphasizing the importance of feature selection and feature engineering. he also highlighted that the goal is to minimize the sum of squared errors (sse).",13,22.029911,4.977296,12.665537,5.1862965,"classification, classifying, classifications"
501,"in today's session we talked about mlr. mlr has more than one independent terms unlike slr. each independent variable has its own coefficient to ensure it's weighted appropriately.
multiple linear regression is suitable when multiple factors affect the outcome. there was also recap of t value .",in today's session we talked about mlr. mlr has more than one independent terms unlike slr. each independent variable has its own coefficient to ensure it's weighted appropriately. multiple linear regression is suitable when multiple factors affect the outcome. there was also recap of t value .,2,19.23068,3.0572824,12.811322,4.828606,"regression, regressions, features"
502,"we started off the class by discussing about outliers and how they can be found out using scatter plots and box plots. they can also be found out by descriptive statistics and line charts. however whether to actually ignore and drop the outliers or to perform some processing on them, depends upon the domain and thus required domain knowledge. sometimes, outliers can be hidden in the data, which can be observed by maybe rescaling the data or by dropping the true outliers and then observing the remaining data again. now we need to smoothen out our data and remove all the noise, in order to observe some trends in the actual data. thus, we perform data smothering, which could be done by various methods. we discussed about simple moving averages, where we consider a window around every data point and average the values in that window. the window width can be adjusted according to the level of smoothening. but in doing so, we need to take care about missing values as well, or else our smoothening algorithm might not work correctly. the higher the window size, the smoother our data becomes. we also have methods called exponential moving average or weighted moving average, which weighs the nearer points more as compared to the farther points. 
we then moved on to handling data where each column has a different scale. suppose we have a data where one column has very large values as compared to another column. this becomes a problem when we try to run a model like multiple regression, as the larger value data creates a very large maxima, which leads to all the other minimas getting overshadowed. hence gradient descent is not able to find the most optimum minima and the algorithm fails. hence we need to scale our data appropriately to prevent this. the most common method of scaling is to scale each column data to lie between 0 and 1. we then discussed that any algorithm which depends upon euclidean distances, like the k-means clustering algorithm, will be affected by the normalisation of the data. we could also perform an operation known as standardisation, which comes from the term â€˜standard normal distributionâ€™ i.e. n (0, 1). standardisation and normalisation do not change the shape of the distribution of the data.
the difference between the mean and the median of the data also gives us an idea about the skewness of the data, where a larger difference indicates more skewness. 
we then discussed about various other transformations like logarithmic transformation, where we de-emphasise the higher values. this transformation actually changes the shape of the distribution of the data. 
we then moved on to data imbalances, which is mainly used in the context of classification. it means that data from one class is highly under-represented as compared to the other classes. in such cases, we can either under-sample the majority class, or generate data belonging to the minority class. we could use smote algorithm, which creates synthetic samples based on linear interpolation between the existing samples. the hyper parameters for this algorithm are the number of neighbours we want and the smote percentage. we also have tomek links, which under sample the majority class data.","we started off the class by discussing about outliers and how they can be found out using scatter plots and box plots. they can also be found out by descriptive statistics and line charts. however whether to actually ignore and drop the outliers or to perform some processing on them, depends upon the domain and thus required domain knowledge. sometimes, outliers can be hidden in the data, which can be observed by maybe rescaling the data or by dropping the true outliers and then observing the remaining data again. now we need to smoothen out our data and remove all the noise, in order to observe some trends in the actual data. thus, we perform data smothering, which could be done by various methods. we discussed about simple moving averages, where we consider a window around every data point and average the values in that window. the window width can be adjusted according to the level of smoothening. but in doing so, we need to take care about missing values as well, or else our smoothening algorithm might not work correctly. the higher the window size, the smoother our data becomes. we also have methods called exponential moving average or weighted moving average, which weighs the nearer points more as compared to the farther points. we then moved on to handling data where each column has a different scale. suppose we have a data where one column has very large values as compared to another column. this becomes a problem when we try to run a model like multiple regression, as the larger value data creates a very large maxima, which leads to all the other minimas getting overshadowed. hence gradient descent is not able to find the most optimum minima and the algorithm fails. hence we need to scale our data appropriately to prevent this. the most common method of scaling is to scale each column data to lie between 0 and 1. we then discussed that any algorithm which depends upon euclidean distances, like the k-means clustering algorithm, will be affected by the normalisation of the data. we could also perform an operation known as standardisation, which comes from the term standard normal distribution i.e. n (0, 1). standardisation and normalisation do not change the shape of the distribution of the data. the difference between the mean and the median of the data also gives us an idea about the skewness of the data, where a larger difference indicates more skewness. we then discussed about various other transformations like logarithmic transformation, where we de-emphasise the higher values. this transformation actually changes the shape of the distribution of the data. we then moved on to data imbalances, which is mainly used in the context of classification. it means that data from one class is highly under-represented as compared to the other classes. in such cases, we can either under-sample the majority class, or generate data belonging to the minority class. we could use smote algorithm, which creates synthetic samples based on linear interpolation between the existing samples. the hyper parameters for this algorithm are the number of neighbours we want and the smote percentage. we also have tomek links, which under sample the majority class data.",9,-19.24028,10.634722,10.069998,9.991144,"dataâ, analyse, analyses"
503,"in today's lecture we discussed about sample and population differences, we discussed about statistics i.e. when the data is calculated for the sample and it is called parameter when it is calculated for the population. we can find the parameters (estimation) using linear regression to find the best fit line for the data. the equation of this best fit (regression) line is y = b0 + b1x, where b0,b1 are parameters. we also discussed on how to find the confidence interval for these parameter's estimates and using the sum of squares method to minimize the error. in this squares method we use sum of squares of (errors) distances as the errors can be +ve or -ve so directly the sum won't give minimum error as sum can be minimsed from -ve values of errors which may be of greater absolute values.
so now we will minimize this summation ei ^2    by partially differentiating w.r.t. b0 and b1. this will help us to determine the estimates of the parameters for the data, where ei = error = (yi- b0 + b1x). hence we get the final estimated values of these parameters as:
b0 = [mean(y)- b1 * mean(x)    and   b1 = mean(x*y) - mean(x)*mean(y)] / [mean(x^2)- (mean(x))^2]","in today's lecture we discussed about sample and population differences, we discussed about statistics i.e. when the data is calculated for the sample and it is called parameter when it is calculated for the population. we can find the parameters (estimation) using linear regression to find the best fit line for the data. the equation of this best fit (regression) line is y = b0 + b1x, where b0,b1 are parameters. we also discussed on how to find the confidence interval for these parameter's estimates and using the sum of squares method to minimize the error. in this squares method we use sum of squares of (errors) distances as the errors can be +ve or -ve so directly the sum won't give minimum error as sum can be minimsed from -ve values of errors which may be of greater absolute values. so now we will minimize this summation ei ^2 by partially differentiating w.r.t. b0 and b1. this will help us to determine the estimates of the parameters for the data, where ei = error = (yi- b0 + b1x). hence we get the final estimated values of these parameters as: b0 = [mean(y)- b1 * mean(x) and b1 = mean(x*y) - mean(x)*mean(y)] / [mean(x^2)- (mean(x))^2]",1,32.339325,-10.382587,16.710814,4.144241,"population, models, estimating"
504,"in the class first we started by taking some doubts in which we learned about the standard deviation of mean of samples which is equal to sigma divided by root x and its square is the variance of mean whereas sigma square is the variance of sample. can we learn about different methods like sturges theorem to calculate the number of bins in the histogram.
then we started with logistic regression . there are two different classifications then it is easy to classify them when the clusters are far away and when the clusters are not far away then we use probability like if probability is greater than 0.5 then it is x1 otherwise it is x2. our goal is to minimise the error between the predicted value and the actual value of all the samples. then we learnt thatp(y/x) = sigma(wtx+b). we want to maximise the likelihood of predicted outcomes being closed to targets if we get 100 percent correct classification of training data then there can be higher error in the test data we need to generalise the boundary of classification. then we learnt about confusion matrix and how to calculate it then we learn some definitions like accuracy which is how often is the classifier correct precision of the events you have detected like how many have you detected correctly recall is for the class events like how many of a class 1 events have you correctly detected. and f1 value is just the harmonic mean of precision and recall so that we get a performance mean of the precision and recall. then then our assignments were being assessed and the reviews were given about them in the later part of the class.",in the class first we started by taking some doubts in which we learned about the standard deviation of mean of samples which is equal to sigma divided by root x and its square is the variance of mean whereas sigma square is the variance of sample. can we learn about different methods like sturges theorem to calculate the number of bins in the histogram. then we started with logistic regression . there are two different classifications then it is easy to classify them when the clusters are far away and when the clusters are not far away then we use probability like if probability is greater than 0.5 then it is x1 otherwise it is x2. our goal is to minimise the error between the predicted value and the actual value of all the samples. then we learnt thatp(y/x) = sigma(wtx+b). we want to maximise the likelihood of predicted outcomes being closed to targets if we get 100 percent correct classification of training data then there can be higher error in the test data we need to generalise the boundary of classification. then we learnt about confusion matrix and how to calculate it then we learn some definitions like accuracy which is how often is the classifier correct precision of the events you have detected like how many have you detected correctly recall is for the class events like how many of a class 1 events have you correctly detected. and f1 value is just the harmonic mean of precision and recall so that we get a performance mean of the precision and recall. then then our assignments were being assessed and the reviews were given about them in the later part of the class.,10,13.437145,-17.223434,8.985048,-1.319901,"classifications, histograms, histogram"
505,"when predicting with a given sample size using linear regression (or any regression), there are limits. we can make predictions, but only up to a certain point , not far beyond the sample.

time series analysis: time series analysis helps to predict beyond the sample size. 

understanding f(x): in the equation ð‘“(ð‘¥)=ð‘¦ , the trend of the data is the signal, while the deviations are the noise. if there is a trend in the noise, it means your model has not captured the trend perfectly.

histogram: a histogram is simply a frequency chart that shows how often different values appear in a dataset.

a good model: a good model is one that explains most of the variation in the data.

in simple linear regression (slr):
sst=sse+ssr
where:
sse: sum of squares of errors, or the variation not explained by the regression line.
ssr: sum of squares of regression, or the total variation explained by the regression line.

r-squared (râ²):
râ² = ssr/sst = 1 - sse/sst 
râ² is the coefficient of determination, which measures how well the regression model explains the variation in the data.

why râ² and not r? in simple linear regression (slr), the coefficient of determination (râ²) is exactly the same as the square of the correlation coefficient between ð‘¥ and ð‘¦ this is why we use râ²  rather than ð‘….

we also created an excel program for linear regression and learned about some of the major terms used to evaluate how well the model explains the variation in the data.","when predicting with a given sample size using linear regression (or any regression), there are limits. we can make predictions, but only up to a certain point , not far beyond the sample. time series analysis: time series analysis helps to predict beyond the sample size. understanding f(x): in the equation ( )= , the trend of the data is the signal, while the deviations are the noise. if there is a trend in the noise, it means your model has not captured the trend perfectly. histogram: a histogram is simply a frequency chart that shows how often different values appear in a dataset. a good model: a good model is one that explains most of the variation in the data. in simple linear regression (slr): sst=sse+ssr where: sse: sum of squares of errors, or the variation not explained by the regression line. ssr: sum of squares of regression, or the total variation explained by the regression line. r-squared (r ): r = ssr/sst = 1 - sse/sst r is the coefficient of determination, which measures how well the regression model explains the variation in the data. why r and not r? in simple linear regression (slr), the coefficient of determination (r ) is exactly the same as the square of the correlation coefficient between and this is why we use r rather than . we also created an excel program for linear regression and learned about some of the major terms used to evaluate how well the model explains the variation in the data.",5,21.124071,0.027069895,13.664074,4.6654778,"regression, statistical, statistics"
506,"multi-class confusion matrix-
we started off with investigating the confusion matrix in multi-class classification. it is an important measure to gauge the performance of models by marking the correct predictions as well as the misclassifications across multiple classes, thus helping us identify where the model can be improved.

data understanding and preparation challenges-
much of the session was devoted to the issues involved in comprehending and preparing data, with a close look at problems concerning both the target variable (y) and feature variables (x).

issues with y:
  data unavailability: in case of missing target data, unsupervised learning methods can be employed.
  incorrect data: incorrect values need to be checked manually or automatically.
  not enough data: this requires collecting more data or simulating/generating it.
  too much data: dealing with too much data can involve statistical sampling, big data 
   techniques, or binning techniques.

issues with x:
  within columns: problems like inconsistent formatting and missing values.
  across columns: inter-feature relationships can cause redundancy or bias, necessitating 
   proper correlation analysis and suitable adjustments.


crisp-dm methodology-
the crisp-dm methodology was presented as a guide for data science projects. its six stagesâ€”business understanding, data understanding, data preparation, modeling, evaluation, and deploymentâ€”ensure that each step, from setting objectives to deploying the final model, is methodically tackled.

exploratory data analysis (eda)-
eda was emphasized as an important process for summarizing data attributes and discovering patterns or trends. it plays a critical role in:
- developing insights into data
- detecting anomalies
- hypothesis testing

practical eda presentation-
our ta shared real-life examples with multiple datasets. one interesting example was the ""aqi readings for maharashtra"" dataset, where missing values were handled. methods like imputation with mean, median, or mode, and even more sophisticated model-based methods were proposed to efficiently handle missing data.

handling outliers-
detection and treatment of outliers were also covered in the session:
detection methods: techniques such as box plots, standard deviation analysis, dbscan, and isolation forest were illustrated.
handling methods: techniques such as data trimming or capping by quartile ranges were explained, stating that although the median is resistant to outliers, the mean is greatly impacted.","multi-class confusion matrix- we started off with investigating the confusion matrix in multi-class classification. it is an important measure to gauge the performance of models by marking the correct predictions as well as the misclassifications across multiple classes, thus helping us identify where the model can be improved. data understanding and preparation challenges- much of the session was devoted to the issues involved in comprehending and preparing data, with a close look at problems concerning both the target variable (y) and feature variables (x). issues with y: data unavailability: in case of missing target data, unsupervised learning methods can be employed. incorrect data: incorrect values need to be checked manually or automatically. not enough data: this requires collecting more data or simulating/generating it. too much data: dealing with too much data can involve statistical sampling, big data techniques, or binning techniques. issues with x: within columns: problems like inconsistent formatting and missing values. across columns: inter-feature relationships can cause redundancy or bias, necessitating proper correlation analysis and suitable adjustments. crisp-dm methodology- the crisp-dm methodology was presented as a guide for data science projects. its six stages business understanding, data understanding, data preparation, modeling, evaluation, and deployment ensure that each step, from setting objectives to deploying the final model, is methodically tackled. exploratory data analysis (eda)- eda was emphasized as an important process for summarizing data attributes and discovering patterns or trends. it plays a critical role in: - developing insights into data - detecting anomalies - hypothesis testing practical eda presentation- our ta shared real-life examples with multiple datasets. one interesting example was the ""aqi readings for maharashtra"" dataset, where missing values were handled. methods like imputation with mean, median, or mode, and even more sophisticated model-based methods were proposed to efficiently handle missing data. handling outliers- detection and treatment of outliers were also covered in the session: detection methods: techniques such as box plots, standard deviation analysis, dbscan, and isolation forest were illustrated. handling methods: techniques such as data trimming or capping by quartile ranges were explained, stating that although the median is resistant to outliers, the mean is greatly impacted.",9,-16.48201,19.488567,8.583061,8.895642,"dataâ, analyse, analyses"
507,"we studied how to determine the population mean from sample means in today's session, taking into account both single and multiple samples. in order to estimate a population mean and determine the likelihood that a population mean will fall within a particular interval at a certain probability, we investigated confidence intervals. the formula for calculating the standard error of the mean is ïƒ/âˆšn. the sample mean is an estimate of the population mean, and the means of several samples combine to form a normal distribution. we also spoke about how to assess probability by calculating the p-value. we also learned how to calculate a linear regression model's coefficients (î²â‚€ and î²â‚). the anova table, which determines if there are statistically significant differences between the means of three or more samples, was the last topic we discussed.","we studied how to determine the population mean from sample means in today's session, taking into account both single and multiple samples. in order to estimate a population mean and determine the likelihood that a population mean will fall within a particular interval at a certain probability, we investigated confidence intervals. the formula for calculating the standard error of the mean is / n. the sample mean is an estimate of the population mean, and the means of several samples combine to form a normal distribution. we also spoke about how to assess probability by calculating the p-value. we also learned how to calculate a linear regression model's coefficients ( and ). the anova table, which determines if there are statistically significant differences between the means of three or more samples, was the last topic we discussed.",7,35.106655,4.743422,14.9711485,2.3701513,"statistics, statistical, statisticsâ"
508,"the lecture focused on statistical concepts in linear regression. if a modelâ€™s errors (residuals) are predictable (e.g., follow a pattern), it suggests the model failed to capture underlying trends, leading to biased predictions. to estimate the population mean from a single sample (e.g., 30 observations), we rely on the central limit theorem: even if the population isnâ€™t normal, the distribution of sample means becomes normal. using the sample mean and standard error (ïƒ/âˆšn), we build confidence intervals. a 95% confidence interval means 95% of samples taken repeatedly would have means within this range, reflecting uncertainty in our estimate.
in regression, the f-score evaluates if the modelâ€™s overall relationship is significant (comparing explained vs. unexplained variance). anova tables summarize this with metrics like degrees of freedom (df) and significance f (p-value for the model). standard error measures their precision, and p-values test if they meaningfully impact the outcome. these tools together assess the modelâ€™s validity and variable importance.","the lecture focused on statistical concepts in linear regression. if a model s errors (residuals) are predictable (e.g., follow a pattern), it suggests the model failed to capture underlying trends, leading to biased predictions. to estimate the population mean from a single sample (e.g., 30 observations), we rely on the central limit theorem: even if the population isn t normal, the distribution of sample means becomes normal. using the sample mean and standard error ( / n), we build confidence intervals. a 95% confidence interval means 95% of samples taken repeatedly would have means within this range, reflecting uncertainty in our estimate. in regression, the f-score evaluates if the model s overall relationship is significant (comparing explained vs. unexplained variance). anova tables summarize this with metrics like degrees of freedom (df) and significance f (p-value for the model). standard error measures their precision, and p-values test if they meaningfully impact the outcome. these tools together assess the model s validity and variable importance.",7,31.170813,2.2787344,14.657693,3.3422046,"statistics, statistical, statisticsâ"
509,"plotted a regression line of the data uploaded on moodle. the data contained x and y values, and we used excel formulas to find the values of a and b of the line y=ax+b. the regression line is computed and it provides the estimated function that is plotted alongside the scatter plot that illustrates the relationship between the x and y variables. the metrics used to evaluate the models' accuracyâ€”sse, mse, rmse, and mae were computed. to determine the degree of unpredictability, we plotted the histogram. next, we performed regression using anova utilizing the data analysis tool pak, uploading the t-statistics and p-values at specific confidence ranges. the standard error is represented by the standard deviation; hence, the larger the sample size, the smaller the error. after that, we looked at the central limit theorem which helps in deriving inferences about population from sample means.","plotted a regression line of the data uploaded on moodle. the data contained x and y values, and we used excel formulas to find the values of a and b of the line y=ax+b. the regression line is computed and it provides the estimated function that is plotted alongside the scatter plot that illustrates the relationship between the x and y variables. the metrics used to evaluate the models' accuracy sse, mse, rmse, and mae were computed. to determine the degree of unpredictability, we plotted the histogram. next, we performed regression using anova utilizing the data analysis tool pak, uploading the t-statistics and p-values at specific confidence ranges. the standard error is represented by the standard deviation; hence, the larger the sample size, the smaller the error. after that, we looked at the central limit theorem which helps in deriving inferences about population from sample means.",5,20.799421,-4.3689165,14.403146,4.9478583,"regression, statistical, statistics"
510,"in today's lecture we discussed that given a dataset, we can use various methods to make predictions of certain parameters using tools like simple linear regression, multiple linear regression, logistic regression, random forest, etc. we also discussed that we will move back and forth between ml and statistics in this course as they are closely related. then we had a look at levels a measurement (4 levels) which are nominal, ordinal, interval and ratio. nominal and ordinal are discrete while interval and ratio are continuos. there is no ordering in nominal measurement we can always assign numbers to the categories say for example male=0 and female=1. this comes with a very fundamental problem, it allows ordering because 0<1. we discussed a solution to this and it was by the use of vectors. we then had a look at 'interval' and understood how we cannot compare 10 degree c to be twice as 5 degree c because of its dependency on a reference. we then took a look at 'ratio' which allows us such comparisons. we discussed the use of 'levels of measurements' in 'ml category' as nominal and ordinal being useful for 'classification' and interval and ratio being useful for 'regression'. then we moved to the difference between supervised and unsupervised learning which essentially was the presence and absence of labels respectively. there are various models for unsupervised learning like k-means clustering, hierarchial clustering. we concluded our lecture with the difference between population and sample while talking about the duality of lack of data and abundance of data for a computer to handle.","in today's lecture we discussed that given a dataset, we can use various methods to make predictions of certain parameters using tools like simple linear regression, multiple linear regression, logistic regression, random forest, etc. we also discussed that we will move back and forth between ml and statistics in this course as they are closely related. then we had a look at levels a measurement (4 levels) which are nominal, ordinal, interval and ratio. nominal and ordinal are discrete while interval and ratio are continuos. there is no ordering in nominal measurement we can always assign numbers to the categories say for example male=0 and female=1. this comes with a very fundamental problem, it allows ordering because 0<1. we discussed a solution to this and it was by the use of vectors. we then had a look at 'interval' and understood how we cannot compare 10 degree c to be twice as 5 degree c because of its dependency on a reference. we then took a look at 'ratio' which allows us such comparisons. we discussed the use of 'levels of measurements' in 'ml category' as nominal and ordinal being useful for 'classification' and interval and ratio being useful for 'regression'. then we moved to the difference between supervised and unsupervised learning which essentially was the presence and absence of labels respectively. there are various models for unsupervised learning like k-means clustering, hierarchial clustering. we concluded our lecture with the difference between population and sample while talking about the duality of lack of data and abundance of data for a computer to handle.",4,-22.543339,-13.832371,1.8975364,0.49357304,"classification, classifying, classifications"
511,"practically we never have whole population data, we only have a sample data (called dataset). we should never use complete sample data to train the model rather only around 80% of data randomly, rest 20% is used to test and compare different models developed for the task.
multiple r = sqrt(r^2), this is a metric which tells us some kind of correlation between independent and dependent variables.
in general linear regression, we could have multiple features and even multiple outputs which have matrix form solution.
we want to know variance captured per independent variable(degree of freedom). if we know sample mean then we require one less sample so degree of freedom reduces. we use sample mean in both ssr and sst so when we adjust them with degree of freedom and use them in r^3 formula, it is called adjusted r^2.
adjusted r^2 = 1 - ((ssr/(n-k-1))/(sst/(n-1)))

quantile-quantile (qq) plot tells us how close distribution is to normal distribution, ideal plot is y=x.","practically we never have whole population data, we only have a sample data (called dataset). we should never use complete sample data to train the model rather only around 80% of data randomly, rest 20% is used to test and compare different models developed for the task. multiple r = sqrt(r^2), this is a metric which tells us some kind of correlation between independent and dependent variables. in general linear regression, we could have multiple features and even multiple outputs which have matrix form solution. we want to know variance captured per independent variable(degree of freedom). if we know sample mean then we require one less sample so degree of freedom reduces. we use sample mean in both ssr and sst so when we adjust them with degree of freedom and use them in r^3 formula, it is called adjusted r^2. adjusted r^2 = 1 - ((ssr/(n-k-1))/(sst/(n-1))) quantile-quantile (qq) plot tells us how close distribution is to normal distribution, ideal plot is y=x.",2,13.869123,1.5271751,11.835055,4.689688,"regression, regressions, features"
512,"todayâ€™s discussion is mainly based on principal component analysis and t-sne. vif is used to detect multicollinearity by calculating how much the variance of a regression coefficient increases. so, the highest vif values indicates that the column in very dependent on others. in these we set a threshold and column having vif value higher than that will be drop out. 
having too many features in data can cause problems like overfitting, slower computation, and lower accuracy. this is called the curse of dimensionality where more features exponentially increase the data needed for reliable results. to tackle this problem we use dimensionality reduction in which we reduces the number of features. pca is one of the methods to do it. it works by transforming high-dimensional data into a lower-dimensional space while maximizing the variance.
we create principal components which is the new axis where the data is most spread out. pcs are orthogonal to each others. the number of pcs are equal to the number of dimensions we have. pc1 are that axis which captures best variance in data. the general equation of principal components is pc1=c1x1 + c2x2 + c3x3 + c4x4 + .... where c1 is called loadings. predictions can be made through pca but we loses explain ability i.e. we can explain the results. principal component analysis have many features like data reduction, dimension reduction, prediction models, visualization (understanding the structure of data).
pca is sensitive to the data scale, we have to normalize the data before applying pca. we have seen an example on mnist dataset consists of handwritten digits. another method we learn is t-sne which stands for t-distribution stochastic neighbor encoding. it is a lossy transformation. it helps us to map multi-dimensional data to 2 dimensions or 3 dimensions.","today s discussion is mainly based on principal component analysis and t-sne. vif is used to detect multicollinearity by calculating how much the variance of a regression coefficient increases. so, the highest vif values indicates that the column in very dependent on others. in these we set a threshold and column having vif value higher than that will be drop out. having too many features in data can cause problems like overfitting, slower computation, and lower accuracy. this is called the curse of dimensionality where more features exponentially increase the data needed for reliable results. to tackle this problem we use dimensionality reduction in which we reduces the number of features. pca is one of the methods to do it. it works by transforming high-dimensional data into a lower-dimensional space while maximizing the variance. we create principal components which is the new axis where the data is most spread out. pcs are orthogonal to each others. the number of pcs are equal to the number of dimensions we have. pc1 are that axis which captures best variance in data. the general equation of principal components is pc1=c1x1 + c2x2 + c3x3 + c4x4 + .... where c1 is called loadings. predictions can be made through pca but we loses explain ability i.e. we can explain the results. principal component analysis have many features like data reduction, dimension reduction, prediction models, visualization (understanding the structure of data). pca is sensitive to the data scale, we have to normalize the data before applying pca. we have seen an example on mnist dataset consists of handwritten digits. another method we learn is t-sne which stands for t-distribution stochastic neighbor encoding. it is a lossy transformation. it helps us to map multi-dimensional data to 2 dimensions or 3 dimensions.",11,-18.387663,4.1851616,10.402362,13.336496,"pca, heatmap, heatmaps"
513,"in the class, i learned about the four levels of measurement: nominal (discrete, classification, e.g., gender or color), ordinal (discrete, classification, e.g., grades), interval (continuous, regression, e.g., temperature), and ratio (continuous, regression with a true zero). i explored the concept of a label as y = f(x), where x is the features vector, and unsupervised learning methods like k-means and hierarchical clustering. additionally, the idea of a sample as a subset of a population for analysis was discussed.","in the class, i learned about the four levels of measurement: nominal (discrete, classification, e.g., gender or color), ordinal (discrete, classification, e.g., grades), interval (continuous, regression, e.g., temperature), and ratio (continuous, regression with a true zero). i explored the concept of a label as y = f(x), where x is the features vector, and unsupervised learning methods like k-means and hierarchical clustering. additionally, the idea of a sample as a subset of a population for analysis was discussed.",4,-21.337114,-16.336517,2.0989602,0.37077177,"classification, classifying, classifications"
514,"today's lecture started with a discussion of the objectives of exploratory data analysis, which are to gain insights, identify anomalies, test hypotheses, and validate assumptions. we then looked at a dataset of course summary submissions from previous lectures, looking at features such as the length of each submission. some of the entries were much longer than others, suggesting potential outliers.  we used pivot tables to analyse this data.
then, we examined a problem of real interest involving optimization for a chemical plant, where several parameters were analysed through various plots. among the observations was a distribution that seemed to be normal, except for some odd readings at the low extreme, perhaps caused by faulty sensors. but instead of throwing them away, the value of domain knowledge was highlighted, with a mention of an instance where early data exclusion resulted in incorrect conclusions. another instance involving transformer failures was also mentioned. towards the end, the teaching assistants participated in the session to give feedback on an assignment e2 bringing the lecture to a close.","today's lecture started with a discussion of the objectives of exploratory data analysis, which are to gain insights, identify anomalies, test hypotheses, and validate assumptions. we then looked at a dataset of course summary submissions from previous lectures, looking at features such as the length of each submission. some of the entries were much longer than others, suggesting potential outliers. we used pivot tables to analyse this data. then, we examined a problem of real interest involving optimization for a chemical plant, where several parameters were analysed through various plots. among the observations was a distribution that seemed to be normal, except for some odd readings at the low extreme, perhaps caused by faulty sensors. but instead of throwing them away, the value of domain knowledge was highlighted, with a mention of an instance where early data exclusion resulted in incorrect conclusions. another instance involving transformer failures was also mentioned. towards the end, the teaching assistants participated in the session to give feedback on an assignment e2 bringing the lecture to a close.",9,-6.4134045,25.32256,8.312429,9.508661,"dataâ, analyse, analyses"
515,"in todayâ€™s class we continued our discussion on data sets. we seen plot of data set . there were cases when there was clear clusters of different types of points . there was case where there was no clear seperation of clusters. we then saw roc. roc (receiver operating characteristic) curve is a graphical representation used to evaluate the performance of a classification model, particularly for binary classification problems. it plots the true positive  rate (tpr) against the false positive rate (fpr) at various threshold settings. also we discussed about supervised and unsupervised clustering. types of clustering : k-means and hierarchical. k-means is an unsupervised machine learning algorithm used to group data into k clusters. it works by initializing k centroids, assigning each data point to the nearest centroid, and iteratively updating centroids by computing the mean of assigned points. hierarchical clustering is an unsupervised learning algorithm that builds a hierarchy of clusters without requiring a predefined number of clusters. the results are represented as a dendogram.","in today s class we continued our discussion on data sets. we seen plot of data set . there were cases when there was clear clusters of different types of points . there was case where there was no clear seperation of clusters. we then saw roc. roc (receiver operating characteristic) curve is a graphical representation used to evaluate the performance of a classification model, particularly for binary classification problems. it plots the true positive rate (tpr) against the false positive rate (fpr) at various threshold settings. also we discussed about supervised and unsupervised clustering. types of clustering : k-means and hierarchical. k-means is an unsupervised machine learning algorithm used to group data into k clusters. it works by initializing k centroids, assigning each data point to the nearest centroid, and iteratively updating centroids by computing the mean of assigned points. hierarchical clustering is an unsupervised learning algorithm that builds a hierarchy of clusters without requiring a predefined number of clusters. the results are represented as a dendogram.",8,-5.316657,-20.021694,6.404471,0.48871198,"classification, clusterings, classifying"
516,"we began the class with logistic regression and calculating weights of parameters in order to maximize likelihood of predicted outcomes being equal to targets. if t=1 we maximize p and if t=0 we maximize 1-p. maximizing likelihood simply means minimizing error. since there are product terms which are difficult to work with we take logarithm to get sum. a confusion matrix is created- true negative, true positive, false negative, false positive with the accuracy found using the formula- (true positive+ true negative)/total no. of events, precision defined as true positive/(true positive+true negative), recall as true positive + true negative /total and f1 value as harmonic mean of precision and accuracy","we began the class with logistic regression and calculating weights of parameters in order to maximize likelihood of predicted outcomes being equal to targets. if t=1 we maximize p and if t=0 we maximize 1-p. maximizing likelihood simply means minimizing error. since there are product terms which are difficult to work with we take logarithm to get sum. a confusion matrix is created- true negative, true positive, false negative, false positive with the accuracy found using the formula- (true positive+ true negative)/total no. of events, precision defined as true positive/(true positive+true negative), recall as true positive + true negative /total and f1 value as harmonic mean of precision and accuracy",10,14.716773,-22.068197,8.691901,-1.6135259,"classifications, histograms, histogram"
517,"in today's lecture, we started the discussion with population vs sample that the sample must be good and representative of the entire population. we use the sample to predict/estimate various attributes of the population.

attributes of population:
1. count (frequency)
2. mode
3. mean
4. median
5. standard deviation
6. variance

operations:
1. count
2. add
3. subtract
4. multiply
5. divide

if these attributes are calculated from the population they are known as parameters and if they are calculated from a sample they are known as statistic.
we want to estimate the parameters based on the statistics.

slr -> simple linear regression:
(it has only one predictor) 

we find the best fit line for the given data in the slr model.
y = b0 + b1x
where, y -> dependent variable, response variable, label
             x -> independent variable, feature, predictor

even a point can be considered as a model - although a very naive model.

here, b0 is called as bias as it accounts for all the other features which we didn't account for in the model.
b0 and b1 are the estimates of the population parameters (i.e. statistics).
since these are calculated from the sample their confidence is very low. so, we need to find a confidence interval containing these estimates in which we can confidently say that the real population parameters would lie in.
on increasing the interval width, the confidence also increases.

we have y(hat) = ax+b, for each data value we define error ei = yi - yi(hat).
now to get the best fit line we need to minimize the errors.
for that taking the sum of all the individual errors won't work as the positive and negative errors might get cancelled leading to zero net error even if the individual error values are large.
to address this issue we can minimize the sum of either |ei| or (ei)^2. 
we prefer the sum of (ei)^2 as:
1. it magnifies the error for a better fit.
2. it doesn't differentiate between different directions.

summation(ei)^2 = summation(yi - yi(hat))^2 
                               = summation (yi - ax -b)^2
here, we can minimize the error by making the derivative of error zero with respect to both a and b to get their values.
we have 'closed form' solution to calculate the values of a and b.
we can also observe from the equations that the point with average values of features as x-coordinate and average values of labels as y-coordinate lies on the best fit line.

b0 and b1 are 'point estimates' so we need to arrive at the possible interval within which these values lie such that there is a very high chance that b0p and b1p (i.e. the population parameters) will lie within these intervals respectively.","in today's lecture, we started the discussion with population vs sample that the sample must be good and representative of the entire population. we use the sample to predict/estimate various attributes of the population. attributes of population: 1. count (frequency) 2. mode 3. mean 4. median 5. standard deviation 6. variance operations: 1. count 2. add 3. subtract 4. multiply 5. divide if these attributes are calculated from the population they are known as parameters and if they are calculated from a sample they are known as statistic. we want to estimate the parameters based on the statistics. slr -> simple linear regression: (it has only one predictor) we find the best fit line for the given data in the slr model. y = b0 + b1x where, y -> dependent variable, response variable, label x -> independent variable, feature, predictor even a point can be considered as a model - although a very naive model. here, b0 is called as bias as it accounts for all the other features which we didn't account for in the model. b0 and b1 are the estimates of the population parameters (i.e. statistics). since these are calculated from the sample their confidence is very low. so, we need to find a confidence interval containing these estimates in which we can confidently say that the real population parameters would lie in. on increasing the interval width, the confidence also increases. we have y(hat) = ax+b, for each data value we define error ei = yi - yi(hat). now to get the best fit line we need to minimize the errors. for that taking the sum of all the individual errors won't work as the positive and negative errors might get cancelled leading to zero net error even if the individual error values are large. to address this issue we can minimize the sum of either |ei| or (ei)^2. we prefer the sum of (ei)^2 as: 1. it magnifies the error for a better fit. 2. it doesn't differentiate between different directions. summation(ei)^2 = summation(yi - yi(hat))^2 = summation (yi - ax -b)^2 here, we can minimize the error by making the derivative of error zero with respect to both a and b to get their values. we have 'closed form' solution to calculate the values of a and b. we can also observe from the equations that the point with average values of features as x-coordinate and average values of labels as y-coordinate lies on the best fit line. b0 and b1 are 'point estimates' so we need to arrive at the possible interval within which these values lie such that there is a very high chance that b0p and b1p (i.e. the population parameters) will lie within these intervals respectively.",1,35.252422,-9.468937,16.685541,4.0073776,"population, models, estimating"
518,"in today's class, we mainly discussed our mid-semester test. we first analyzed the data, which has 24 columns of blood parameters, 1 ailment column, and 2371 rows, providing information about the population residing in an area. based on blood samples taken, we identified whether they are healthy or have certain types of disease. we gave an open test so that we can use tools, but do not rely on them so much that they will solve the problem effectively like an expert. in the test, we can get solutions through multiple paths, we need to take one and justify it. based on the nature of the data, define what kind of problem you are going to solve. in the test, a simple classification model was needed, and we also needed to create a model and state why it is not working on the validation dataset. also, as per vp's choice, we should have made kde, not histogram. heading towards the solution, we must report missing values, either not consider them in model training or replace them with some values. for this, we need to know about the trend/distribution of the data, for this we can plot a scatter plot of all the columns, which shows the data was all over the plot, as there was no time series in the question, and hence randomness was there in the data. the histogram should be normal. we can use the mean to fix the missing values in certain cases only, but not always. then we need to consider the range of values across the columns, do we need to normalize or standardize the data, for this we plotted the histogram of all the blood parameters, in them some were unimodal, some were bimodal, and not normal and hence we do not get anything from these but these will be helpful when comparing with new dataset. we then see the descriptive statistics for the parameters and also compare with the validation data statistics and plots. at the minimum, we should get the data normalized between 0 and 1, hence we should have applied normalization on the datasets at least. we also see the box plot of numerical data, in which we see one column that was higher than others, hence the need for normalization is there. we also get to know about outliers through the columns and saw that there were no outliers in the columns. then we analyzed the ailment column using a bar chart, which can be made through a pivot table or counting in excel, and also through python. the heart disease population was much less compared to others, and thrombocytosis was also less than another disease in the initial dataset. hence, the model will not identify heart disease effectively and needs more data of these in the dataset for a better model. we plotted a simple correlation heatmap, which tells us that the columns are not correlated much and is good for us. we also discussed that tree-based models are good for modelling such kinds of data. then we split the dataset into test and train data (20% and 80%). we applied the random forest model on the data, and we saw that there was no misclassification for other diseases except heart diseases. then we looked at the confusion matrix of the validation data, which was not very good compared to the original dataset given. then we perform the distribution analysis to see why the model performed badly on the validation dataset. in that, we compare the plots of the original and validation dataset, which majorly shows the distributions are different for the datasets, which at last states that the data are from different populations and training the model on one will not perform well on the other. then we get the assessment of e3, which majorly states all have done it better, and the main problem was mse loss in q3 part d. then, at last we saw regarding the curse of dimensionality, in which we calculate vif and based on its value we can remove features.","in today's class, we mainly discussed our mid-semester test. we first analyzed the data, which has 24 columns of blood parameters, 1 ailment column, and 2371 rows, providing information about the population residing in an area. based on blood samples taken, we identified whether they are healthy or have certain types of disease. we gave an open test so that we can use tools, but do not rely on them so much that they will solve the problem effectively like an expert. in the test, we can get solutions through multiple paths, we need to take one and justify it. based on the nature of the data, define what kind of problem you are going to solve. in the test, a simple classification model was needed, and we also needed to create a model and state why it is not working on the validation dataset. also, as per vp's choice, we should have made kde, not histogram. heading towards the solution, we must report missing values, either not consider them in model training or replace them with some values. for this, we need to know about the trend/distribution of the data, for this we can plot a scatter plot of all the columns, which shows the data was all over the plot, as there was no time series in the question, and hence randomness was there in the data. the histogram should be normal. we can use the mean to fix the missing values in certain cases only, but not always. then we need to consider the range of values across the columns, do we need to normalize or standardize the data, for this we plotted the histogram of all the blood parameters, in them some were unimodal, some were bimodal, and not normal and hence we do not get anything from these but these will be helpful when comparing with new dataset. we then see the descriptive statistics for the parameters and also compare with the validation data statistics and plots. at the minimum, we should get the data normalized between 0 and 1, hence we should have applied normalization on the datasets at least. we also see the box plot of numerical data, in which we see one column that was higher than others, hence the need for normalization is there. we also get to know about outliers through the columns and saw that there were no outliers in the columns. then we analyzed the ailment column using a bar chart, which can be made through a pivot table or counting in excel, and also through python. the heart disease population was much less compared to others, and thrombocytosis was also less than another disease in the initial dataset. hence, the model will not identify heart disease effectively and needs more data of these in the dataset for a better model. we plotted a simple correlation heatmap, which tells us that the columns are not correlated much and is good for us. we also discussed that tree-based models are good for modelling such kinds of data. then we split the dataset into test and train data (20% and 80%). we applied the random forest model on the data, and we saw that there was no misclassification for other diseases except heart diseases. then we looked at the confusion matrix of the validation data, which was not very good compared to the original dataset given. then we perform the distribution analysis to see why the model performed badly on the validation dataset. in that, we compare the plots of the original and validation dataset, which majorly shows the distributions are different for the datasets, which at last states that the data are from different populations and training the model on one will not perform well on the other. then we get the assessment of e3, which majorly states all have done it better, and the main problem was mse loss in q3 part d. then, at last we saw regarding the curse of dimensionality, in which we calculate vif and based on its value we can remove features.",9,-8.239197,15.123992,9.686757,8.376744,"dataâ, analyse, analyses"
519,the lecture started with brief discussion on differences between population and samples. further we moved on with discussion on simple linear regression further discussing dependent and independent variables and via examples we learnt to plot scatter plots in order to gain insights of the data points and even used a point to model a collection of data. further we discussed liner regression in more detail talking about beta 1 and beta 0 which are used as estimates of population variables to gain insights about the data through an example of sales and advertisement as how sales depend on advertisement. the concept of bias was also introduced which accounts for independent variables/predictors which are not considered while modeling for the data. later the concept of error was introduced and the optimisation of beta 1 and beta 0 was carried out to get values a and b which is called a closed form solution. a concept of confidence intervals was also introduced briefly which is better than point estimates as of possible values of variables is given in terms of intervals with a specific confidence related to them.,the lecture started with brief discussion on differences between population and samples. further we moved on with discussion on simple linear regression further discussing dependent and independent variables and via examples we learnt to plot scatter plots in order to gain insights of the data points and even used a point to model a collection of data. further we discussed liner regression in more detail talking about beta 1 and beta 0 which are used as estimates of population variables to gain insights about the data through an example of sales and advertisement as how sales depend on advertisement. the concept of bias was also introduced which accounts for independent variables/predictors which are not considered while modeling for the data. later the concept of error was introduced and the optimisation of beta 1 and beta 0 was carried out to get values a and b which is called a closed form solution. a concept of confidence intervals was also introduced briefly which is better than point estimates as of possible values of variables is given in terms of intervals with a specific confidence related to them.,2,23.70314,8.105041,15.553211,4.371219,"regression, regressions, features"
520,"in todays class (5/3/25)
we started analyzing all the expectations that had to be carried out for midsem solutions.
it basically included performing eda (missing vales replaced with means, scatter plots to understand the nature of variables, watch out for the description of the data) we foundd that normalization was required to do so. next, computing outliers using box plots. also analyzing ailments to understand bars of diseases, where it was seen to drop the heart disease in the model. 
next there was discussion on exercise e3
in the later part we started with the curse of dimensionality to answer questions such as what will happen if features>>datapoints or features<<datapoints. increased sparsity, computational complexity and overfitting were some general issues. than we entered into collinearity and multi-collinearity ending with the variance inflation pattern discussion","in todays class (5/3/25) we started analyzing all the expectations that had to be carried out for midsem solutions. it basically included performing eda (missing vales replaced with means, scatter plots to understand the nature of variables, watch out for the description of the data) we foundd that normalization was required to do so. next, computing outliers using box plots. also analyzing ailments to understand bars of diseases, where it was seen to drop the heart disease in the model. next there was discussion on exercise e3 in the later part we started with the curse of dimensionality to answer questions such as what will happen if features>>datapoints or features<<datapoints. increased sparsity, computational complexity and overfitting were some general issues. than we entered into collinearity and multi-collinearity ending with the variance inflation pattern discussion",13,-5.5720944,12.543603,9.415307,7.925026,"classification, classifying, classifications"
521,"today we started off with talking a bit more about linear regression and looked at an application of feature engineering like how higher powers of independent variables could be used and regressed. it is also called polynomial regression. and we discussed about how to select features, and a way to remove them is by excluding the ones with low p-value. we moved on to talk about models which can handle more complex data like random forest and neural networks. we were also given a look into how neural networks work. and lastly we saw a bit of classification the form of logistic regression.","today we started off with talking a bit more about linear regression and looked at an application of feature engineering like how higher powers of independent variables could be used and regressed. it is also called polynomial regression. and we discussed about how to select features, and a way to remove them is by excluding the ones with low p-value. we moved on to talk about models which can handle more complex data like random forest and neural networks. we were also given a look into how neural networks work. and lastly we saw a bit of classification the form of logistic regression.",0,-3.1548972,-4.387009,8.969536,4.541542,"models, feature, features"
522,"in todays class (24/1/25),
we started with stating out a formal assumption: in closed form solution, errors are always normally distributed. whatever be the distribution of the population, when we take out multiple samples and plot a histogram as a continuous frequency function, it forms a normal distribution.
next we discussed about the 3 steps to perform while estimating population mean from a sample
1. calculate the mean of the sample
2. get the sample standard deviation and assume it to be close to population standard deviation
3. now you can get the standard error
than based on the confidence interval, you can get the most likely population mean based on your confidence interval.
we completed and understood the following using an exercise of a researcher planning to estimate the extra hours put forward by the manager towards the company.
the key note for the following was, lower the confidence you want for your measurement, the value estimation will turn down to a point.
later, using the experiment we understood the significance of t-value, p-value, analysis on variance (anova) and relation with sample variables and gaussian distribution.","in todays class (24/1/25), we started with stating out a formal assumption: in closed form solution, errors are always normally distributed. whatever be the distribution of the population, when we take out multiple samples and plot a histogram as a continuous frequency function, it forms a normal distribution. next we discussed about the 3 steps to perform while estimating population mean from a sample 1. calculate the mean of the sample 2. get the sample standard deviation and assume it to be close to population standard deviation 3. now you can get the standard error than based on the confidence interval, you can get the most likely population mean based on your confidence interval. we completed and understood the following using an exercise of a researcher planning to estimate the extra hours put forward by the manager towards the company. the key note for the following was, lower the confidence you want for your measurement, the value estimation will turn down to a point. later, using the experiment we understood the significance of t-value, p-value, analysis on variance (anova) and relation with sample variables and gaussian distribution.",7,36.006687,3.4130123,15.096892,2.3745828,"statistics, statistical, statisticsâ"
523,"we revised score, precision, recall, confusion matrix, accuracy and also saw playground website. we saw tpr and fpr and their importance. the classifier already detects true positives before detecting false positive (vertical). receiver operating characteristics: classifier discriminate between classes in a very nice way is reflected in roc curve. if probability greater than 0.5 than 1 otheriwise 0. at different number like 0.1 or 0.5 ; what will be the behaviour of classifier (value or probability) - sharpness and quality of classifier. more random curve is more random classifier. we saw definition of clustering and depth of clusters. sometimes part of eda : unsupervised learning in reality. two kinds : k means and hierarchical methods. how to decide cluster is good and matrix associated with it. assignments: pdf with size big no text.. we do eda on data. and then subject it to clustering: hierarchies and means (mean is representative of observations). randomly assign to mean to different points then reassign and repeat.( k means algo) so we discussed basically the algorithm of both clustering and visualisation.","we revised score, precision, recall, confusion matrix, accuracy and also saw playground website. we saw tpr and fpr and their importance. the classifier already detects true positives before detecting false positive (vertical). receiver operating characteristics: classifier discriminate between classes in a very nice way is reflected in roc curve. if probability greater than 0.5 than 1 otheriwise 0. at different number like 0.1 or 0.5 ; what will be the behaviour of classifier (value or probability) - sharpness and quality of classifier. more random curve is more random classifier. we saw definition of clustering and depth of clusters. sometimes part of eda : unsupervised learning in reality. two kinds : k means and hierarchical methods. how to decide cluster is good and matrix associated with it. assignments: pdf with size big no text.. we do eda on data. and then subject it to clustering: hierarchies and means (mean is representative of observations). randomly assign to mean to different points then reassign and repeat.( k means algo) so we discussed basically the algorithm of both clustering and visualisation.",8,-0.80399793,-20.01399,6.882565,0.3653535,"classification, clusterings, classifying"
524,"in today's session, we covered important data science concepts: regression metrics, sampling distributions, and the central limit theorem (clt). we learned some of the most important regression metrics: sse, mse, rmse, and mae. these metrics assess model performance and prediction accuracy.

we also discussed how regression coefficients from sample data estimate population parameters and the importance of evaluating their reliability. sampling distributions help in this by showing how sample means form a normal distribution, as explained by the clt, enabling population inferences and confidence interval calculations.

finally, we applied these concepts in excel by plotting error histograms and drawing the best-fit line using linear regression, reinforcing their practical relevance in building reliable models.","in today's session, we covered important data science concepts: regression metrics, sampling distributions, and the central limit theorem (clt). we learned some of the most important regression metrics: sse, mse, rmse, and mae. these metrics assess model performance and prediction accuracy. we also discussed how regression coefficients from sample data estimate population parameters and the importance of evaluating their reliability. sampling distributions help in this by showing how sample means form a normal distribution, as explained by the clt, enabling population inferences and confidence interval calculations. finally, we applied these concepts in excel by plotting error histograms and drawing the best-fit line using linear regression, reinforcing their practical relevance in building reliable models.",5,26.209915,1.998717,14.564575,4.307059,"regression, statistical, statistics"
525,"learnt about how output is related to input(empirical equations) and about levels of measurements which are:
1)nominal(discrete)-- classification
2)ordinal(discrete)-- classification
3)interval(continuous)-- regression
4)ratio(continuous)-- regression

y=f(x) where y- label and x- features ------  supervised learning
y=f(x) where y- label(not present) and x- features ------  unsupervised learning",learnt about how output is related to input(empirical equations) and about levels of measurements which are: 1)nominal(discrete)-- classification 2)ordinal(discrete)-- classification 3)interval(continuous)-- regression 4)ratio(continuous)-- regression y=f(x) where y- label and x- features ------ supervised learning y=f(x) where y- label(not present) and x- features ------ unsupervised learning,4,-17.92903,-15.590659,2.3795276,0.7967891,"classification, classifying, classifications"
526,"at the start of class, sir discussed the midsem question paper. sir explained the question paper and shared the approach of solving the question and problems associated with it. 
first step is exploratory data analysis. we first try to find the missing values in the data. there were four rows with missing values. one way to solve this is to drop the four rows. we can justify it because we have 2371 rows. another way to solve it is to fill in the missing values. plotting the parameters, we noticed that there is randomness in the data. the histogram would show a normal distribution. this suggests that even if we replace the missing data with the mean it would be ok. before filling the value, we check the range of value of parameters to check if we need to do standardization or normalization. we find that for some parameters the range is very small while for some the range is really high. we still don't have visualization for outliers. for this we use the box-plots. we first use box plot on all the columns to check if any column is different. then we check for individual outliers (surprisingly there are no outliers). for the column with ailments, we use a bar-chart or pie-chart. we notice that heart-diseases is under-represented. one way is oversampling or under sampling. as the difference is very large for the under sampled data, we can conclude that the model would not be able to heart-disease properly. 
heatmap is the first step in finding the correlation between the columns. we notice that there is no correlation between the columns. the point is that doing pair-wise correlation is suicidal. we need additional steps to decide if there is any correlation between columns. the problem here is of multi-collinearity. we can use variance inflation factor analysis. 

when we have two entirely different samples, we try going for kde plots. after creating the kde plots, we notice that the distribution of each column is different for both the samples. this suggests that the model fitted on one sample cannot be used on the other sample. this means that both the samples come from different populations. 

by the end of the class we started with a new topic: the curse of dimensionality. too less data chasing too much features. we don't have enough data for representation of the columns. this is a problem when dealing with high dimensional data. this problem increases sparsity. this results in loss in the discriminative power of the model. the consequences are: overfitting (model stick to the given data and are not much reliable). for this we can try to reduce the number of features. we can do feature selection, regularization or increase the amount of data. these solutions are easier said than done. we checked for variance inflation factor. if a feature has a vif more than 10, then it can be expressed as a linear combination of other features. we can hence eliminate the features with vif greater than 10.","at the start of class, sir discussed the midsem question paper. sir explained the question paper and shared the approach of solving the question and problems associated with it. first step is exploratory data analysis. we first try to find the missing values in the data. there were four rows with missing values. one way to solve this is to drop the four rows. we can justify it because we have 2371 rows. another way to solve it is to fill in the missing values. plotting the parameters, we noticed that there is randomness in the data. the histogram would show a normal distribution. this suggests that even if we replace the missing data with the mean it would be ok. before filling the value, we check the range of value of parameters to check if we need to do standardization or normalization. we find that for some parameters the range is very small while for some the range is really high. we still don't have visualization for outliers. for this we use the box-plots. we first use box plot on all the columns to check if any column is different. then we check for individual outliers (surprisingly there are no outliers). for the column with ailments, we use a bar-chart or pie-chart. we notice that heart-diseases is under-represented. one way is oversampling or under sampling. as the difference is very large for the under sampled data, we can conclude that the model would not be able to heart-disease properly. heatmap is the first step in finding the correlation between the columns. we notice that there is no correlation between the columns. the point is that doing pair-wise correlation is suicidal. we need additional steps to decide if there is any correlation between columns. the problem here is of multi-collinearity. we can use variance inflation factor analysis. when we have two entirely different samples, we try going for kde plots. after creating the kde plots, we notice that the distribution of each column is different for both the samples. this suggests that the model fitted on one sample cannot be used on the other sample. this means that both the samples come from different populations. by the end of the class we started with a new topic: the curse of dimensionality. too less data chasing too much features. we don't have enough data for representation of the columns. this is a problem when dealing with high dimensional data. this problem increases sparsity. this results in loss in the discriminative power of the model. the consequences are: overfitting (model stick to the given data and are not much reliable). for this we can try to reduce the number of features. we can do feature selection, regularization or increase the amount of data. these solutions are easier said than done. we checked for variance inflation factor. if a feature has a vif more than 10, then it can be expressed as a linear combination of other features. we can hence eliminate the features with vif greater than 10.",9,-8.602298,14.136948,9.782113,8.39984,"dataâ, analyse, analyses"
527,"in todays class (29/1/25),
first the discussion started with the doubts i raised in my last summary form about the real significance of statistically 0 which was clearly explained using the 95% confidence bound and marking out points a, b, c and d where we said d as statistically insignificant as it was out of the confidence interval whereas a and b being required interval and if there's a point c = 0, a and b can also be statistically 0 due to the parametric estimation in the confidence interval. then sir showed us how he calibrates and check out the submissions using the natural language processing. 
then we moved out to understanding feature engineering where we learned what exactly does feature mean about extracting data from the basic data values we have. like pixels from the images, embedded vector in the texts. 
lastly we started the discussion about the multiple linear regression (mlr), discussing about multiple features and equal number of variables (coefficients).
the closed form solution which we used into the slr was not on much into the fitting the mlr, thus sir initiated discussion on gradient descent algorithm engaging from the newton-raphson method by moving towards the minimizing loss moving towards negative gradient.","in todays class (29/1/25), first the discussion started with the doubts i raised in my last summary form about the real significance of statistically 0 which was clearly explained using the 95% confidence bound and marking out points a, b, c and d where we said d as statistically insignificant as it was out of the confidence interval whereas a and b being required interval and if there's a point c = 0, a and b can also be statistically 0 due to the parametric estimation in the confidence interval. then sir showed us how he calibrates and check out the submissions using the natural language processing. then we moved out to understanding feature engineering where we learned what exactly does feature mean about extracting data from the basic data values we have. like pixels from the images, embedded vector in the texts. lastly we started the discussion about the multiple linear regression (mlr), discussing about multiple features and equal number of variables (coefficients). the closed form solution which we used into the slr was not on much into the fitting the mlr, thus sir initiated discussion on gradient descent algorithm engaging from the newton-raphson method by moving towards the minimizing loss moving towards negative gradient.",2,15.493058,9.14233,12.525376,4.095987,"regression, regressions, features"
528,"today we learned that there are three ways to improve the quality of results: improving the sample, improving the method, and fine-tuning the method. then, we moved on to understanding mlr for non-linear cases. we used feature engineering by adding more terms, like polynomials and trigonometric functions which is called polynomial regression. 
we explored backward and forward feature selection and the problem of overfitting. then, we looked at non-linear examples involving trigonometry and straight lines. instead of using two different models for this, we wanted a single model that could handle both, and random forest turned out to be a good method for this.  we also learned about parametric and non-parametric methods and delta analysis.  we talked about classification, the difference between regression and logistic regression, and started using the term ""weights."" we also saw graphs and explanations for the sigmoid function.","today we learned that there are three ways to improve the quality of results: improving the sample, improving the method, and fine-tuning the method. then, we moved on to understanding mlr for non-linear cases. we used feature engineering by adding more terms, like polynomials and trigonometric functions which is called polynomial regression. we explored backward and forward feature selection and the problem of overfitting. then, we looked at non-linear examples involving trigonometry and straight lines. instead of using two different models for this, we wanted a single model that could handle both, and random forest turned out to be a good method for this. we also learned about parametric and non-parametric methods and delta analysis. we talked about classification, the difference between regression and logistic regression, and started using the term ""weights."" we also saw graphs and explanations for the sigmoid function.",0,-1.2061568,-2.0913138,9.20999,4.5548534,"models, feature, features"
529,"we learnt to estimate population parameters using sample statistics as samples are representative set of the population. depending on the data, models, like line and point, are employed. linear regression is used as an example to predict sales, with b0 and b1 being parameters for prediction of the population characteristics. then, learnt to find a confidence interval by minimizing the b0 term, that is bias,  which consider the extra variables too that influence sales. the best fit line is where the mean of the y variable and predictor values is located, and the sum of the squares of the errors is minimized.","we learnt to estimate population parameters using sample statistics as samples are representative set of the population. depending on the data, models, like line and point, are employed. linear regression is used as an example to predict sales, with b0 and b1 being parameters for prediction of the population characteristics. then, learnt to find a confidence interval by minimizing the b0 term, that is bias, which consider the extra variables too that influence sales. the best fit line is where the mean of the y variable and predictor values is located, and the sum of the squares of the errors is minimized.",1,31.86671,-5.833741,15.974888,3.975726,"population, models, estimating"
530,"we began by addressing a doubt from the previous lecture summaries. it was about the ways in which we can improve our results from the model apart from just increasing the sample size. so, we can also consider and try out different models before fixing one. we can compare the various metrics of these models and figure out the best one. if we want to stick at our original model, we can fine tune/ use it more appropriately to improve the quality of our results. one of the solutions suggested was that of grid search.
after having a brief discussion about the statistics from the summary submissions of the previous classes, we then moved on to understand the significance of feature engineering in improving our results from the model. by using feature engineering, we can create more features or destroy the already existing ones, to arrive at a set of most relevant and appropriate features that significantly describe the data. we can use something like polynomial regression. we already have one feature, x1 with us. we can raise it to higher and higher powers and introduce these as additional features in the mlr model. we can also have a combination of the polynomial and trigonometric functions like sin(x1). we observe that as we start adding more and more of these features to our model through feature engineering, the r2 value increases, however the adjusted r2 decreases. this is because we are adding more and more features to the model which do not significantly improve the results. hence, by using feature engineering and analyzing the scatter plots and histograms of errors, we can arrive at the best set of features for our model, which can explain much of the variation in the data.
all this is a part of exploratory data analysis (eda).
there are two types of feature engineering techniques - forward feature engineering and backward feature engineering. in forward feature engineering, we keep on adding more and more features in our model, by transforming the existing features. in backward feature engineering, we start eliminating these features one by one from the model, already having many features to arrive at the best set of features. the elimination is done on the basis of the p-values of the corresponding coefficients.
after this, we talked about multiple model creation. so, in real life scenarios we first have to get the data, then perform eda and preprocess it to get â€˜good dataâ€™. this good data can now be fed to multiple models. we can simultaneously compare the metrics of all the models and decide the best one from that. we can use multiple or single models to fit our data, depending on the variations in it. however, if a single model describes the data well, then it is always preferred over handling multiple models.
so, mainly there are two types of models- parametric and non-parametric. in parametric models, we can control the various parameters (regression coefficients) and we can also perform â€˜delta analysisâ€™ on it. this means we can find out how much the y value would change if the x value changes by an amount, say delta. slr and mlr are examples of such a model. however, in non-parametric models, like random forest, we cannot perform delta analysis, instead they give us better predictions than the parametric models. these models are also more flexible, i.e. they can adapt to different types of data well.
one of the models which we used on the data was the artificial neural networks (ann). ann consists of â€˜hidden layer(s)â€™ which have nodes. these nodes map/ link to each and every other node in the network to form â€˜linksâ€™. these links have certain weights associated with them. the y value is a function of these weights and the x values. these weights keep changing and are recalculated as more and more features are introduced. when there are more than 1 hidden layer in the network, we call it the â€˜deep learningâ€™ network.
more the layers, more capable the model becomes but at the same time we need to feed more data in the network. data (x values) is converted into information, which is represented by the weights.
we fitted many models on the same data set, and compared various metrics like r2, mse, etc. of these to determine the best model for the data. apart from comparing between models, we can also use the metrics, like mse within the model, to assess its validity.
we compared the relative difference between the metrics of the test and train data for a single model, to arrive at meaningful conclusions.
lastly, we started with classification models and discussed logistic regression in that. these classification models are used on discrete data like nominal and ordinal data to classify it into various classes. the y values are called â€˜labelsâ€™ and they determine a particular class. using logistic regression, we are trying to find out the line/ curve which separates or segregates the data into different clusters, such that misclassification despite of overlapping is minimized.
so, every point will have feature values x1,x2,... associated with it based on which it is given a y value, i.e. a label.
we concluded by discussing the sigmoid function and how it can be used to identify the equation of the line. further discussions to be continued in the next class.","we began by addressing a doubt from the previous lecture summaries. it was about the ways in which we can improve our results from the model apart from just increasing the sample size. so, we can also consider and try out different models before fixing one. we can compare the various metrics of these models and figure out the best one. if we want to stick at our original model, we can fine tune/ use it more appropriately to improve the quality of our results. one of the solutions suggested was that of grid search. after having a brief discussion about the statistics from the summary submissions of the previous classes, we then moved on to understand the significance of feature engineering in improving our results from the model. by using feature engineering, we can create more features or destroy the already existing ones, to arrive at a set of most relevant and appropriate features that significantly describe the data. we can use something like polynomial regression. we already have one feature, x1 with us. we can raise it to higher and higher powers and introduce these as additional features in the mlr model. we can also have a combination of the polynomial and trigonometric functions like sin(x1). we observe that as we start adding more and more of these features to our model through feature engineering, the r2 value increases, however the adjusted r2 decreases. this is because we are adding more and more features to the model which do not significantly improve the results. hence, by using feature engineering and analyzing the scatter plots and histograms of errors, we can arrive at the best set of features for our model, which can explain much of the variation in the data. all this is a part of exploratory data analysis (eda). there are two types of feature engineering techniques - forward feature engineering and backward feature engineering. in forward feature engineering, we keep on adding more and more features in our model, by transforming the existing features. in backward feature engineering, we start eliminating these features one by one from the model, already having many features to arrive at the best set of features. the elimination is done on the basis of the p-values of the corresponding coefficients. after this, we talked about multiple model creation. so, in real life scenarios we first have to get the data, then perform eda and preprocess it to get good data . this good data can now be fed to multiple models. we can simultaneously compare the metrics of all the models and decide the best one from that. we can use multiple or single models to fit our data, depending on the variations in it. however, if a single model describes the data well, then it is always preferred over handling multiple models. so, mainly there are two types of models- parametric and non-parametric. in parametric models, we can control the various parameters (regression coefficients) and we can also perform delta analysis on it. this means we can find out how much the y value would change if the x value changes by an amount, say delta. slr and mlr are examples of such a model. however, in non-parametric models, like random forest, we cannot perform delta analysis, instead they give us better predictions than the parametric models. these models are also more flexible, i.e. they can adapt to different types of data well. one of the models which we used on the data was the artificial neural networks (ann). ann consists of hidden layer(s) which have nodes. these nodes map/ link to each and every other node in the network to form links . these links have certain weights associated with them. the y value is a function of these weights and the x values. these weights keep changing and are recalculated as more and more features are introduced. when there are more than 1 hidden layer in the network, we call it the deep learning network. more the layers, more capable the model becomes but at the same time we need to feed more data in the network. data (x values) is converted into information, which is represented by the weights. we fitted many models on the same data set, and compared various metrics like r2, mse, etc. of these to determine the best model for the data. apart from comparing between models, we can also use the metrics, like mse within the model, to assess its validity. we compared the relative difference between the metrics of the test and train data for a single model, to arrive at meaningful conclusions. lastly, we started with classification models and discussed logistic regression in that. these classification models are used on discrete data like nominal and ordinal data to classify it into various classes. the y values are called labels and they determine a particular class. using logistic regression, we are trying to find out the line/ curve which separates or segregates the data into different clusters, such that misclassification despite of overlapping is minimized. so, every point will have feature values x1,x2,... associated with it based on which it is given a y value, i.e. a label. we concluded by discussing the sigmoid function and how it can be used to identify the equation of the line. further discussions to be continued in the next class.",0,1.1668674,-4.340761,9.387014,3.9249704,"models, feature, features"
531,"in today's session, we learn about the difference between sample and population. the variables in population are known as parameters and we use statistics on samples to estimate these parameters. there are various models and methods to estimate the parameters based on the type of data. the estimated function can be simple or difficult depending on the attributes. after estimating the parameters we give a confidence interval i.e how sure we are on the estimates we have provided. the estimates are generally range of values of a and b and larger the range higher is the confidence interval.
one such model was linear regression, where the best-fit representation is a straight line of the form y=a+ b*x where a and b are point estimates.
'a' is also known as bias which represents for all other variables/attributes we haven't taken into account. in linear regression we minimise the sum of square of errors as minimising the   sum of errors does not give accurate measurement. we also don't take absolute values of error as it is having manhattan distance and square area of influence. using this model we can interpolate this to predict the output based on the provided input with a given error limit.","in today's session, we learn about the difference between sample and population. the variables in population are known as parameters and we use statistics on samples to estimate these parameters. there are various models and methods to estimate the parameters based on the type of data. the estimated function can be simple or difficult depending on the attributes. after estimating the parameters we give a confidence interval i.e how sure we are on the estimates we have provided. the estimates are generally range of values of a and b and larger the range higher is the confidence interval. one such model was linear regression, where the best-fit representation is a straight line of the form y=a+ b*x where a and b are point estimates. 'a' is also known as bias which represents for all other variables/attributes we haven't taken into account. in linear regression we minimise the sum of square of errors as minimising the sum of errors does not give accurate measurement. we also don't take absolute values of error as it is having manhattan distance and square area of influence. using this model we can interpolate this to predict the output based on the provided input with a given error limit.",1,33.75399,-8.61633,16.529278,3.9884095,"population, models, estimating"
532,"in todays class (5/2/2025);
we started with discussion on doubt about how to improve the quality of data without increasing the data incorporating the principles of sample randomness. next we continued with the discussion about multiple linear regression recapping of previous classes.
then we moved on to the topic of polynomial regression where understood the inclusion of higher order variable creating a complex polynomial equation and performing regression analysis on the same. we then started our discussion on feature selection where we understood about forward(adding most relevant independent variables) and backward(eliminate variables with low significance) selection and their significance. 
we further dived into understanding the parametric and non parametric models which involves distinction on limiting on number of parameters to optimise the loss function. parametric methods are simples with limited parameters but hallucinate or find it difficult to cope up with complexities and tend to underfit. non parametric models like knn, random forest use the complete data capturing more intricate relationships with the rosk of overfitting. thus we need to analyse our data clearly to get the best optimisation possible.
lastly, a brief introduction on neural networks and classification problems was provided about how deep neurons are framed to carry out the complex tasks and role of activation functions to effectively achieve what we want.","in todays class (5/2/2025); we started with discussion on doubt about how to improve the quality of data without increasing the data incorporating the principles of sample randomness. next we continued with the discussion about multiple linear regression recapping of previous classes. then we moved on to the topic of polynomial regression where understood the inclusion of higher order variable creating a complex polynomial equation and performing regression analysis on the same. we then started our discussion on feature selection where we understood about forward(adding most relevant independent variables) and backward(eliminate variables with low significance) selection and their significance. we further dived into understanding the parametric and non parametric models which involves distinction on limiting on number of parameters to optimise the loss function. parametric methods are simples with limited parameters but hallucinate or find it difficult to cope up with complexities and tend to underfit. non parametric models like knn, random forest use the complete data capturing more intricate relationships with the rosk of overfitting. thus we need to analyse our data clearly to get the best optimisation possible. lastly, a brief introduction on neural networks and classification problems was provided about how deep neurons are framed to carry out the complex tasks and role of activation functions to effectively achieve what we want.",0,-3.2303712,-2.7211852,9.007622,4.5655212,"models, feature, features"
533,"in today's class we first discussed how to know from the confusion matrix that whether the column or the row represents the actual or the predicted data? and for that we can calculate the sum of the actual number of total data sets from the confusion matrix and if it is equal to the total values of the data which is known as the support in the data analysis hence that value is the actual data set. then we learnt about crisp-dm which is crocus industry stranded process for data mining. this includes majorly 6 steps - business understanding, data understanding which means collecting describing and exploring data and then verifying the data quality. then comes data preparation which means selecting cleaning constructing and then integrating the data and also formatting the data after it comes modelling evaluation and deployment which we will learn later in this course. then we learnt about exploratory data analysis and its basic definition. can we learnt about heteroscedasticity - which is that when there is a large difference in the variance throughout the data set and we need to convert them into homoscedasticity. then the ta taught us about box plots like if the median lies roughly in the middle of the boxplot then the distribution is certainly normal distribution when we learn about different types of missing data like mcar , mar , mnar and some examples for each of their scenarios when we learnt about how to fill the missing data like when to use mean and when to use median. then we learnt about outliers which are the data sets that differ significantly from the rest of the data sets and how to detect them . they are of also two types univariate and multivariate . for multivariate we can use dbscan. we can just isolate outliers make two different buckets of data and treat them differently just so that they don't affect our original model significantly. means are influenced by outliers but median is not so median is a good method for calculating outliers and for median we need to first arrange our data in increasing order","in today's class we first discussed how to know from the confusion matrix that whether the column or the row represents the actual or the predicted data? and for that we can calculate the sum of the actual number of total data sets from the confusion matrix and if it is equal to the total values of the data which is known as the support in the data analysis hence that value is the actual data set. then we learnt about crisp-dm which is crocus industry stranded process for data mining. this includes majorly 6 steps - business understanding, data understanding which means collecting describing and exploring data and then verifying the data quality. then comes data preparation which means selecting cleaning constructing and then integrating the data and also formatting the data after it comes modelling evaluation and deployment which we will learn later in this course. then we learnt about exploratory data analysis and its basic definition. can we learnt about heteroscedasticity - which is that when there is a large difference in the variance throughout the data set and we need to convert them into homoscedasticity. then the ta taught us about box plots like if the median lies roughly in the middle of the boxplot then the distribution is certainly normal distribution when we learn about different types of missing data like mcar , mar , mnar and some examples for each of their scenarios when we learnt about how to fill the missing data like when to use mean and when to use median. then we learnt about outliers which are the data sets that differ significantly from the rest of the data sets and how to detect them . they are of also two types univariate and multivariate . for multivariate we can use dbscan. we can just isolate outliers make two different buckets of data and treat them differently just so that they don't affect our original model significantly. means are influenced by outliers but median is not so median is a good method for calculating outliers and for median we need to first arrange our data in increasing order",9,-12.750267,18.7466,9.204645,9.070392,"dataâ, analyse, analyses"
534,"in todays class we had a a session which delved us back into excel and its data analysis tools. we also re visited the definition of histogram, which is a frequency chart. we also had a discussion about the apparent randomness of errors. following we talked about r square and the coefficient of relation an a bit more statistics.","in todays class we had a a session which delved us back into excel and its data analysis tools. we also re visited the definition of histogram, which is a frequency chart. we also had a discussion about the apparent randomness of errors. following we talked about r square and the coefficient of relation an a bit more statistics.",6,17.24676,-2.5319014,13.850868,5.2546606,"summarizing, summarize, summarization"
535,"heatmaps are useful visualization tools for pairwise analysis when plotting all parameters individually is impractical, although they lack detailed numeric insights. variance inflation factor (vif) helps detect multicollinearity by measuring how much a feature's variance increases due to correlations with other features; features with high vif values above a certain threshold should be removed first to reduce multicollinearity. principal component analysis (pca), derived from singular value decomposition (svd), transforms correlated variables into orthogonal principal components (pcs), each being a weighted sum of original parameters (weights known as loadings). pca reduces dimensionality, aids prediction models, data compression, visualization, and exploratory data analysis (eda), but cannot perform sensitivity or ""what-if"" analysis since pcs aren't directly interpretable as original parameters. pca is sensitive to data scaling, thus normalization is necessary. in contrast, t-distributed stochastic neighbor embedding (t-sne) employs a stochastic gradient descent approach based on t-distribution for nonlinear dimensionality reduction and visualization, preserving local relationships but losing exact numeric information (lossy transformation).","heatmaps are useful visualization tools for pairwise analysis when plotting all parameters individually is impractical, although they lack detailed numeric insights. variance inflation factor (vif) helps detect multicollinearity by measuring how much a feature's variance increases due to correlations with other features; features with high vif values above a certain threshold should be removed first to reduce multicollinearity. principal component analysis (pca), derived from singular value decomposition (svd), transforms correlated variables into orthogonal principal components (pcs), each being a weighted sum of original parameters (weights known as loadings). pca reduces dimensionality, aids prediction models, data compression, visualization, and exploratory data analysis (eda), but cannot perform sensitivity or ""what-if"" analysis since pcs aren't directly interpretable as original parameters. pca is sensitive to data scaling, thus normalization is necessary. in contrast, t-distributed stochastic neighbor embedding (t-sne) employs a stochastic gradient descent approach based on t-distribution for nonlinear dimensionality reduction and visualization, preserving local relationships but losing exact numeric information (lossy transformation).",11,-16.919634,4.391085,10.31326,13.082586,"pca, heatmap, heatmaps"
536,"we started our lecture with a doubt asked by a student. the question was how to improve the quality of results. we looked at 3 ways which can be used to do this. first is improving the sample. it can be done by either increasing the sample size or by increasing the quality of the sample. second was to improve the method in which we use multiple methods, compare them and select the best one. third one was an extension of the second point which was fine tuning the method which basically means using the method properly. we then began the theory by understanding the fact that multiple linear regression is not regression of linear parameters, but the linear combination of any parameter. we then looked at an example where the error was following some sort of sinusoidal pattern. we took multiple feature like x1 = x1, x2= x1^2, ....., x5 = sin(x1) and by intuition we know that the p values of x1,x2,x3,x4 will be high and that of x5 will be close to a zero. these feature making is called as feature engineering. there are two types of feature engineering - forward and backward. in forward feature engineering, we keep on adding feature one by one till we get a good model. in case of backward feature engineering, we add all the features once and remove irrelevant features one by one. we then started number crunching on excel and looked at models like slr, randomforest, xgboost, knn, neural networks. we looked at how the parametric models can be used for delta analysis and non parametric models like random forest can't. we then had a brief on neural networks where the dependent variable is a function of features and associated weights. neural network consists of nodes and links and they make up hidden layers. if there are more than one hidden layer, it is called as deep learning network. more layers with a small sample leads to overfit. we concluded our lecture with a small introduction to logistic regression, which involves finding boundries for classification.","we started our lecture with a doubt asked by a student. the question was how to improve the quality of results. we looked at 3 ways which can be used to do this. first is improving the sample. it can be done by either increasing the sample size or by increasing the quality of the sample. second was to improve the method in which we use multiple methods, compare them and select the best one. third one was an extension of the second point which was fine tuning the method which basically means using the method properly. we then began the theory by understanding the fact that multiple linear regression is not regression of linear parameters, but the linear combination of any parameter. we then looked at an example where the error was following some sort of sinusoidal pattern. we took multiple feature like x1 = x1, x2= x1^2, ....., x5 = sin(x1) and by intuition we know that the p values of x1,x2,x3,x4 will be high and that of x5 will be close to a zero. these feature making is called as feature engineering. there are two types of feature engineering - forward and backward. in forward feature engineering, we keep on adding feature one by one till we get a good model. in case of backward feature engineering, we add all the features once and remove irrelevant features one by one. we then started number crunching on excel and looked at models like slr, randomforest, xgboost, knn, neural networks. we looked at how the parametric models can be used for delta analysis and non parametric models like random forest can't. we then had a brief on neural networks where the dependent variable is a function of features and associated weights. neural network consists of nodes and links and they make up hidden layers. if there are more than one hidden layer, it is called as deep learning network. more layers with a small sample leads to overfit. we concluded our lecture with a small introduction to logistic regression, which involves finding boundries for classification.",0,0.8703741,-3.4589365,9.406578,4.2423477,"models, feature, features"
537,"in todayâ€™s class we spent our major time on discussing mid sem paper. we discussed about methods and procedures to be followed to solve the problems. the first step would be exploring dataset like the number of columns and rows, any missing values present or not. the second step would be data preprocessing steps like handling missing values, checking the descriptive statistics of dataset. if the data is random then replace missing values with statistics value. to find any outliers present in the dataset, we should create the box plot of each column. we would also see the distribution of â€œailmentsâ€ columns by creating bar chart, pie chart or any other plot. by creating this we would see that heart disease comes in very small count as compared to any other ailments. the best way to handle these problems is by looking manually on this. the scatter plot of columns should be created to see that the distribution is random or not. another plot we created is the heatmap which shows that columns are not correlated, but it is not sufficient. as there is correlation between some columns which can be checked by the variance inflation factor. for validation dataset, kde plot shows that the data is very different from that of training data.
the new topic we learned is curse of dimensionality â€”
it describes the problems that occur when the number of features in a dataset increases significantly. it arises when dealing with data in high-dimensional space. we can address this curse by
1. dimensionality reduction
2. increase the number of data points
3. feature selection 
4. regularisation 
variance inflation factor is a statistical measurement that indicates how correlated variables are in a regression model. it's used to detect multicollinearity between variables.","in today s class we spent our major time on discussing mid sem paper. we discussed about methods and procedures to be followed to solve the problems. the first step would be exploring dataset like the number of columns and rows, any missing values present or not. the second step would be data preprocessing steps like handling missing values, checking the descriptive statistics of dataset. if the data is random then replace missing values with statistics value. to find any outliers present in the dataset, we should create the box plot of each column. we would also see the distribution of ailments columns by creating bar chart, pie chart or any other plot. by creating this we would see that heart disease comes in very small count as compared to any other ailments. the best way to handle these problems is by looking manually on this. the scatter plot of columns should be created to see that the distribution is random or not. another plot we created is the heatmap which shows that columns are not correlated, but it is not sufficient. as there is correlation between some columns which can be checked by the variance inflation factor. for validation dataset, kde plot shows that the data is very different from that of training data. the new topic we learned is curse of dimensionality it describes the problems that occur when the number of features in a dataset increases significantly. it arises when dealing with data in high-dimensional space. we can address this curse by 1. dimensionality reduction 2. increase the number of data points 3. feature selection 4. regularisation variance inflation factor is a statistical measurement that indicates how correlated variables are in a regression model. it's used to detect multicollinearity between variables.",9,-9.133245,14.314027,9.746238,8.417741,"dataâ, analyse, analyses"
538,"today's lecture focused on the standard operational process for problem-solving, which consists of six cyclic steps: business understanding, data understanding, data preparation, modeling, evaluation, and deployment.

data understanding involves collecting initial data, describing it, exploring patterns, and verifying data quality. exploratory data analysis (eda) was discussed, emphasizing the identification of outliers using boxplots, studying feature distributions, correlations, and inter-feature relationships. recognizing trends in data is essential for understanding patterns and making informed decisions.

data preparation includes selecting, cleaning, constructing, integrating, and formatting data to ensure consistency and usability. missing values and quartiles were highlighted as crucial aspects of data preprocessing. the discussion also introduced univariate and multivariate data, emphasizing their significance in statistical analysis.","today's lecture focused on the standard operational process for problem-solving, which consists of six cyclic steps: business understanding, data understanding, data preparation, modeling, evaluation, and deployment. data understanding involves collecting initial data, describing it, exploring patterns, and verifying data quality. exploratory data analysis (eda) was discussed, emphasizing the identification of outliers using boxplots, studying feature distributions, correlations, and inter-feature relationships. recognizing trends in data is essential for understanding patterns and making informed decisions. data preparation includes selecting, cleaning, constructing, integrating, and formatting data to ensure consistency and usability. missing values and quartiles were highlighted as crucial aspects of data preprocessing. the discussion also introduced univariate and multivariate data, emphasizing their significance in statistical analysis.",6,-18.95908,21.152956,7.8667235,8.862875,"summarizing, summarize, summarization"
539,"today we learnt topic about data smoothening to reduce noise. one of the method used in data smoothening is moving averages. this method makes the data less noise and  helps in detecting the trend better , another method is exponential moving averages these are mainly utilised in time series here weighted averages are taken. its important to handle missing values and outliers before applying moving averages. few techniques to handle missing values are winzorisation , imputation, trimming, cap data and robust estimation. we then saw the importance of data scaling and its various methods like normalisation, standardisation and box-cox methods. all euclidian distance models are not immune to the data without data scaling. few models assume normal data distribution in such scenarios we have to use box-cox scaling method. standardisation and normalisation doesn't change the shape of the original distribution. log transformation is used when there is heteroscedasticity in the data that is variance keeps on changing across the dataset. log deemphasises large values. we later saw how to handle data imbalance which primarily occurs in clustering problems in this we have three options 
1) under sample the majority 
2)oversample the minority- that is duplication of the same minority data many times
3) produce synthetic data.
the first two methods are naã¯ve and not usually recommended.
to do the third method we use smote method which helps us in synthesising new data points for the minority class. it works on euclidean distance method it connects two minority points and forms a new point on the line (uses k-nearest neighbours method to do this) the data should be scaled before passing it on to smote, other methods used for synthetic data production are adasyn, borderline-smote. tomek links is a method used to under sample the majority class it deletes the majority points which are very close to the border of the minority points. usually smote and then tomek links are used to form a good dataset","today we learnt topic about data smoothening to reduce noise. one of the method used in data smoothening is moving averages. this method makes the data less noise and helps in detecting the trend better , another method is exponential moving averages these are mainly utilised in time series here weighted averages are taken. its important to handle missing values and outliers before applying moving averages. few techniques to handle missing values are winzorisation , imputation, trimming, cap data and robust estimation. we then saw the importance of data scaling and its various methods like normalisation, standardisation and box-cox methods. all euclidian distance models are not immune to the data without data scaling. few models assume normal data distribution in such scenarios we have to use box-cox scaling method. standardisation and normalisation doesn't change the shape of the original distribution. log transformation is used when there is heteroscedasticity in the data that is variance keeps on changing across the dataset. log deemphasises large values. we later saw how to handle data imbalance which primarily occurs in clustering problems in this we have three options 1) under sample the majority 2)oversample the minority- that is duplication of the same minority data many times 3) produce synthetic data. the first two methods are na ve and not usually recommended. to do the third method we use smote method which helps us in synthesising new data points for the minority class. it works on euclidean distance method it connects two minority points and forms a new point on the line (uses k-nearest neighbours method to do this) the data should be scaled before passing it on to smote, other methods used for synthetic data production are adasyn, borderline-smote. tomek links is a method used to under sample the majority class it deletes the majority points which are very close to the border of the minority points. usually smote and then tomek links are used to form a good dataset",9,-21.134928,11.713827,10.010756,10.155341,"dataâ, analyse, analyses"
540,"statistical significance vs. statistical similarity:-
statistically significant: indicates that the observed result is unlikely to be due to random chance, suggesting the effect or difference is meaningful.
statistically similar: implies no significant difference between the groups being compared, meaning the results are likely due to random variability rather than a true effect.

multiple linear regression (mlr):-
embedding vector: in machine learning, text is converted into vectors to allow models to process and learn from textual data.

feature and feature engineering:-
understanding and creating features is essential for building effective models. this involves selecting, transforming, and generating features that improve model performance.

slr (simple linear regression) vs. mlr (multiple linear regression):-
unlike simple linear regression, mlr does not have a closed-form solution for parameters. instead, an iterative approach is used to estimate the parameters.

gradient descent:-
this method is used to minimize error by selecting an initial random value for parameters and adjusting them iteratively in the direction that reduces error (e.g., adjusting beta values). the formula is:
beta_new = beta_old âˆ’ âˆ‡(ð½)ã—î·
where âˆ‡(ð½) is the gradient and î· is the learning rate.
solvers: solvers are algorithms used to optimize parameter values and minimize model error.

good regression model: a good regression model is indicated by a large f-value, suggesting that the model explains a significant portion of the variance in the data.

model evaluation context:-
model evaluation can be approached in two ways:
between two models: comparing the performance of two models to determine which performs better.
single model evaluation: assessing a single model's performance by analyzing its error metrics (e.g., mean squared error, r-squared, etc.).","statistical significance vs. statistical similarity:- statistically significant: indicates that the observed result is unlikely to be due to random chance, suggesting the effect or difference is meaningful. statistically similar: implies no significant difference between the groups being compared, meaning the results are likely due to random variability rather than a true effect. multiple linear regression (mlr):- embedding vector: in machine learning, text is converted into vectors to allow models to process and learn from textual data. feature and feature engineering:- understanding and creating features is essential for building effective models. this involves selecting, transforming, and generating features that improve model performance. slr (simple linear regression) vs. mlr (multiple linear regression):- unlike simple linear regression, mlr does not have a closed-form solution for parameters. instead, an iterative approach is used to estimate the parameters. gradient descent:- this method is used to minimize error by selecting an initial random value for parameters and adjusting them iteratively in the direction that reduces error (e.g., adjusting beta values). the formula is: beta_new = beta_old ( ) where ( ) is the gradient and is the learning rate. solvers: solvers are algorithms used to optimize parameter values and minimize model error. good regression model: a good regression model is indicated by a large f-value, suggesting that the model explains a significant portion of the variance in the data. model evaluation context:- model evaluation can be approached in two ways: between two models: comparing the performance of two models to determine which performs better. single model evaluation: assessing a single model's performance by analyzing its error metrics (e.g., mean squared error, r-squared, etc.).",0,2.1760778,-0.7293655,9.783189,4.443652,"models, feature, features"
541,"in today's class, we begin with a question that we have onlyâ€‚1 sample, e.g., 30 observations, so how do you estimate the population mean based on only 1 sample? is it possible. however,  when errors become predictive thenâ€‚it is not a regression model. not it will take normalâ€‚distribution up to the limit of law => universally at a point whatever the sample could be -> as at some threshold of our samples we will always get their multiple samples normal distribution. if we assume that we have obtained a sample from a population, the first thing we would do is calculate the mean and(assume that this mean is similarâ€‚to the population mean).

standard of deviation of the sampling distribution is population/rootâ€‚under number of observations. in the next step we studied about confidence interval which is actually the areaâ€‚under curve. when we have 95% confidence, auc is 0.95 and the observations lying outside this area areâ€‚0.025 on each side. another interesting point that we also found out was that, if the number of observations isâ€‚less than 30 we do not get normal distribution and the t distribution.","in today's class, we begin with a question that we have only 1 sample, e.g., 30 observations, so how do you estimate the population mean based on only 1 sample? is it possible. however, when errors become predictive then it is not a regression model. not it will take normal distribution up to the limit of law => universally at a point whatever the sample could be -> as at some threshold of our samples we will always get their multiple samples normal distribution. if we assume that we have obtained a sample from a population, the first thing we would do is calculate the mean and(assume that this mean is similar to the population mean). standard of deviation of the sampling distribution is population/root under number of observations. in the next step we studied about confidence interval which is actually the area under curve. when we have 95% confidence, auc is 0.95 and the observations lying outside this area are 0.025 on each side. another interesting point that we also found out was that, if the number of observations is less than 30 we do not get normal distribution and the t distribution.",7,37.98182,1.3032033,15.272224,2.4418592,"statistics, statistical, statisticsâ"
542,"sir started with the topic â€œpopulation vs. sampleâ€. sample should be good and representative. probably the two should mean the same thing. using the sample we predict/estimate certain characteristics of the population. 
analysing the entire population to understand its behaviour/characteristics is not feasible both â€“ time wise and money wise. so, we make use of (representative) samples.
attributes
attributes associated with samples as well as population are:
1.	count (frequency)
2.	mode
3.	mean
4.	standard deviation
5.	variance, and many others
operations
operations associated with samples as well as population are:
1.	count
2.	add
3.	subtract
4.	multiply
5.	divide and many others
then sir asked us to make a table which contains attributes and operations associated with each of the four levels of measurement.

using a sample, we can estimate the mean of the population.
attributes associated with the population are known as parameters while those associated with the sample as statistics.
we want to estimate the parameters based on the statistics.
sir then showed us a scatter plot (between sales and advertising expenditure) and presented a case of simple linear regression. simple linear regression has only one predictor.
y is the response variable/dependent variable/label
x is the predictor variable/independent variable/feature
sir was then showing us a scatter plot in which it looked like the points were scattered on the surface of a circular disc. then he pointed out a few things, which are:
1.	simple linear regression is not required in such cases.
2.	it would not be the best possible method in such cases.
3.	but if someone wants to apply it, then it can be applied.
4.	in such a case a point is a better approximation than a line, this can be understood by taking any value of the feature (x) and then by looking at the difference in its actual value at that x and the predicted value.
5.	each point can be considered as a model â€“ although a very naã¯ve one.

now sir started talking about bias which is the value of the y â€“ intercept in the equation obtained by simple linear regression. he said bias is the net some of all un-accounted variables. so, if we develop such a model in which we have taken into account all the variables which affect the dependent variable (y) then we would obtain an equation which would pass through the origin. 

then he said that as the size of the sample which we are using to estimate the population parameters increases the estimates become better. think of this by thinking that if the entire population is the sample, then the estimates would be exactly equal to the parameters, if we decrease the size of this sample by some amount then the new estimates would be lesser good estimates than they were previously. now keeping on decreasing the size of the sample and think about the estimatesâ€™ reliability (it decreases). prediction error would increase.

then sir talked about estimation interval or confidence interval. he said that finding î²0 and î²1 was just the start of machine learning. we need to take into account the error that would be there in our estimation. we have no basis to say that î²0 = î²0p.
then he gave the definition of confidence interval which is that with a% probability we can say that the parameter value will lie in an interval, that interval is known as confidence interval. typically, confidence intervals are of 90% or 95% and sometimes even 99%. 100% confidence interval is the interval from -infinity to +infinity.
then sir talked about how to define the best fit line. now there were four options:
1.	minimize sum of all ei
2.	minimize the sum of squares of all ei.
3.	minimize the sum of perpendicular distance between all pairs of {(xi,,yi) and (xi,yi-bar)}.
4.	minimize the sum of absolute value of all ei.
here ei = yi â€“ (yi-bar)    
(1) is rejected (4) is rejected because we donâ€™t want our solutions to be biased towards any direction so we choose (2).
a professor sir/maâ€™am from iit kanpur has done (3).
now refer sirâ€™s linear regression derivation and from there we see that the equation of the line that is obtained after (2) is such that the point made up of the mean of the labels and features lies on that line.
now for more notes about this class refer class notes, sirâ€™s lecture notes and simple linear regression proof. 
it would be better if one read them from these 3 places.","sir started with the topic population vs. sample . sample should be good and representative. probably the two should mean the same thing. using the sample we predict/estimate certain characteristics of the population. analysing the entire population to understand its behaviour/characteristics is not feasible both time wise and money wise. so, we make use of (representative) samples. attributes attributes associated with samples as well as population are: 1. count (frequency) 2. mode 3. mean 4. standard deviation 5. variance, and many others operations operations associated with samples as well as population are: 1. count 2. add 3. subtract 4. multiply 5. divide and many others then sir asked us to make a table which contains attributes and operations associated with each of the four levels of measurement. using a sample, we can estimate the mean of the population. attributes associated with the population are known as parameters while those associated with the sample as statistics. we want to estimate the parameters based on the statistics. sir then showed us a scatter plot (between sales and advertising expenditure) and presented a case of simple linear regression. simple linear regression has only one predictor. y is the response variable/dependent variable/label x is the predictor variable/independent variable/feature sir was then showing us a scatter plot in which it looked like the points were scattered on the surface of a circular disc. then he pointed out a few things, which are: 1. simple linear regression is not required in such cases. 2. it would not be the best possible method in such cases. 3. but if someone wants to apply it, then it can be applied. 4. in such a case a point is a better approximation than a line, this can be understood by taking any value of the feature (x) and then by looking at the difference in its actual value at that x and the predicted value. 5. each point can be considered as a model although a very na ve one. now sir started talking about bias which is the value of the y intercept in the equation obtained by simple linear regression. he said bias is the net some of all un-accounted variables. so, if we develop such a model in which we have taken into account all the variables which affect the dependent variable (y) then we would obtain an equation which would pass through the origin. then he said that as the size of the sample which we are using to estimate the population parameters increases the estimates become better. think of this by thinking that if the entire population is the sample, then the estimates would be exactly equal to the parameters, if we decrease the size of this sample by some amount then the new estimates would be lesser good estimates than they were previously. now keeping on decreasing the size of the sample and think about the estimates reliability (it decreases). prediction error would increase. then sir talked about estimation interval or confidence interval. he said that finding 0 and 1 was just the start of machine learning. we need to take into account the error that would be there in our estimation. we have no basis to say that 0 = 0p. then he gave the definition of confidence interval which is that with a% probability we can say that the parameter value will lie in an interval, that interval is known as confidence interval. typically, confidence intervals are of 90% or 95% and sometimes even 99%. 100% confidence interval is the interval from -infinity to +infinity. then sir talked about how to define the best fit line. now there were four options: 1. minimize sum of all ei 2. minimize the sum of squares of all ei. 3. minimize the sum of perpendicular distance between all pairs of {(xi,,yi) and (xi,yi-bar)}. 4. minimize the sum of absolute value of all ei. here ei = yi (yi-bar) (1) is rejected (4) is rejected because we don t want our solutions to be biased towards any direction so we choose (2). a professor sir/ma am from iit kanpur has done (3). now refer sir s linear regression derivation and from there we see that the equation of the line that is obtained after (2) is such that the point made up of the mean of the labels and features lies on that line. now for more notes about this class refer class notes, sir s lecture notes and simple linear regression proof. it would be better if one read them from these 3 places.",1,36.68145,-7.17089,16.501854,3.7547534,"population, models, estimating"
543,"today in class, i learned about feature engineering for images,  in the context of convolutional neural networks (cnns). the preparatory steps are image processing and text processing. it was explained that simply converting an image into a long vector of pixel values is not a good approach because it ignores how pixels relate to each other. instead, we need to consider these spatial relationships when creating features for machine learning models.
i also learned about convolutions, which are mathematical operations used in image processing. they help with tasks like edge detection, feature extraction, and applying filters to highlight important parts of an image.","today in class, i learned about feature engineering for images, in the context of convolutional neural networks (cnns). the preparatory steps are image processing and text processing. it was explained that simply converting an image into a long vector of pixel values is not a good approach because it ignores how pixels relate to each other. instead, we need to consider these spatial relationships when creating features for machine learning models. i also learned about convolutions, which are mathematical operations used in image processing. they help with tasks like edge detection, feature extraction, and applying filters to highlight important parts of an image.",13,-3.7749906,-11.926533,8.53504,4.35691,"classification, classifying, classifications"
544,"we analyzed different types of roc curves today. an roc curve charts the true positive rate against the false positive rate for classifying instances at different threshold levels. we established that a curve that is flat signifies that the classifier performs poorly. the flat curve means that even if the threshold is raised, there will be no significant increase in true positive rates, but the false positive rate will increase. it means that the classifier is overpredicting. during these instances, the auc value is low, meaning that the classifier does not perform well. in most cases, the diagonal line y=x which represents a random classifier and has an auc of 0.5 is weak, therefore, any part of the graph that is above this line represents improved model performance.","we analyzed different types of roc curves today. an roc curve charts the true positive rate against the false positive rate for classifying instances at different threshold levels. we established that a curve that is flat signifies that the classifier performs poorly. the flat curve means that even if the threshold is raised, there will be no significant increase in true positive rates, but the false positive rate will increase. it means that the classifier is overpredicting. during these instances, the auc value is low, meaning that the classifier does not perform well. in most cases, the diagonal line y=x which represents a random classifier and has an auc of 0.5 is weak, therefore, any part of the graph that is above this line represents improved model performance.",12,2.207682,-24.783546,7.4720335,0.747765,"classifiers, logistic, roc"
545,"we started by learning about the tool ""playground.tensorflow"" where we visualized the dataset and how we can adjust the neural network to get it as accurate as possible. we did changes by increasing and decreasing the number of layers, as we keep increasing the degree of freedom, the ann adjust to more changes and the boundary is closer to the actual boundary until a point, after which it leads to overfitting of data. we saw graphs of false positive and false negative and them overlapping. we would like to have classifier which can distinguish between two classes even if they overlap. we see this in the roc curve (receiver operator characteristic curve), the importance and significance of the area under roc curve. we saw different roc curves and understood what shape indicates best fit. sometimes, there is lesser data for one class as compared to the others, such cases in real life are when there are fraudulent cases, and the classifier usually misses this out. we observed a code in python which can plot the roc curves and saw the output figures. clustering is unsupervised type, and we looked at the code and algorithm for it. in the end we learnt about k means clustering. and in the end we saw hierarchical clustering method.","we started by learning about the tool ""playground.tensorflow"" where we visualized the dataset and how we can adjust the neural network to get it as accurate as possible. we did changes by increasing and decreasing the number of layers, as we keep increasing the degree of freedom, the ann adjust to more changes and the boundary is closer to the actual boundary until a point, after which it leads to overfitting of data. we saw graphs of false positive and false negative and them overlapping. we would like to have classifier which can distinguish between two classes even if they overlap. we see this in the roc curve (receiver operator characteristic curve), the importance and significance of the area under roc curve. we saw different roc curves and understood what shape indicates best fit. sometimes, there is lesser data for one class as compared to the others, such cases in real life are when there are fraudulent cases, and the classifier usually misses this out. we observed a code in python which can plot the roc curves and saw the output figures. clustering is unsupervised type, and we looked at the code and algorithm for it. in the end we learnt about k means clustering. and in the end we saw hierarchical clustering method.",8,-1.4355785,-20.954216,6.9512105,0.42575723,"classification, clusterings, classifying"
546,"class started with sir giving us a solution to the modem question explaining every step of the open ended question. here in question two the fresh dataset given was from a totally different population this was to be determined via plotting smooth curves which substitute histograms. then sir solved the problem himself starting with eda by checking missing values and dropping the rows with missing values. plotting the scatter plots for each columns suggest no specific trend in the data which suggests that the missing values can be filled with average values of the columns. then we checked the range of data the range varied drastically which suggested the need for normalisation. then we moved on to check for outliers by plotting box plots. a boxplot of all the columns suggested that there was a need for normalisation again as one of the columns were drastically different than others. the individual box plots suggested that there were no outliers. now the bar plots suggested that class heart diseases was under represented now the solution for this to drop the prediction of heart diseases due to under representation of heart diseases in the dataset here artificial data generation or under sampling are not feasible due to the huge difference in number of values of different classes relavtive to each other. next we generate inter correlation heatmap which concluded that the variables are not correlated to each other but a definitive test is needed to confirm this. further sir used random forest and gbm to fit the data. here the model predicted the second dataset poorly to justify this distributions of both the sample datasets were compared which suggested that they were from different. now sir emphasized the need for fundamentals. later e3 was reviewed by one of the teaching assistants and sir took review for the midsem test. later sir moved on to teach us about across the column problems starting with the curse of dimensionality which leads to overfitting, increased computation, etc these can be solved via dimensionality reduction done via feature selection and also regularisation to tune models to prevent over fitting. here the amount of data can also be increased. now to check interdependence of xi on xj, linear regression is done for each xi and r^2 is determined and variance inflation factors are calculated. now features with large vif are removed until the remaining features show a vif of less than 10.","class started with sir giving us a solution to the modem question explaining every step of the open ended question. here in question two the fresh dataset given was from a totally different population this was to be determined via plotting smooth curves which substitute histograms. then sir solved the problem himself starting with eda by checking missing values and dropping the rows with missing values. plotting the scatter plots for each columns suggest no specific trend in the data which suggests that the missing values can be filled with average values of the columns. then we checked the range of data the range varied drastically which suggested the need for normalisation. then we moved on to check for outliers by plotting box plots. a boxplot of all the columns suggested that there was a need for normalisation again as one of the columns were drastically different than others. the individual box plots suggested that there were no outliers. now the bar plots suggested that class heart diseases was under represented now the solution for this to drop the prediction of heart diseases due to under representation of heart diseases in the dataset here artificial data generation or under sampling are not feasible due to the huge difference in number of values of different classes relavtive to each other. next we generate inter correlation heatmap which concluded that the variables are not correlated to each other but a definitive test is needed to confirm this. further sir used random forest and gbm to fit the data. here the model predicted the second dataset poorly to justify this distributions of both the sample datasets were compared which suggested that they were from different. now sir emphasized the need for fundamentals. later e3 was reviewed by one of the teaching assistants and sir took review for the midsem test. later sir moved on to teach us about across the column problems starting with the curse of dimensionality which leads to overfitting, increased computation, etc these can be solved via dimensionality reduction done via feature selection and also regularisation to tune models to prevent over fitting. here the amount of data can also be increased. now to check interdependence of xi on xj, linear regression is done for each xi and r^2 is determined and variance inflation factors are calculated. now features with large vif are removed until the remaining features show a vif of less than 10.",9,-7.9129395,13.886801,9.692141,8.218818,"dataâ, analyse, analyses"
547,"we first did a comparative study
 between features and r-squared where we want to reduce the number of features because of the difficulty presented by dimensionality.
principal component analysis (pca) uses singular value decomposition (svd). in a data set with variables x1 and x2, it is feasible to obtain as many principal components (pcs) as the number of dimensions, i.e., pc_1 and pc_2, which are orthogonal to each other. the direction of these principal component axes is so oriented that the first component explains the maximum variability, and the next components explain successively less variability. at first, this is a two-dimensional problem, and pca successfully simplifies it into one dimension.  pca reduces the dimension of the data set. it is necessary to calculate the variance inflation factor (vif) before pca is applied. while principal components increase predictive power, their interpretability is severely compromised. ""what-if"" analyses are not possible with principal components since they are not real features but mathematical entities used for prediction. therefore, tracing predictions back to the original variables is compromised when pca is used.
pca is also used in exploratory data analysis (eda), which dictates the choice between going for ordinary regression or pca regression.
limitations of pca are its interpretative difficulties. but one strong point is that it can be used to visualize large datasets because the original data, which might have many dimensions (columns), can be reduced very much using the corresponding principal components.
another well-known dimensionality reduction method is t-distributed stochastic neighbor embedding (t-sne), which compares t-distribution with gaussian normal distribution. t-sne is a probabilistic method.
the process of converting raw data to labeled data through t-sne is achieved by developing a probability distribution for every data point that reflects its closeness to all other points in the dataset. in high-dimensional spaces, gaussian normal distribution is used, while t-distribution is used in low-dimensional situations. the probability of a point being close to another is highlighted in t-sne over its gaussian equivalent.","we first did a comparative study between features and r-squared where we want to reduce the number of features because of the difficulty presented by dimensionality. principal component analysis (pca) uses singular value decomposition (svd). in a data set with variables x1 and x2, it is feasible to obtain as many principal components (pcs) as the number of dimensions, i.e., pc_1 and pc_2, which are orthogonal to each other. the direction of these principal component axes is so oriented that the first component explains the maximum variability, and the next components explain successively less variability. at first, this is a two-dimensional problem, and pca successfully simplifies it into one dimension. pca reduces the dimension of the data set. it is necessary to calculate the variance inflation factor (vif) before pca is applied. while principal components increase predictive power, their interpretability is severely compromised. ""what-if"" analyses are not possible with principal components since they are not real features but mathematical entities used for prediction. therefore, tracing predictions back to the original variables is compromised when pca is used. pca is also used in exploratory data analysis (eda), which dictates the choice between going for ordinary regression or pca regression. limitations of pca are its interpretative difficulties. but one strong point is that it can be used to visualize large datasets because the original data, which might have many dimensions (columns), can be reduced very much using the corresponding principal components. another well-known dimensionality reduction method is t-distributed stochastic neighbor embedding (t-sne), which compares t-distribution with gaussian normal distribution. t-sne is a probabilistic method. the process of converting raw data to labeled data through t-sne is achieved by developing a probability distribution for every data point that reflects its closeness to all other points in the dataset. in high-dimensional spaces, gaussian normal distribution is used, while t-distribution is used in low-dimensional situations. the probability of a point being close to another is highlighted in t-sne over its gaussian equivalent.",11,-17.618847,3.1150796,10.311552,13.223982,"pca, heatmap, heatmaps"
548,"in today's lecture, we started with the plan of the schedule for the next 10 lectures and the details regarding our next group project. we then proceeded to function encoding, beginning with an example wherein a categorical variable (red, blue, green) was encoded into a function of three variables (). we considered two main types of problemsâ€”multiclass and multilabelâ€”and how they influence our approach. next we discussed binary encoding, a dense method that tidily encodes categorical data. we learned the sequential process of converting data with the help of this approach. proceeding, we discussed frequency encoding, where the category values are substituted with the frequency in the dataset, and target encoding, where all the occurrences of a category are substituted with the mean score over a cut-off point (2.5 in our case). other encoding techniques such as label encoding, one-hot encoding, and image encoding were discussed as well. finally, we examined feature binning using an example of data distributed randomly and discussed the method of processing test data.","in today's lecture, we started with the plan of the schedule for the next 10 lectures and the details regarding our next group project. we then proceeded to function encoding, beginning with an example wherein a categorical variable (red, blue, green) was encoded into a function of three variables (). we considered two main types of problems multiclass and multilabel and how they influence our approach. next we discussed binary encoding, a dense method that tidily encodes categorical data. we learned the sequential process of converting data with the help of this approach. proceeding, we discussed frequency encoding, where the category values are substituted with the frequency in the dataset, and target encoding, where all the occurrences of a category are substituted with the mean score over a cut-off point (2.5 in our case). other encoding techniques such as label encoding, one-hot encoding, and image encoding were discussed as well. finally, we examined feature binning using an example of data distributed randomly and discussed the method of processing test data.",3,-40.337055,5.8009925,0.47460803,6.048533,"categorical, categorization, categorise"
549,"in todays class (19/2/25), we learnt about the use of pivot tables. calculating max, min, average, sum and how mean deviates from the min and max. creating charts and understanding that how the tasks can be simplified using the pivot table in excel rather going for python codes. we carried out the following task using 2 case studies, 1 about our summaries and other was about the chemical plant which was having a lot of data around 241 columns. in a line, we performed eda using pivot tables and also learnt about the use of different charts to find the outliers, people with giving summary of 6k+ characters to 71 characters. finally delved into the discussion about exercise 2 assignment and ended the class.","in todays class (19/2/25), we learnt about the use of pivot tables. calculating max, min, average, sum and how mean deviates from the min and max. creating charts and understanding that how the tasks can be simplified using the pivot table in excel rather going for python codes. we carried out the following task using 2 case studies, 1 about our summaries and other was about the chemical plant which was having a lot of data around 241 columns. in a line, we performed eda using pivot tables and also learnt about the use of different charts to find the outliers, people with giving summary of 6k+ characters to 71 characters. finally delved into the discussion about exercise 2 assignment and ended the class.",6,-11.574208,28.36078,7.501766,10.205719,"summarizing, summarize, summarization"
550,"the concept of statistically significant and statistically similar was discussed in today's lecture. in a linear regression equation of form y=ax+b, there will be a gaussian distribution of ""a"" value and a particular zone (95%) symmetric around the mean. if any value lies in that particular zone, then that value is not significant and is called a statistically similar value, while those values lying outside the zone are called statistically significant. 
multiple linear regression (mlr) was discussed. multiple linear regression deals with more than one independent variable and talies the form as : y=a+bx1+cx2+dx3+.... . on the other hand simple linear regression deals with only one independent variable and talies the form as : y=ax+b. then mlr gradient descent approach was introduced, an optimization algorithm that helps to reach an optimal value. ( similar to newton raphson method)","the concept of statistically significant and statistically similar was discussed in today's lecture. in a linear regression equation of form y=ax+b, there will be a gaussian distribution of ""a"" value and a particular zone (95%) symmetric around the mean. if any value lies in that particular zone, then that value is not significant and is called a statistically similar value, while those values lying outside the zone are called statistically significant. multiple linear regression (mlr) was discussed. multiple linear regression deals with more than one independent variable and talies the form as : y=a+bx1+cx2+dx3+.... . on the other hand simple linear regression deals with only one independent variable and talies the form as : y=ax+b. then mlr gradient descent approach was introduced, an optimization algorithm that helps to reach an optimal value. ( similar to newton raphson method)",2,18.553888,7.315231,12.729489,4.3736787,"regression, regressions, features"
551,"we continued (by mainly looking at different statistical conditions) where we had stopped in the last class, and discussed the terms coming up from our extension (data analysis toolpack) and then looked at their interconnections and terminologies, what these values mean, their graphical interpretation, and conclusions with certain errors or uncertainties which come along with it.
we looked at a few cases of beta and beta 0, and their different specific case conditions
some of these terms were the p-value (and itâ€™s basic use, like it helps in our selection of feature selection) 
we also looked at multiple linear regression, and how some of the terms coming in our table were mainly relevant for multiple linear regression. we also looked at what the term anova means and the terms which it involves (f statistic and why it should be large, to be continued in the next class)","we continued (by mainly looking at different statistical conditions) where we had stopped in the last class, and discussed the terms coming up from our extension (data analysis toolpack) and then looked at their interconnections and terminologies, what these values mean, their graphical interpretation, and conclusions with certain errors or uncertainties which come along with it. we looked at a few cases of beta and beta 0, and their different specific case conditions some of these terms were the p-value (and it s basic use, like it helps in our selection of feature selection) we also looked at multiple linear regression, and how some of the terms coming in our table were mainly relevant for multiple linear regression. we also looked at what the term anova means and the terms which it involves (f statistic and why it should be large, to be continued in the next class)",13,4.4214473,17.35577,12.551606,5.8595014,"classification, classifying, classifications"
552,"sir began the class by summarising the progress of syllabus till date and also discussed future plans, announcing the group project which is going to be based on assessment of assignments submitted by the students themselves. the main focus of the project is feature engineering.
then we were taught feature encoding. here one hot encoding was taught. this was don't by taking an example of classification of colours via logistic regression further multi class and multi label problems were discussed and the application scenarios for these problems.  further label encoding was discussed were the independent variables are not supposed to be label encoding whereas the dependent variable works were with label encoding next a similar type of encoding ie integer encoding was also introduced where specific integers are encoded when the variable are associated to those specific values based on domain knowledge. effects of one hot encoding were discussed where the dataset becomes more sparse as more columns are added in one hot encoding and examples of use cases were discussed.
next binary encoding was discussed where it's a type of sudo one hot encoding as it eliminates the extra addition of columns. next other methods like frequency encoding and targent modelling were surfaces upon and their use cases were also explained were frequency modelling is the variables are assigned the frequency of their occorance and in target encoding the average of target value is taken. here emphasis was put on choosing the correct encoding method while considering repercussions. next the classification on continuous problems was discussed and the application scenario where the r2 value is less in a dataset independent of the fitting model was explained as the variance of the dataset is to high to be explained by curve fitting and r2 values drop here binning is done. next sir starting discussion on text processing ie how to convert text into numbers so that processing is efficient and useful here sir briefly introduced us to characterisation of words which mainly depends upon the context. tree bank and word net dictionaries are used to find synonyms, words with same meanings or different meanings for the same word. further the overall approach was discussed via an example were a dictionary is formed and the document is represented via the dictionary formed.","sir began the class by summarising the progress of syllabus till date and also discussed future plans, announcing the group project which is going to be based on assessment of assignments submitted by the students themselves. the main focus of the project is feature engineering. then we were taught feature encoding. here one hot encoding was taught. this was don't by taking an example of classification of colours via logistic regression further multi class and multi label problems were discussed and the application scenarios for these problems. further label encoding was discussed were the independent variables are not supposed to be label encoding whereas the dependent variable works were with label encoding next a similar type of encoding ie integer encoding was also introduced where specific integers are encoded when the variable are associated to those specific values based on domain knowledge. effects of one hot encoding were discussed where the dataset becomes more sparse as more columns are added in one hot encoding and examples of use cases were discussed. next binary encoding was discussed where it's a type of sudo one hot encoding as it eliminates the extra addition of columns. next other methods like frequency encoding and targent modelling were surfaces upon and their use cases were also explained were frequency modelling is the variables are assigned the frequency of their occorance and in target encoding the average of target value is taken. here emphasis was put on choosing the correct encoding method while considering repercussions. next the classification on continuous problems was discussed and the application scenario where the r2 value is less in a dataset independent of the fitting model was explained as the variance of the dataset is to high to be explained by curve fitting and r2 values drop here binning is done. next sir starting discussion on text processing ie how to convert text into numbers so that processing is efficient and useful here sir briefly introduced us to characterisation of words which mainly depends upon the context. tree bank and word net dictionaries are used to find synonyms, words with same meanings or different meanings for the same word. further the overall approach was discussed via an example were a dictionary is formed and the document is represented via the dictionary formed.",3,-40.352535,4.1963634,0.27795342,6.4306607,"categorical, categorization, categorise"
553,"in today's class, we revised some points, such as the p-value being less than 0.05 for the regression coefficient beta1 to be significant when zero is inside the confidence interval. then we saw that we did not use the model completely, we split it into 80% and 20% random parts, the 80% part serves as our training data, while the 20% part is used by us as test data. we first develop the model using training data and then apply the model to testing data. if the result of the training data is not as accurate as the testing data, the data is called overfit data. doesn't matter how good the results or graphs of training data are; if the testing graphs or results are not good, that model is not best suited for the data. multiple r, the term we got during data analysis using regression in excel, is the square root of r-squared. then we saw about adjusted r^2 which tells about how effective the adding of a new variable is. then we saw that during the calculation of the variance of the sample, we used (n-1) in the denominator instead of n, as we are using the mean already which has n observations in it already, so we have only n-1 unknowns. then we learned about the quantile-quantile plot which is plotted by separating the distribution of the data(errors) in some quantile. then we perform regression analysis using python in which we use python libraries like numpy, pandas, and sci-kit-learn. at last, we see the analysis of errors using various tests like omnibus which tells about the normality of tests, skewness, kurtosis, and jarque-bera test.","in today's class, we revised some points, such as the p-value being less than 0.05 for the regression coefficient beta1 to be significant when zero is inside the confidence interval. then we saw that we did not use the model completely, we split it into 80% and 20% random parts, the 80% part serves as our training data, while the 20% part is used by us as test data. we first develop the model using training data and then apply the model to testing data. if the result of the training data is not as accurate as the testing data, the data is called overfit data. doesn't matter how good the results or graphs of training data are; if the testing graphs or results are not good, that model is not best suited for the data. multiple r, the term we got during data analysis using regression in excel, is the square root of r-squared. then we saw about adjusted r^2 which tells about how effective the adding of a new variable is. then we saw that during the calculation of the variance of the sample, we used (n-1) in the denominator instead of n, as we are using the mean already which has n observations in it already, so we have only n-1 unknowns. then we learned about the quantile-quantile plot which is plotted by separating the distribution of the data(errors) in some quantile. then we perform regression analysis using python in which we use python libraries like numpy, pandas, and sci-kit-learn. at last, we see the analysis of errors using various tests like omnibus which tells about the normality of tests, skewness, kurtosis, and jarque-bera test.",2,7.762735,3.2390008,10.85696,4.9148064,"regression, regressions, features"
554,"in today's class, we discussed levels of measurement for data, which include four types: 1) nominal, 2) ordinal, 3) interval, and 4) ratio. of these, nominal and ordinal are categorical data, while interval and ratio are numerical data.

nominal data has no inherent order among categories. examples include gender and nationality. ordinal data, on the other hand, is categorical with an inherent order but lacks meaningful intervals between categories, such as grades (a, b, c). for nominal and ordinal data, techniques like one-hot encoding or label encoding are used for feature representation. however, one-hot encoding can significantly increase the dimensionality of the dataset.

interval and ratio data are both continuous and numerical. interval data, such as temperature in â°c or â°f, has an arbitrary zero, meaning zero does not represent the absence of the quantity. in contrast, ratio data has an absolute zero, as in the case of temperature in kelvin (0k) or height, where zero indicates the complete absence of the quantity.

we also covered the difference between supervised and unsupervised machine learning. in supervised learning, the target variable (y) is provided, and the goal is to learn a function f that maps input features (x) to the output (y). techniques like linear regression and multiple regression are examples. in unsupervised learning, the target variable is not available, and the focus is on finding patterns in the data, such as clustering based on distance metrics like euclidean distance. examples include k-means and hierarchical clustering.

finally, we discussed the distinction between population and samples. all the data available on the internet represents samples of a larger population. machine learning models are trained on these samples and generalized to the population using proper validation techniques.","in today's class, we discussed levels of measurement for data, which include four types: 1) nominal, 2) ordinal, 3) interval, and 4) ratio. of these, nominal and ordinal are categorical data, while interval and ratio are numerical data. nominal data has no inherent order among categories. examples include gender and nationality. ordinal data, on the other hand, is categorical with an inherent order but lacks meaningful intervals between categories, such as grades (a, b, c). for nominal and ordinal data, techniques like one-hot encoding or label encoding are used for feature representation. however, one-hot encoding can significantly increase the dimensionality of the dataset. interval and ratio data are both continuous and numerical. interval data, such as temperature in c or f, has an arbitrary zero, meaning zero does not represent the absence of the quantity. in contrast, ratio data has an absolute zero, as in the case of temperature in kelvin (0k) or height, where zero indicates the complete absence of the quantity. we also covered the difference between supervised and unsupervised machine learning. in supervised learning, the target variable (y) is provided, and the goal is to learn a function f that maps input features (x) to the output (y). techniques like linear regression and multiple regression are examples. in unsupervised learning, the target variable is not available, and the focus is on finding patterns in the data, such as clustering based on distance metrics like euclidean distance. examples include k-means and hierarchical clustering. finally, we discussed the distinction between population and samples. all the data available on the internet represents samples of a larger population. machine learning models are trained on these samples and generalized to the population using proper validation techniques.",4,-25.435091,-15.98999,1.6663954,0.14136039,"classification, classifying, classifications"
555,"firstly 4 levels of measurement  - nominal, ordinal, interval and ratio and difference between them.
then about features and labels and the relation y = f(x)
supervised and unsupervised learning 
a glimpse of regression and clustering","firstly 4 levels of measurement - nominal, ordinal, interval and ratio and difference between them. then about features and labels and the relation y = f(x) supervised and unsupervised learning a glimpse of regression and clustering",4,-18.141436,-15.1092415,2.3613427,0.8458745,"classification, classifying, classifications"
556,"firstly we got to know about some algorithms by which assignments are checked.the idea is to find the similarity between the answers.then we studied about multiple linear regression where our dependent variable depends on many independent variables, also called features. these features are represented as a vector (x1 x2 ... xn).its general equation is y=b0+b1x1+b2x2+...+bnxn
apart from r and p values there is another statistical parameter, f value, by which we can infer how good the model is. f value is the ratio of explained variance to unexplained variance so higher is the f value, better is the prediction. we took some dataset and calculated the error metrics and other parameters. from this we learnt that since there is no reference point for f-value so rather than calling a particular f value as a good value it makes more sense to compare f values for two models and conclude that the one with the higher f value is a better model.","firstly we got to know about some algorithms by which assignments are checked.the idea is to find the similarity between the answers.then we studied about multiple linear regression where our dependent variable depends on many independent variables, also called features. these features are represented as a vector (x1 x2 ... xn).its general equation is y=b0+b1x1+b2x2+...+bnxn apart from r and p values there is another statistical parameter, f value, by which we can infer how good the model is. f value is the ratio of explained variance to unexplained variance so higher is the f value, better is the prediction. we took some dataset and calculated the error metrics and other parameters. from this we learnt that since there is no reference point for f-value so rather than calling a particular f value as a good value it makes more sense to compare f values for two models and conclude that the one with the higher f value is a better model.",2,10.69316,-4.155266,12.115785,4.2260776,"regression, regressions, features"
557,"in today's class, we continued the discussion of vif, in which we saw that if a feature's vif value is high, then that feature is highly dependent on other features. vif is used for determining multicollinearity. in vif analysis, we set up a threshold, any feature having vif above it can be removed. then we move onto principal component analysis(pca), in which we plot vif against r^2. we see that the principal components are orthogonal to each other. as the number of principal components increases, the number of dimensions in the data also increases. pc are the axis which captures the best variance in the data. the function y=f(x) can be interpreted as y=f(pc) and it has mainly two uses, for prediction and answering what-if type questions. pc analysis can be used for data reduction, dimension reduction, in prediction models, and visualization in eda to understand the structure of data. pca is sensitive to data scale and hence we need to normalize the data. we saw two examples of this in the class, in which the blood parameter data was one. at last, we see about t-sne, which is t-distribution schematic neighbour encoding, which is a lossy transformation and helps mapping of multi-dimensional data to one-dimensional or two-dimensional data.","in today's class, we continued the discussion of vif, in which we saw that if a feature's vif value is high, then that feature is highly dependent on other features. vif is used for determining multicollinearity. in vif analysis, we set up a threshold, any feature having vif above it can be removed. then we move onto principal component analysis(pca), in which we plot vif against r^2. we see that the principal components are orthogonal to each other. as the number of principal components increases, the number of dimensions in the data also increases. pc are the axis which captures the best variance in the data. the function y=f(x) can be interpreted as y=f(pc) and it has mainly two uses, for prediction and answering what-if type questions. pc analysis can be used for data reduction, dimension reduction, in prediction models, and visualization in eda to understand the structure of data. pca is sensitive to data scale and hence we need to normalize the data. we saw two examples of this in the class, in which the blood parameter data was one. at last, we see about t-sne, which is t-distribution schematic neighbour encoding, which is a lossy transformation and helps mapping of multi-dimensional data to one-dimensional or two-dimensional data.",11,-18.085157,5.4234157,10.504207,13.413069,"pca, heatmap, heatmaps"
558,"in today's session we talked about core statistical concepts required for machine learning. 
we started our discussion with confidence intervals. what it actually means. we cannot say or predict the trends of population based on a small sample with a 100% confidence hence we use the concepts of confidence intervals to say that we are alpha percent sure that this value will be in this interval. then we talked about upper and lower confidence intervals and two sided interval tests.
also discussed concepts like if the error is taken from population that is the standard deviations of population are followed then it follows a normal distribution and if not then we take standard deviations of sample and it follows a t-distribuion. we also touched upon the concept of p-value. and hypothesis testing.",in today's session we talked about core statistical concepts required for machine learning. we started our discussion with confidence intervals. what it actually means. we cannot say or predict the trends of population based on a small sample with a 100% confidence hence we use the concepts of confidence intervals to say that we are alpha percent sure that this value will be in this interval. then we talked about upper and lower confidence intervals and two sided interval tests. also discussed concepts like if the error is taken from population that is the standard deviations of population are followed then it follows a normal distribution and if not then we take standard deviations of sample and it follows a t-distribuion. we also touched upon the concept of p-value. and hypothesis testing.,7,35.214603,2.8160956,14.957131,2.4353096,"statistics, statistical, statisticsâ"
559,"we firstly learnt about polynomial regression, and learnt that it models relationships by extending linear regression with polynomial terms, while the p-value helps determine the significance of those terms in predicting the outcome. to a function similar to sin curve, and with one x1, we got the p value and stuff, and then we added sin(x1) to it, and in this case we got a different value. so it made a significant difference to the model. sometimes some features are more important than the other, it's not more important to have more features, you should have features which better explain the model. so adding new features only gives better results upto a point, then after that it's unnecessary. also, neural network was talked about in class today, and we learnt how it has layers and neurons and ties to mimic the human form of thinking. multiple layers and all are there in deep learning. so basically, a neural network is like a bunch of connected math equations that learn patterns from data, kind of like how our brain learns from experience. we also saw overfitting graphs with example, and how it looks when data is overfit. and how it's related to degress of freedom too, that more degrees means overfitting.","we firstly learnt about polynomial regression, and learnt that it models relationships by extending linear regression with polynomial terms, while the p-value helps determine the significance of those terms in predicting the outcome. to a function similar to sin curve, and with one x1, we got the p value and stuff, and then we added sin(x1) to it, and in this case we got a different value. so it made a significant difference to the model. sometimes some features are more important than the other, it's not more important to have more features, you should have features which better explain the model. so adding new features only gives better results upto a point, then after that it's unnecessary. also, neural network was talked about in class today, and we learnt how it has layers and neurons and ties to mimic the human form of thinking. multiple layers and all are there in deep learning. so basically, a neural network is like a bunch of connected math equations that learn patterns from data, kind of like how our brain learns from experience. we also saw overfitting graphs with example, and how it looks when data is overfit. and how it's related to degress of freedom too, that more degrees means overfitting.",0,-1.4364316,0.79363036,9.09728,3.9262354,"models, feature, features"
560,"in  today's session, we focused on key concepts in data science and statistical analysis, including a review of regression metrics, sampling distributions, and the central limit theorem (clt). from the uploaded document, we explored metrics such as sse (sum of squared errors), mse (mean squared error), rmse (root mean squared error), and mae (mean absolute error), which are essential for assessing the performance of linear regression models. these metrics help quantify the model's ability to explain variations in the data and its accuracy in predicting outcomes. the document also introduced the idea that regression coefficients derived from sample data are estimates of population parameters, emphasizing the importance of evaluating their reliability.

to understand this reliability, we examined the concept of sampling distributions, where multiple representative samples are drawn from a population to calculate sample means. these means tend to form a normal distribution as per the central limit theorem, regardless of the original population distribution, provided the sample size is sufficiently large. this principle allows statisticians to infer population characteristics and calculate confidence intervals for model parameters. by reducing uncertainty with larger sample sizes, the standard error of the sampling distribution decreases, improving the precision of estimates. and then we made this on excel sheet and plotted histogram for errors and drew best fit line using linear regression. the discussion highlighted the theoretical and practical importance of these concepts for creating generalizable and reliable models in data science.","in today's session, we focused on key concepts in data science and statistical analysis, including a review of regression metrics, sampling distributions, and the central limit theorem (clt). from the uploaded document, we explored metrics such as sse (sum of squared errors), mse (mean squared error), rmse (root mean squared error), and mae (mean absolute error), which are essential for assessing the performance of linear regression models. these metrics help quantify the model's ability to explain variations in the data and its accuracy in predicting outcomes. the document also introduced the idea that regression coefficients derived from sample data are estimates of population parameters, emphasizing the importance of evaluating their reliability. to understand this reliability, we examined the concept of sampling distributions, where multiple representative samples are drawn from a population to calculate sample means. these means tend to form a normal distribution as per the central limit theorem, regardless of the original population distribution, provided the sample size is sufficiently large. this principle allows statisticians to infer population characteristics and calculate confidence intervals for model parameters. by reducing uncertainty with larger sample sizes, the standard error of the sampling distribution decreases, improving the precision of estimates. and then we made this on excel sheet and plotted histogram for errors and drew best fit line using linear regression. the discussion highlighted the theoretical and practical importance of these concepts for creating generalizable and reliable models in data science.",5,26.755201,1.3692263,14.658767,4.0687575,"regression, statistical, statistics"
561,"we define the boundary for classification as a linear combination of features. this is then passed through the sigmoid function so that we can treat the output as a probability, specifically the probability that the label is 1 given the features. the prediction output is done by rounding this probability to the nearest integer (0 or 1).
the four possible cases of predicted and actual labels (both can be 0 or 1) are represented in a matrix form called the confusion matrix. many metrics can be derived here, for e.g. precision, recall, accuracy. f1 score is the harmonic mean of precision and recall and it is a better metric than accuracy in the case where the data is imbalanced. when there are more sample from a particular class in the training dataset, then the classifier can achieve a high accuracy by trivially detecting every input as belonging to the dominant class. f1 score counters this by providing a value based on the precision and recall which themselves focus on number of correct predictions from those predicted and for each class how many were identified correctly respectively.","we define the boundary for classification as a linear combination of features. this is then passed through the sigmoid function so that we can treat the output as a probability, specifically the probability that the label is 1 given the features. the prediction output is done by rounding this probability to the nearest integer (0 or 1). the four possible cases of predicted and actual labels (both can be 0 or 1) are represented in a matrix form called the confusion matrix. many metrics can be derived here, for e.g. precision, recall, accuracy. f1 score is the harmonic mean of precision and recall and it is a better metric than accuracy in the case where the data is imbalanced. when there are more sample from a particular class in the training dataset, then the classifier can achieve a high accuracy by trivially detecting every input as belonging to the dominant class. f1 score counters this by providing a value based on the precision and recall which themselves focus on number of correct predictions from those predicted and for each class how many were identified correctly respectively.",10,12.867252,-20.090136,8.829182,-1.5841858,"classifications, histograms, histogram"
562,"we started the lecture where we left off last time, discussing about the vif values and the iterative procedure of finding the most independent features. we understood that the threshold value chosen should have some justification. we then moved on to pca i.e. principal component analysis. we discussed that pca helps us to reduce the dimensionality of our problem, by somewhat changing the axes of our data such that each axis helps to describe the maximum variance in the data. thus pc1 axes captures maximum variance, pc2 captures the next maximum variance and so on. a simple example could be a two dimensional linear data, which could be reduced to one dimension by taking the axis along the line. we need to check how many principal components we need in order to explain majority of the variance of the data. each principal component is a linear combination of each of the original features. the coefficient of each feature in the linear combination, is known as loadings. now in between pca and vif, the vif procedure should be done first as it eliminates multicolinearity which is a bigger issue. also, pca hinders with our interpretability as the features are not what they actually meant, but a combination of multiple features. however, pca does help us in dimension reduction, for making good predictions and also in visualisation and understanding the structure of the data. when used for visualisation, pca becomes a part of eda. pca requires normalisation of the data. if pc1 and pc2 cover 70 to 80 percent of the variance of the data, then the multidimensional data can be visualised in two dimensions by plotting the data on a pc1 vs pc2 plot. now for better visualisation, we have another tool called tsne. tsne is a lossy transformation which helps us to visualise our data in two dimensions. it is based on the t-distribution and also uses gradient descent, and is hence slow and not consistent. tsne calculates a probability distribution of all the other points being close to one certain point. it does this for each and every point. this data is taken into a matrix and then the dimension is reduced. tsne means t-distributed stochastic neighbour encoding. t-distribution helps us to magnify the difference between the points if there exists any, and thus helps in classification and clustering.","we started the lecture where we left off last time, discussing about the vif values and the iterative procedure of finding the most independent features. we understood that the threshold value chosen should have some justification. we then moved on to pca i.e. principal component analysis. we discussed that pca helps us to reduce the dimensionality of our problem, by somewhat changing the axes of our data such that each axis helps to describe the maximum variance in the data. thus pc1 axes captures maximum variance, pc2 captures the next maximum variance and so on. a simple example could be a two dimensional linear data, which could be reduced to one dimension by taking the axis along the line. we need to check how many principal components we need in order to explain majority of the variance of the data. each principal component is a linear combination of each of the original features. the coefficient of each feature in the linear combination, is known as loadings. now in between pca and vif, the vif procedure should be done first as it eliminates multicolinearity which is a bigger issue. also, pca hinders with our interpretability as the features are not what they actually meant, but a combination of multiple features. however, pca does help us in dimension reduction, for making good predictions and also in visualisation and understanding the structure of the data. when used for visualisation, pca becomes a part of eda. pca requires normalisation of the data. if pc1 and pc2 cover 70 to 80 percent of the variance of the data, then the multidimensional data can be visualised in two dimensions by plotting the data on a pc1 vs pc2 plot. now for better visualisation, we have another tool called tsne. tsne is a lossy transformation which helps us to visualise our data in two dimensions. it is based on the t-distribution and also uses gradient descent, and is hence slow and not consistent. tsne calculates a probability distribution of all the other points being close to one certain point. it does this for each and every point. this data is taken into a matrix and then the dimension is reduced. tsne means t-distributed stochastic neighbour encoding. t-distribution helps us to magnify the difference between the points if there exists any, and thus helps in classification and clustering.",11,-18.540936,2.440358,10.395083,13.354166,"pca, heatmap, heatmaps"
563,"we continued to follow up on our statistical concepts. we explored terms from our data analysis toolpack, discussed their meanings, interconnections, how to represent them graphically, and the potential sources of uncertainty or errors in applying them.
the significant portion of the class covers cases involving beta and beta 0 under specific conditions, with emphasis on how they feature in different statistical models. discussed the concept of the p-value, bringing into light its role on critical decisions especially regarding feature selection.
at the end of the class, we started to touch on anova, or analysis of variance, and focused on the f-statistic. we discussed why it was important for this value to be large, also relating to how it relates to evaluating modelâ performance.","we continued to follow up on our statistical concepts. we explored terms from our data analysis toolpack, discussed their meanings, interconnections, how to represent them graphically, and the potential sources of uncertainty or errors in applying them. the significant portion of the class covers cases involving beta and beta 0 under specific conditions, with emphasis on how they feature in different statistical models. discussed the concept of the p-value, bringing into light its role on critical decisions especially regarding feature selection. at the end of the class, we started to touch on anova, or analysis of variance, and focused on the f-statistic. we discussed why it was important for this value to be large, also relating to how it relates to evaluating model performance.",13,2.9427257,16.212032,12.579094,5.891561,"classification, classifying, classifications"
564,"in this session, we began by addressing doubts from the previous class, where expectation algebra was briefly introduced. we then derived the relationship between the variance of sample means and the variance of the population. moving forward, we covered the basics of logistic regression, discussing its mathematical formulation and applying gradient descent to minimize the defined function. it explained the concept of confusion matrix, elaborating that the explanation of misclassifications is an important part of understanding false negatives, considered to be worse, such as being declared ""ok"" when not ok. additional key evaluation metrics discussed include precision and how many of the detected events were correctly identified. the recall measures how instances of a particular class are successful in being detected. then, the f1 score represents the harmonic mean of precision and recall. we emphasized how the f1 score is more reliable than simple accuracy, as the later sometimes misleads when implemented as a selection criterion.","in this session, we began by addressing doubts from the previous class, where expectation algebra was briefly introduced. we then derived the relationship between the variance of sample means and the variance of the population. moving forward, we covered the basics of logistic regression, discussing its mathematical formulation and applying gradient descent to minimize the defined function. it explained the concept of confusion matrix, elaborating that the explanation of misclassifications is an important part of understanding false negatives, considered to be worse, such as being declared ""ok"" when not ok. additional key evaluation metrics discussed include precision and how many of the detected events were correctly identified. the recall measures how instances of a particular class are successful in being detected. then, the f1 score represents the harmonic mean of precision and recall. we emphasized how the f1 score is more reliable than simple accuracy, as the later sometimes misleads when implemented as a selection criterion.",10,13.842139,-15.954621,9.0312,-1.2606419,"classifications, histograms, histogram"
565,"we learnt why its important to check for multi-collinearity. if there is multi collinearity among the variables then our model becomes unstable and it starts giving different answers when the model is run. next we learnt about splitting the sample data into train and test sets. usually the split is 80% and 20% but if the sample size is less we can go for 90-10 split. and there is hard rule for the splits it depends on the data size we have. to compare trained model on the test data we can use r squared. if the values are quite close then we can say the model is good but if starts diverging from each other it might be a case of overfitting on the train data and in this case we should think of some other simpler models. multiple r is the square root of r squared and kind of gives the corelation between the y and set of x. usually when the number of variables are more the r^2 value might increase because it can explain more variance ,therefore its good to have appropriate and less variables. we can use rmse to compare the results of train and test data results. p-value is used for parametrized model and not for unparametrized models like random forests and therefore its important to have performance metrics like rmse, r^2 and f stat which can be used to compare. we used sklearn library to make multilinear regression line and got to know there are not many performance metrices available therefore started with stats model and learnt some new metrices like omnibus ,jarque bera test which tell us how the residuals are distributed that is whether they are close to the normal distribution or not. and learnt about a new plot that is quantile-quantile plot from which we can visually get to know how close the residual distribution is to the normal distribution.","we learnt why its important to check for multi-collinearity. if there is multi collinearity among the variables then our model becomes unstable and it starts giving different answers when the model is run. next we learnt about splitting the sample data into train and test sets. usually the split is 80% and 20% but if the sample size is less we can go for 90-10 split. and there is hard rule for the splits it depends on the data size we have. to compare trained model on the test data we can use r squared. if the values are quite close then we can say the model is good but if starts diverging from each other it might be a case of overfitting on the train data and in this case we should think of some other simpler models. multiple r is the square root of r squared and kind of gives the corelation between the y and set of x. usually when the number of variables are more the r^2 value might increase because it can explain more variance ,therefore its good to have appropriate and less variables. we can use rmse to compare the results of train and test data results. p-value is used for parametrized model and not for unparametrized models like random forests and therefore its important to have performance metrics like rmse, r^2 and f stat which can be used to compare. we used sklearn library to make multilinear regression line and got to know there are not many performance metrices available therefore started with stats model and learnt some new metrices like omnibus ,jarque bera test which tell us how the residuals are distributed that is whether they are close to the normal distribution or not. and learnt about a new plot that is quantile-quantile plot from which we can visually get to know how close the residual distribution is to the normal distribution.",2,8.478108,3.887453,10.893685,4.768591,"regression, regressions, features"
566,"today's session focused on logistic regression, a fundamental algorithm for classification problems. it predicts probabilities using the sigmoid function, which maps inputs to a range between 0 and 1. a decision boundary, typically set at 0.5, determines class labels.

we also explored clustering as a technique for organizing data before classification, improving model performance. the importance of true positive and true negative rates in evaluating classification accuracy was discussed, along with outlier detection techniques to handle irregular data points. additionally, we examined loss functions, which guide model optimization by minimizing errors. the session concluded with practical approaches to implementing logistic regression and improving classification performance.","today's session focused on logistic regression, a fundamental algorithm for classification problems. it predicts probabilities using the sigmoid function, which maps inputs to a range between 0 and 1. a decision boundary, typically set at 0.5, determines class labels. we also explored clustering as a technique for organizing data before classification, improving model performance. the importance of true positive and true negative rates in evaluating classification accuracy was discussed, along with outlier detection techniques to handle irregular data points. additionally, we examined loss functions, which guide model optimization by minimizing errors. the session concluded with practical approaches to implementing logistic regression and improving classification performance.",13,-5.7239494,-9.381596,8.512994,5.2379723,"classification, classifying, classifications"
567,"today's class was about feature engineering and different encoding techniques. we learned about one-hot, label, integer, binary, frequency, and target encoding, along with feature binning and how to handle text data.","today's class was about feature engineering and different encoding techniques. we learned about one-hot, label, integer, binary, frequency, and target encoding, along with feature binning and how to handle text data.",3,-42.70848,7.6250243,0.25989422,6.170241,"categorical, categorization, categorise"
568,"we worked on understanding regression analysis using a dataset of 100 sample points in excel. the primary focus was on calculating the regression line, represented by b (intercept) and a (slope), and using excelâ€™s data analysis toolkit to derive and analyze the regression function. we evaluated the errors by plotting a scatter plot and a histogram. interestingly, the histogram revealed a roughly uniform error distribution, emphasizing that visual interpretation of scatter plots can sometimes be misleading. 
we then discussed the characteristics of a good regression model:
the error distribution should be random.
the model should explain a significant portion of the data's variation.
finally, we explored confidence intervals, specifically the upper and lower 95% bounds, and related them to the gaussian distribution.","we worked on understanding regression analysis using a dataset of 100 sample points in excel. the primary focus was on calculating the regression line, represented by b (intercept) and a (slope), and using excel s data analysis toolkit to derive and analyze the regression function. we evaluated the errors by plotting a scatter plot and a histogram. interestingly, the histogram revealed a roughly uniform error distribution, emphasizing that visual interpretation of scatter plots can sometimes be misleading. we then discussed the characteristics of a good regression model: the error distribution should be random. the model should explain a significant portion of the data's variation. finally, we explored confidence intervals, specifically the upper and lower 95% bounds, and related them to the gaussian distribution.",5,19.76272,-5.0383573,14.351772,5.039375,"regression, statistical, statistics"
569,"today's lecture covered key concepts in linear regression, including interpreting regression outputs such as r-squared, p-values, and confidence intervals. the anova table was introduced to explain variance analysis and the significance of regression models. the professor emphasized the importance of checking if zero lies within confidence intervals to determine statistical significance. additionally, sampling distributions of regression coefficients were discussed. an introduction to multiple linear regression (mlr) was given, highlighting how multiple independent variables can be used to predict a dependent variable. the lecture concluded with insights on how anova helps compare multiple averages simultaneously.","today's lecture covered key concepts in linear regression, including interpreting regression outputs such as r-squared, p-values, and confidence intervals. the anova table was introduced to explain variance analysis and the significance of regression models. the professor emphasized the importance of checking if zero lies within confidence intervals to determine statistical significance. additionally, sampling distributions of regression coefficients were discussed. an introduction to multiple linear regression (mlr) was given, highlighting how multiple independent variables can be used to predict a dependent variable. the lecture concluded with insights on how anova helps compare multiple averages simultaneously.",13,20.910814,4.14009,12.777562,5.045415,"classification, classifying, classifications"
570,"multiple linear regression there are multiple predictor variables also called features. what 'stuff' to use as features, how to calculate / derive features from some other data and using domain knowledge form what is known as feature engineering.
some error metrics like rmse and mae may have a physical meaning as they have the same dimension as the data but others like sse and mse are used only form 'optimization purposes'. also, these values themselves may not be very helpful, but they aid in model selection; we decide which model is better based on these metric (lower is better for error and higher is better for something else).
even when metrics such as r-squared and f-statistic are high, the model may still be bad. saw an example where the regression coefficients were statistically equal to zero. after performing some feature selection (successively dropping the ones with the highest p-value) we become more confident with the model even though the value of the metrics may have decreased slightly.
mlr doesn't have a closed form solution hence we need to use gradient descent to arrive at a solution (that is potentially correct). it is similar to newton-rhapson method. we start with a randomly initialized value as the answer and the iteratively move towards the 'optimal' solution. multiple solvers are available that implement some form of gradient descent and it forms an integral part in the functioning of many machine learning algorithms / techniques.","multiple linear regression there are multiple predictor variables also called features. what 'stuff' to use as features, how to calculate / derive features from some other data and using domain knowledge form what is known as feature engineering. some error metrics like rmse and mae may have a physical meaning as they have the same dimension as the data but others like sse and mse are used only form 'optimization purposes'. also, these values themselves may not be very helpful, but they aid in model selection; we decide which model is better based on these metric (lower is better for error and higher is better for something else). even when metrics such as r-squared and f-statistic are high, the model may still be bad. saw an example where the regression coefficients were statistically equal to zero. after performing some feature selection (successively dropping the ones with the highest p-value) we become more confident with the model even though the value of the metrics may have decreased slightly. mlr doesn't have a closed form solution hence we need to use gradient descent to arrive at a solution (that is potentially correct). it is similar to newton-rhapson method. we start with a randomly initialized value as the answer and the iteratively move towards the 'optimal' solution. multiple solvers are available that implement some form of gradient descent and it forms an integral part in the functioning of many machine learning algorithms / techniques.",0,3.6825466,-6.7153955,9.874305,4.0470695,"models, feature, features"
571,"feature encoding
if any of the independent variable or dependent variable is categorical, it needs to be encoded before pushing it into the model.
we then got into one hot encoding, where we saw the difference between multiclass and multilable problems, where multiclass aims to process one of the classes and multilable processes and gives many of the possible labels 
we also understood that it is more covinient of encode the dependent variable and not mostly encode the independable variable as it is directly present in the algorithm
whatâ€™s the problem with one hot encoding, it invites increase in the dimensions and we all tend to reduce the dimensionality
we then saw about binary encoding, label encoding, one hot encoding, frequency encoding and target encoding 
then we gone into feature binning which essentially converts continuous quantities into discrete items
we then delved into text processing, the whole sole basis of text processing is statistics","feature encoding if any of the independent variable or dependent variable is categorical, it needs to be encoded before pushing it into the model. we then got into one hot encoding, where we saw the difference between multiclass and multilable problems, where multiclass aims to process one of the classes and multilable processes and gives many of the possible labels we also understood that it is more covinient of encode the dependent variable and not mostly encode the independable variable as it is directly present in the algorithm what s the problem with one hot encoding, it invites increase in the dimensions and we all tend to reduce the dimensionality we then saw about binary encoding, label encoding, one hot encoding, frequency encoding and target encoding then we gone into feature binning which essentially converts continuous quantities into discrete items we then delved into text processing, the whole sole basis of text processing is statistics",3,-44.681034,5.134299,0.25330234,6.502988,"categorical, categorization, categorise"
572,"today in class, sir talked about the metrics we use to check how good a linear regression model is, like sse,râ²,etc. he said that when we use tools like excel, many more numbers appear, and to understand them, we need to go back to basic statistics. sir explained that the data we use is just a sample (99 observations in this case), so the values we calculate for the model, like a and b, are only guesses about the real values for the entire population. he said our goal is to create a model that works well for data outside the sample.
sir then explained the idea of sampling distributions and confidence intervals. he said that if we take many small samples from the population and find their means, those means would form a normal distribution, even if the population data isnâ€™t normally distributed. this is called the central limit theorem (clt). he also said that bigger samples give more accurate results, while smaller samples create more uncertainty. sir explained that this normal distribution helps us make good guesses about the population, even when we donâ€™t know everything about it.","today in class, sir talked about the metrics we use to check how good a linear regression model is, like sse,r ,etc. he said that when we use tools like excel, many more numbers appear, and to understand them, we need to go back to basic statistics. sir explained that the data we use is just a sample (99 observations in this case), so the values we calculate for the model, like a and b, are only guesses about the real values for the entire population. he said our goal is to create a model that works well for data outside the sample. sir then explained the idea of sampling distributions and confidence intervals. he said that if we take many small samples from the population and find their means, those means would form a normal distribution, even if the population data isn t normally distributed. this is called the central limit theorem (clt). he also said that bigger samples give more accurate results, while smaller samples create more uncertainty. sir explained that this normal distribution helps us make good guesses about the population, even when we don t know everything about it.",7,30.236809,0.48930854,14.950049,2.788302,"statistics, statistical, statisticsâ"
573,"first 10 lecs schedule was discussed. we started the course with discussing function encoding, where we saw an example of turning a categorical variable (y) with three variables (red, blue, and green) into a function f(x) using y1, y2, and y3).  additionally, we learnt about the various problem typesâ€”multiclass and multilabelâ€”that influence our strategy.

binary encoding, which produces a more condensed form, was the next topic we discussed.  we looked at how to determine the final product and the conversion process step-by-step.  with the introduction of frequency encoding, category values are swapped out for the frequency with which they occur in the dataset.  target encoding substitutes the average score over a predetermined threshold for data in a column.","first 10 lecs schedule was discussed. we started the course with discussing function encoding, where we saw an example of turning a categorical variable (y) with three variables (red, blue, and green) into a function f(x) using y1, y2, and y3). additionally, we learnt about the various problem types multiclass and multilabel that influence our strategy. binary encoding, which produces a more condensed form, was the next topic we discussed. we looked at how to determine the final product and the conversion process step-by-step. with the introduction of frequency encoding, category values are swapped out for the frequency with which they occur in the dataset. target encoding substitutes the average score over a predetermined threshold for data in a column.",3,-40.09395,6.4741325,0.4102033,6.02714,"categorical, categorization, categorise"
574,"we discussed  about eda today through various case studies. the first  case study was on our summary data itself where we learnt how to use pivot tables in the excel, here we could perform various operations like finding mean of the data , max, min of the columns. its a great tool and makes life easier during eda when we have small datasets because excel is interactive. we also used the data analysis tool of the excel to get the summary statistics of the data set. we discussed how the outliers could be made out in different methods like we were able to see the in box plot lying above and below the whiskers , in scatter plot they were lying very far from the actual data which formed a band and in histogram where we could see some skewness confirming presence of outliers. we then saw the eda report of the chemical factory and transformers. and learnt how to present the data and how to pose relevant questions to the clients based on the missing data and preliminary data analysis through eda. here we saw the relevance of using line plots to find the outliers and plotting them pairwise to know there corelation etc. thus in conclusion we learnt how important eda is how to do it effectively. after this the ta's discussed about the last assignment. the key points of the discussion were writing the report in well structured manner and using the right tool as mentioned in the question and learnt how to introduce noise in the data.","we discussed about eda today through various case studies. the first case study was on our summary data itself where we learnt how to use pivot tables in the excel, here we could perform various operations like finding mean of the data , max, min of the columns. its a great tool and makes life easier during eda when we have small datasets because excel is interactive. we also used the data analysis tool of the excel to get the summary statistics of the data set. we discussed how the outliers could be made out in different methods like we were able to see the in box plot lying above and below the whiskers , in scatter plot they were lying very far from the actual data which formed a band and in histogram where we could see some skewness confirming presence of outliers. we then saw the eda report of the chemical factory and transformers. and learnt how to present the data and how to pose relevant questions to the clients based on the missing data and preliminary data analysis through eda. here we saw the relevance of using line plots to find the outliers and plotting them pairwise to know there corelation etc. thus in conclusion we learnt how important eda is how to do it effectively. after this the ta's discussed about the last assignment. the key points of the discussion were writing the report in well structured manner and using the right tool as mentioned in the question and learnt how to introduce noise in the data.",6,-11.481202,26.475197,7.651864,10.154096,"summarizing, summarize, summarization"
575,"the session highlighted the use of principal component analysis (pca) in feature reduction, focusing on dimensionality reduction while retaining the most variance. a brief introduction was provided on the need to remove correlated factors using vif before pca can be performed. the applications of pca included visualization (eda), embedding in prediction models, and analysis of the structure of data as a comparison between classic regression models and pc regression, improved interpretability, and computational efficiency. this includes how to extract the pcs and how to determine the number of pcs by the elbow method. then, the session efficiently packs out with the role of pca in the analysis of real-world data and predictive modeling workflows.","the session highlighted the use of principal component analysis (pca) in feature reduction, focusing on dimensionality reduction while retaining the most variance. a brief introduction was provided on the need to remove correlated factors using vif before pca can be performed. the applications of pca included visualization (eda), embedding in prediction models, and analysis of the structure of data as a comparison between classic regression models and pc regression, improved interpretability, and computational efficiency. this includes how to extract the pcs and how to determine the number of pcs by the elbow method. then, the session efficiently packs out with the role of pca in the analysis of real-world data and predictive modeling workflows.",11,-12.58009,2.2602785,10.046929,12.649122,"pca, heatmap, heatmaps"
576,"today's class covered the concepts of population and sample data, with an emphasis on the practicality of using samples to estimate population parameters. key statistical measures such as mean, median, and variance were discussed, as well as their use in data analysis. confidence intervals were created to estimate population parameters with a certain level of certainty. the discussion then shifted to simple linear regression, in which we found the best-fit line by minimizing the sum of squared errors. we discovered that the mean of the predictor and response variables is on the best-fit line. deriving coefficients b0 and b1â€‹ for simple regression is straightforward, but extending this to multiple predictors requires more advanced algorithms, which will be explored in the following class.","today's class covered the concepts of population and sample data, with an emphasis on the practicality of using samples to estimate population parameters. key statistical measures such as mean, median, and variance were discussed, as well as their use in data analysis. confidence intervals were created to estimate population parameters with a certain level of certainty. the discussion then shifted to simple linear regression, in which we found the best-fit line by minimizing the sum of squared errors. we discovered that the mean of the predictor and response variables is on the best-fit line. deriving coefficients b0 and b1 for simple regression is straightforward, but extending this to multiple predictors requires more advanced algorithms, which will be explored in the following class.",1,30.944735,-6.5354776,15.747853,4.2220135,"population, models, estimating"
577,"we started with pivot tables in excel. we learned how pivot tables help to summarize large collections of data by grouping and totaling values. we learned how to create a pivot table in excel, drag and drop fields, and calculate measurements like sum, average, count, and percentages to effectively analyze data.

then we moved on to other eda (exploratory data analysis) methods. we talked about how summary statistics like mean, median, variance, and standard deviation give an idea of the data. we also covered visualization methods like histograms, box plots, and scatter plots to identify trends, skewness, and outliers.

lastly, we touched on how data issues like class imbalance could impact analysis. when one of the classes significantly outweighs all the rest, it can skew results. we also did a quick overview of feature engineering and transformation, which are utilized to assist in improving data quality before analysis.","we started with pivot tables in excel. we learned how pivot tables help to summarize large collections of data by grouping and totaling values. we learned how to create a pivot table in excel, drag and drop fields, and calculate measurements like sum, average, count, and percentages to effectively analyze data. then we moved on to other eda (exploratory data analysis) methods. we talked about how summary statistics like mean, median, variance, and standard deviation give an idea of the data. we also covered visualization methods like histograms, box plots, and scatter plots to identify trends, skewness, and outliers. lastly, we touched on how data issues like class imbalance could impact analysis. when one of the classes significantly outweighs all the rest, it can skew results. we also did a quick overview of feature engineering and transformation, which are utilized to assist in improving data quality before analysis.",6,-13.710149,27.007671,7.2185535,10.002714,"summarizing, summarize, summarization"
578,"in this class, much was not done. we moved further in exploratory data analysis and basically studied on outliers. we referered to 2 or more problems regarding fault in transformer etc and how data collection can help me create a good explanatory data set. later we moved to, how we should deal with outliers. there were several times when the outliers were 0 or very far away from the actual data. we just remove them directly to avoid unnecessary deviations. later, it was shown how various huge numbers of columns of data can be converted to minimum data size with the help of principal component analysis and on the basis of independent variables. it was shown how very heavy data file of more than 200 columns were converted to only 17 principle components and how other columns were clustered together due to some kind of clustering in between them.","in this class, much was not done. we moved further in exploratory data analysis and basically studied on outliers. we referered to 2 or more problems regarding fault in transformer etc and how data collection can help me create a good explanatory data set. later we moved to, how we should deal with outliers. there were several times when the outliers were 0 or very far away from the actual data. we just remove them directly to avoid unnecessary deviations. later, it was shown how various huge numbers of columns of data can be converted to minimum data size with the help of principal component analysis and on the basis of independent variables. it was shown how very heavy data file of more than 200 columns were converted to only 17 principle components and how other columns were clustered together due to some kind of clustering in between them.",9,-10.062681,19.649061,9.14122,9.200611,"dataâ, analyse, analyses"
579,"at the beginning of the class logistic regression was visualized through graph on playground.tensorflow.org. confusion matrix was discussed upon . ratios such as true positive rate and false positive rate were introduced . then we discussed plot of tpr v/s fpr which is called roc ( receiver operating characteristic) curve , which tells us about the quality of the regression. auc of roc is between 0.5 and 1 , wher area=1 depicts a good classifier and a curve with area 0.5 is not accurate as it randomly classifies observations. after that we moved on to clustering (an unsupervised technique) , which are of two types k-means clustering and hierarchical clustering. in k-means clustering we need to know about the no. of clusters while in hierarchical we can get the no. of clusters depending upon where we cut the dendrogram. maximum no. of clusters being equal to the no. of observations.","at the beginning of the class logistic regression was visualized through graph on playground.tensorflow.org. confusion matrix was discussed upon . ratios such as true positive rate and false positive rate were introduced . then we discussed plot of tpr v/s fpr which is called roc ( receiver operating characteristic) curve , which tells us about the quality of the regression. auc of roc is between 0.5 and 1 , wher area=1 depicts a good classifier and a curve with area 0.5 is not accurate as it randomly classifies observations. after that we moved on to clustering (an unsupervised technique) , which are of two types k-means clustering and hierarchical clustering. in k-means clustering we need to know about the no. of clusters while in hierarchical we can get the no. of clusters depending upon where we cut the dendrogram. maximum no. of clusters being equal to the no. of observations.",8,-3.481815,-19.824661,6.3342233,0.5508205,"classification, clusterings, classifying"
580,"today in class we got into principal component analysis. it's advantages and disadvantages. it greatly reduces the dimensionality of the data sets given. we had two things pc1 and pc2. first captures maximum variance in the data and 2nd one next max variance. then we also discussed about vif, it removes multi collinearity in the data. also we again got to know the importance of eda and it's really great even if we are giving 80% time cleaning the data.","today in class we got into principal component analysis. it's advantages and disadvantages. it greatly reduces the dimensionality of the data sets given. we had two things pc1 and pc2. first captures maximum variance in the data and 2nd one next max variance. then we also discussed about vif, it removes multi collinearity in the data. also we again got to know the importance of eda and it's really great even if we are giving 80% time cleaning the data.",11,-12.655705,4.9614162,10.208166,12.822215,"pca, heatmap, heatmaps"
581,"today, we started by how different samples could produce different regression lines but that all such lines would go through the mean. we went on to look at the notions of statistical significance and statistical similarity. in this context, we pointed out that any value falling within a confidence interval (ci) is statistically similar, implying that any such value could have been obtained, depending on our sample.
a model is poor if the confidence interval contains zero because a regression coefficient of zero in linear regression produces a horizontal line, which is undesirable.
we then learned about multiple linear regression, where the goal is to model
y as a linear combination of various variables. we want to find the correct coefficients for the variables. as there is no closed-form solution, we will use an method called gradient descent.
we then looked at an example in excel, where we iteratively eliminated statistically insignificant estimates and saw that the performance of the model was not greatly affected. we then briefly touched on the f-value.","today, we started by how different samples could produce different regression lines but that all such lines would go through the mean. we went on to look at the notions of statistical significance and statistical similarity. in this context, we pointed out that any value falling within a confidence interval (ci) is statistically similar, implying that any such value could have been obtained, depending on our sample. a model is poor if the confidence interval contains zero because a regression coefficient of zero in linear regression produces a horizontal line, which is undesirable. we then learned about multiple linear regression, where the goal is to model y as a linear combination of various variables. we want to find the correct coefficients for the variables. as there is no closed-form solution, we will use an method called gradient descent. we then looked at an example in excel, where we iteratively eliminated statistically insignificant estimates and saw that the performance of the model was not greatly affected. we then briefly touched on the f-value.",2,17.467327,6.9309072,12.670369,4.114281,"regression, regressions, features"
582,"crisp-dm is a process-oriented, iterative data mining approach beginning with business and data understanding, preparing data, constructing models, assessing results, and implementing solutions. exploratory data analysis (eda) assists in revealing patterns, identifying anomalies, and visualizing distributionsâ€”such as glucose and bmi are reasonably normal, whereas insulin contains numerous outliers. missing values can be random or caused by certain problems, and methods such as knn can impute the missing values. class imbalance is also a problem, which needs to be handled carefully.","crisp-dm is a process-oriented, iterative data mining approach beginning with business and data understanding, preparing data, constructing models, assessing results, and implementing solutions. exploratory data analysis (eda) assists in revealing patterns, identifying anomalies, and visualizing distributions such as glucose and bmi are reasonably normal, whereas insulin contains numerous outliers. missing values can be random or caused by certain problems, and methods such as knn can impute the missing values. class imbalance is also a problem, which needs to be handled carefully.",9,-18.956985,22.881998,8.400639,8.733279,"dataâ, analyse, analyses"
583,"discussion upon logistic regression was taken further ,logistic regression is used to predict binary outcomes (0 or 1). it operates by taking input values, applying weights, adding a bias, and passing the result through a sigmoid function to obtain a probability. training involves adjusting the weights using gradient descent to minimize errors. the likelihood function aids in optimizing weights by maximizing the number of correct predictions. the modelâ€™s performance is assessed using a confusion matrix, accuracy, precision, and recall. a predicted probability above 0.5 results in an output of 1; otherwise, it is 0.","discussion upon logistic regression was taken further ,logistic regression is used to predict binary outcomes (0 or 1). it operates by taking input values, applying weights, adding a bias, and passing the result through a sigmoid function to obtain a probability. training involves adjusting the weights using gradient descent to minimize errors. the likelihood function aids in optimizing weights by maximizing the number of correct predictions. the model s performance is assessed using a confusion matrix, accuracy, precision, and recall. a predicted probability above 0.5 results in an output of 1; otherwise, it is 0.",12,8.494071,-21.27603,9.071169,-1.9270022,"classifiers, logistic, roc"
584,"linear regression in ms excel 
error matrics that can be calculated are mse, rmse, sse, mae.
sst is measure of total variation in given dataset. sse is variations not explained by model, they are attributed to random errors. ssr is total variation explained by lr model. sst = sse + ssr.
a good model is the model that explains most of the variations in the data.
the central limit theorem (clt) is a fundamental concept in statistics that describes theâ 
distribution of sample means for a sufficiently large sample, regardless of the shape of theâ 
original population distribution.","linear regression in ms excel error matrics that can be calculated are mse, rmse, sse, mae. sst is measure of total variation in given dataset. sse is variations not explained by model, they are attributed to random errors. ssr is total variation explained by lr model. sst = sse + ssr. a good model is the model that explains most of the variations in the data. the central limit theorem (clt) is a fundamental concept in statistics that describes the distribution of sample means for a sufficiently large sample, regardless of the shape of the original population distribution.",5,24.746136,0.8757476,14.301187,4.28882,"regression, statistical, statistics"
585,learnt analysing data in excel and different new terms of data analysis,learnt analysing data in excel and different new terms of data analysis,6,-15.918544,30.708454,7.2245064,9.583129,"summarizing, summarize, summarization"
586,firstly we discuss the classes how much are left and and how much we have to attend the reamaining of the 10 classes to get the marks then we discuss about midsem answers and project work . then we discussed about feature engineering which is a crucial aspect of machine learning and data analysis where you transform raw data into meaningful features that can improve model performance. it involves creating new features from existing ones or selecting the most relevant features for your model.,firstly we discuss the classes how much are left and and how much we have to attend the reamaining of the 10 classes to get the marks then we discuss about midsem answers and project work . then we discussed about feature engineering which is a crucial aspect of machine learning and data analysis where you transform raw data into meaningful features that can improve model performance. it involves creating new features from existing ones or selecting the most relevant features for your model.,13,-2.7764034,5.2877345,9.321457,3.8394322,"classification, classifying, classifications"
587,"we learned to use the data analysis pack in excel a bit. also we were given the dataset and we made the regression plot in the excel and using data analysis pack got the summary of data and then learned abt the confidence interval, errors etc. then we learned abt the r^2 number","we learned to use the data analysis pack in excel a bit. also we were given the dataset and we made the regression plot in the excel and using data analysis pack got the summary of data and then learned abt the confidence interval, errors etc. then we learned abt the r^2 number",6,17.85141,-4.788442,14.125091,5.3216066,"summarizing, summarize, summarization"
588,"we started with logistic regression and learnt how to calculate the weights. we want to maximize the likelihood of our predicted outcomes being close to the targets. if t=1 we would be maximizing p and if t=0 we would be maximizing (1-p).in order to find w there should be the goals across all the data(i.e. training data=n observations). maximizing likelihood is same as minimizing the error function.since there are product terms and it is difficult to work with those hence we take logarithm so that we get sums.
we create a confusion matrix:-true negative,false positive,false negative,true positive.
then we defined some terms:-
accuracy=(true positive+true negative)/total no. of events
precision:of the events we have detected how many have we detected correctly 
recall:of a specific class how many events have we identified correctly
f1 value-harmonic mean of precision and accuracy(doesn't give false hope unlike accuracy)","we started with logistic regression and learnt how to calculate the weights. we want to maximize the likelihood of our predicted outcomes being close to the targets. if t=1 we would be maximizing p and if t=0 we would be maximizing (1-p).in order to find w there should be the goals across all the data(i.e. training data=n observations). maximizing likelihood is same as minimizing the error function.since there are product terms and it is difficult to work with those hence we take logarithm so that we get sums. we create a confusion matrix:-true negative,false positive,false negative,true positive. then we defined some terms:- accuracy=(true positive+true negative)/total no. of events precision:of the events we have detected how many have we detected correctly recall:of a specific class how many events have we identified correctly f1 value-harmonic mean of precision and accuracy(doesn't give false hope unlike accuracy)",10,15.454836,-20.366343,8.651991,-1.7523768,"classifications, histograms, histogram"
589,"data analysis begins with understanding the problem, which can be achieved through both visual and mathematical visualization. visual tools like scatter plots, histograms, and box plots help in identifying trends and patterns, while mathematical visualization through statistical summaries and correlation matrices provides deeper insights into data distributions. 
a structured approach to data mining follows the crisp-dm framework, which consists of six cyclical steps: business understanding, data understanding, data preparation, modeling, evaluation, and deployment. during data exploration, heteroscedasticity (unequal variance of errors) and homoscedasticity (constant variance) must be examined, as heteroscedasticity can lead to unreliable models. another concern is multicollinearity, where highly correlated features can distort regression models. this can be addressed by removing variables, or using pca.

handling missing data is also crucial, using methods like mean/mode imputation or predictive modeling. similarly, outliers need proper treatment using techniques like transformation, or removal to ensure accurate and reliable data analysis.","data analysis begins with understanding the problem, which can be achieved through both visual and mathematical visualization. visual tools like scatter plots, histograms, and box plots help in identifying trends and patterns, while mathematical visualization through statistical summaries and correlation matrices provides deeper insights into data distributions. a structured approach to data mining follows the crisp-dm framework, which consists of six cyclical steps: business understanding, data understanding, data preparation, modeling, evaluation, and deployment. during data exploration, heteroscedasticity (unequal variance of errors) and homoscedasticity (constant variance) must be examined, as heteroscedasticity can lead to unreliable models. another concern is multicollinearity, where highly correlated features can distort regression models. this can be addressed by removing variables, or using pca. handling missing data is also crucial, using methods like mean/mode imputation or predictive modeling. similarly, outliers need proper treatment using techniques like transformation, or removal to ensure accurate and reliable data analysis.",9,-17.836655,20.013144,8.302946,8.604299,"dataâ, analyse, analyses"
590,"our first focus was on pivot tables in excel, and how they assist in summarizing datasets by sorting and grouping values. we learned how to build the pivot tables, how to set fields by dragging and dropping, and how to calculate totals, averages, count etc in order to analyze data .

then we studied different eda techniques. we calculated summary statistics such as mean and median, variance, and standard deviation in order to understand the dataâ€™s distribution. we also studied some tools of visualization like histograms, box plots, and scatter plots to see how they help to show patterns and shapes of distribution and outliers within a dataset.

we also discussed the analysis issue where one category of class dominates the other. lastly, we touched upon feature engineering and data transformation techniques that are needed to improve data quality before any further analysis is performed.","our first focus was on pivot tables in excel, and how they assist in summarizing datasets by sorting and grouping values. we learned how to build the pivot tables, how to set fields by dragging and dropping, and how to calculate totals, averages, count etc in order to analyze data . then we studied different eda techniques. we calculated summary statistics such as mean and median, variance, and standard deviation in order to understand the data s distribution. we also studied some tools of visualization like histograms, box plots, and scatter plots to see how they help to show patterns and shapes of distribution and outliers within a dataset. we also discussed the analysis issue where one category of class dominates the other. lastly, we touched upon feature engineering and data transformation techniques that are needed to improve data quality before any further analysis is performed.",6,-14.010891,26.937931,7.1903067,9.986194,"summarizing, summarize, summarization"
591,"the lecture was focused on various key concepts in data measurement and machine learning. it began with the levels of measurement, which include nominal, ordinal, interval, and ratio levels. nominal scales categorize data without any order, while ordinal scales add meaningful order but lack measurable distances. interval scales offer numeric measurement with equal intervals but no true zero, and ratio scales provide both equal intervals and a true zero, enabling magnitude comparison.
the lecture distinguished between supervised and unsupervised learning. supervised learning involves labeled data where the dependent variable is known, and examples include regression and classification techniques. in contrast, unsupervised learning deals with unlabeled data, focusing on clustering methods such as k-means and hierarchical clustering to identify patterns within the data.
the discussion on regression covered various types, including single linear regression, multiple regression, logistic regression, polynomial regression, and random forest models. 
the lecture also touched upon the concepts of population and sample data, stressing the need for a representative sample in data analysis","the lecture was focused on various key concepts in data measurement and machine learning. it began with the levels of measurement, which include nominal, ordinal, interval, and ratio levels. nominal scales categorize data without any order, while ordinal scales add meaningful order but lack measurable distances. interval scales offer numeric measurement with equal intervals but no true zero, and ratio scales provide both equal intervals and a true zero, enabling magnitude comparison. the lecture distinguished between supervised and unsupervised learning. supervised learning involves labeled data where the dependent variable is known, and examples include regression and classification techniques. in contrast, unsupervised learning deals with unlabeled data, focusing on clustering methods such as k-means and hierarchical clustering to identify patterns within the data. the discussion on regression covered various types, including single linear regression, multiple regression, logistic regression, polynomial regression, and random forest models. the lecture also touched upon the concepts of population and sample data, stressing the need for a representative sample in data analysis",4,-21.364222,-14.881875,2.1856883,0.51562065,"classification, classifying, classifications"
592,"we continued with the correlation problems. we observed the heat maps for perfectly co-related data and partially correlated data. heatmaps are more coherent if they are created after sorting the data. when we calculate vif, we do not process the values of y but only x-values. vif is simply the function of x. f(x(i != j)). in this method, we progressively eliminate the features based on threshold vif. we have to decide on the threshold value of r^2 because we only choose the vif values below the threshold. we have to check if the feature being eliminated is important for the data or not. if it is important, we can't just eliminate it. 
lesson: don't do think mechanically. 
principal component analysis: principal components are necessarily orthogonal to each other. number of principal components is equal number of dimensions of the data. out of these principal components, we have to decide which among these are important. pc1 is the axis that captures the best variance in the data, pc2 is the second best axis to capture variance. pca actually helps us to reduce the dimensionality of the data. we plot the fraction of variance for these components. these plots are called elbow plot. each principal component is a mixture of features multiplied by loading. but do not blindly follow the process. first check if the dimensionality should need to be reduced or not. in general vif needs to be done first, because inter-collinearity is a big problem. 
after pca, instead of representing y as a function of x, we represent it in the form of function of pca. after representing we do what-if analysis on the function. we later explain the results however with pca we lose explainability. we can't do any sensitive analyses. vif reduced the number of features in real space. and if we still have many methods, we can go for pca. 
pca helps in dimension reduction, helps predicting model, helps in visualization of data. when pca is used for visualization, which comes under eda. pca pre-supposes normalization in the data. we need to normalize the data before using pca. we then tried these method on the data from midsem and noticed that out of 24, 18 features were needed to explain the data. pca is a lossless method, data is not lost. 
we started with t-sne (t-distributed stochastic neighbor encoding). t stands for t-distribution. probability wise, t-distribution predicts more probability for wide spread data; gaussian on the other hand is tighter. it is stochastic in nature and gradient descent is involved in the process, hence while running itâ€™s code, it is likely to take time. here we lose the exactness of data but gain the closeness of data. we can map multi-dimensional data onto lower dimensional data. we usually do not prefer to plot beyond 3 dimensions.","we continued with the correlation problems. we observed the heat maps for perfectly co-related data and partially correlated data. heatmaps are more coherent if they are created after sorting the data. when we calculate vif, we do not process the values of y but only x-values. vif is simply the function of x. f(x(i != j)). in this method, we progressively eliminate the features based on threshold vif. we have to decide on the threshold value of r^2 because we only choose the vif values below the threshold. we have to check if the feature being eliminated is important for the data or not. if it is important, we can't just eliminate it. lesson: don't do think mechanically. principal component analysis: principal components are necessarily orthogonal to each other. number of principal components is equal number of dimensions of the data. out of these principal components, we have to decide which among these are important. pc1 is the axis that captures the best variance in the data, pc2 is the second best axis to capture variance. pca actually helps us to reduce the dimensionality of the data. we plot the fraction of variance for these components. these plots are called elbow plot. each principal component is a mixture of features multiplied by loading. but do not blindly follow the process. first check if the dimensionality should need to be reduced or not. in general vif needs to be done first, because inter-collinearity is a big problem. after pca, instead of representing y as a function of x, we represent it in the form of function of pca. after representing we do what-if analysis on the function. we later explain the results however with pca we lose explainability. we can't do any sensitive analyses. vif reduced the number of features in real space. and if we still have many methods, we can go for pca. pca helps in dimension reduction, helps predicting model, helps in visualization of data. when pca is used for visualization, which comes under eda. pca pre-supposes normalization in the data. we need to normalize the data before using pca. we then tried these method on the data from midsem and noticed that out of 24, 18 features were needed to explain the data. pca is a lossless method, data is not lost. we started with t-sne (t-distributed stochastic neighbor encoding). t stands for t-distribution. probability wise, t-distribution predicts more probability for wide spread data; gaussian on the other hand is tighter. it is stochastic in nature and gradient descent is involved in the process, hence while running it s code, it is likely to take time. here we lose the exactness of data but gain the closeness of data. we can map multi-dimensional data onto lower dimensional data. we usually do not prefer to plot beyond 3 dimensions.",11,-18.949152,3.7903657,10.460431,13.4144,"pca, heatmap, heatmaps"
593,"in today's class, we first started with a website named playground.tensorflow.org in which we had several options of several features and different kinds of data types and we came to our conclusion that by increasing the number of features we can even use the simplest of the model to get the appropriate results that is the correct model. delhi came to know about true positive rate and false positive rate and their formulae. then we discussed the receiver operating characteristic curve which shows when does the classifier start detecting incorrect or the false positives like if the curve is steeply rising in the starting and then when it completely rows then it is turning towards the right then it is accurate rather than when it is less steeply rising or rising like a curve. if we add more features roc will get better this is called feature engineering. auc is the area under the roc curve and when auc is 5 this means that there is no classifier or there is no need of any classifier because it is something like tossing a coin. we used to do like if probability is greater than equal to 0.5 then it is class one otherwise class zero so what happens if we change this probability to 0.4 or 0.3, so for that the different points on the roc curve correspond to these various different thresholds. then using a simple data set we calculated precision and recall of different classes. we can convert a regression problem to a classification problem like if we have heights we can create bins like 2 - 4 feet ,  4 - 6 feet like that. but classification to regression is not possible because data loss happens in such a case. when we started with the clustering which is a kind of unsupervised learning which means that we dont have y value we only have x value. we then learned about king's clustering in which we pre decide the number of clusters and how this mechanism works. then we learned hierarchical clustering in which there is no need to specify the number of clusters and which results in dendrogram.","in today's class, we first started with a website named playground.tensorflow.org in which we had several options of several features and different kinds of data types and we came to our conclusion that by increasing the number of features we can even use the simplest of the model to get the appropriate results that is the correct model. delhi came to know about true positive rate and false positive rate and their formulae. then we discussed the receiver operating characteristic curve which shows when does the classifier start detecting incorrect or the false positives like if the curve is steeply rising in the starting and then when it completely rows then it is turning towards the right then it is accurate rather than when it is less steeply rising or rising like a curve. if we add more features roc will get better this is called feature engineering. auc is the area under the roc curve and when auc is 5 this means that there is no classifier or there is no need of any classifier because it is something like tossing a coin. we used to do like if probability is greater than equal to 0.5 then it is class one otherwise class zero so what happens if we change this probability to 0.4 or 0.3, so for that the different points on the roc curve correspond to these various different thresholds. then using a simple data set we calculated precision and recall of different classes. we can convert a regression problem to a classification problem like if we have heights we can create bins like 2 - 4 feet , 4 - 6 feet like that. but classification to regression is not possible because data loss happens in such a case. when we started with the clustering which is a kind of unsupervised learning which means that we dont have y value we only have x value. we then learned about king's clustering in which we pre decide the number of clusters and how this mechanism works. then we learned hierarchical clustering in which there is no need to specify the number of clusters and which results in dendrogram.",8,1.0564121,-19.729603,6.9185047,0.28887135,"classification, clusterings, classifying"
594,"first today we learned abt the playgroundtensor site. it was related to neural network but as in data science, ml models are often used for predicting outcomes based on data similarly in tensorflow allows you to explore how different neural network architectures affect the ability of model to learn data.then we learned abt the confusion matrix which provides summary of prediction of results of comparing actual vs. predicted values. it is abt false negative and true positive etc. then we saw the dendogram which is the hierarchy clustering tree. we also discussed abt data imbalance which occurs when one class in a dataset has significantly more samples than other","first today we learned abt the playgroundtensor site. it was related to neural network but as in data science, ml models are often used for predicting outcomes based on data similarly in tensorflow allows you to explore how different neural network architectures affect the ability of model to learn data.then we learned abt the confusion matrix which provides summary of prediction of results of comparing actual vs. predicted values. it is abt false negative and true positive etc. then we saw the dendogram which is the hierarchy clustering tree. we also discussed abt data imbalance which occurs when one class in a dataset has significantly more samples than other",13,-1.7966932,-9.284643,9.001687,3.8009694,"classification, classifying, classifications"
595,"we explored how empirical equations were traditionally used in the past and how modern machine learning algorithms have now taken their place. we also learned about the levels of measurement, namely nominal, ordinal, interval, and ratio, along with the encoding methods for ordinal data. additionally, we discussed the differences between supervised and unsupervised learning, particularly how predictions are made in the absence of label information. the session further covered key concepts such as data, population, and sample, emphasizing their importance in analysis and prediction.","we explored how empirical equations were traditionally used in the past and how modern machine learning algorithms have now taken their place. we also learned about the levels of measurement, namely nominal, ordinal, interval, and ratio, along with the encoding methods for ordinal data. additionally, we discussed the differences between supervised and unsupervised learning, particularly how predictions are made in the absence of label information. the session further covered key concepts such as data, population, and sample, emphasizing their importance in analysis and prediction.",4,-20.114122,-11.997989,2.333548,0.6732849,"classification, classifying, classifications"
596,"the class started with the recall of t-sne algorithm. though we can create clusters but we should not create prediction models based on this. because t-sne converts n-dimensional data into two  dimensional and then form clusters on top of it. 
sir, then discussed about the project and we started a new topic: feature encoding.  
when y is categorical and some of x is categorical, they need to be encoded prior to training model. we need the values to be numerical and not categorical. one way to encode is vectorize (one hot encoding). if y has three values red, blue, green then we have y1, y2, y3 and assign 0 and 1 depending on the value of y. it works like y1 = f(x), y2 = f(x) and y3 = f(x). if we use logistic regression, then we predict three models. but the problem with this is that the three models are independent of each other. the main problems that arise are: multiclass problem, multilabel problem. 
when dealing with multilabel problem: expected outcome is one class out of many class. when dealing with multilabel problem: expected outcome is the total labels that are possible. the way in which we encode y is different for both the problems. in multiclass we encode with differential ways, like 0, 1, 2â€¦, in multilabel problem we encode such that values depend on one another. 
when we are working in multiclass problem and the dependent variable is categorical, we go with label encoding. giving numerical values to data. we should not do this for x, only y should be encoded in this since. another way is integer encoding: it has inherent sense of hierarchy. 
one- hot encoding has the problem that it brings more dimensions. example: when we are dealing with pin-codes. when we have nominal data we can use one hot encoding. we do not use one hot encoding for ordinal data. 
we then move to binary encoding. this is similar to one-hot encoding except there are more options to describe data. 
then we have frequency encoding. it uses the frequency measure as the encoding. we do not use this encoding for y, because we need distinct values for y. it can happen that two different y values get in same frequency. in target encoding, the values are replaced by the average value. 
then we looked at feature binning. it is used when continuous features need to be converted into categorical features. 

then we started: how to process text data? 
most llm are the examples of the manner in which statistical processing can generate deterministic output (code generation). the basic question is: how to convert text to numbers so that processing can be made efficient.","the class started with the recall of t-sne algorithm. though we can create clusters but we should not create prediction models based on this. because t-sne converts n-dimensional data into two dimensional and then form clusters on top of it. sir, then discussed about the project and we started a new topic: feature encoding. when y is categorical and some of x is categorical, they need to be encoded prior to training model. we need the values to be numerical and not categorical. one way to encode is vectorize (one hot encoding). if y has three values red, blue, green then we have y1, y2, y3 and assign 0 and 1 depending on the value of y. it works like y1 = f(x), y2 = f(x) and y3 = f(x). if we use logistic regression, then we predict three models. but the problem with this is that the three models are independent of each other. the main problems that arise are: multiclass problem, multilabel problem. when dealing with multilabel problem: expected outcome is one class out of many class. when dealing with multilabel problem: expected outcome is the total labels that are possible. the way in which we encode y is different for both the problems. in multiclass we encode with differential ways, like 0, 1, 2 , in multilabel problem we encode such that values depend on one another. when we are working in multiclass problem and the dependent variable is categorical, we go with label encoding. giving numerical values to data. we should not do this for x, only y should be encoded in this since. another way is integer encoding: it has inherent sense of hierarchy. one- hot encoding has the problem that it brings more dimensions. example: when we are dealing with pin-codes. when we have nominal data we can use one hot encoding. we do not use one hot encoding for ordinal data. we then move to binary encoding. this is similar to one-hot encoding except there are more options to describe data. then we have frequency encoding. it uses the frequency measure as the encoding. we do not use this encoding for y, because we need distinct values for y. it can happen that two different y values get in same frequency. in target encoding, the values are replaced by the average value. then we looked at feature binning. it is used when continuous features need to be converted into categorical features. then we started: how to process text data? most llm are the examples of the manner in which statistical processing can generate deterministic output (code generation). the basic question is: how to convert text to numbers so that processing can be made efficient.",3,-45.426044,3.9465313,0.16384022,6.473483,"categorical, categorization, categorise"
597,"we started our lecture by visiting an interesting website playground.tensorflow.org. where we can create different types of samples and apply ann to classify the elements of this sample. we then jumped into our jupyter notebook where we learnt that we don't have to trust the logist_regr.score and instead we prefer to look at true positive rate. we then roc curve of tp rate vs fp rate. we would like it to have auc = 1. even a random classification has auc = 0.5. we then had a look at a case where a particular class is under represented. it generally happens when a particular class is representing fraudulent entries. it is a huge problem as this class usually gets misclassified. precision and recall values of this class were very low. we concluded our lecture with a brief discussion on k-means clustering where we had a look on linkages, number of nodes.","we started our lecture by visiting an interesting website playground.tensorflow.org. where we can create different types of samples and apply ann to classify the elements of this sample. we then jumped into our jupyter notebook where we learnt that we don't have to trust the logist_regr.score and instead we prefer to look at true positive rate. we then roc curve of tp rate vs fp rate. we would like it to have auc = 1. even a random classification has auc = 0.5. we then had a look at a case where a particular class is under represented. it generally happens when a particular class is representing fraudulent entries. it is a huge problem as this class usually gets misclassified. precision and recall values of this class were very low. we concluded our lecture with a brief discussion on k-means clustering where we had a look on linkages, number of nodes.",8,2.09035,-18.648901,6.9785423,0.20507933,"classification, clusterings, classifying"
598,"this lecture was mostly based on feature encoding. feature encoding is done when the dependent or independent variables in your data is categorical. you need to convert this categorical data type to numerical so that it can be processed by the computer. there are different ways by which this can be done. we can expect two types of output through this. one is that there are multiple classes, out of which there is only one class which the y represents at a time. this is known as â€˜multiclass problemâ€™. another is the one in which two or more labels can be present in a single y value. this is known as â€˜multilabel problemâ€™. for example of multiclass problems, we can consider a situation in which the y value is some color. so, the color can be any of the decided number of color options. we can use label encoding for this type of problem. it just assigns labels to each of the possible options (in this case, the colors). these labels donâ€™t have any inherent value, they just represent the different classes. 
for multilabel problems, we can consider the situation in which the y value is an image which consists of both dog and cat. so, here for single y we can associate it with two different labels, cat and the dog.
for such data sets where y values are nominal, we use label encoding. 
but, if the y values have ordinal level of measurement, then we can use integer encoding. it assigns integer values to each of the class and these integer values also have inherent value associated with them, by which we can compare between two distinct values. 
the other types of encoding methods include:
1)	one- hot encoding: in this, we assign values of 1s and 0s to all the available classes. we basically create vectors, whose components can be either 1 or 0, depending on the actual class. so, for one feature, we create n different columns. â€˜nâ€™ depends on the number of classes associated with that feature. this increases the dimensionality. so, it shouldnâ€™t be used when there are large number of classes associated with a feature.
2)	binary encoding: it is also known as â€˜pseudo one-hot encodingâ€™. instead of using separate column for each class and unnecessarily increasing the dimensionality, we can reduce the number of columns used by using a combination of 1s and 0s to represent the different classes, instead of using just one column with 1 for each class.
3)	frequency encoding: in this type of encoding, the value associated with each class is just equal to its frequency in the data. this is generally not used for y variable as it may happen that two different classes in y have the same frequency. so, in this case, we wonâ€™t get â€˜distinctâ€™ label for each class.
4)	target encoding: in this, the value for each class is just the average of the corresponding y values associated with all the occurrences of that class.
5)	feature binning: in this, the numerical values are categorized (divided into categories) to turn the problem into a classification problem.
next, we just discussed a bit on how we can process text data. all the above techniques were used when the features were already known to us. but in case of text data, we donâ€™t have the features, we need to create them.
also, we need to convert a word to some numerical value, so that the computer can deal with it. a problem which can be encountered here is that one word can have multiple meanings in english language. so, we should be very careful while determining the meaning of a statement. the context of the statement should be clear before making any kind of predictions.","this lecture was mostly based on feature encoding. feature encoding is done when the dependent or independent variables in your data is categorical. you need to convert this categorical data type to numerical so that it can be processed by the computer. there are different ways by which this can be done. we can expect two types of output through this. one is that there are multiple classes, out of which there is only one class which the y represents at a time. this is known as multiclass problem . another is the one in which two or more labels can be present in a single y value. this is known as multilabel problem . for example of multiclass problems, we can consider a situation in which the y value is some color. so, the color can be any of the decided number of color options. we can use label encoding for this type of problem. it just assigns labels to each of the possible options (in this case, the colors). these labels don t have any inherent value, they just represent the different classes. for multilabel problems, we can consider the situation in which the y value is an image which consists of both dog and cat. so, here for single y we can associate it with two different labels, cat and the dog. for such data sets where y values are nominal, we use label encoding. but, if the y values have ordinal level of measurement, then we can use integer encoding. it assigns integer values to each of the class and these integer values also have inherent value associated with them, by which we can compare between two distinct values. the other types of encoding methods include: 1) one- hot encoding: in this, we assign values of 1s and 0s to all the available classes. we basically create vectors, whose components can be either 1 or 0, depending on the actual class. so, for one feature, we create n different columns. n depends on the number of classes associated with that feature. this increases the dimensionality. so, it shouldn t be used when there are large number of classes associated with a feature. 2) binary encoding: it is also known as pseudo one-hot encoding . instead of using separate column for each class and unnecessarily increasing the dimensionality, we can reduce the number of columns used by using a combination of 1s and 0s to represent the different classes, instead of using just one column with 1 for each class. 3) frequency encoding: in this type of encoding, the value associated with each class is just equal to its frequency in the data. this is generally not used for y variable as it may happen that two different classes in y have the same frequency. so, in this case, we won t get distinct label for each class. 4) target encoding: in this, the value for each class is just the average of the corresponding y values associated with all the occurrences of that class. 5) feature binning: in this, the numerical values are categorized (divided into categories) to turn the problem into a classification problem. next, we just discussed a bit on how we can process text data. all the above techniques were used when the features were already known to us. but in case of text data, we don t have the features, we need to create them. also, we need to convert a word to some numerical value, so that the computer can deal with it. a problem which can be encountered here is that one word can have multiple meanings in english language. so, we should be very careful while determining the meaning of a statement. the context of the statement should be clear before making any kind of predictions.",3,-44.832634,2.4842825,0.091912426,6.4796195,"categorical, categorization, categorise"
599,"today, we went over crisp-dm, a six-step framework for solving data science problems, and the importance of exploratory data analysis (eda) with examples like pima india diabetes, ganga water quality (2012), and mumbai aqi. we explored box plots, matrix plots, and how class imbalance affects analysis.
the ta explained handling missing data (mcar, mar, mnar) using univariate methods (dropping or filling with mean/median/mode) and multivariate methods (knn, mice, regression models). we also covered outliers, their causes, and ways to handle them using iqr, isolation forests, and db scan. the median is more reliable than the mean when dealing with outliers.
lastly, we touched on the confusion matrix for multi-class classification, and sir mentioned that the midsem will focus on concepts, eda, and visualizationsâ€”no derivations.","today, we went over crisp-dm, a six-step framework for solving data science problems, and the importance of exploratory data analysis (eda) with examples like pima india diabetes, ganga water quality (2012), and mumbai aqi. we explored box plots, matrix plots, and how class imbalance affects analysis. the ta explained handling missing data (mcar, mar, mnar) using univariate methods (dropping or filling with mean/median/mode) and multivariate methods (knn, mice, regression models). we also covered outliers, their causes, and ways to handle them using iqr, isolation forests, and db scan. the median is more reliable than the mean when dealing with outliers. lastly, we touched on the confusion matrix for multi-class classification, and sir mentioned that the midsem will focus on concepts, eda, and visualizations no derivations.",9,-15.637034,15.588309,8.861274,8.798459,"dataâ, analyse, analyses"
600,"consider a sample s1 from a population. we used this data to fit a line using linear regression. we have a value of a (y=ax+b). we then estimate the 95% confidence interval of a. any two values which lie in the 95% confidence interval are considered statistically similar and not significant. because a value outside the 95% confidence interval has very low chance of occurring it is statistically significant. in linear regression we don't want value of a to be zero . so for a good model we want zero to lie outside 95% ci. 
multiple linear regression deals with more than one independent variable y=a0+a1x1+a2x2+a3x3+......+anxn...   . converting text into a vector is called embedding vector. for sales--->(age, earnings, location,.... ). (x1,x2,x3,....)-->features. feature engineering is calculating useful additional features obtained by operating on existing features to draw meaningful conclusions. we use mlr gradient descent to find the features. matrices are used in derivations y(mx1)=x(mxl)b(lx1)+e(mx1). e-error term. here we describing all points not  the regression line.(in slr we describe y=ax+b, but here y=xb+e where e is error term).m= number of observations, k= number of features. we need to minimise the cost function.  the gradient descent process: 1) assume some value for b.(generally initialised to all 0s or 1s) 2) using y_hat=xb evaluate y_hat. 3) calculate dj/db as per the above expression by assuming some value for n. $) calculate new values for b using the following expression b_new=b_old-n(grad_j). 5) repeat steps 2 to 4 until abs (b_new-b_old) reaches a threshold level(eg:0.0001). finally we end up with a b(column vector) which are our features. f value=msr(mean square due to regression)/mse(mean square due to random error). sse, mse, rmse, mae are useful for comparing between two samples. to qualitatively make some conclusion from data, we need to calculate some other values from these. in mlr based on the coefficients we can understand which features effect the output most. anova- analysis of variance. we drop variable with highest p value. more variables mean more r squared value. when we drop variables with higher p values we see a decrease(very less) in r squared value and f value increases. we want p value to be less than 0.05 for the coefficients to be significant.","consider a sample s1 from a population. we used this data to fit a line using linear regression. we have a value of a (y=ax+b). we then estimate the 95% confidence interval of a. any two values which lie in the 95% confidence interval are considered statistically similar and not significant. because a value outside the 95% confidence interval has very low chance of occurring it is statistically significant. in linear regression we don't want value of a to be zero . so for a good model we want zero to lie outside 95% ci. multiple linear regression deals with more than one independent variable y=a0+a1x1+a2x2+a3x3+......+anxn... . converting text into a vector is called embedding vector. for sales--->(age, earnings, location,.... ). (x1,x2,x3,....)-->features. feature engineering is calculating useful additional features obtained by operating on existing features to draw meaningful conclusions. we use mlr gradient descent to find the features. matrices are used in derivations y(mx1)=x(mxl)b(lx1)+e(mx1). e-error term. here we describing all points not the regression line.(in slr we describe y=ax+b, but here y=xb+e where e is error term).m= number of observations, k= number of features. we need to minimise the cost function. the gradient descent process: 1) assume some value for b.(generally initialised to all 0s or 1s) 2) using y_hat=xb evaluate y_hat. 3) calculate dj/db as per the above expression by assuming some value for n. $) calculate new values for b using the following expression b_new=b_old-n(grad_j). 5) repeat steps 2 to 4 until abs (b_new-b_old) reaches a threshold level(eg:0.0001). finally we end up with a b(column vector) which are our features. f value=msr(mean square due to regression)/mse(mean square due to random error). sse, mse, rmse, mae are useful for comparing between two samples. to qualitatively make some conclusion from data, we need to calculate some other values from these. in mlr based on the coefficients we can understand which features effect the output most. anova- analysis of variance. we drop variable with highest p value. more variables mean more r squared value. when we drop variables with higher p values we see a decrease(very less) in r squared value and f value increases. we want p value to be less than 0.05 for the coefficients to be significant.",2,15.408745,6.3318467,12.614457,4.015689,"regression, regressions, features"
601,evaluated metrics for midsem and exercise.,evaluated metrics for midsem and exercise.,6,11.552178,19.019476,9.389709,5.3984375,"summarizing, summarize, summarization"
602,"discussed the purpose of histograms in visualizing data distribution. learned that when the output is influenced by many unknown random causes, the observed distribution may follow a normal distribution. highlighted that an error histogram not matching the norma distribution indicates the model fails to capture the trend in the error function.

uploaded a csv file into excel for analysis. performed linear regression to fit a model and plotted scatterplots:scatterplots for x vs. y and for errors were created. errors appeared random in the scatterplot but showed a uniform distribution in the histogram.

   - defined sst (sum of squares total), which is the total variation in the data.
     - sst = ssr (sum of squares regression) + sse (sum of squares error).
     - sse captures variation not explained by the model (random error).
     - ssr captures variation explained by the regression line.
   - introduced râ² (coefficient of determination)
     - râ² = ssr / sst.
     - a higher râ² indicates that more variance is captured by the model.
   - correlation coefficient (r) in simple linear regression:
     - r = âˆš(râ²), representing the strength and direction of the relationship.

explained how to derive confidence intervals using regression coefficients (bâ‚€ and bâ‚).divided the sample into subsets, calculated their means, and plotted a histogram of means, which resembled a normal distribution due to randomness.

  explored terms like râ², 95% confidence interval, and their interpretation in regression output. discussed positive and negative correlations.","discussed the purpose of histograms in visualizing data distribution. learned that when the output is influenced by many unknown random causes, the observed distribution may follow a normal distribution. highlighted that an error histogram not matching the norma distribution indicates the model fails to capture the trend in the error function. uploaded a csv file into excel for analysis. performed linear regression to fit a model and plotted scatterplots:scatterplots for x vs. y and for errors were created. errors appeared random in the scatterplot but showed a uniform distribution in the histogram. - defined sst (sum of squares total), which is the total variation in the data. - sst = ssr (sum of squares regression) + sse (sum of squares error). - sse captures variation not explained by the model (random error). - ssr captures variation explained by the regression line. - introduced r (coefficient of determination) - r = ssr / sst. - a higher r indicates that more variance is captured by the model. - correlation coefficient (r) in simple linear regression: - r = (r ), representing the strength and direction of the relationship. explained how to derive confidence intervals using regression coefficients (b and b ).divided the sample into subsets, calculated their means, and plotted a histogram of means, which resembled a normal distribution due to randomness. explored terms like r , 95% confidence interval, and their interpretation in regression output. discussed positive and negative correlations.",5,22.595652,-6.0188346,14.361204,4.814812,"regression, statistical, statistics"
603,"in this lecture, we learn about simple linear regression, what are parameters and statistics, how to find the coefficients bo and b1 in the prediction line y=mx+c. also both the mean of observations line on the prediction line. to find the prediction line, we have to minimize the sum of square of the errors sse. error = distance between the predicted label (from prediction line)and the actual label(from data). attributes calculated from sample are called statistic (e.g, mean, meadian, mode, sd, etc of sample), using this we try to predict/estimate the same for the entire population. different level of measurements  have different attributes and operations to be performed on the data. no one can say that the attributes of sample are exactly same as that of population with 100% confidence. if we took a point estimatie instead of prediction line or anyother possible bestfitted curve, the confidence is likely to be 0%.thus here come the importance of confidence interval(higher ci leads to larger ranges). also, we learned about what is bias (c-intercept of prediction line). bias accounts for the influences of all unknown variables on the  label(y). bias is zero only when we known the system entire, we know all the independent variables and their influence on the dependent variable.","in this lecture, we learn about simple linear regression, what are parameters and statistics, how to find the coefficients bo and b1 in the prediction line y=mx+c. also both the mean of observations line on the prediction line. to find the prediction line, we have to minimize the sum of square of the errors sse. error = distance between the predicted label (from prediction line)and the actual label(from data). attributes calculated from sample are called statistic (e.g, mean, meadian, mode, sd, etc of sample), using this we try to predict/estimate the same for the entire population. different level of measurements have different attributes and operations to be performed on the data. no one can say that the attributes of sample are exactly same as that of population with 100% confidence. if we took a point estimatie instead of prediction line or anyother possible bestfitted curve, the confidence is likely to be 0%.thus here come the importance of confidence interval(higher ci leads to larger ranges). also, we learned about what is bias (c-intercept of prediction line). bias accounts for the influences of all unknown variables on the label(y). bias is zero only when we known the system entire, we know all the independent variables and their influence on the dependent variable.",1,31.93876,-13.250677,16.346622,3.9486215,"population, models, estimating"
604,"today's class start with a discussion on the value of beta_1, which is statistically significant or not. instead of checking if zero lies between beta_1l and  beta_1h, we can alternatively check if beta_1 lies within /  outside the 90% or 95% confidence interval defined  around zero. so, if beta_1 is within the confidence interval around zero, it is not statistically significant, and if it outside the ci around  zero it is said to be statistically significant.
the p_value associated with beta_1 tells us about the  position of beta_1 w.r.t. zero. if p_values is less than  0.05, then beta_1 is outside the 95% ci around zero and hence it is statistically significant. the opposite is true if the  p_values is greater than 0.05. further ahead we talked about the multiple linear regression, we have y=b0 + b1x1 + b2x2 + b3x3... here [x1, x2, x3, x4....] are the independent variables, also known as features. in mlr our goal is to express 'y' as a linear combination of x1, x2, x3..... . examples like -> a) photos-> the pixels in photos can be considered as features. b) in case of ""text"" we convert it into vectors by a method embedding. c) in sales prediction we have features like age, earning and location. next we learned about the gradient descent method which is an optimization technique which are used to find the minimum of function by updating it values until minimum not found. 
another term is f- statistics which is equal to msr/mse where msr should be as large as possible and mse should be as small as possible. for good regression f-statistics should be large. we were shown an example in ms excel in class where we done feature selection based on p-value (for good features the p_value should be less than 0.05) we remove one by one that features whose p_value is less than 0.05 and hence the f value is increasing.","today's class start with a discussion on the value of beta_1, which is statistically significant or not. instead of checking if zero lies between beta_1l and beta_1h, we can alternatively check if beta_1 lies within / outside the 90% or 95% confidence interval defined around zero. so, if beta_1 is within the confidence interval around zero, it is not statistically significant, and if it outside the ci around zero it is said to be statistically significant. the p_value associated with beta_1 tells us about the position of beta_1 w.r.t. zero. if p_values is less than 0.05, then beta_1 is outside the 95% ci around zero and hence it is statistically significant. the opposite is true if the p_values is greater than 0.05. further ahead we talked about the multiple linear regression, we have y=b0 + b1x1 + b2x2 + b3x3... here [x1, x2, x3, x4....] are the independent variables, also known as features. in mlr our goal is to express 'y' as a linear combination of x1, x2, x3..... . examples like -> a) photos-> the pixels in photos can be considered as features. b) in case of ""text"" we convert it into vectors by a method embedding. c) in sales prediction we have features like age, earning and location. next we learned about the gradient descent method which is an optimization technique which are used to find the minimum of function by updating it values until minimum not found. another term is f- statistics which is equal to msr/mse where msr should be as large as possible and mse should be as small as possible. for good regression f-statistics should be large. we were shown an example in ms excel in class where we done feature selection based on p-value (for good features the p_value should be less than 0.05) we remove one by one that features whose p_value is less than 0.05 and hence the f value is increasing.",2,15.725522,5.9797263,12.669728,3.8795686,"regression, regressions, features"
605,"started with identifying and eliminating outliers.
covered ways to reduce noise using smoothing methods like sma and ema, noting that bigger windows give smoother results.
explained how gradient descent works for optimization.
introduced scaling methods and transformations like box-cox and log transforms to adjust variance and prepare data for time series.
discussed maximum likelihood estimation (mle).
mentioned the kepler exoplanet dataset.
talked about smote to manage unbalanced datasets.","started with identifying and eliminating outliers. covered ways to reduce noise using smoothing methods like sma and ema, noting that bigger windows give smoother results. explained how gradient descent works for optimization. introduced scaling methods and transformations like box-cox and log transforms to adjust variance and prepare data for time series. discussed maximum likelihood estimation (mle). mentioned the kepler exoplanet dataset. talked about smote to manage unbalanced datasets.",13,-23.988726,8.583561,9.951996,10.255288,"classification, classifying, classifications"
606,"in today's session we discussed briefly about closed form solutions for mlr where we got to know that these solutions exist but we don't use them because of the large size of the matrices and computing the inverses of those matrices would consume too much time and computational power and also because multicollinearity exists between two or more variables. when we want to generalise our model for the entire population but we dont have access to entire population's data so the sample we have is divided into train and test data. depending upon the size of the sample we can choose different proportions of train and test data. some parameters used for training data might not be used for the test data like the ci and other error metrics like sse and tse. we use mostly r squared value and mae or rmse to decide if our model is working good enough on the unseen(test data). an overfit model fails to generalise for unseen data and hence lacks the ability to perform good on the test data. adjusted r^2 value is used for mlr to see if our r^2 value is increasing significantly when we add a new variable and that helps us to decide how many independent variables we would want to have in our data. also the term linear regression does not always result in a straight line, it just means that the dependent variable would be a linear combination of all the independent variables. also we later jumped into python and see how to code for a lr problem using different libraries like pandas, matplotlib, sklearn etc and their functions. for a problem we have we should always try to solve the problem with different models and decide which one's better in terms of the less error values and more accurate results.","in today's session we discussed briefly about closed form solutions for mlr where we got to know that these solutions exist but we don't use them because of the large size of the matrices and computing the inverses of those matrices would consume too much time and computational power and also because multicollinearity exists between two or more variables. when we want to generalise our model for the entire population but we dont have access to entire population's data so the sample we have is divided into train and test data. depending upon the size of the sample we can choose different proportions of train and test data. some parameters used for training data might not be used for the test data like the ci and other error metrics like sse and tse. we use mostly r squared value and mae or rmse to decide if our model is working good enough on the unseen(test data). an overfit model fails to generalise for unseen data and hence lacks the ability to perform good on the test data. adjusted r^2 value is used for mlr to see if our r^2 value is increasing significantly when we add a new variable and that helps us to decide how many independent variables we would want to have in our data. also the term linear regression does not always result in a straight line, it just means that the dependent variable would be a linear combination of all the independent variables. also we later jumped into python and see how to code for a lr problem using different libraries like pandas, matplotlib, sklearn etc and their functions. for a problem we have we should always try to solve the problem with different models and decide which one's better in terms of the less error values and more accurate results.",2,6.53497,3.0466604,10.682633,4.968919,"regression, regressions, features"
607,"in the previous lecture, we discussed how to use a confusion matrix to evaluate the quality of results. itâ€™s important to verify whether the rows in the matrix represent actual or predicted values, as they can differ.

exploratory data analysis (eda) and crisp-dm
the crisp-dm (cross-industry standard process for data mining) methodology is broken down into six steps:

business understanding: define the objective, set goals, and plan.
data understanding: collect, describe, explore, and verify data quality.
data preparation: select, clean, construct, integrate, and format data.
modeling: choose techniques, design tests, build, and assess models.
evaluation: review results, evaluate the process, and plan next steps.
deployment: plan deployment, monitoring, maintenance, and report creation.
challenges in data
dependent variable (y): common problems include incorrect data (manual or automated checks), insufficient data (addressed by collecting more, simulating, or using undersampling/oversampling), and too much data (handled through sampling).
independent variable (x): issues include incorrect representation (solved by encoding), heteroskedasticity, insufficient features (resolved by feature engineering), too many features (dimensionality reduction), feature scaling problems (normalized via standardization), and collinearity (addressed through correlation analysis).
eda on pima indians diabetes dataset
box plots: help identify feature variability and outliers.
feature correlation: shows strong relationships between features.
matrix plots: highlight deeper relationships between features.
clusters and trends: identifying trends or clusters within data can show seasonality or recurring patterns.
handling missing data
types of missing data:

mcar: missing completely at random.
mar: missing at random (data missing due to external factors, e.g., sensor failure).
mnar: missing not at random (e.g., missing data because it exceeds a threshold).
methods to handle missing data:

delete instances: remove rows with missing data.
impute values: replace missing values with mean, median, etc.
multivariate imputation: use algorithms like k-nn or mice (multivariate imputation by chained equations) to predict missing values based on other data.
for time series data, missing values can be filled in with close temporal values, assuming small variations.

handling outliers
univariate outliers: identify outliers using the interquartile range (iqr), defined as q3 - q1. outliers fall outside the range of [q1 - 1.5iqr, q3 + 1.5iqr].

normal distribution method: points outside the normal distribution range can be considered outliers.

multivariate outliers: use density-based clustering methods (e.g., dbscan), where points with few nearby neighbors are treated as outliers.

handling outliers:

drop outliers: simply remove them from the dataset.
capping: apply limits to outlier values.

true outliers: some extreme values may not be erroneous and provide valuable insights and should be handled based on domain knowledge. such outliers should be treated independently and cautiously based on their impact on the model and their relationship with the data.","in the previous lecture, we discussed how to use a confusion matrix to evaluate the quality of results. it s important to verify whether the rows in the matrix represent actual or predicted values, as they can differ. exploratory data analysis (eda) and crisp-dm the crisp-dm (cross-industry standard process for data mining) methodology is broken down into six steps: business understanding: define the objective, set goals, and plan. data understanding: collect, describe, explore, and verify data quality. data preparation: select, clean, construct, integrate, and format data. modeling: choose techniques, design tests, build, and assess models. evaluation: review results, evaluate the process, and plan next steps. deployment: plan deployment, monitoring, maintenance, and report creation. challenges in data dependent variable (y): common problems include incorrect data (manual or automated checks), insufficient data (addressed by collecting more, simulating, or using undersampling/oversampling), and too much data (handled through sampling). independent variable (x): issues include incorrect representation (solved by encoding), heteroskedasticity, insufficient features (resolved by feature engineering), too many features (dimensionality reduction), feature scaling problems (normalized via standardization), and collinearity (addressed through correlation analysis). eda on pima indians diabetes dataset box plots: help identify feature variability and outliers. feature correlation: shows strong relationships between features. matrix plots: highlight deeper relationships between features. clusters and trends: identifying trends or clusters within data can show seasonality or recurring patterns. handling missing data types of missing data: mcar: missing completely at random. mar: missing at random (data missing due to external factors, e.g., sensor failure). mnar: missing not at random (e.g., missing data because it exceeds a threshold). methods to handle missing data: delete instances: remove rows with missing data. impute values: replace missing values with mean, median, etc. multivariate imputation: use algorithms like k-nn or mice (multivariate imputation by chained equations) to predict missing values based on other data. for time series data, missing values can be filled in with close temporal values, assuming small variations. handling outliers univariate outliers: identify outliers using the interquartile range (iqr), defined as q3 - q1. outliers fall outside the range of [q1 - 1.5iqr, q3 + 1.5iqr]. normal distribution method: points outside the normal distribution range can be considered outliers. multivariate outliers: use density-based clustering methods (e.g., dbscan), where points with few nearby neighbors are treated as outliers. handling outliers: drop outliers: simply remove them from the dataset. capping: apply limits to outlier values. true outliers: some extreme values may not be erroneous and provide valuable insights and should be handled based on domain knowledge. such outliers should be treated independently and cautiously based on their impact on the model and their relationship with the data.",9,-16.820871,18.324429,8.797347,8.615189,"dataâ, analyse, analyses"
608,"we started off with a display of a site that gives us visualization of classification, named playground.tensorflow.org. after that we returned back to metrics oof logistics regression. we saw that the the given score that tells us about the accuracy can't be trusted totally. moving on we saw the receiver operating characteristics (roc) curve. the roc curve indicates the quality of the model. the sharper the better. there is another term called auc which is basically the area under the roc curve. ideally it should be near to 1 for a good classifier.
we then came to clustering. this is a part of unsupervised learning, that is it doesn't have labels. there are algorithms like k-means and other.","we started off with a display of a site that gives us visualization of classification, named playground.tensorflow.org. after that we returned back to metrics oof logistics regression. we saw that the the given score that tells us about the accuracy can't be trusted totally. moving on we saw the receiver operating characteristics (roc) curve. the roc curve indicates the quality of the model. the sharper the better. there is another term called auc which is basically the area under the roc curve. ideally it should be near to 1 for a good classifier. we then came to clustering. this is a part of unsupervised learning, that is it doesn't have labels. there are algorithms like k-means and other.",8,-1.223958,-21.998684,7.017767,0.4902407,"classification, clusterings, classifying"
609,"1. we saw the methods of improving quality of results : improve the sample or method or fine tune the existing samples .
2.mlr non linear cases : using trigonometry and straight line
3. as we use more terms , adjusted r2 will decrease and at last only significant term will be there 
4.  backward and forward feature engineering 
5. we learnt that it is better to have one model rather then 2 subordinate models 
6. neural networks 
7. explanation for sigmoid function 
8. definition of logistic regression","1. we saw the methods of improving quality of results : improve the sample or method or fine tune the existing samples . 2.mlr non linear cases : using trigonometry and straight line 3. as we use more terms , adjusted r2 will decrease and at last only significant term will be there 4. backward and forward feature engineering 5. we learnt that it is better to have one model rather then 2 subordinate models 6. neural networks 7. explanation for sigmoid function 8. definition of logistic regression",0,0.0039873146,-0.5789869,9.267761,4.3461504,"models, feature, features"
610,"the class started with an emphasis on how closely related ml and statistics are. it was told that machine learning is all about finding 'f' in  y = f(x).then we learnt about 4 levels of measures, which determines the kind of ml problem to be solved. they are nominal, ordinal, interval and ratio. nominal is categorical, no order, discrete. ordinal is similar but each of the category has got an order. interval is continuous, there is no fixed zero(can change with context), ratio of these intervals can't be interpreted. ratio similar to interval, with a fixed zero, where both difference and ratio makes sense. it was explained why learning these measures was important as it helps understand supervised ml algorithms. any ml problem where y is either nominal or ordinal is a classification problem and if it is else, it is a regression problem. in unsupervised learning one has to bring out patterns from the data, mostly clustering similar ones. the discussion went on to  how using numbers to represent categories in nominal and ordinal causes problems and how one hot encoding, which basically vectorizes the labels solves it. then an idea about what statistics was discussed. it is a study that aims to better understand the population, studying a smaller subset of it - sample.","the class started with an emphasis on how closely related ml and statistics are. it was told that machine learning is all about finding 'f' in y = f(x).then we learnt about 4 levels of measures, which determines the kind of ml problem to be solved. they are nominal, ordinal, interval and ratio. nominal is categorical, no order, discrete. ordinal is similar but each of the category has got an order. interval is continuous, there is no fixed zero(can change with context), ratio of these intervals can't be interpreted. ratio similar to interval, with a fixed zero, where both difference and ratio makes sense. it was explained why learning these measures was important as it helps understand supervised ml algorithms. any ml problem where y is either nominal or ordinal is a classification problem and if it is else, it is a regression problem. in unsupervised learning one has to bring out patterns from the data, mostly clustering similar ones. the discussion went on to how using numbers to represent categories in nominal and ordinal causes problems and how one hot encoding, which basically vectorizes the labels solves it. then an idea about what statistics was discussed. it is a study that aims to better understand the population, studying a smaller subset of it - sample.",4,-23.183298,-13.215595,1.8081261,0.49925128,"classification, classifying, classifications"
611,"vif vs r-square
features vs r-square trade-off 
our goal is to reduce the number of features because of the curse of dimensionality. 
pca: an application of svd (singular value decomposition). if we have a data set like x1 vs x2. we can have as many pcs as the dimensions, pc1 and pc2, ensuring they are orthogonal. the orientation of these pc axes is such that the 1st one accounts for the maximum variability and then less as we follow. orginally, this problem is a 2-dimensional problem which has been reduced to 1 dimension through pca!
pca helps us to reduce the dimensionality of the problem. vif (needs to be done first) vs pca 
interpretability: pcs help with prediction but their interpretability is very low. we can't perform what if analysis with pcs as they are not even real features, they are just mathematical features used for predictability. we can't link predictions with original variables if we use pca.
pca analysis is used for (1) dimension reduction (2) prediction model (3) visualization (understanding the structure of data) 
pca is a part of eda. then we select if we have to go for normal regression vs pca regression.
disadvantages of pca
advantage: if we have very big data, we can visualize it using pca. original data had many dimensions (columns) which can be significantly compressed using relevant pcs
another important dimensionality reduction method is tsne; t distribution vs gaussian normal distribution; tsne is a probabilistic method.
raw data to labelled data using tsne. tsne creates a probability distribution for each point that tells us the closeness of that point with every other point in the dataset. in higher dimensions, we tend to use gaussian normal distribution whereas in lower dimensions, we tend to use t-distribution. the probability of a point being close to another point is magnified in tsne when compared with that probability in gaussian normal distribution","vif vs r-square features vs r-square trade-off our goal is to reduce the number of features because of the curse of dimensionality. pca: an application of svd (singular value decomposition). if we have a data set like x1 vs x2. we can have as many pcs as the dimensions, pc1 and pc2, ensuring they are orthogonal. the orientation of these pc axes is such that the 1st one accounts for the maximum variability and then less as we follow. orginally, this problem is a 2-dimensional problem which has been reduced to 1 dimension through pca! pca helps us to reduce the dimensionality of the problem. vif (needs to be done first) vs pca interpretability: pcs help with prediction but their interpretability is very low. we can't perform what if analysis with pcs as they are not even real features, they are just mathematical features used for predictability. we can't link predictions with original variables if we use pca. pca analysis is used for (1) dimension reduction (2) prediction model (3) visualization (understanding the structure of data) pca is a part of eda. then we select if we have to go for normal regression vs pca regression. disadvantages of pca advantage: if we have very big data, we can visualize it using pca. original data had many dimensions (columns) which can be significantly compressed using relevant pcs another important dimensionality reduction method is tsne; t distribution vs gaussian normal distribution; tsne is a probabilistic method. raw data to labelled data using tsne. tsne creates a probability distribution for each point that tells us the closeness of that point with every other point in the dataset. in higher dimensions, we tend to use gaussian normal distribution whereas in lower dimensions, we tend to use t-distribution. the probability of a point being close to another point is magnified in tsne when compared with that probability in gaussian normal distribution",11,-19.707151,2.999522,10.44724,13.44367,"pca, heatmap, heatmaps"
612,"the session covered key steps in data preprocessing, starting with outlier analysis and removal. it then explored the kepler exoplanet dataset before discussing the synthetic minority over-sampling technique (smote) for handling imbalanced data. noise reduction techniques like simple moving average (sma) and exponential moving average (ema) were introduced, where a larger window size results in more smoothing. gradient descent was discussed as an optimization technique, followed by normalization methods, including the box-cox and logarithmic transformations, which help stabilize variance and normalize data before time series analysis. finally, maximum likelihood estimation (mle) was covered.","the session covered key steps in data preprocessing, starting with outlier analysis and removal. it then explored the kepler exoplanet dataset before discussing the synthetic minority over-sampling technique (smote) for handling imbalanced data. noise reduction techniques like simple moving average (sma) and exponential moving average (ema) were introduced, where a larger window size results in more smoothing. gradient descent was discussed as an optimization technique, followed by normalization methods, including the box-cox and logarithmic transformations, which help stabilize variance and normalize data before time series analysis. finally, maximum likelihood estimation (mle) was covered.",11,-23.87538,8.183418,9.91239,10.328677,"pca, heatmap, heatmaps"
613,"we started with eda(exploratory data analysis). i learnt that any data related job involves 6 cyclic steps/phases.
1. business understanding/ domain knowledge
2. data understanding
3. data preparation
4. building a model
5. model evaluation
6. deployment of the final model
then one of the ta presented about eda on the following topics
a) data visulaization
b) handling missing fields in the data: one can ignore those data points, replace with mean or median or mode based on the distribution of the missing field, cluster data and take mean. in case of missing values in time series, one can use linear interpolation, spline interpolation
c) handling outliers: using box plots really helps. i learnt an interesting idea: median is not influenced by outliers but mean is.","we started with eda(exploratory data analysis). i learnt that any data related job involves 6 cyclic steps/phases. 1. business understanding/ domain knowledge 2. data understanding 3. data preparation 4. building a model 5. model evaluation 6. deployment of the final model then one of the ta presented about eda on the following topics a) data visulaization b) handling missing fields in the data: one can ignore those data points, replace with mean or median or mode based on the distribution of the missing field, cluster data and take mean. in case of missing values in time series, one can use linear interpolation, spline interpolation c) handling outliers: using box plots really helps. i learnt an interesting idea: median is not influenced by outliers but mean is.",9,-15.107945,18.214186,9.260769,9.0114565,"dataâ, analyse, analyses"
614,"in today's lecture we talked about moving averages for data smoothing. in it we discussed about simple moving average(sma). in it higher window size implies more smoothing. we also talked about term imputation. in it we focus on a single column (aqi). linear imputation seems more natural.what are better methods ? there are exponential moving average that weigh nearly samples more while averages. also utilized in time series forecasting to understand trends seasonality in the data and forecast it. we then talked about normalization that forces column values to lie between 0 and 1 , there avoiding the problem of overshadowing. clustering algorithms are greatly influenced by normalizing of data . thus same input of data one with normalization and other without it will give different cluster plots.
then we talked about data imbalance.algorithm trained on data imbalance can be biased towards the majority class( that is class with higher frequency).
then we discussed about t-nse plot. it  converts multidimensional data into lower dimensional data.","in today's lecture we talked about moving averages for data smoothing. in it we discussed about simple moving average(sma). in it higher window size implies more smoothing. we also talked about term imputation. in it we focus on a single column (aqi). linear imputation seems more natural.what are better methods ? there are exponential moving average that weigh nearly samples more while averages. also utilized in time series forecasting to understand trends seasonality in the data and forecast it. we then talked about normalization that forces column values to lie between 0 and 1 , there avoiding the problem of overshadowing. clustering algorithms are greatly influenced by normalizing of data . thus same input of data one with normalization and other without it will give different cluster plots. then we talked about data imbalance.algorithm trained on data imbalance can be biased towards the majority class( that is class with higher frequency). then we discussed about t-nse plot. it converts multidimensional data into lower dimensional data.",9,-18.882097,10.054222,10.098476,10.009888,"dataâ, analyse, analyses"
615,"we got to learn about how in the distribution of beta_1 if zero lies in the 95% or 90% ci, zero is statistically similar to any value in that data range. any value of beta_1 lying outside of the  ci is said to be statistically significant. any value in the ci is obtained at random and it is said to change when we change our dataset which is a sample. if the ci contains zero, then there could be a sample dataset that could give beta_1 =0 and if that occurs then beta_1 is said to be statistically zero and hence no regression line with a slope would exist. then we moved on to multiple linear regression where we discussed about the formula for mlr and the technique used to solve this and how we dont have a closed form solution for mlr and hence we take the help of gradient descent to reach to a point where all our errors are minimized and that point tells us the values of beta_1, beta_2,......,beta_k. later we used excel to calculate the p-values, f-statistic for our mlr problem and how the variables with p values greater then 0.05 can be ignored and removed from our regression line and that would increase the f-stats value and the higher the f-statistics value better is our model.","we got to learn about how in the distribution of beta_1 if zero lies in the 95% or 90% ci, zero is statistically similar to any value in that data range. any value of beta_1 lying outside of the ci is said to be statistically significant. any value in the ci is obtained at random and it is said to change when we change our dataset which is a sample. if the ci contains zero, then there could be a sample dataset that could give beta_1 =0 and if that occurs then beta_1 is said to be statistically zero and hence no regression line with a slope would exist. then we moved on to multiple linear regression where we discussed about the formula for mlr and the technique used to solve this and how we dont have a closed form solution for mlr and hence we take the help of gradient descent to reach to a point where all our errors are minimized and that point tells us the values of beta_1, beta_2,......,beta_k. later we used excel to calculate the p-values, f-statistic for our mlr problem and how the variables with p values greater then 0.05 can be ignored and removed from our regression line and that would increase the f-stats value and the higher the f-statistics value better is our model.",2,16.768505,6.827303,12.681001,3.9845898,"regression, regressions, features"
616,"focused on principal component analysis (pca) as a feature reduction technique.
key concepts included the identification of principal components (pcs), ensuring that they capture the highest variance in data, and how pca effectively reduces dimensionality.
use of the elbow method to determine the optimal number of pcs for a dataset.  comparison was made between normal regression models and pc regressionthe session concluded with a discussion on the significance of pca in real-world data analysis and its integration into predictive modelingâ workflows.","focused on principal component analysis (pca) as a feature reduction technique. key concepts included the identification of principal components (pcs), ensuring that they capture the highest variance in data, and how pca effectively reduces dimensionality. use of the elbow method to determine the optimal number of pcs for a dataset. comparison was made between normal regression models and pc regressionthe session concluded with a discussion on the significance of pca in real-world data analysis and its integration into predictive modeling workflows.",11,-12.61988,1.8206136,10.084177,12.713672,"pca, heatmap, heatmaps"
617,"we started off with discussion the right way to go about our midterm. so we basically had to go through eda and then comment on it by using a classifier model. major pointss are, looking for missing values, and decided whether to drop them or fill them. if we fill them what logic should be used. next we saw how one of the labels was very, very low, so then to represent it we either ask for more genuine data or let it be and get some wrong results. making copies is not the way around here. next was to find outliers, lucky our data didn't have any.  next was to see what happens when we fit it in a model. the the validation data part, we were to figure out why the model will predict bad here. so we can look at the features and compare them and it was inferred that the data was mostly from different samples.
next we had a small review of e3.
then we talked about the dilemma of dimensionality. more dimensions mean more information, but also more computation time and cost, more data to look after and all. so to deal with this we they to make features as a function of other features. and to look at this we use a metric, variance inflation factor.","we started off with discussion the right way to go about our midterm. so we basically had to go through eda and then comment on it by using a classifier model. major pointss are, looking for missing values, and decided whether to drop them or fill them. if we fill them what logic should be used. next we saw how one of the labels was very, very low, so then to represent it we either ask for more genuine data or let it be and get some wrong results. making copies is not the way around here. next was to find outliers, lucky our data didn't have any. next was to see what happens when we fit it in a model. the the validation data part, we were to figure out why the model will predict bad here. so we can look at the features and compare them and it was inferred that the data was mostly from different samples. next we had a small review of e3. then we talked about the dilemma of dimensionality. more dimensions mean more information, but also more computation time and cost, more data to look after and all. so to deal with this we they to make features as a function of other features. and to look at this we use a metric, variance inflation factor.",0,1.5113269,2.199593,9.736267,4.6422653,"models, feature, features"
618,"in this class, we mainly discussed the midsem paper. sir showed us how we could have attempted the questions. the first part involved performing eda on the given data set to find out the missing values and outliers. the early exploratory stage involves creating scatter plots and histograms of distribution of the parameters to understand how they are distributed. next, to handle the missing values, we can either drop them completely or we can approximate them to some value. it depends on the trend in the data (which can be seen through the scatter plots), whether to use simple mean or some moving average. in this case, the data was randomly spread in the entire region, hence we could use the mean value of each column to fill the missing values. 
there can be mainly two types of problems- problem within the column, which involves missing values and the problem across the columns, which involves significant differences in the values of the parameters in the two columns. in such a normalization is required, to ensure that a particular class is not under-expressed/ suppressed by the others.
box plots give us idea about the outliers. in this case, there werenâ€™t any outliers.
we can visualize the categorical data using bar charts/ pie charts.
 correlation heat maps can also be used to check for the relations between the different parameters. the problem with this is that it only checks for the correlation between a feature and only one other feature at a time. however, it is possible that a feature depends on multiple other features and is a linear combination of those. correlation maps do not provide this information. hence, they are sufficient but not necessary check of correlation.
for solving this problem, we use â€˜variance inflation factorâ€™(vif). it compares each feature with all other features taken together. to find out the vif, we express each feature as a linear combination of the others and fit a regression model on it. we then find the r2 values for each of the feature. vif is then calculated as 1/(1-r2). so, as r2 increases, vif also increases. hence, if r2 values is high, it indicates strong correlation among the features.
if vif of a feature is >10 then that feature can be expressed as a linear combination of the others, hence we remove that feature column from our data.
in this way, we reduce the dimensionality of the data set, as high dimensionality is a curse. it spreads out the data more, thereby making it difficult to find out important patterns from the data.
the next part in the question was to check whether the derived regression model worked well for another different data set as well. by plotting kde plots and analyzing the descriptive statistics of the new data set, we observed that it had completely different distribution than the original data set, which suggested that it belonged to a completely different population. this was also evident by the looking at the values of accuracy, precision, recall and f1 scores of the model, fitted to the new data. hence, the original model did not work well for the new data, which belonged to a different population.","in this class, we mainly discussed the midsem paper. sir showed us how we could have attempted the questions. the first part involved performing eda on the given data set to find out the missing values and outliers. the early exploratory stage involves creating scatter plots and histograms of distribution of the parameters to understand how they are distributed. next, to handle the missing values, we can either drop them completely or we can approximate them to some value. it depends on the trend in the data (which can be seen through the scatter plots), whether to use simple mean or some moving average. in this case, the data was randomly spread in the entire region, hence we could use the mean value of each column to fill the missing values. there can be mainly two types of problems- problem within the column, which involves missing values and the problem across the columns, which involves significant differences in the values of the parameters in the two columns. in such a normalization is required, to ensure that a particular class is not under-expressed/ suppressed by the others. box plots give us idea about the outliers. in this case, there weren t any outliers. we can visualize the categorical data using bar charts/ pie charts. correlation heat maps can also be used to check for the relations between the different parameters. the problem with this is that it only checks for the correlation between a feature and only one other feature at a time. however, it is possible that a feature depends on multiple other features and is a linear combination of those. correlation maps do not provide this information. hence, they are sufficient but not necessary check of correlation. for solving this problem, we use variance inflation factor (vif). it compares each feature with all other features taken together. to find out the vif, we express each feature as a linear combination of the others and fit a regression model on it. we then find the r2 values for each of the feature. vif is then calculated as 1/(1-r2). so, as r2 increases, vif also increases. hence, if r2 values is high, it indicates strong correlation among the features. if vif of a feature is >10 then that feature can be expressed as a linear combination of the others, hence we remove that feature column from our data. in this way, we reduce the dimensionality of the data set, as high dimensionality is a curse. it spreads out the data more, thereby making it difficult to find out important patterns from the data. the next part in the question was to check whether the derived regression model worked well for another different data set as well. by plotting kde plots and analyzing the descriptive statistics of the new data set, we observed that it had completely different distribution than the original data set, which suggested that it belonged to a completely different population. this was also evident by the looking at the values of accuracy, precision, recall and f1 scores of the model, fitted to the new data. hence, the original model did not work well for the new data, which belonged to a different population.",9,-8.389576,13.764365,9.816992,8.360333,"dataâ, analyse, analyses"
619,"hands-on linear regression analysis with excel

1. introduction

this lecture focuses on practical application of linear regression concepts using excel.
we utilize the linear regression formula derived in the previous lecture to calculate the slope and intercept of the best-fit line directly within excel.
the best-fit line is then plotted on the data.
2. limitations of extrapolation

question: can we make reliable predictions for data points outside the observed range (edge points)?
answer: no. accurate extrapolation requires sufficient data points within the target region to ensure the linear regression model remains valid.
3. analyzing prediction errors

we plot the error (difference between predicted and actual values) for each data point.
observation: the errors appear random.
question: what is the expected distribution of these random errors?
4. distribution of random errors

central limit theorem: when an outcome is influenced by numerous independent random factors, the distribution of the observed results tends to follow a gaussian (normal) distribution.
we create a histogram of the prediction errors to visually examine their distribution.
5. automated linear regression with excel

we utilize the ""data analysis"" tool in excel to perform linear regression automatically.
this provides a wealth of statistical output.
6. evaluating model fit

key concept: a good model effectively explains the maximum variation in the data.
decomposition of variance:
sst (total sum of squares): total variation in the dependent variable (y).
ssr (regression sum of squares): variation in y explained by the regression model.
sse (error sum of squares): variation in y not explained by the model, attributed to random error.
coefficient of determination (r-squared):
r-squared = ssr / sst
r-squared quantifies the proportion of total variation explained by the regression model.
in simple linear regression, r-squared is also equal to the square of the correlation coefficient (r) between the independent (x) and dependent (y) variables.","hands-on linear regression analysis with excel 1. introduction this lecture focuses on practical application of linear regression concepts using excel. we utilize the linear regression formula derived in the previous lecture to calculate the slope and intercept of the best-fit line directly within excel. the best-fit line is then plotted on the data. 2. limitations of extrapolation question: can we make reliable predictions for data points outside the observed range (edge points)? answer: no. accurate extrapolation requires sufficient data points within the target region to ensure the linear regression model remains valid. 3. analyzing prediction errors we plot the error (difference between predicted and actual values) for each data point. observation: the errors appear random. question: what is the expected distribution of these random errors? 4. distribution of random errors central limit theorem: when an outcome is influenced by numerous independent random factors, the distribution of the observed results tends to follow a gaussian (normal) distribution. we create a histogram of the prediction errors to visually examine their distribution. 5. automated linear regression with excel we utilize the ""data analysis"" tool in excel to perform linear regression automatically. this provides a wealth of statistical output. 6. evaluating model fit key concept: a good model effectively explains the maximum variation in the data. decomposition of variance: sst (total sum of squares): total variation in the dependent variable (y). ssr (regression sum of squares): variation in y explained by the regression model. sse (error sum of squares): variation in y not explained by the model, attributed to random error. coefficient of determination (r-squared): r-squared = ssr / sst r-squared quantifies the proportion of total variation explained by the regression model. in simple linear regression, r-squared is also equal to the square of the correlation coefficient (r) between the independent (x) and dependent (y) variables.",5,21.452393,-5.563218,14.326472,4.93079,"regression, statistical, statistics"
620,"we started by acknowledging the base of data science, y=f(x). that is finding the relation between features and labels. following up we were told about a few algorithms, named: simple linear regression, multiple linear regression, random forest, and logistic regression. 
there was a discussion on measurements and their types, named: nominal, ordinal, interval, and ratio.
we also talked about encoding of data, and how and when we should not use number directly. finally, we had a peek into the part of ml with no label, ie unsupervised learning, and some of the techniques used,","we started by acknowledging the base of data science, y=f(x). that is finding the relation between features and labels. following up we were told about a few algorithms, named: simple linear regression, multiple linear regression, random forest, and logistic regression. there was a discussion on measurements and their types, named: nominal, ordinal, interval, and ratio. we also talked about encoding of data, and how and when we should not use number directly. finally, we had a peek into the part of ml with no label, ie unsupervised learning, and some of the techniques used,",4,-20.935654,-13.359354,2.3415573,0.69975275,"classification, classifying, classifications"
621,"data has levels of measurement. there are 4 levels of measurement:
1. nominal: they are descrete and there is no ordering between data categories. eg colour, gender 
2. ordinal: they are descrete, but unlike nominal, they have ordering between data categories. eg grades
3. interval: they are continuous. eg temperature, height 
4. ratio: those which can be expressed as one is x times of another. measurement of unit should not matter. eg height can be categorised as ratio level of measurement, but temperature can not. 

from the given data, we may be interested in identifying underlying distribution. y = f(x). where x is data and y is label. when y is descrete, the required task is named as classification and when y is continuous, it's called as regression. nominal and ordinal data are used in classification task, whereas interval and ratio are used in regression task. 

while learning the underlying distribution and data is nominal, we are required to convert it into numerical values. in such case, we use one hot encoding.","data has levels of measurement. there are 4 levels of measurement: 1. nominal: they are descrete and there is no ordering between data categories. eg colour, gender 2. ordinal: they are descrete, but unlike nominal, they have ordering between data categories. eg grades 3. interval: they are continuous. eg temperature, height 4. ratio: those which can be expressed as one is x times of another. measurement of unit should not matter. eg height can be categorised as ratio level of measurement, but temperature can not. from the given data, we may be interested in identifying underlying distribution. y = f(x). where x is data and y is label. when y is descrete, the required task is named as classification and when y is continuous, it's called as regression. nominal and ordinal data are used in classification task, whereas interval and ratio are used in regression task. while learning the underlying distribution and data is nominal, we are required to convert it into numerical values. in such case, we use one hot encoding.",4,-27.656672,-16.92228,1.3211269,0.20586395,"classification, classifying, classifications"
622,"logistic regression is a method for predicting binary outcomes (0 or 1). it multiplies inputs by weights, adds a bias, and applies a sigmoid function to get a probability. if the probability is above 0.5, the output is 1; otherwise, itâ€™s 0.

the model learns by adjusting weights with gradient descent to minimize errors. a likelihood function helps improve accuracy by optimizing the weights. performance is evaluated using metrics like accuracy, precision, recall, and a confusion matrix. itâ€™s commonly used in classification tasks such as spam detection and medical diagnosis.","logistic regression is a method for predicting binary outcomes (0 or 1). it multiplies inputs by weights, adds a bias, and applies a sigmoid function to get a probability. if the probability is above 0.5, the output is 1; otherwise, it s 0. the model learns by adjusting weights with gradient descent to minimize errors. a likelihood function helps improve accuracy by optimizing the weights. performance is evaluated using metrics like accuracy, precision, recall, and a confusion matrix. it s commonly used in classification tasks such as spam detection and medical diagnosis.",12,8.155757,-21.628292,9.080877,-2.0118887,"classifiers, logistic, roc"
623,today we learnt about qualities of a sample that it should be representative of the population. the main funda of calculation on the sample is statistics and estimation on population is parameter. certain statistics on mean are count mode std dev median and mean etc. where we also made a tabulation of level of measurement and their attributes possible and operations allowed. then we moved on to simple linear regression. also an interesting thing that points are also fitting of data...though a bad fit as it's like cr of a class of data. we learnt about bias in a slr which indicates the effect of other parameters on the y or output. last part was derivation of linear fitting line with best fit condition to minimize error sqaure.,today we learnt about qualities of a sample that it should be representative of the population. the main funda of calculation on the sample is statistics and estimation on population is parameter. certain statistics on mean are count mode std dev median and mean etc. where we also made a tabulation of level of measurement and their attributes possible and operations allowed. then we moved on to simple linear regression. also an interesting thing that points are also fitting of data...though a bad fit as it's like cr of a class of data. we learnt about bias in a slr which indicates the effect of other parameters on the y or output. last part was derivation of linear fitting line with best fit condition to minimize error sqaure.,1,33.114727,-3.919109,15.885327,3.5373313,"population, models, estimating"
624,"we were taught about crisp-dm (cross industry standard process for data mining), a cyclical six-step process. we begin with business understanding, defining the problem and examining pertinent statistics. next is data understanding, gathering and understanding the dataset. the modeling stage entails constructing and testing various models, followed by evaluation, where we test the outcomes to make sure they meet business requirements. last but not least, in the deployment phase, the model is completed, and reports are produced.then, we learned about exploratory data analysis (eda), an important statistic and data science method for examining datasets. we also considered outliers and quartiles and recognized how boxplots can display variability and identify outliers. we also learned about inter-feature relationships with matrix plots to look for correlations among various features. we then discovered three categories of missing data: missing completely at random (mcar), in which the missing values are not patterned; missing at random (mar), in which missing data is a function of some observed variables; and missing not at random (mnar), in which the missing values are a function of unobserved variables. finally, we talked about true outliers, which are outliers in a dataset that are not errors but real observations.","we were taught about crisp-dm (cross industry standard process for data mining), a cyclical six-step process. we begin with business understanding, defining the problem and examining pertinent statistics. next is data understanding, gathering and understanding the dataset. the modeling stage entails constructing and testing various models, followed by evaluation, where we test the outcomes to make sure they meet business requirements. last but not least, in the deployment phase, the model is completed, and reports are produced.then, we learned about exploratory data analysis (eda), an important statistic and data science method for examining datasets. we also considered outliers and quartiles and recognized how boxplots can display variability and identify outliers. we also learned about inter-feature relationships with matrix plots to look for correlations among various features. we then discovered three categories of missing data: missing completely at random (mcar), in which the missing values are not patterned; missing at random (mar), in which missing data is a function of some observed variables; and missing not at random (mnar), in which the missing values are a function of unobserved variables. finally, we talked about true outliers, which are outliers in a dataset that are not errors but real observations.",9,-12.981078,20.799282,8.747614,8.658857,"dataâ, analyse, analyses"
625,"in this lecture we learnt about exploratory data analysis. this is the first step towards creating any machine learning model. eda involves cleaning, transforming, understanding and analyzing the raw data through basic and primary techniques. data can be analyzed to reveal patterns in it, if any. this can be done using either mathematical formulae or by making visual charts. also, many times we donâ€™t get a single data file, instead we have to merge a lot of files to get a compact, single data file on which we perform further analysis. 
there is crisp-dm: cross industry standard process for data mining, in which there are 6 steps to be followed for building a good ml model. 
the steps are:
1- business understanding- this involves acquiring domain knowledge, clearly defining our goal and objectives, and understanding the problems that are to be tackled by the model. 
2- data understanding- this step involves, collection, analysis, and assessment of the relevancy and quality of data.
3- data preparation- it involves cleaning and transforming data into suitable form for analysis. feature engineering is done to select the most relevant/ appropriate features.
4- modelling- after performing the data analysis and cleaning steps, we move towards fitting a basic model to our data. the model is trained based on the available data. if y-values are not available, we used unsupervised learning algorithms, to cluster the data and assign labels to it.
5- evaluation - assessment of the model performance using various metrics and implementing changes in it, if any.
6- deployment- implement the final model and monitor its performance to make improvements with time. also, effectively present the insights and results of our model.
the entire process is iterative, we need to repeat the steps again and again, as and when required.
we need to establish a certain acceptance criterion for our model. for many algorithms, we assume that the distribution of the features is normal, however this may not be the case for every x. heteroscedasticity is another problem, in which the variance of the residuals is not constant and keeps varying. 
in eda, we have to clean the data, account for the missing values and also detect the outliers and take necessary actions to handle these. 
the example we discussed in class was that of clustering patients into diabetic/ non-diabetic, by considering various features including, insulin levels, glucose levels, age, bmi, etc. these features, in the very basic step, can be analyzed using histograms, which may give us some (if not complete) idea about the distribution. we can also use correlation maps (heat maps) to find out the relation between these features. however, we cannot completely rely on these, and we should take into consideration numerous other factors as well. we looked upon matrix plot, box plots, quartile plots and some others to find the relationship in the data. the box plots, histograms can be used to detect outliers. 
it is important to know/ find out what we should do with the missing values and outliers. in some cases, it is reasonable to ignore the entire row/ observation and in some others, it is better to fill up the missing values. all this depends on the count of missing values. if there are a large number of such missing values, then completely deleting those observations, would leave us with merely 10-15 rows which is not at all sufficient to build up a model. 
for filling up the missing values we can use either mean, mode or median, depending on the level of measurement. for example, when we have nominal data, we use mode because the concept of mean is not defined for nominal data. 
the next step is to detect the outliers. there are several methods to do so. we can plot histograms, box plots and find out the points that lie far beyond the region where maximum of the points lie. dbscan- density based spatial clustering of applications with noise is yet another method used for detecting outliers. it considers a point and finds out other points around it which are in close proximity to it. likewise, it clusters all other points, thereby forming groups. the outliers remain ungrouped. 
median is not affected by the outliers, but the mean is. hence, median is preferred over mean for detecting outliers. however, in some cases, use of mean is not justifiable as it deviates largely from the values nearby the missing values. considering this as the approximate missing value would lead to larger errors. 
instead of using mean or median for filling the missing values, we can also use a regression model in that region and determine the missing values. so, even for filling the missing data and for data cleaning (eda), we use various models. this suggests that all the steps involved in building a model from the basic/ raw data are iterative and are performed in loops. 
by carrying out these repetitive operations we keep refining our model, so as to get the best possible model for our data set.","in this lecture we learnt about exploratory data analysis. this is the first step towards creating any machine learning model. eda involves cleaning, transforming, understanding and analyzing the raw data through basic and primary techniques. data can be analyzed to reveal patterns in it, if any. this can be done using either mathematical formulae or by making visual charts. also, many times we don t get a single data file, instead we have to merge a lot of files to get a compact, single data file on which we perform further analysis. there is crisp-dm: cross industry standard process for data mining, in which there are 6 steps to be followed for building a good ml model. the steps are: 1- business understanding- this involves acquiring domain knowledge, clearly defining our goal and objectives, and understanding the problems that are to be tackled by the model. 2- data understanding- this step involves, collection, analysis, and assessment of the relevancy and quality of data. 3- data preparation- it involves cleaning and transforming data into suitable form for analysis. feature engineering is done to select the most relevant/ appropriate features. 4- modelling- after performing the data analysis and cleaning steps, we move towards fitting a basic model to our data. the model is trained based on the available data. if y-values are not available, we used unsupervised learning algorithms, to cluster the data and assign labels to it. 5- evaluation - assessment of the model performance using various metrics and implementing changes in it, if any. 6- deployment- implement the final model and monitor its performance to make improvements with time. also, effectively present the insights and results of our model. the entire process is iterative, we need to repeat the steps again and again, as and when required. we need to establish a certain acceptance criterion for our model. for many algorithms, we assume that the distribution of the features is normal, however this may not be the case for every x. heteroscedasticity is another problem, in which the variance of the residuals is not constant and keeps varying. in eda, we have to clean the data, account for the missing values and also detect the outliers and take necessary actions to handle these. the example we discussed in class was that of clustering patients into diabetic/ non-diabetic, by considering various features including, insulin levels, glucose levels, age, bmi, etc. these features, in the very basic step, can be analyzed using histograms, which may give us some (if not complete) idea about the distribution. we can also use correlation maps (heat maps) to find out the relation between these features. however, we cannot completely rely on these, and we should take into consideration numerous other factors as well. we looked upon matrix plot, box plots, quartile plots and some others to find the relationship in the data. the box plots, histograms can be used to detect outliers. it is important to know/ find out what we should do with the missing values and outliers. in some cases, it is reasonable to ignore the entire row/ observation and in some others, it is better to fill up the missing values. all this depends on the count of missing values. if there are a large number of such missing values, then completely deleting those observations, would leave us with merely 10-15 rows which is not at all sufficient to build up a model. for filling up the missing values we can use either mean, mode or median, depending on the level of measurement. for example, when we have nominal data, we use mode because the concept of mean is not defined for nominal data. the next step is to detect the outliers. there are several methods to do so. we can plot histograms, box plots and find out the points that lie far beyond the region where maximum of the points lie. dbscan- density based spatial clustering of applications with noise is yet another method used for detecting outliers. it considers a point and finds out other points around it which are in close proximity to it. likewise, it clusters all other points, thereby forming groups. the outliers remain ungrouped. median is not affected by the outliers, but the mean is. hence, median is preferred over mean for detecting outliers. however, in some cases, use of mean is not justifiable as it deviates largely from the values nearby the missing values. considering this as the approximate missing value would lead to larger errors. instead of using mean or median for filling the missing values, we can also use a regression model in that region and determine the missing values. so, even for filling the missing data and for data cleaning (eda), we use various models. this suggests that all the steps involved in building a model from the basic/ raw data are iterative and are performed in loops. by carrying out these repetitive operations we keep refining our model, so as to get the best possible model for our data set.",9,-12.602043,17.282244,9.125494,8.607134,"dataâ, analyse, analyses"
626,"the class began with a quick review of gradient descent and then moved to logistic regression.
the confusion matrix was explained, and the teacher showed a website to try neural network clustering.
after that, students looked at logistic regression code and learned about important measures like precision, recall, and the f1 score.
the roc curve was introduced, where the x-axis shows the false positive rate (fpr) and the y-axis shows the true positive rate (tpr).
it was also said that each class has its own roc curve.
next, the session moved to clustering, starting with k-means, where the number of groups is decided before.
in the end, the discussion moved to hierarchical clustering and different ways to do clustering were explained.","the class began with a quick review of gradient descent and then moved to logistic regression. the confusion matrix was explained, and the teacher showed a website to try neural network clustering. after that, students looked at logistic regression code and learned about important measures like precision, recall, and the f1 score. the roc curve was introduced, where the x-axis shows the false positive rate (fpr) and the y-axis shows the true positive rate (tpr). it was also said that each class has its own roc curve. next, the session moved to clustering, starting with k-means, where the number of groups is decided before. in the end, the discussion moved to hierarchical clustering and different ways to do clustering were explained.",8,-5.783598,-17.422138,6.212688,0.7481464,"classification, clusterings, classifying"
627,"today's lecture begin with a discussion of how to increase the quality of results, there are some methods for it, one of them is by improving the sample (either by upgrading the quality of sample or by increasing the sample size). then we also came to know about what is forward(start with minimal number of features and then increase features based on requirement or according to further domain knowledge) and backward(start with large number of features and then eliminating those which are unwanted) feature engineering. also, it is not necessary that given a data set, only one model will be able to fit the data (i.e., one can fit multiple models for a given dataset based on requirement). after this, we discussed parametric(regression, classification) and non-parametric(random forest, k-nearest neighbor (knn)) models. both this models can predict values, but sometimes non-parametric model unable to find what will be the value at some delta(x). we also had a brief intro on neural networks (like what does it means, how does it works, etc. in short), and like what are links, nodes, weights, features, degrees of freedom, etc. for neural networking systems. the outcome in neural network is a function of weights and features (y_i = f (w_i, x_i)). then, we compared different models like linear regression, svm, random forest, xgboost, knn, neural network, etc.  representing same dataset with the help of r^2 values, rmse, etc., the model with higher r^2 value and lower rmse value is generally a better choice. then we moved on to classification of data (when data is of nominal or ordinal form, classification model is used; when its is of interval or ratio form, regression model is used).
 outcome in classification is called y = class. after this, we first discussed the definition of regression and then what is meant by logistic regression. in logistic regression, we are not interested in finding the best fit line or to predict values, but we are rather interested in finding the boundary between the group/clusters of values. outcome in logistic regression is known as classifiers or class labels. the number or distinct labels = number of boundary lines = number of classes one want to classify. at last, we also discussed the function used by logistic regression to classify the data (in 0/1 or left/right or up/down, etc.) which is called sigmoidal function where outcome is either zero or one based on which class your given value belongs. in order to find a sigmoidal function (s=1/(1-e^(-a))), we need to find weights(w; where a = w1x1 + w2x2 + ... + wnxn) just as we were findings regression coefficients for linear regression.","today's lecture begin with a discussion of how to increase the quality of results, there are some methods for it, one of them is by improving the sample (either by upgrading the quality of sample or by increasing the sample size). then we also came to know about what is forward(start with minimal number of features and then increase features based on requirement or according to further domain knowledge) and backward(start with large number of features and then eliminating those which are unwanted) feature engineering. also, it is not necessary that given a data set, only one model will be able to fit the data (i.e., one can fit multiple models for a given dataset based on requirement). after this, we discussed parametric(regression, classification) and non-parametric(random forest, k-nearest neighbor (knn)) models. both this models can predict values, but sometimes non-parametric model unable to find what will be the value at some delta(x). we also had a brief intro on neural networks (like what does it means, how does it works, etc. in short), and like what are links, nodes, weights, features, degrees of freedom, etc. for neural networking systems. the outcome in neural network is a function of weights and features (y_i = f (w_i, x_i)). then, we compared different models like linear regression, svm, random forest, xgboost, knn, neural network, etc. representing same dataset with the help of r^2 values, rmse, etc., the model with higher r^2 value and lower rmse value is generally a better choice. then we moved on to classification of data (when data is of nominal or ordinal form, classification model is used; when its is of interval or ratio form, regression model is used). outcome in classification is called y = class. after this, we first discussed the definition of regression and then what is meant by logistic regression. in logistic regression, we are not interested in finding the best fit line or to predict values, but we are rather interested in finding the boundary between the group/clusters of values. outcome in logistic regression is known as classifiers or class labels. the number or distinct labels = number of boundary lines = number of classes one want to classify. at last, we also discussed the function used by logistic regression to classify the data (in 0/1 or left/right or up/down, etc.) which is called sigmoidal function where outcome is either zero or one based on which class your given value belongs. in order to find a sigmoidal function (s=1/(1-e^(-a))), we need to find weights(w; where a = w1x1 + w2x2 + ... + wnxn) just as we were findings regression coefficients for linear regression.",0,4.1837125,-2.5391662,9.917404,4.177126,"models, feature, features"
628,paper discussion of midsem,paper discussion of midsem,6,-2.6487496,21.508278,9.3611555,6.680293,"summarizing, summarize, summarization"
629,if we have only one sample and want to predict the population mean and other attributes of the population we can establish that by assuming the mean of the sample which we have to be very close to the actual dataset and then use standard error using the mean and the number of observations in it. we saw how 95% confidence level is defined and what it means. 0.95 also represents the probability of the values in the normal distribution. from the end points of this 95% confidence level we are able to define the upper and lower limit in which the actual values of the population will lie. we applied the same approach in determining the beta values but when beta becomes there is no regression there was a requirement of  transforming the plot to origin and then look for the probability values less than 5% to get the value of beta.,if we have only one sample and want to predict the population mean and other attributes of the population we can establish that by assuming the mean of the sample which we have to be very close to the actual dataset and then use standard error using the mean and the number of observations in it. we saw how 95% confidence level is defined and what it means. 0.95 also represents the probability of the values in the normal distribution. from the end points of this 95% confidence level we are able to define the upper and lower limit in which the actual values of the population will lie. we applied the same approach in determining the beta values but when beta becomes there is no regression there was a requirement of transforming the plot to origin and then look for the probability values less than 5% to get the value of beta.,7,36.980484,4.9140964,15.111122,2.4582345,"statistics, statistical, statisticsâ"
630,"playground.tensorflow.org- allows us to play with datasets and helps us understands how classification happens. if we want to get a non linear boundary in logistic regression, we need to add new variable such as x1^2,x1^3 and so on. by adding a node in hidden layer in neural network we can get a better boundary. next sir went on to code where he explained about logistic regression implementation and test metrics. don't believe the accuracy values, see other values in confusion metrics and other test metrics. true positive rate(trp)= (tp)/(fn+tp). receiver operating characteristic curve(roc) curve- it is a plot of tpr against fpr for different thresholds(value at which we change classes for eg if p>=0.5 => class 1 else class 0). the area under this curve measures how well the model can distinguish between two groups. so base roc curve we can find threshold values. in roc plot there is a 45 degree line which occurs when we randomly classify data. so when our plot is more close to this 45 degree line then our classification is more worse. auc should be as close to 1 as possible for good classification. the classifier also depends on the quality of data. if the data itself is intermeshed, any classifier wont be able to get a proper boundary.  for such datasets we can carry out data separation by using some transformations. any kind of fraud that is occurring is an example of data imbalance. precision: of the observations that have been detected as class 1 how many actually belong to class 1. so if there are multiple values there will be multiple precision values. support- how many values have been classified as a particular class. when there is a class imbalance, it will be difficult to detect anomalies. if we have multiple classes, we have multiple roc curves. many times problems will be easier to convert a regression problem into a classification problem by binning. 
clustering: unsupervised. we don't have labelled data. we will only have features. we will have to detect potential number of classes and number of points in each class. we will see at k means clustering and hierarchal clustering. metrics part is homework, we will only look at algorithms in class. in k means clustering number of cluster is an input we need to give. in hierarchical clustering we get a dendrogram in which we will be able to know how clusters can be formed based on closeness of groups of data. after clustering we will be able to identify some outliers. clustering is usually done as a part of exploratory data analysis. algorithms are explained step by step in  slides. k-means - you are trying to identify k mean points around which a cluster can be formed. we need to specify desired number of clusters. sir then explained how this works step by step. step-1:distance calculation and reassignment of clusters. step-2,3: cluster re assignment. initial cluster assignments are random. so we run this multiple times to get a nice model. hierarchical clustering- each observation is a cluster and identify which clusters are close to each other. this group goes on till we get one cluster. we can cut the grouping at any point to get required number of clusters. by default euclidean metrics is used as default, distance between clusters is small.","playground.tensorflow.org- allows us to play with datasets and helps us understands how classification happens. if we want to get a non linear boundary in logistic regression, we need to add new variable such as x1^2,x1^3 and so on. by adding a node in hidden layer in neural network we can get a better boundary. next sir went on to code where he explained about logistic regression implementation and test metrics. don't believe the accuracy values, see other values in confusion metrics and other test metrics. true positive rate(trp)= (tp)/(fn+tp). receiver operating characteristic curve(roc) curve- it is a plot of tpr against fpr for different thresholds(value at which we change classes for eg if p>=0.5 => class 1 else class 0). the area under this curve measures how well the model can distinguish between two groups. so base roc curve we can find threshold values. in roc plot there is a 45 degree line which occurs when we randomly classify data. so when our plot is more close to this 45 degree line then our classification is more worse. auc should be as close to 1 as possible for good classification. the classifier also depends on the quality of data. if the data itself is intermeshed, any classifier wont be able to get a proper boundary. for such datasets we can carry out data separation by using some transformations. any kind of fraud that is occurring is an example of data imbalance. precision: of the observations that have been detected as class 1 how many actually belong to class 1. so if there are multiple values there will be multiple precision values. support- how many values have been classified as a particular class. when there is a class imbalance, it will be difficult to detect anomalies. if we have multiple classes, we have multiple roc curves. many times problems will be easier to convert a regression problem into a classification problem by binning. clustering: unsupervised. we don't have labelled data. we will only have features. we will have to detect potential number of classes and number of points in each class. we will see at k means clustering and hierarchal clustering. metrics part is homework, we will only look at algorithms in class. in k means clustering number of cluster is an input we need to give. in hierarchical clustering we get a dendrogram in which we will be able to know how clusters can be formed based on closeness of groups of data. after clustering we will be able to identify some outliers. clustering is usually done as a part of exploratory data analysis. algorithms are explained step by step in slides. k-means - you are trying to identify k mean points around which a cluster can be formed. we need to specify desired number of clusters. sir then explained how this works step by step. step-1:distance calculation and reassignment of clusters. step-2,3: cluster re assignment. initial cluster assignments are random. so we run this multiple times to get a nice model. hierarchical clustering- each observation is a cluster and identify which clusters are close to each other. this group goes on till we get one cluster. we can cut the grouping at any point to get required number of clusters. by default euclidean metrics is used as default, distance between clusters is small.",8,-4.070776,-21.9389,6.322245,0.32075694,"classification, clusterings, classifying"
631,"knns and varying behaviour with changing feature inputs and first layer depths. used playground.tensorflow.org.
introduction tnr, tpr (aka specficity, sensitivity), fpr and fnr. read roc curve and auc value for a mediocre classifier. 

k-means algorithm and hierarchial clusteting. exploratory data analysis with e2 assignments from two years before.  an overview of different linkage options - complete single and average. steps in clustering and maximal distance. silhouette score and other performance metrics for self study.","knns and varying behaviour with changing feature inputs and first layer depths. used playground.tensorflow.org. introduction tnr, tpr (aka specficity, sensitivity), fpr and fnr. read roc curve and auc value for a mediocre classifier. k-means algorithm and hierarchial clusteting. exploratory data analysis with e2 assignments from two years before. an overview of different linkage options - complete single and average. steps in clustering and maximal distance. silhouette score and other performance metrics for self study.",8,-6.778421,-18.832296,6.3939404,0.66575176,"classification, clusterings, classifying"
632,"midsem discussion:
we discussed about the questions that came up in midsems. the first question was all about exploratory data analysis (eda), we donâ€™t have to create an entire model as such, but we needed to analyze all the data that was given to understand what itâ€™s all about. stuff like, how many rows, how many columns, basics. and then we moved on to missing rows, in this case, since the number is very small, we could drop those rows. else, we could replace those with the mean of the data. this can only be done when data has no particular trend, and it has missing values. we observed a scatter plot to see that. then we saw the histogram as per each column. some are bimodal, some are unimodal, but those didnâ€™t give many insights. then we plotted a pie chart. also, we saw the heat correlation map, it shows a very ideal case, no correlation between the columns so we donâ€™t have to drop anything. however, this analysis is necessary, but not sufficient to say that the columns are not related. sometimes there might be a more serious problem of multi collinearity. 
for the second part of the question we learnt that the main reason why the model created wasnâ€™t working for the new data, was because it was for a different population. performing descriptive statistics on both the data side by side revealed this to us.
important points learnt:
â€¢	on any raw data, perform eda to get a complete understanding of data. 
â€¢	if the data is random, if there is no particular trend, only then you can replace the missing values with mean of the data. 
â€¢	if heat correlation map indicates correlation, it has correlation. but if it doesnâ€™t, then thatâ€™s not a good enough conclusion to say that the columns are not related. sometimes there might be a more serious problem of multi collinearity. so itâ€™s necessary, but not sufficient.","midsem discussion: we discussed about the questions that came up in midsems. the first question was all about exploratory data analysis (eda), we don t have to create an entire model as such, but we needed to analyze all the data that was given to understand what it s all about. stuff like, how many rows, how many columns, basics. and then we moved on to missing rows, in this case, since the number is very small, we could drop those rows. else, we could replace those with the mean of the data. this can only be done when data has no particular trend, and it has missing values. we observed a scatter plot to see that. then we saw the histogram as per each column. some are bimodal, some are unimodal, but those didn t give many insights. then we plotted a pie chart. also, we saw the heat correlation map, it shows a very ideal case, no correlation between the columns so we don t have to drop anything. however, this analysis is necessary, but not sufficient to say that the columns are not related. sometimes there might be a more serious problem of multi collinearity. for the second part of the question we learnt that the main reason why the model created wasn t working for the new data, was because it was for a different population. performing descriptive statistics on both the data side by side revealed this to us. important points learnt: on any raw data, perform eda to get a complete understanding of data. if the data is random, if there is no particular trend, only then you can replace the missing values with mean of the data. if heat correlation map indicates correlation, it has correlation. but if it doesn t, then that s not a good enough conclusion to say that the columns are not related. sometimes there might be a more serious problem of multi collinearity. so it s necessary, but not sufficient.",9,-8.183247,16.191639,9.665464,8.448565,"dataâ, analyse, analyses"
633,"in today's class we took a sample data with 100 data points and found out the best fit line using simple linear regression in ms excel. we calculated the values of 'a' and 'b' using the data to get the regression line y(hat) = a*x + b. we plotted a scatter plot between x and y and also added the y(hat) points on the same graph. then we calculated the error values
ei = yi - yi(hat) and plotted the error values on a scatter plot and a histogram as well. for a perfectly random data the error values should be a normal distribution (bell curve) on the histogram, which is not the case as we observed so we say that the model failed to pick up the pattern in the data.
then we used the data analysis tools for linear regression to get various information about the sample data.

then we moved on to discuss ""what is a good model?"" 
the model that explains most of the variations in the data.
sst = summation(yi - ybar)^2
sst = sse + ssr 
1 = sse/sst + ssr/sst
1 = r^2 + sse/sst
r^2 = 1 - sse/sst
r^2 : coefficient of determination
in case of simple linear regression coefficient of determination is same as the square of correlation coefficient 'r'.  
r^2 = r^2


when drawing k representative samples (si) of size n from a population, the means of these samples (mi) are expected to be similar due to their representativeness. if we plot the frequency distribution of mi, we typically get a histogram that resembles a normal distribution, as suggested by the central limit theorem. this histogram illustrates the sampling distribution of the sample mean. important characteristics of this distribution include that its mean is close to the population mean, and its standard deviation, known as the standard error (sx), is connected to the population standard deviation (sigma) and the sample size (n). these characteristics highlight the advantage of using multiple smaller samples rather than relying on a single large one, as the sampling distribution offers valuable insights into the population's traits.","in today's class we took a sample data with 100 data points and found out the best fit line using simple linear regression in ms excel. we calculated the values of 'a' and 'b' using the data to get the regression line y(hat) = a*x + b. we plotted a scatter plot between x and y and also added the y(hat) points on the same graph. then we calculated the error values ei = yi - yi(hat) and plotted the error values on a scatter plot and a histogram as well. for a perfectly random data the error values should be a normal distribution (bell curve) on the histogram, which is not the case as we observed so we say that the model failed to pick up the pattern in the data. then we used the data analysis tools for linear regression to get various information about the sample data. then we moved on to discuss ""what is a good model?"" the model that explains most of the variations in the data. sst = summation(yi - ybar)^2 sst = sse + ssr 1 = sse/sst + ssr/sst 1 = r^2 + sse/sst r^2 = 1 - sse/sst r^2 : coefficient of determination in case of simple linear regression coefficient of determination is same as the square of correlation coefficient 'r'. r^2 = r^2 when drawing k representative samples (si) of size n from a population, the means of these samples (mi) are expected to be similar due to their representativeness. if we plot the frequency distribution of mi, we typically get a histogram that resembles a normal distribution, as suggested by the central limit theorem. this histogram illustrates the sampling distribution of the sample mean. important characteristics of this distribution include that its mean is close to the population mean, and its standard deviation, known as the standard error (sx), is connected to the population standard deviation (sigma) and the sample size (n). these characteristics highlight the advantage of using multiple smaller samples rather than relying on a single large one, as the sampling distribution offers valuable insights into the population's traits.",5,24.56612,-3.2036152,14.3115835,4.41945,"regression, statistical, statistics"
634,"logistic regression-
most of terms used here are derived from medical field. we have to assign label to each point based on output. we need to find boundaries which separate regions. expectation algebra - same as discussed in probability course last semster. if we don't have standard deviation of population, we assume that standard deviation of sample is same as that of population. to find the number of bins in a histogram there are different methods such as struges rule, rice rule etc based on what we want to infer from the data. even when there are overlapping clusters, we might need a boundary to separate out non overlapping points, overlapping points are dealt later. 
logistic regression doesn't directly predict the class to which the point belongs, it predicts the probability with which it belongs to a particular class. if the number of classes are two it is simple, if probability of y=1 given x is greater than 0.5.  we need to solve the problem of finding a boundary so that misclassification is minimum. our goal is to minimise the difference between .
calculate the weights wi such that the likelihood of getting the desired targets is maximised given the observations. in training phase input data points xj and the output points yj are known. predicted probability is probability of y given x which is sigmoid of w x + b. if there are two classes, if the predicted probability lies close to 1 then it belongs to class 1. we want to maximise the likelihood of our predicted outcomes being close to the targets. log(l) is known as the log likelihood. minimising likelihood is same as maximising log likelihood. see derivation in slides. apply gradient descent to minimise likelihood. the boundary will be decided based on the flexibility. we have random forest classification and random forest regression. we are give a dataset we have to apply some model and then compare the metrics of both models. we have to create a confusion matrix which contains true positive, false positive , true negatives and false negatives. false negative is more dangerous. false positive might incur some additional costs. accuracy=true positive+ true negative / total.( how many observations you have correctly classified. when there is a data imbalance, there is not much impact on accuracy, so it is not correct measure to compare. precision is of the events that you have predicted how many you have predicted correctly. recall value of a specific class how many could you correctly identify. f1 score is the harmonic mean of precision and recall. other metrics include true positive rate, false positive rate, true negative rate.","logistic regression- most of terms used here are derived from medical field. we have to assign label to each point based on output. we need to find boundaries which separate regions. expectation algebra - same as discussed in probability course last semster. if we don't have standard deviation of population, we assume that standard deviation of sample is same as that of population. to find the number of bins in a histogram there are different methods such as struges rule, rice rule etc based on what we want to infer from the data. even when there are overlapping clusters, we might need a boundary to separate out non overlapping points, overlapping points are dealt later. logistic regression doesn't directly predict the class to which the point belongs, it predicts the probability with which it belongs to a particular class. if the number of classes are two it is simple, if probability of y=1 given x is greater than 0.5. we need to solve the problem of finding a boundary so that misclassification is minimum. our goal is to minimise the difference between . calculate the weights wi such that the likelihood of getting the desired targets is maximised given the observations. in training phase input data points xj and the output points yj are known. predicted probability is probability of y given x which is sigmoid of w x + b. if there are two classes, if the predicted probability lies close to 1 then it belongs to class 1. we want to maximise the likelihood of our predicted outcomes being close to the targets. log(l) is known as the log likelihood. minimising likelihood is same as maximising log likelihood. see derivation in slides. apply gradient descent to minimise likelihood. the boundary will be decided based on the flexibility. we have random forest classification and random forest regression. we are give a dataset we have to apply some model and then compare the metrics of both models. we have to create a confusion matrix which contains true positive, false positive , true negatives and false negatives. false negative is more dangerous. false positive might incur some additional costs. accuracy=true positive+ true negative / total.( how many observations you have correctly classified. when there is a data imbalance, there is not much impact on accuracy, so it is not correct measure to compare. precision is of the events that you have predicted how many you have predicted correctly. recall value of a specific class how many could you correctly identify. f1 score is the harmonic mean of precision and recall. other metrics include true positive rate, false positive rate, true negative rate.",10,12.397362,-18.861908,8.973544,-1.5468866,"classifications, histograms, histogram"
635,"sir started with how to read confusion matrix and how to interpret it. till now the data given to us was ordered and structured data. but in real data will be different. we have to understand the nature of data. we have some tools to understand data. after understanding this data we have to think and apply proper transformations so that data will be converted into a form suitable for analysis. data formats can be text, binary, files, database, from internet. crisp-dm : cross industry standard process for data mining. it has six steps running cyclically :1. business understanding 2. data understanding  3. data preparation 4. modelling 5. evaluation 6. deployment. currently we see data understanding and data preparation. don't use deep learning networks when you are not supposed to use. use the most appropriate model suitable. data understanding: collect initial data, describe data, verify data quality. data preparation: select data, clean data, construct data, integrate data, format data. eda-performing initial investigations on the data to gain insights, spot anomalies, test hypothesis and check assumptions. then sir showed a mind map about data problems and explained it. heteroscedasticity: different variance at different points of dataset. then one of the teaching assistant gave explanation about eda. diabetes data set was analysed. the features are glucose level, bmi,bp and insulin.  the output is whether a person is diabetic or not. a boxplot can help understand variability in the features and any outliers present in them. feature correlations helps us filter data better. matrix plot between can show more such relationships between the features-two at a time. we have to do time based analysis: how data evolves in time. find any underlying trend or seasonality in the data. handling missing values: missing data is shown as na. it is of three formats: missing completely at random(mcar)-completely random data points missing, missing at random(mar)-some relationships between missing point and values in different columns , missing not at random(mnar). to solve this we can just leave those missing points like that. we can replace the missing point with mean, median or mode. we can use knn, we can see the nearest points near the missing point and take their average and replace the missing value. mice-multiple imputation by chained equations. so in filling missing data we might have to use machine learning models. time series data is of particular interest because of the temporal aspects that we can utilize. forward and backward fill, linear interpolation, simple moving averages. we have to understand which method is best for which type  of missing points. handling outliers- datapoint that significantly differs from other data. detecting outliers: inter quartile range, standard deviation.  for multivariate data, dbscan- density based spatial clustering of applications with noise can be used to detect outliers. to visualise higher dimensional data we can use 2d t-sne plot(discussed in detail in further lectures). to remove outliers: data trimming and data capping.","sir started with how to read confusion matrix and how to interpret it. till now the data given to us was ordered and structured data. but in real data will be different. we have to understand the nature of data. we have some tools to understand data. after understanding this data we have to think and apply proper transformations so that data will be converted into a form suitable for analysis. data formats can be text, binary, files, database, from internet. crisp-dm : cross industry standard process for data mining. it has six steps running cyclically :1. business understanding 2. data understanding 3. data preparation 4. modelling 5. evaluation 6. deployment. currently we see data understanding and data preparation. don't use deep learning networks when you are not supposed to use. use the most appropriate model suitable. data understanding: collect initial data, describe data, verify data quality. data preparation: select data, clean data, construct data, integrate data, format data. eda-performing initial investigations on the data to gain insights, spot anomalies, test hypothesis and check assumptions. then sir showed a mind map about data problems and explained it. heteroscedasticity: different variance at different points of dataset. then one of the teaching assistant gave explanation about eda. diabetes data set was analysed. the features are glucose level, bmi,bp and insulin. the output is whether a person is diabetic or not. a boxplot can help understand variability in the features and any outliers present in them. feature correlations helps us filter data better. matrix plot between can show more such relationships between the features-two at a time. we have to do time based analysis: how data evolves in time. find any underlying trend or seasonality in the data. handling missing values: missing data is shown as na. it is of three formats: missing completely at random(mcar)-completely random data points missing, missing at random(mar)-some relationships between missing point and values in different columns , missing not at random(mnar). to solve this we can just leave those missing points like that. we can replace the missing point with mean, median or mode. we can use knn, we can see the nearest points near the missing point and take their average and replace the missing value. mice-multiple imputation by chained equations. so in filling missing data we might have to use machine learning models. time series data is of particular interest because of the temporal aspects that we can utilize. forward and backward fill, linear interpolation, simple moving averages. we have to understand which method is best for which type of missing points. handling outliers- datapoint that significantly differs from other data. detecting outliers: inter quartile range, standard deviation. for multivariate data, dbscan- density based spatial clustering of applications with noise can be used to detect outliers. to visualise higher dimensional data we can use 2d t-sne plot(discussed in detail in further lectures). to remove outliers: data trimming and data capping.",9,-13.813844,19.018848,9.254771,8.954698,"dataâ, analyse, analyses"
636,"today first thing i learned is how to find similarities in two things like submissions and all.
sir also gave a recap of what he taught previous time and also talked about f= msr/mse.
then i learned about the mlr and how to do it on excel and how to interpret the data.
also, i learned that python is very imp and as i am a beginner in it so i have to do efforts and learn it to some extent so as to be at par in class.","today first thing i learned is how to find similarities in two things like submissions and all. sir also gave a recap of what he taught previous time and also talked about f= msr/mse. then i learned about the mlr and how to do it on excel and how to interpret the data. also, i learned that python is very imp and as i am a beginner in it so i have to do efforts and learn it to some extent so as to be at par in class.",6,-0.69619495,8.673785,7.2204704,9.365259,"summarizing, summarize, summarization"
637,"in class we discussed the steps to be applied in data science, which are: understanding the problem, exploratory data analysis, and visualization. then we discussed about the cyclic running process called: crisp dm. it stands for ""cross industry standard process for data mining"". its six steps are: business understanding, data understanding, data preparation, modelling, evaluation, deployment. we need to budget our time accordingly as we do not want to spend too much time on unimportant tasks. we need to create a project plan to judicially assign time to each of these six steps. 
later, we looked into exploratory data analysis. we saw a rigorous mind map for problems related to 'x' and 'y'. for the x part, we saw that problems can be either less features or too many features. when we have too many features we can't just go on eliminating features. there is possibility that the feature we eliminate might actually be a controlling part for the process. hence, we need to be very careful. other problems are feature scaling problem, collinearity and multicollinearity. 
later, we had a presentation by shubham. he explained his approach on the data analysis for diabetic data. he explained the problems that arose and also their solutions. he suggested a method of density based spatial clustering of application with noise (dbscan). 
later, sir cleared some points regarding outliers. we focus more on median when dealing with outliers than the mean. this is because outliers do not affect median but it affects mean.","in class we discussed the steps to be applied in data science, which are: understanding the problem, exploratory data analysis, and visualization. then we discussed about the cyclic running process called: crisp dm. it stands for ""cross industry standard process for data mining"". its six steps are: business understanding, data understanding, data preparation, modelling, evaluation, deployment. we need to budget our time accordingly as we do not want to spend too much time on unimportant tasks. we need to create a project plan to judicially assign time to each of these six steps. later, we looked into exploratory data analysis. we saw a rigorous mind map for problems related to 'x' and 'y'. for the x part, we saw that problems can be either less features or too many features. when we have too many features we can't just go on eliminating features. there is possibility that the feature we eliminate might actually be a controlling part for the process. hence, we need to be very careful. other problems are feature scaling problem, collinearity and multicollinearity. later, we had a presentation by shubham. he explained his approach on the data analysis for diabetic data. he explained the problems that arose and also their solutions. he suggested a method of density based spatial clustering of application with noise (dbscan). later, sir cleared some points regarding outliers. we focus more on median when dealing with outliers than the mean. this is because outliers do not affect median but it affects mean.",9,-13.601157,14.229623,9.004297,8.378588,"dataâ, analyse, analyses"
638,"the class started with a recap of what had been done in the past few classes, specifically doubts regarding p-value. p-values can be used to reject those input variables that don't contribute significant enough to the output. then it was discussed how the summary data that we submit is being analyzed. a similarity heatmap was shown indicating how similar two submissions were. basically, every submission can be considered as a vector in a higher dimensional space. two similar submissions will be close to each other in that space and this idea was used to create that heatmap. this was really amazing to know. it was discussed that even if a student tries to slightly modify another submission, he leaves out trails of it and that can be identified easily using modern ai tools. a little discussion on feature engineering was done. it is making up new features from the existing features to reduce better model the function f. then we started discussing mlr. we discussed that all problems may not have closed form solutions and we use a method called gradient descent to numerically compute the optimal parameters. then it was discussed how quantities like mse, mae, f-score don't give much information about the model and are rather mostly used in comparing two models. finally it was shown how removing those independent variables with higher values of p, lead finally to a model with higher f-score and comparison metric.","the class started with a recap of what had been done in the past few classes, specifically doubts regarding p-value. p-values can be used to reject those input variables that don't contribute significant enough to the output. then it was discussed how the summary data that we submit is being analyzed. a similarity heatmap was shown indicating how similar two submissions were. basically, every submission can be considered as a vector in a higher dimensional space. two similar submissions will be close to each other in that space and this idea was used to create that heatmap. this was really amazing to know. it was discussed that even if a student tries to slightly modify another submission, he leaves out trails of it and that can be identified easily using modern ai tools. a little discussion on feature engineering was done. it is making up new features from the existing features to reduce better model the function f. then we started discussing mlr. we discussed that all problems may not have closed form solutions and we use a method called gradient descent to numerically compute the optimal parameters. then it was discussed how quantities like mse, mae, f-score don't give much information about the model and are rather mostly used in comparing two models. finally it was shown how removing those independent variables with higher values of p, lead finally to a model with higher f-score and comparison metric.",0,-1.0194746,4.2267795,9.225403,4.848584,"models, feature, features"
639,"in this class, we performed exploratory data analysis on our summary data collected. we discussed a few functions of the pivot table and how it can be helpful, as well as a little more convenient than to code. we performed some analysis on our outlier data using multiple graphs to get inferences on the outlier nature.
at the end, the tas came and discussed some common mistakes which we performed in our exercise 2","in this class, we performed exploratory data analysis on our summary data collected. we discussed a few functions of the pivot table and how it can be helpful, as well as a little more convenient than to code. we performed some analysis on our outlier data using multiple graphs to get inferences on the outlier nature. at the end, the tas came and discussed some common mistakes which we performed in our exercise 2",6,-6.9288177,23.158115,7.8609824,9.798677,"summarizing, summarize, summarization"
640,"we started a new topic vif which is used to determine the most important and unique parameters. in vif vs r squared graph, we fix a vif threshold corresponding to r squared = 0.9 and reject the parameters having vif greater than the threshold value. we start eliminating features one by one such that there are no features with vif greater than say 10. our motivation for reducing the features is to minimize the curse of dimensionality. we then looked at principal component analysis which is an application of single value decomposition. it also works similar to vif by reducing dimensionality. the remaining components are known as principal components. principal components are always orthogonal to each other. the essential difference vif and pca are the parameters which they test. we generally perform vif first as it increases the interpretability. vif is useful for making predictions and pca is more useful for ""what if"" analysis or delta analysis. the uses of pca include dimension reduction, making prediction model and visualisation which is done in eda stage. the thing to be cautious about is the scale dependency of pca and some parameters might need normalisation. we then concluded the lecture with discussion on tsne.","we started a new topic vif which is used to determine the most important and unique parameters. in vif vs r squared graph, we fix a vif threshold corresponding to r squared = 0.9 and reject the parameters having vif greater than the threshold value. we start eliminating features one by one such that there are no features with vif greater than say 10. our motivation for reducing the features is to minimize the curse of dimensionality. we then looked at principal component analysis which is an application of single value decomposition. it also works similar to vif by reducing dimensionality. the remaining components are known as principal components. principal components are always orthogonal to each other. the essential difference vif and pca are the parameters which they test. we generally perform vif first as it increases the interpretability. vif is useful for making predictions and pca is more useful for ""what if"" analysis or delta analysis. the uses of pca include dimension reduction, making prediction model and visualisation which is done in eda stage. the thing to be cautious about is the scale dependency of pca and some parameters might need normalisation. we then concluded the lecture with discussion on tsne.",11,-16.3132,1.5560172,10.316211,12.987764,"pca, heatmap, heatmaps"
641,"discussion apon principle component analysis was taken further , v.i.f was also discussed upon i.e how the features can be dropped , the ones with high v.i.f value correlated factors could be removed , role of principle component analysis in exploratory data analysis. elbow method can be used to determine the the number of optimal principle components . transformed model can be used to make more efficient model.comparison between principle component regression and linear regression was also done .","discussion apon principle component analysis was taken further , v.i.f was also discussed upon i.e how the features can be dropped , the ones with high v.i.f value correlated factors could be removed , role of principle component analysis in exploratory data analysis. elbow method can be used to determine the the number of optimal principle components . transformed model can be used to make more efficient model.comparison between principle component regression and linear regression was also done .",11,-11.320322,2.4318087,10.060771,12.658328,"pca, heatmap, heatmaps"
642,"crisp-dm is used for data mining. exploratory data analysis is used to analyse anomalies, test assumptions, gain insights. eda involves statistical graphics. inter feature relation between features gives data.mcar, mnar,mar are types of missing data in aqi dataset of maharashtra. we can also do multivariate data approaches such as knn and mice","crisp-dm is used for data mining. exploratory data analysis is used to analyse anomalies, test assumptions, gain insights. eda involves statistical graphics. inter feature relation between features gives data.mcar, mnar,mar are types of missing data in aqi dataset of maharashtra. we can also do multivariate data approaches such as knn and mice",9,-16.493553,22.341585,8.243262,8.985414,"dataâ, analyse, analyses"
643,"in today's class, we saw that the sample we selected for testing out of the population must be good and representative. the attributes we calculate for the sample are called statistics, and those we estimate for the population are called parameters. we estimate parameters based on the statistics. if y is a dependent/response variable, which is dependent on x as independent variables, features, or predictors and we have their sample data then we can predict a model through which we can find y for values of x that were not in the sample data, i.e. we are predicting population attributes using sample attributes. if there are various dependent variables, like x1,x2, etc. we can estimate the model using mlr while for only x we can predict the model using slr. the very basic/naive model of the sample data can be a point, but the error in the predicted and observed value will be significant in most cases. then we move on towards slr, for data, if we can plot a best-fit line then the line can be predicted by simple linear regression as y= b0+b1x. as we increase the sample size, our model will become better as we move toward the population as a sample. similarly, different sample has different best-fit lines. b0 and b1 are point estimates and we need to arrive at the possible interval within which these values lie such that there is a very high chance that b0 and b1  we can get b0 and b1 using the sample data will be within the interval respectively. we can get the values of b0 and b1 using the sample data.","in today's class, we saw that the sample we selected for testing out of the population must be good and representative. the attributes we calculate for the sample are called statistics, and those we estimate for the population are called parameters. we estimate parameters based on the statistics. if y is a dependent/response variable, which is dependent on x as independent variables, features, or predictors and we have their sample data then we can predict a model through which we can find y for values of x that were not in the sample data, i.e. we are predicting population attributes using sample attributes. if there are various dependent variables, like x1,x2, etc. we can estimate the model using mlr while for only x we can predict the model using slr. the very basic/naive model of the sample data can be a point, but the error in the predicted and observed value will be significant in most cases. then we move on towards slr, for data, if we can plot a best-fit line then the line can be predicted by simple linear regression as y= b0+b1x. as we increase the sample size, our model will become better as we move toward the population as a sample. similarly, different sample has different best-fit lines. b0 and b1 are point estimates and we need to arrive at the possible interval within which these values lie such that there is a very high chance that b0 and b1 we can get b0 and b1 using the sample data will be within the interval respectively. we can get the values of b0 and b1 using the sample data.",1,36.2453,-5.8352633,16.338978,3.6859355,"population, models, estimating"
644,"we started the discussion with logistic regression. we saw the logistic regression model in the sklearn library. how to implement it etc. logit_regression score gives the accuracy of the fit. but accuracy of the model is not a good metric to decide whether the model is good or bad. hence we make a confusion matrix. by this we get the false positives false negatives etc. using this we can get the recall, precision and f1-score by these which are a better metric to decide whether a model is good or bad. then we discussed about roc curve which tells when the classifier starts detecting incorrect values/false positives. auc signifies area under the roc curve. we need to set thresholds on classifier best classifier will have an auc tending to 1 and worst will have an auc tending to 0.5. 

then we touched the topic of feature engineering and feature transformation. we also discussed that if there is a class imbalance in the dataset then the classifier will not be able to perform well and generalize well.

then we discussed clustering algorithms which are a class of unsupervised learning. we discussed to methods:
1) heirarchial clustering - make dendograms
2) kmeans clustering - randomly initiate centroids and then reassign based on distances. we initially declare that how many clusters we want to achieve and then finally reach that target.

then we also talked about sillhoutte score which tells whether the ckustering is good or not.","we started the discussion with logistic regression. we saw the logistic regression model in the sklearn library. how to implement it etc. logit_regression score gives the accuracy of the fit. but accuracy of the model is not a good metric to decide whether the model is good or bad. hence we make a confusion matrix. by this we get the false positives false negatives etc. using this we can get the recall, precision and f1-score by these which are a better metric to decide whether a model is good or bad. then we discussed about roc curve which tells when the classifier starts detecting incorrect values/false positives. auc signifies area under the roc curve. we need to set thresholds on classifier best classifier will have an auc tending to 1 and worst will have an auc tending to 0.5. then we touched the topic of feature engineering and feature transformation. we also discussed that if there is a class imbalance in the dataset then the classifier will not be able to perform well and generalize well. then we discussed clustering algorithms which are a class of unsupervised learning. we discussed to methods: 1) heirarchial clustering - make dendograms 2) kmeans clustering - randomly initiate centroids and then reassign based on distances. we initially declare that how many clusters we want to achieve and then finally reach that target. then we also talked about sillhoutte score which tells whether the ckustering is good or not.",8,0.101436466,-18.746563,7.0252614,0.37372836,"classification, clusterings, classifying"
645,"in the review of their mid-semester exam and problem-solving approach, the primary steps of exploratory data analysis (eda) were discussed, including data visualization and handling missing values. the analysis also covered the limitations in prediction accuracy due to an under-sampled target variable, ""heart diseases."" insights were drawn from histograms and heat maps, guiding the decision to drop certain columns or apply feature scaling where necessary. based on the data type and problem, appropriate models such as tree-based methods, svm, and logistic regression were selected. considerations of the curse of dimensionality and variance inflation factor (vif) were also addressed in the process.","in the review of their mid-semester exam and problem-solving approach, the primary steps of exploratory data analysis (eda) were discussed, including data visualization and handling missing values. the analysis also covered the limitations in prediction accuracy due to an under-sampled target variable, ""heart diseases."" insights were drawn from histograms and heat maps, guiding the decision to drop certain columns or apply feature scaling where necessary. based on the data type and problem, appropriate models such as tree-based methods, svm, and logistic regression were selected. considerations of the curse of dimensionality and variance inflation factor (vif) were also addressed in the process.",13,-12.628992,10.558022,8.509962,8.115932,"classification, classifying, classifications"
646,"crisp-dm is the process we covered today for addressing data mining, progressing through learning about business and data, preparing it, developing models, testing them, and ultimately deploying solutions. eda allows us to play with data, identify patterns, and see problems such as insulin outliers. there isn't always random missing data, so we apply methods such as knn or mice to manage it. class imbalance can also distort results, so there are special techniques. visualization tools such as histograms and boxplots make analysis more intuitive and clearer.","crisp-dm is the process we covered today for addressing data mining, progressing through learning about business and data, preparing it, developing models, testing them, and ultimately deploying solutions. eda allows us to play with data, identify patterns, and see problems such as insulin outliers. there isn't always random missing data, so we apply methods such as knn or mice to manage it. class imbalance can also distort results, so there are special techniques. visualization tools such as histograms and boxplots make analysis more intuitive and clearer.",9,-18.904463,23.18316,8.215739,8.87192,"dataâ, analyse, analyses"
647,"summary 2

in this lecture, we studied logistic regression, which helps classify data using probability scores. clustering techniques were introduced to segment similar data points. we learned about true positive and true negative classifications, essential for assessing model accuracy. outlier detection methods were explored to identify unusual data points. the role of loss functions in minimizing prediction errors was discussed. finally, solving logistic regression using optimization techniques was demonstrated.","summary 2 in this lecture, we studied logistic regression, which helps classify data using probability scores. clustering techniques were introduced to segment similar data points. we learned about true positive and true negative classifications, essential for assessing model accuracy. outlier detection methods were explored to identify unusual data points. the role of loss functions in minimizing prediction errors was discussed. finally, solving logistic regression using optimization techniques was demonstrated.",13,-6.300092,-8.811195,8.500467,5.29157,"classification, classifying, classifications"
648,"today's session explored key machine learning concepts, starting with optimization and recall parameters. we examined how regression problems can sometimes be reframed as classification tasks, though the reverse isnâ€™t feasible due to information loss. in unsupervised learning, we covered clustering techniques, including hierarchical clustering, which organizes data into a tree-like structure using a dendrogram, and k-means clustering, where points are iteratively assigned to predefined clusters based on their proximity to centroids. we then experimented with neural networks using tensorflow playground, adjusting parameters to observe changes in predictions. lastly, we discussed classification metrics such as auc-roc, which evaluates model performance by comparing true and false positive rates, and explored how transformations and feature engineering can enhance data separation, making classification models more effective than simple regression.","today's session explored key machine learning concepts, starting with optimization and recall parameters. we examined how regression problems can sometimes be reframed as classification tasks, though the reverse isn t feasible due to information loss. in unsupervised learning, we covered clustering techniques, including hierarchical clustering, which organizes data into a tree-like structure using a dendrogram, and k-means clustering, where points are iteratively assigned to predefined clusters based on their proximity to centroids. we then experimented with neural networks using tensorflow playground, adjusting parameters to observe changes in predictions. lastly, we discussed classification metrics such as auc-roc, which evaluates model performance by comparing true and false positive rates, and explored how transformations and feature engineering can enhance data separation, making classification models more effective than simple regression.",8,-3.9055262,-13.959308,6.634997,1.065606,"classification, clusterings, classifying"
649,"we started of with understanding about population vs samples and how the we can use a sample to make 'estimates' about the population. a point to be noted was that a sample should be picked from an appropriate place, that is the location or the center from where we gather our data shouldn't be biased. like the example given during class, it is better to take the sample from a public place than a school, when we are sampling for that area's census. moving on we saw about how and when a clustering or simple linear regression would be better. the key is to minizine the error. we went ahead and talked about the meaning of intercept with the example of sales and how it shows that there is some features we don't know and it gives a bias. in an ideal case we will know all the features and then the line will pass through the (0,0) point in the graph. we also discussed a bit about how to define the error and what could be beneficial and what wouldn't.","we started of with understanding about population vs samples and how the we can use a sample to make 'estimates' about the population. a point to be noted was that a sample should be picked from an appropriate place, that is the location or the center from where we gather our data shouldn't be biased. like the example given during class, it is better to take the sample from a public place than a school, when we are sampling for that area's census. moving on we saw about how and when a clustering or simple linear regression would be better. the key is to minizine the error. we went ahead and talked about the meaning of intercept with the example of sales and how it shows that there is some features we don't know and it gives a bias. in an ideal case we will know all the features and then the line will pass through the (0,0) point in the graph. we also discussed a bit about how to define the error and what could be beneficial and what wouldn't.",2,30.527676,-1.9332541,15.257644,3.3829055,"regression, regressions, features"
650,"in today's session, we learned how to do simple linear regression in excel using its built-in functions. we used scatter plots and histograms to analyze the nature of the data and check whether it is normally distributed. in addition, we explored various statistical parameters of the given csv file to understand the dataset better.
another point of discussion was the coefficient of determination, râ², versus the correlation coefficient, r. râ² is preferred because it gives a measure of the proportion of the variance in the dependent variable, y, explained by the independent variable, x, making it a more precise measure of the strength of their relationship.","in today's session, we learned how to do simple linear regression in excel using its built-in functions. we used scatter plots and histograms to analyze the nature of the data and check whether it is normally distributed. in addition, we explored various statistical parameters of the given csv file to understand the dataset better. another point of discussion was the coefficient of determination, r , versus the correlation coefficient, r. r is preferred because it gives a measure of the proportion of the variance in the dependent variable, y, explained by the independent variable, x, making it a more precise measure of the strength of their relationship.",5,19.165478,-0.22265358,13.674625,5.0261173,"regression, statistical, statistics"
651,"in today's class first we started by stating the fact that when y is a function of a large number of variables then multicolinearity may be possible that is there can be some kind of interdependence between these values. then we came to a conclusion that beta value can be used as a closed form in theory but it is impractical for real life scenarios. free don't take test and training to be 50 50% ratio because this does not allow our model to check the whole data and give better variance. but if we have lower number of observations then we can take the ratio of training set to be 90% otherwise ideally the training set should be around 80%. tell me talked about overfit situation which will be very very good for the training data but it won't be able to fit the testing data and our model will not be generalised. sometimes we also introduce bias to make it better which is also called bias variance tradeoff. then we talked about multiple r from the data analysis in spreadsheet. we also talked about r square and that one degree of freedom should be minus one, as the adjusted r square value will drop if we add the variables. then we started with some basic python functions on the jupyter notebook .if errors are following normal distribution then in the qq plot the point should lie in the straight line. he also learned about omnibus statistics and omnibus p value and how we use them in the first exercise as a skewness. we also learned ols and came to the fact that lower the value of aic and bic better is the model.","in today's class first we started by stating the fact that when y is a function of a large number of variables then multicolinearity may be possible that is there can be some kind of interdependence between these values. then we came to a conclusion that beta value can be used as a closed form in theory but it is impractical for real life scenarios. free don't take test and training to be 50 50% ratio because this does not allow our model to check the whole data and give better variance. but if we have lower number of observations then we can take the ratio of training set to be 90% otherwise ideally the training set should be around 80%. tell me talked about overfit situation which will be very very good for the training data but it won't be able to fit the testing data and our model will not be generalised. sometimes we also introduce bias to make it better which is also called bias variance tradeoff. then we talked about multiple r from the data analysis in spreadsheet. we also talked about r square and that one degree of freedom should be minus one, as the adjusted r square value will drop if we add the variables. then we started with some basic python functions on the jupyter notebook .if errors are following normal distribution then in the qq plot the point should lie in the straight line. he also learned about omnibus statistics and omnibus p value and how we use them in the first exercise as a skewness. we also learned ols and came to the fact that lower the value of aic and bic better is the model.",2,8.709102,4.663924,10.979125,4.8038826,"regression, regressions, features"
652,"today were started by looking into eda again, but with our session summary data. we were told about pivot table which provides us with data like skewness, max min and more. using these we can make some inferences. we spent some time in this using the session summary data as mentioned. then we headed on how to treat outliners a bit and we were advised not too totally discard them as the question why this outliner arises too is valuable and can give us insights.
then the tas came and gave us the insights on the reports of e2.","today were started by looking into eda again, but with our session summary data. we were told about pivot table which provides us with data like skewness, max min and more. using these we can make some inferences. we spent some time in this using the session summary data as mentioned. then we headed on how to treat outliners a bit and we were advised not too totally discard them as the question why this outliner arises too is valuable and can give us insights. then the tas came and gave us the insights on the reports of e2.",6,-7.567984,22.531607,7.989558,9.859961,"summarizing, summarize, summarization"
653,"we discussed and looked into the various numbers of statistics that come out from a standard process. there is p value which state how close close is our estimate and the it also tells us about the confidence of our estimate. and for that we use a interval as a point has a zero probability, but an interval can have a probability with the setting that the area under the distribution is kept at 1 and ratio can be used and appropriate values can be found. we also looked at the t-stat distribution. it is like normal but a bit more spread out at the extremes and flat at the mean.","we discussed and looked into the various numbers of statistics that come out from a standard process. there is p value which state how close close is our estimate and the it also tells us about the confidence of our estimate. and for that we use a interval as a point has a zero probability, but an interval can have a probability with the setting that the area under the distribution is kept at 1 and ratio can be used and appropriate values can be found. we also looked at the t-stat distribution. it is like normal but a bit more spread out at the extremes and flat at the mean.",7,37.500175,-0.6707115,15.240308,2.6133785,"statistics, statistical, statisticsâ"
654,"the sample from the population should be a good representation of the whole population. we want to estimate the parameters based on the statistics.

model - a mathematical representation of a real-world problem.  
the most basic model - a point (but not a good enough prediction because it's a very naive model).

simple linear regression -  
y = dependent variable or label, and x = independent variable or feature.  
y = b(0) + b(1)*x (only 1 predictor).  
b(0) --> the bias, net sum of all unaccounted variables on prediction.  
b(0) and b(1) are nothing but the estimates of population parameters.  
as the sample size increases, the prediction error keeps on reducing.  
confidence increases if we increase the interval in the confidence interval (c.i.).

criteria for the best fit line - minimize the sum of squares of errors.  
then, we derived the formula for slr (we have a closed-form solution to calculate b(0) and b(1)).

the mean of the observation lies on the prediction line.  
b(0) and b(1) are just point estimates, and we need to arrive at the interval of b(0) and b(1).","the sample from the population should be a good representation of the whole population. we want to estimate the parameters based on the statistics. model - a mathematical representation of a real-world problem. the most basic model - a point (but not a good enough prediction because it's a very naive model). simple linear regression - y = dependent variable or label, and x = independent variable or feature. y = b(0) + b(1)*x (only 1 predictor). b(0) --> the bias, net sum of all unaccounted variables on prediction. b(0) and b(1) are nothing but the estimates of population parameters. as the sample size increases, the prediction error keeps on reducing. confidence increases if we increase the interval in the confidence interval (c.i.). criteria for the best fit line - minimize the sum of squares of errors. then, we derived the formula for slr (we have a closed-form solution to calculate b(0) and b(1)). the mean of the observation lies on the prediction line. b(0) and b(1) are just point estimates, and we need to arrive at the interval of b(0) and b(1).",1,33.801956,-11.079511,16.485575,4.270455,"population, models, estimating"
655,"we learnt about the levels of measurement. there are 4 levels of measurement. (nominal, ordinal, interval, ratio). nominal and ordinal being discrete and interval and ratio being continuous. ml category: classification for nominal and ordinal and regression for interval and ratio. 
there are various algos we use: linear regression, multiple linear regression, logistic regression, random forest, k-means clustering, heirarchial clustering.","we learnt about the levels of measurement. there are 4 levels of measurement. (nominal, ordinal, interval, ratio). nominal and ordinal being discrete and interval and ratio being continuous. ml category: classification for nominal and ordinal and regression for interval and ratio. there are various algos we use: linear regression, multiple linear regression, logistic regression, random forest, k-means clustering, heirarchial clustering.",4,-19.67405,-15.692935,2.2354882,0.6350866,"classification, classifying, classifications"
656,"in todayâ€™s session, the instructor provided a detailed tutorial on using pivot tables to conduct exploratory data analysis (eda). the session began with a hands-on demonstration of pivot table functions applied to our summary analysis, where we explored plots of averages, maximums, and minimums, such as analyzing the number of words in summaries. this practical approach enabled us to see firsthand how pivot tables can reveal underlying data trends and discrepancies.

building on that, the instructor delved into broader aspects of eda, discussing various techniques and strategies for handling different types of challenges encountered during data exploration. he emphasized that understanding the nuances of eda is critical for diagnosing data issues, cleaning datasets, and ensuring the accuracy of the analysis. key points included methods for managing missing values, outliers, and other common data irregularities that can skew results.

the session then transitioned to a real-world demonstration, where the instructor showcased eda on datasets from a chemical plant and a solar plant project he had worked on. through this demonstration, he illustrated how comprehensive data analysis, when well-documented and carefully presented, can drive insightful decision-making and operational improvements. the importance of creating clear, detailed reports was highlighted, ensuring that the insights derived from eda are accessible and actionable for stakeholders.

finally, the teaching assistant supplemented the session with a presentation on exercise e2, further reinforcing the practical application of the techniques discussed.","in today s session, the instructor provided a detailed tutorial on using pivot tables to conduct exploratory data analysis (eda). the session began with a hands-on demonstration of pivot table functions applied to our summary analysis, where we explored plots of averages, maximums, and minimums, such as analyzing the number of words in summaries. this practical approach enabled us to see firsthand how pivot tables can reveal underlying data trends and discrepancies. building on that, the instructor delved into broader aspects of eda, discussing various techniques and strategies for handling different types of challenges encountered during data exploration. he emphasized that understanding the nuances of eda is critical for diagnosing data issues, cleaning datasets, and ensuring the accuracy of the analysis. key points included methods for managing missing values, outliers, and other common data irregularities that can skew results. the session then transitioned to a real-world demonstration, where the instructor showcased eda on datasets from a chemical plant and a solar plant project he had worked on. through this demonstration, he illustrated how comprehensive data analysis, when well-documented and carefully presented, can drive insightful decision-making and operational improvements. the importance of creating clear, detailed reports was highlighted, ensuring that the insights derived from eda are accessible and actionable for stakeholders. finally, the teaching assistant supplemented the session with a presentation on exercise e2, further reinforcing the practical application of the techniques discussed.",6,-18.3491,27.157833,7.4736724,9.4338455,"summarizing, summarize, summarization"
657,"in today's lecture, sir explained how we were supposed to approach midsem problem. we were required to essentially perform eda and then try to fit a classification model (note clustering was not required at all). in order to perform eda, we had first find missing value and then drop or replace it(dropping values was not a big deal as only 4 rows had missing values in 2000+ rows). we then had to plot box plot to detect outliers but there was no outlier. check if there was a need to normalise the data by checking the range of parameters.(large difference in range proposes a need for normalisation), plot scatter plots and histogram, and state descriptive statistics, generate heatmap of all the parameters (there was not much linear correlation between them, but there was possibility of multi-collinearity) count the number of cases of each ailment (heart diseasewas under represented); after all this, fitting a random forest classification model was required. then we had to plot kde plots for both the data set (q1 and q2) to see whether the distribution is same in both cases or not. the kde plots were not same and classification was not predicting the disease properly which suggests that the dataset were essentially of two different populations.
then the tas discussed e3 evaluation with us. at last, sir gave an introduction and explained 
 about the topic the curse of dimensionality. what are the reasons for its occurance and how to tackle it? dropping features, features selection, dimensionality reduction, regularisation, etc.","in today's lecture, sir explained how we were supposed to approach midsem problem. we were required to essentially perform eda and then try to fit a classification model (note clustering was not required at all). in order to perform eda, we had first find missing value and then drop or replace it(dropping values was not a big deal as only 4 rows had missing values in 2000+ rows). we then had to plot box plot to detect outliers but there was no outlier. check if there was a need to normalise the data by checking the range of parameters.(large difference in range proposes a need for normalisation), plot scatter plots and histogram, and state descriptive statistics, generate heatmap of all the parameters (there was not much linear correlation between them, but there was possibility of multi-collinearity) count the number of cases of each ailment (heart diseasewas under represented); after all this, fitting a random forest classification model was required. then we had to plot kde plots for both the data set (q1 and q2) to see whether the distribution is same in both cases or not. the kde plots were not same and classification was not predicting the disease properly which suggests that the dataset were essentially of two different populations. then the tas discussed e3 evaluation with us. at last, sir gave an introduction and explained about the topic the curse of dimensionality. what are the reasons for its occurance and how to tackle it? dropping features, features selection, dimensionality reduction, regularisation, etc.",9,-7.2051578,14.109224,9.593715,8.23417,"dataâ, analyse, analyses"
658,"we started with the revision of some concepts from the previous class, whereby we had discussed about the sampling distribution of the sample mean. today we defined some new terms with regards to that histogram. we defined the standard deviation of the sampling distribution or the standard error s_x_bar, which is equal to the population standard deviation upon the square root of the number of observation in the sample. today's discussion revolved around the idea that we might not have a lot of samples for each and every data. we may have only one sample in some cases. in that case, how can we make predictions or analyse the parameters of the population, using the statistics of the given sample. one highlight point is that the sampling distribution is always normally distributed i.e. it has a normal distribution which can be expressed as n(mu, sigma), where mu is the population mean and sigma is the population standard deviation. suppose we have only one sample. we can find the mean of all the observations in this sample and assume that it is the mean of the population. then we can also calculate the standard deviation of our sample and we again assume that it is the standard deviation of the population. both these assumptions rely on the fact that ideally, a sample should be a good representation of the population. then we get both mu and sigma for the sampling distribution and since it is normally distributed, we get the distribution as well. now we can calculate the boundary points of the area of 95% confidence level. we also studied that the area under the normal distribution curve, is actually the probability that the mean of the sample lies in that range. hence, we can get two boundary points, within which the area under the curve is 0.95. we also divide our frequency of sample means by the total number of means, so that our graph is normalised and we directly get the probabilities from the area under the curve. the total area under the curve is 1. 
we went on to say that if our no. of observations in the sample are less than 30, then we get a t distribution instead of normal distribution. hence, we understood the terms lower 95% and upper 95%. we also talked about p value, which is basically 2 times the area under the distribution curve beyond the point where our regression coefficient lies. we said that the lower the p value, the better is our regression model.","we started with the revision of some concepts from the previous class, whereby we had discussed about the sampling distribution of the sample mean. today we defined some new terms with regards to that histogram. we defined the standard deviation of the sampling distribution or the standard error s_x_bar, which is equal to the population standard deviation upon the square root of the number of observation in the sample. today's discussion revolved around the idea that we might not have a lot of samples for each and every data. we may have only one sample in some cases. in that case, how can we make predictions or analyse the parameters of the population, using the statistics of the given sample. one highlight point is that the sampling distribution is always normally distributed i.e. it has a normal distribution which can be expressed as n(mu, sigma), where mu is the population mean and sigma is the population standard deviation. suppose we have only one sample. we can find the mean of all the observations in this sample and assume that it is the mean of the population. then we can also calculate the standard deviation of our sample and we again assume that it is the standard deviation of the population. both these assumptions rely on the fact that ideally, a sample should be a good representation of the population. then we get both mu and sigma for the sampling distribution and since it is normally distributed, we get the distribution as well. now we can calculate the boundary points of the area of 95% confidence level. we also studied that the area under the normal distribution curve, is actually the probability that the mean of the sample lies in that range. hence, we can get two boundary points, within which the area under the curve is 0.95. we also divide our frequency of sample means by the total number of means, so that our graph is normalised and we directly get the probabilities from the area under the curve. the total area under the curve is 1. we went on to say that if our no. of observations in the sample are less than 30, then we get a t distribution instead of normal distribution. hence, we understood the terms lower 95% and upper 95%. we also talked about p value, which is basically 2 times the area under the distribution curve beyond the point where our regression coefficient lies. we said that the lower the p value, the better is our regression model.",7,38.16904,1.3026226,15.399634,2.469899,"statistics, statistical, statisticsâ"
659,"1. recap of the gradient descent function, 
2. introduction to logistic regression. 
3. confusion matrix 
4. performance metrics such as precision, recall, and the f1 score. 
5. the roc curve , with the x-axis representing the false positive rate (fpr) and the y-axis representing the true positive rate (tpr). 
6. clustering, starting with k-means clustering,
7. hierarchical clustering","1. recap of the gradient descent function, 2. introduction to logistic regression. 3. confusion matrix 4. performance metrics such as precision, recall, and the f1 score. 5. the roc curve , with the x-axis representing the false positive rate (fpr) and the y-axis representing the true positive rate (tpr). 6. clustering, starting with k-means clustering, 7. hierarchical clustering",8,-5.476678,-16.343126,6.240727,0.67144233,"classification, clusterings, classifying"
660,"we learnt about different ml techniques such as simple linear regression, multiple linear regression, logistic regression, k-means clustering. we learned about the levels of measurement. they are of 4 types: 1)nominal : in this categorization without a specific order is done and it is discrete. for eg.: gender, colour. 2) ordinal : in this categorization is done in ordered way without equal intervals and it is discrete. for eg.: grades, rankings 3) interval : in this categorization, it is ordered with equal intervals but no true zero. it is continuous in nature. for eg.: temperature. 4) ratio: this categorization is similar to the interval, just here the the zero is true zero. for eg.: height, weight. next, we learned about the general type of equal in ml : y =f(x), where y is label and x are features. if both of them are known then it is known as supervised learning. if labels are unknown, it is called unsupervised learning and then using k-means clustering and hierarchical clustering, we create labels for it.","we learnt about different ml techniques such as simple linear regression, multiple linear regression, logistic regression, k-means clustering. we learned about the levels of measurement. they are of 4 types: 1)nominal : in this categorization without a specific order is done and it is discrete. for eg.: gender, colour. 2) ordinal : in this categorization is done in ordered way without equal intervals and it is discrete. for eg.: grades, rankings 3) interval : in this categorization, it is ordered with equal intervals but no true zero. it is continuous in nature. for eg.: temperature. 4) ratio: this categorization is similar to the interval, just here the the zero is true zero. for eg.: height, weight. next, we learned about the general type of equal in ml : y =f(x), where y is label and x are features. if both of them are known then it is known as supervised learning. if labels are unknown, it is called unsupervised learning and then using k-means clustering and hierarchical clustering, we create labels for it.",4,-24.081526,-17.900469,1.4826092,0.06140733,"classification, classifying, classifications"
661,"the discussion on closed form solution of mlr was revised , it was not ideal as we had to deal with matrix inversion and occurrence of multi collinearity. p-value<= .05 , feature is omitted as it doesn't contribute much. r2 multiple r and the reason behind n-1 degree of freedom in tss was discussed. training and testing data was discussed , as we want model to represent population. the issue of overfit can occur if r2 values for training and testing matrices are not close enough.","the discussion on closed form solution of mlr was revised , it was not ideal as we had to deal with matrix inversion and occurrence of multi collinearity. p-value<= .05 , feature is omitted as it doesn't contribute much. r2 multiple r and the reason behind n-1 degree of freedom in tss was discussed. training and testing data was discussed , as we want model to represent population. the issue of overfit can occur if r2 values for training and testing matrices are not close enough.",2,7.940511,9.405714,11.427148,5.0521793,"regression, regressions, features"
662,"today's session discussed variance inflation factor (vif) and principal component analysis (pca) in-depth.

we first discussed vif, a statistical factor to check for multicollinearity among independent variables in regression models. we understood that high vif suggests high correlation between independent variables, which tends to bias the coefficient estimates and decrease model interpretability. the session discussed vif, its interpretation, and how to deal with multicollinearity. the method consists of removing features individually, beginning from the largest vif, so that all remaining features have a vif value less than an arbitrarily selected value. this both reduces the curse of dimensionality and enhances the performance of the model. we also saw that vif scores would tend to form an elbow-like pattern upon plotting, from which a threshold can be conveniently determined.

then, we discussed pca, which is an unsupervised method of reducing dimension with retaining the maximum variance in the data. pca is dependent on singular value decomposition (svd) and assists in transforming data to a new set of uncorrelated principal components. principal components are orthogonal to one another, thus ensuring that multicollinearity is removed. we also observed how pca has the capability of dimension reduction from higher dimensions to lower dimensions, as in the example where data was reduced from two dimensions to one.

a major difference between vif and pca was explored. vif is usually performed first to provide better interpretability because it maintains the original features, which would make prediction simpler to interpret. pca, however, converts features into principal components, so it is better used for ""what-if"" analysis or delta analysis than for explicit prediction.

the session also touched on various applications of pca, such as dimensionality reduction, predictive modeling, and visualization (particularly in exploratory data analysis - eda). but we observed that pca is scale-dependent, and thus normalization becomes an essential preprocessing step.

near the conclusion, t-sne (t-distributed stochastic neighbor embedding) was introduced as yet another dimension reduction method. it was briefly mentioned how a gaussian and t-distribution comparison on the same dataset highlighted differences between them in picking up data variance.

the session, in summary, illustrated how vif and pca assist with multicollinearity and dimension reduction, ensuring machine learning models are interpretable and effective.","today's session discussed variance inflation factor (vif) and principal component analysis (pca) in-depth. we first discussed vif, a statistical factor to check for multicollinearity among independent variables in regression models. we understood that high vif suggests high correlation between independent variables, which tends to bias the coefficient estimates and decrease model interpretability. the session discussed vif, its interpretation, and how to deal with multicollinearity. the method consists of removing features individually, beginning from the largest vif, so that all remaining features have a vif value less than an arbitrarily selected value. this both reduces the curse of dimensionality and enhances the performance of the model. we also saw that vif scores would tend to form an elbow-like pattern upon plotting, from which a threshold can be conveniently determined. then, we discussed pca, which is an unsupervised method of reducing dimension with retaining the maximum variance in the data. pca is dependent on singular value decomposition (svd) and assists in transforming data to a new set of uncorrelated principal components. principal components are orthogonal to one another, thus ensuring that multicollinearity is removed. we also observed how pca has the capability of dimension reduction from higher dimensions to lower dimensions, as in the example where data was reduced from two dimensions to one. a major difference between vif and pca was explored. vif is usually performed first to provide better interpretability because it maintains the original features, which would make prediction simpler to interpret. pca, however, converts features into principal components, so it is better used for ""what-if"" analysis or delta analysis than for explicit prediction. the session also touched on various applications of pca, such as dimensionality reduction, predictive modeling, and visualization (particularly in exploratory data analysis - eda). but we observed that pca is scale-dependent, and thus normalization becomes an essential preprocessing step. near the conclusion, t-sne (t-distributed stochastic neighbor embedding) was introduced as yet another dimension reduction method. it was briefly mentioned how a gaussian and t-distribution comparison on the same dataset highlighted differences between them in picking up data variance. the session, in summary, illustrated how vif and pca assist with multicollinearity and dimension reduction, ensuring machine learning models are interpretable and effective.",11,-16.302847,2.573628,10.193457,12.90754,"pca, heatmap, heatmaps"
663,"initial clarifications regarding the earlier session summaries: features dropped when p value greater than 0.05
splitting data into two parts randomly, python can do it for you
we briefly looked at different data fitting problems like underfitting and overfitting
we also looked at scikit-learn an open source python library used for machine learning and data analysis, also with this the convenience of using python and excel.
we discussed how the code is written, the lines and their applications. we also analysized the regression results which these lines of code gave us. the durbin watson test was also one of the one returned by the python code.","initial clarifications regarding the earlier session summaries: features dropped when p value greater than 0.05 splitting data into two parts randomly, python can do it for you we briefly looked at different data fitting problems like underfitting and overfitting we also looked at scikit-learn an open source python library used for machine learning and data analysis, also with this the convenience of using python and excel. we discussed how the code is written, the lines and their applications. we also analysized the regression results which these lines of code gave us. the durbin watson test was also one of the one returned by the python code.",13,4.6936417,5.2152944,10.401112,5.043652,"classification, classifying, classifications"
664,"in this lecture we have learnt about what is population and what is sample. majorly the data we will be working on would be sample that is some part of a population since it's very difficult to collect entire set of data for a certain population. r aim is to estimate the different parameters of a population based on the statics that we obtain from sample of that population. there are multiple attributes which are associated with the data for example, if we have data of marbles in a box we can calculate the frequency of a particular colour of marble, mean, median, mode, standard deviations etc. of the data and can also perform various operations on it such as addition, subtraction etc.
there are different types of models like line, point which can be used depending on the type of data we have. point is considered a naive model. then studied the example of simple linear regression with one predictor which was advertisement expenditure, for which equation would be y = b_0 + b_1x in which b_0 and b_1 of the sample are the point estimates. therefore we can say that b_0 and b_1 will be the same values for the population with 0% confidence. find out the confidence interval in which we have good enough confidence to claim that these are only the values for the population. the b_0 is known as bias which accounts for other factors that might affect sales. so if we increase the number of predictive variables then bias will decrease and if we include all the predictive variables then bias would be zero.
then we studied how to find out b_0 and b_1 for the sample. we must try to minimise the sum of squares of errors where error is the distance between actual value and the predicted value. lastly we got the conclusion from the model that the mean of predictor values and response values lie on the best fit line.","in this lecture we have learnt about what is population and what is sample. majorly the data we will be working on would be sample that is some part of a population since it's very difficult to collect entire set of data for a certain population. r aim is to estimate the different parameters of a population based on the statics that we obtain from sample of that population. there are multiple attributes which are associated with the data for example, if we have data of marbles in a box we can calculate the frequency of a particular colour of marble, mean, median, mode, standard deviations etc. of the data and can also perform various operations on it such as addition, subtraction etc. there are different types of models like line, point which can be used depending on the type of data we have. point is considered a naive model. then studied the example of simple linear regression with one predictor which was advertisement expenditure, for which equation would be y = b_0 + b_1x in which b_0 and b_1 of the sample are the point estimates. therefore we can say that b_0 and b_1 will be the same values for the population with 0% confidence. find out the confidence interval in which we have good enough confidence to claim that these are only the values for the population. the b_0 is known as bias which accounts for other factors that might affect sales. so if we increase the number of predictive variables then bias will decrease and if we include all the predictive variables then bias would be zero. then we studied how to find out b_0 and b_1 for the sample. we must try to minimise the sum of squares of errors where error is the distance between actual value and the predicted value. lastly we got the conclusion from the model that the mean of predictor values and response values lie on the best fit line.",1,34.99331,-6.3505645,16.293674,3.6179373,"population, models, estimating"
665,"today, we continued our discussion on statistical significance before shifting focus to python, with assigned tutorials to prepare for our next class.  

we explored how data science algorithms evaluate session summaries using a relative strength method, calculating distances between submissions based on scatter plot coefficients.  

later, we studied multiple linear regression, where a dependent variable is influenced by multiple independent variables. we learned about the f-value, which measures model effectiveness by comparing explained and unexplained varianceâ€”a higher f-value indicating a better model. using a dataset, we calculated error metrics and compared f-values to assess model performance practically.","today, we continued our discussion on statistical significance before shifting focus to python, with assigned tutorials to prepare for our next class. we explored how data science algorithms evaluate session summaries using a relative strength method, calculating distances between submissions based on scatter plot coefficients. later, we studied multiple linear regression, where a dependent variable is influenced by multiple independent variables. we learned about the f-value, which measures model effectiveness by comparing explained and unexplained variance a higher f-value indicating a better model. using a dataset, we calculated error metrics and compared f-values to assess model performance practically.",13,-7.118614,-4.7637744,9.485014,5.2702484,"classification, classifying, classifications"
666,"at the beginning, sir explained what are the ways to determine number of bins in order to get an idea of histogram (there are some formulas to calculate bin number or bin width, but mostly it depends upon how closely we want to get insights from the data(bird-eye view or precise view)). clustering is also useful in cases, even if there is an overlap between two clusters. then topic resumed to logistic regression and sigmoid function. if p(x/y)>0.5, push the outcome to class 1; else, push the outcome to class 2.  learnt about the notations (p, t, w, etc.) in logistic regression. in order to classify (or to come upon with a prediction/outcome), the given things are 1. number of observations(n) and 2. corresponding targets; and our goal is to 1. calculate the weights (w_i) and 2. maximizing l (likelihood) such as the desired weights outcome expression for l satisfies requirements. then we discussed about log likelihood, gradient descent method to solve it (objective function is to minimize log likelihood). then we discussed about confusion matrix (how false positive and false negative affect the outcomes; false negative is disastrous some times). then we saw some quality metrics to assess the logistic regression (like accuracy, precision, recall, f1-metric, etc.); f1 is better quality metric as compared to accuracy( accuracy may fail in case of imbalanced clusters). at the end, tas presented the assessment and insights from of exercise-1.","at the beginning, sir explained what are the ways to determine number of bins in order to get an idea of histogram (there are some formulas to calculate bin number or bin width, but mostly it depends upon how closely we want to get insights from the data(bird-eye view or precise view)). clustering is also useful in cases, even if there is an overlap between two clusters. then topic resumed to logistic regression and sigmoid function. if p(x/y)>0.5, push the outcome to class 1; else, push the outcome to class 2. learnt about the notations (p, t, w, etc.) in logistic regression. in order to classify (or to come upon with a prediction/outcome), the given things are 1. number of observations(n) and 2. corresponding targets; and our goal is to 1. calculate the weights (w_i) and 2. maximizing l (likelihood) such as the desired weights outcome expression for l satisfies requirements. then we discussed about log likelihood, gradient descent method to solve it (objective function is to minimize log likelihood). then we discussed about confusion matrix (how false positive and false negative affect the outcomes; false negative is disastrous some times). then we saw some quality metrics to assess the logistic regression (like accuracy, precision, recall, f1-metric, etc.); f1 is better quality metric as compared to accuracy( accuracy may fail in case of imbalanced clusters). at the end, tas presented the assessment and insights from of exercise-1.",10,10.629803,-17.349073,9.049415,-1.7189329,"classifications, histograms, histogram"
667,"we studied about crisp-dm
(cross industry standard process for data mining) which has 6 steps running cyclically:-(i)business understanding-determine business objectives,(ii)data understanding-explore data,(iii)data preparation-construct data,(iv)modelling-assess model,(v)evaluation-review process and (vi) deployment-review project.
next we studied about exploratory data analysis(eda).it is an approach used in statistics and data science to analyze and investigate datasets.
then we looked at outliers and quartiles. a boxplot can help to understand variability in the features and outlier instances.
then we studied about 3 types of missing data:-(i)missing completely at random(mcar),(ii)missing at random(mar) and (iii)missing not at random(mnar)
lastly we got to know about true outliers, values that lie in the extremes but are not erroneous.","we studied about crisp-dm (cross industry standard process for data mining) which has 6 steps running cyclically:-(i)business understanding-determine business objectives,(ii)data understanding-explore data,(iii)data preparation-construct data,(iv)modelling-assess model,(v)evaluation-review process and (vi) deployment-review project. next we studied about exploratory data analysis(eda).it is an approach used in statistics and data science to analyze and investigate datasets. then we looked at outliers and quartiles. a boxplot can help to understand variability in the features and outlier instances. then we studied about 3 types of missing data:-(i)missing completely at random(mcar),(ii)missing at random(mar) and (iii)missing not at random(mnar) lastly we got to know about true outliers, values that lie in the extremes but are not erroneous.",9,-13.292617,21.585289,8.797204,8.7028055,"dataâ, analyse, analyses"
668,"in today's class we interpreted p-values and confidence intervals in linear regression. we had to find out whether the obtained coefficients are statistically significant.
to make this determination, we used different samples and obtained the distribution of coefficient. this distribution is assumed to be normal, and its spread is quantified by the standard error, calculated from the given data. using this information, we can construct confidence intervals to identify the range within which coefficients is most likely to be in. 
the concept of a p-value also verifies it. a small p-value, typically less than 0.05, indicates that the coefficient is significantly different from zero. this statistical significance tells that  the observed relationship between  x  and  y  is meaningful.
we also learnt that the size of samples also influence these evaluations. larger sample sizes lead to smaller standard errors, smaller confidence intervals, and greater certainty in identifying significant relationships.","in today's class we interpreted p-values and confidence intervals in linear regression. we had to find out whether the obtained coefficients are statistically significant. to make this determination, we used different samples and obtained the distribution of coefficient. this distribution is assumed to be normal, and its spread is quantified by the standard error, calculated from the given data. using this information, we can construct confidence intervals to identify the range within which coefficients is most likely to be in. the concept of a p-value also verifies it. a small p-value, typically less than 0.05, indicates that the coefficient is significantly different from zero. this statistical significance tells that the observed relationship between x and y is meaningful. we also learnt that the size of samples also influence these evaluations. larger sample sizes lead to smaller standard errors, smaller confidence intervals, and greater certainty in identifying significant relationships.",7,27.247261,5.450797,14.207016,3.3546388,"statistics, statistical, statisticsâ"
